I am looking to build a tool that can automate entire process of blog creation using AI. I am looking to use these API's for finding keyword opportuniteis and building production ready content for qualififed keywords for the blog artilce content creation. Here's the external API's i am looking to use and current backend code. Find all the isues you can spot:



## Keyword Ide‌as
The Keyword Ideas endpoint provides search terms that are relevant to the product or service categories of the specified keywords. The algorithm selects the keywords which fall into the same categories as the seed keywords specified in a POST array.
As a result, you will get a list of relevant keyword ideas for up to 200 seed keywords.
Along with each keyword idea, you will get its search volume rate for the last month, search volume trend for the previous 12 months, as well as current cost-per-click and competition values. Moreover, this endpoint supplies minimum, maximum and average values of clicks and CPC for each result.
**Datasource:** DataForSEO Keyword Database segmented by product categories.
**Search algorithm:** relevance-based search for terms that fall into the same category as specified seed keywords.
**Examples:**
Note: no additional sorting parameters applied, `"closely_variants"` set to `false`
Specified seed keywords:
_“keyword research”_, _“content marketing”_
Resulting keyword ideas:
_•”free adword tools”,_
_•”find longtail keywords”,_
_•”how to do keywords research”,_
_•”seo keyword research template”_
> Instead of ‘login’ and ‘password’ use your credentials from https://app.dataforseo.com/api-access
```
# Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access
login="login"
password="password"
cred="$(printf ${login}:${password} | base64)"
curl --location --request POST "https://api.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live" \
--header "Authorization: Basic ${cred}" \
--header "Content-Type: application/json" \
--data-raw '[\
{\
"keywords": [\
"phone",\
"watch"\
],\
"location_code": 2840,\
"language_code": "en",\
"include_serp_info": true,\
"limit": 3\
}\
]'
```
```
// You can download this file from here https://cdn.dataforseo.com/v3/examples/php/php_RestClient.zip
require('RestClient.php');
$api_url = 'https://api.dataforseo.com/';
// Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access
$client = new RestClient($api_url, null, 'login', 'password');
$post_array = array();
// simple way to set a task
$post_array[] = array(
"keywords" => [\
"phone",\
"watch"\
],
"language_name" => "English",
"location_code" => 2840,
"filters" => [\
["keyword_info.search_volume", ">", 10]\
],
"limit" => 3
);
try {
// POST /v3/dataforseo_labs/google/keyword_ideas/live
$result = $client->post('/v3/dataforseo_labs/google/keyword_ideas/live', $post_array);
print_r($result);
// do something with post result
} catch (RestClientException $e) {
echo "n";
print "HTTP code: {$e->getHttpCode()}n";
print "Error code: {$e->getCode()}n";
print "Message: {$e->getMessage()}n";
print $e->getTraceAsString();
echo "n";
}
$client = null;
?>
```
```
from client import RestClient
# You can download this file from here https://cdn.dataforseo.com/v3/examples/python/python_Client.zip
client = RestClient("login", "password")
post_data = dict()
# simple way to set a task
post_data[len(post_data)] = dict(
keywords=[\
"phone",\
"watch"\
],
location_name="United States",
language_name="English",
filters=[\
["keyword_info.search_volume", ">", 10]\
],
limit=3
)
# POST /v3/dataforseo_labs/google/keyword_ideas/live
response = client.post("/v3/dataforseo_labs/google/keyword_ideas/live", post_data)
# you can find the full list of the response codes here https://docs.dataforseo.com/v3/appendix/errors
if response["status_code"] == 20000:
print(response)
# do something with result
else:
print("error. Code: %d Message: %s" % (response["status_code"], response["status_message"]))
```
```
const post_array = [];
post_array.push({
"keywords": [\
"phone",\
"watch"\
],
"location_code": 2840,
"language_name": "English",
"filters": [\
["keyword_info.search_volume", ">", 10]\
],
"limit": 3
});
const axios = require('axios');
axios({
method: 'post',
url: 'https://api.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live',
auth: {
username: 'login',
password: 'password'
},
data: post_array,
headers: {
'content-type': 'application/json'
}
}).then(function (response) {
var result = response['data']['tasks'];
// Result data
console.log(result);
}).catch(function (error) {
console.log(error);
});
```
```
using Newtonsoft.Json;
using System;
using System.Collections.Generic;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Text;
using System.Threading.Tasks;
namespace DataForSeoDemos
{
public static partial class Demos
{
public static async Task dataforseo_labs_google_keyword_ideas_live()
{
var httpClient = new HttpClient
{
BaseAddress = new Uri("https://api.dataforseo.com/"),
// Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access
DefaultRequestHeaders = { Authorization = new AuthenticationHeaderValue("Basic", Convert.ToBase64String(Encoding.ASCII.GetBytes("login:password"))) }
};
var postData = new List();
postData.Add(new
{
keywords = new[]
{
"phone",
"watch"
},
location_name = "United States",
language_name = "English",
filters = new object[]
{
new object[] { "keyword_info.search_volume", ">", 10 }
},
limit = 3
});
// POST /v3/dataforseo_labs/google/keyword_ideas/live
// the full list of possible parameters is available in documentation
var taskPostResponse = await httpClient.PostAsync("/v3/dataforseo_labs/google/keyword_ideas/live", new StringContent(JsonConvert.SerializeObject(postData)));
var result = JsonConvert.DeserializeObject(await taskPostResponse.Content.ReadAsStringAsync());
// you can find the full list of the response codes here https://docs.dataforseo.com/v3/appendix/errors
if (result.status_code == 20000)
{
// do something with result
Console.WriteLine(result);
}
else
Console.WriteLine($"error. Code: {result.status_code} Message: {result.status_message}");
}
}
}
```
> The above command returns JSON structured like this:
```
{
"version": "0.1.20240801",
"status_code": 20000,
"status_message": "Ok.",
"time": "0.7097 sec.",
"cost": 0.0103,
"tasks_count": 1,
"tasks_error": 0,
"tasks": [\
{\
"id": "08221822-1535-0400-0000-8fccf5eb0f23",\
"status_code": 20000,\
"status_message": "Ok.",\
"time": "0.6455 sec.",\
"cost": 0.0103,\
"result_count": 1,\
"path": [\
"v3",\
"dataforseo_labs",\
"google",\
"keyword_ideas",\
"live"\
],\
"data": {\
"api": "dataforseo_labs",\
"function": "keyword_ideas",\
"se_type": "google",\
"keywords": [\
"phone",\
"watch"\
],\
"location_code": 2840,\
"language_code": "en",\
"include_serp_info": true,\
"limit": 3\
},\
"result": [\
{\
"se_type": "google",\
"seed_keywords": [\
"phone",\
"watch"\
],\
"location_code": 2840,\
"language_code": "en",\
"total_count": 533763,\
"items_count": 3,\
"offset": 0,\
"offset_token": "eyJDdXJyZW50T2Zmc2V0IjozLCJSZXF1ZXN0RGF0YSI6eyJrZXl3b3JkcyI6WyJwaG9uZSIsIndhdGNoIl0sImxvY2F0aW9uIjoyODQwLCJsYW5ndWFnZSI6ImVuIiwiY2xvc2VseV92YXJpYW50cyI6ZmFsc2UsIm5ld2VzdCI6ZmFsc2UsImV4dGVuZGVkIjpmYWxzZSwibG9hZF9zZXJwX2luZm8iOnRydWUsImF1dG9jb3JyZWN0Ijp0cnVlLCJJc09sZCI6ZmFsc2UsInNlYXJjaF9hZnRlcl90b2tlbiI6bnVsbCwiaWdub3JlX3N5bm9ueW1zIjpmYWxzZSwic2VhcmNoX2VuZ2luZSI6Imdvb2dsZSIsInVzZV9uZXdfY2F0ZWdvcmllcyI6dHJ1ZSwib3JkZXJfYnkiOnsib3JkZXJfZmllbGQiOiJfc2NvcmUiLCJvcmRlcl90eXBlIjoiRGVzYyIsIm5leHQiOm51bGx9LCJsaW1pdCI6Mywib2Zmc2V0IjowLCJhaWQiOjE1MzV9LCJSYXdRdWVyeSI6bnVsbCwiSWQiOiJiNWEyZGNlOS00Mzk3LTQ3NTgtYWEyOC02NWFiMzY3ZDM5NDgiLCJTZWFyY2hBZnRlckRhdGEiOlszMDQuMTc2NTcsImUwNGZkMDE1LTllY2YtMzcwYi0xZGJmLWY0NGExODVjOWU5ZiJdfQ==",\
"items": [\
{\
"se_type": "google",\
"keyword": "phone",\
"location_code": 2840,\
"language_code": "en",\
"keyword_info": {\
"se_type": "google",\
"last_updated_time": "2024-08-11 13:24:34 +00:00",\
"competition": 1,\
"competition_level": "HIGH",\
"cpc": 5.98,\
"search_volume": 368000,\
"low_top_of_page_bid": 3.08,\
"high_top_of_page_bid": 10.5,\
"categories": [\
10007,\
10878,\
12133,\
13381\
],\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 450000\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 368000\
}\
],\
"search_volume_trend": {\
"monthly": 22,\
"quarterly": 22,\
"yearly": 0\
}\
},\
"clickstream_keyword_info": null,\
"keyword_properties": {\
"se_type": "google",\
"core_keyword": null,\
"synonym_clustering_algorithm": "text_processing",\
"keyword_difficulty": 83,\
"detected_language": "en",\
"is_another_language": false\
},\
"serp_info": {\
"se_type": "google",\
"check_url": "https://www.google.com/search?q=phone&num=100&hl=en&gl=US&gws_rd=cr&ie=UTF-8&oe=UTF-8&glp=1&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",\
"serp_item_types": [\
"popular_products",\
"images",\
"organic",\
"product_considerations",\
"refine_products",\
"top_stories",\
"related_searches"\
],\
"se_results_count": 19880000000,\
"last_updated_time": "2024-07-14 21:43:34 +00:00",\
"previous_updated_time": "2024-05-18 19:29:28 +00:00"\
},\
"avg_backlinks_info": {\
"se_type": "google",\
"backlinks": 6835.7,\
"dofollow": 3775.6,\
"referring_pages": 5352.2,\
"referring_domains": 1100.3,\
"referring_main_domains": 955.1,\
"rank": 369.3,\
"main_domain_rank": 681.2,\
"last_updated_time": "2024-07-14 21:43:39 +00:00"\
},\
"search_intent_info": {\
"se_type": "google",\
"main_intent": "navigational",\
"foreign_intent": [\
"commercial"\
],\
"last_updated_time": "2023-03-02 03:55:21 +00:00"\
},\
"keyword_info_normalized_with_bing": {\
"last_updated_time": "2024-08-17 01:41:37 +00:00",\
"search_volume": 308309,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 377009\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 308309\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 308309\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 308309\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 308309\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 308309\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 308309\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 308309\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 308309\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 308309\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 308309\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 308309\
},\
{\
"year": 2023,\
"month": 7,\
"search_volume": 300631\
}\
]\
},\
"keyword_info_normalized_with_clickstream": {\
"last_updated_time": "2024-08-17 01:41:37 +00:00",\
"search_volume": 324416,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 396705\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 7,\
"search_volume": 324365\
}\
]\
}\
},\
{\
"se_type": "google",\
"keyword": "cell phone signal booster",\
"location_code": 2840,\
"language_code": "en",\
"keyword_info": {\
"se_type": "google",\
"last_updated_time": "2024-08-11 18:11:54 +00:00",\
"competition": 1,\
"competition_level": "HIGH",\
"cpc": 1.05,\
"search_volume": 22200,\
"low_top_of_page_bid": 0.31,\
"high_top_of_page_bid": 1.15,\
"categories": [\
10007,\
10878,\
12133,\
13381\
],\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 33100\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 18100\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 18100\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 18100\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 18100\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 27100\
}\
],\
"search_volume_trend": {\
"monthly": 22,\
"quarterly": 22,\
"yearly": 0\
}\
},\
"clickstream_keyword_info": null,\
"keyword_properties": {\
"se_type": "google",\
"core_keyword": "cell phone signal booster for phone",\
"synonym_clustering_algorithm": "text_processing",\
"keyword_difficulty": 23,\
"detected_language": "en",\
"is_another_language": false\
},\
"serp_info": {\
"se_type": "google",\
"check_url": "https://www.google.com/search?q=cell%20phone%20signal%20booster&num=100&hl=en&gl=US&gws_rd=cr&ie=UTF-8&oe=UTF-8&glp=1&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",\
"serp_item_types": [\
"popular_products",\
"people_also_ask",\
"organic",\
"images",\
"related_searches"\
],\
"se_results_count": 13500000,\
"last_updated_time": "2024-08-04 11:06:04 +00:00",\
"previous_updated_time": "2024-06-22 20:35:21 +00:00"\
},\
"avg_backlinks_info": {\
"se_type": "google",\
"backlinks": 111.6,\
"dofollow": 34.7,\
"referring_pages": 104.5,\
"referring_domains": 29,\
"referring_main_domains": 26.3,\
"rank": 103.5,\
"main_domain_rank": 530.7,\
"last_updated_time": "2024-08-04 11:06:06 +00:00"\
},\
"search_intent_info": {\
"se_type": "google",\
"main_intent": "transactional",\
"foreign_intent": null,\
"last_updated_time": "2023-03-03 12:40:39 +00:00"\
},\
"keyword_info_normalized_with_bing": {\
"last_updated_time": "2024-08-16 10:43:48 +00:00",\
"search_volume": 12895,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 19226\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 15741\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 12895\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 10513\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 10513\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 10513\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 10513\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 12895\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 12895\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 15741\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 15741\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 15741\
},\
{\
"year": 2023,\
"month": 7,\
"search_volume": 19139\
}\
]\
},\
"keyword_info_normalized_with_clickstream": {\
"last_updated_time": "2024-08-16 10:43:48 +00:00",\
"search_volume": 15498,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 23107\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 18918\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 15498\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 12635\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 12635\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 12635\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 12635\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 15498\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 15498\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 18918\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 18918\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 18918\
},\
{\
"year": 2023,\
"month": 7,\
"search_volume": 22930\
}\
]\
}\
},\
{\
"se_type": "google",\
"keyword": "phone charm",\
"location_code": 2840,\
"language_code": "en",\
"keyword_info": {\
"se_type": "google",\
"last_updated_time": "2024-08-12 16:45:18 +00:00",\
"competition": 1,\
"competition_level": "HIGH",\
"cpc": 0.66,\
"search_volume": 27100,\
"low_top_of_page_bid": 0.26,\
"high_top_of_page_bid": 1.93,\
"categories": [\
10007,\
10878,\
12133,\
13381\
],\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 33100\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 27100\
}\
],\
"search_volume_trend": {\
"monthly": 22,\
"quarterly": 22,\
"yearly": 0\
}\
},\
"clickstream_keyword_info": null,\
"keyword_properties": {\
"se_type": "google",\
"core_keyword": "charms for phones",\
"synonym_clustering_algorithm": "text_processing",\
"keyword_difficulty": 0,\
"detected_language": "en",\
"is_another_language": false\
},\
"serp_info": {\
"se_type": "google",\
"check_url": "https://www.google.com/search?q=phone%20charm&num=100&hl=en&gl=US&gws_rd=cr&ie=UTF-8&oe=UTF-8&glp=1&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",\
"serp_item_types": [\
"popular_products",\
"organic",\
"people_also_ask",\
"images",\
"explore_brands",\
"related_searches"\
],\
"se_results_count": 284000000,\
"last_updated_time": "2024-08-04 10:21:18 +00:00",\
"previous_updated_time": "2024-06-22 19:50:30 +00:00"\
},\
"avg_backlinks_info": {\
"se_type": "google",\
"backlinks": 15.9,\
"dofollow": 9,\
"referring_pages": 10.5,\
"referring_domains": 3,\
"referring_main_domains": 2.6,\
"rank": 44.1,\
"main_domain_rank": 491.8,\
"last_updated_time": "2024-08-04 10:21:19 +00:00"\
},\
"search_intent_info": {\
"se_type": "google",\
"main_intent": "transactional",\
"foreign_intent": null,\
"last_updated_time": "2023-03-02 03:55:42 +00:00"\
},\
"keyword_info_normalized_with_bing": {\
"last_updated_time": "2024-08-17 03:47:30 +00:00",\
"search_volume": 14892,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 18190\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 14892\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 14892\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 12200\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 14892\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 12200\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 14892\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 14892\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 14892\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 12200\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 12200\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 14892\
},\
{\
"year": 2023,\
"month": 7,\
"search_volume": 15028\
}\
]\
},\
"keyword_info_normalized_with_clickstream": {\
"last_updated_time": "2024-08-17 03:47:30 +00:00",\
"search_volume": 13826,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 16887\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 13826\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 13826\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 11326\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 13826\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 11326\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 13826\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 13826\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 13826\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 11326\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 11326\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 13826\
},\
{\
"year": 2023,\
"month": 7,\
"search_volume": 13822\
}\
]\
}\
}\
]\
}\
]\
}\
]
}
```
**`POST https://api.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live`**
Your account will be charged for each request.
The cost can be calculated on the [Pricing](https://dataforseo.com/pricing/dataforseo-labs/dataforseo-google-api "Pricing") page.
All POST data should be sent in the [JSON](https://en.wikipedia.org/wiki/JSON) format (UTF-8 encoding). The task setting is done using the POST method. When setting a task, you should send all task parameters in the task array of the generic POST array. You can send up to 2000 API calls per minute. The maximum number of requests that can be sent simultaneously is limited to 30.
You can specify the number of results you want to retrieve and sort them.
Below you will find a detailed description of the fields you can use for setting a task.
**Description of the fields for setting a task:**
| Field name | Type | Description |
| --- | --- | --- |
| `keywords` | array | _keywords_
**required field**
UTF-8 encoding
The maximum number of keywords you can specify: 200.
The keywords will be converted to lowercase format
learn more about rules and limitations of `keyword` and `keywords` fields in DataForSEO APIs in this [Help Center article](https://dataforseo.com/help-center/rules-and-limitations-of-keyword-and-keywords-fields-in-dataforseo-apis) | |
| `location_name` | string | _full name of the location_
**required field if you don’t specify** `location_code`
**Note:** it is required to specify either `location_name` or `location_code`
you can receive the list of available locations with their `location_name` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
example:
`United Kingdom` | |
| `location_code` | integer | _unique location identifier_
**required field if you don’t specify** `location_name`
**Note:** it is required to specify either `location_name` or `location_code`
you can receive the list of available locations with their `location_code` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
example:
`2840` | |
| `language_name` | string | _full name of the language_
optional field
if you use this field, you don’t need to specify `language_code`
you can receive the list of available languages with their `language_name` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
example:
`English`
**Note:** if omitted, results default to the language with the most keyword records in the specified location;
refer to the `available_languages.keywords` field of the [Locations and Languages endpoint](https://docs.dataforseo.com/v3/dataforseo_labs/locations_and_languages) to determine the default language | |
| `language_code` | string | _language code_
optional field
if you use this field, you don’t need to specify `language_name`
you can receive the list of available languages with their `language_code` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
example:
`en`
**Note:** if omitted, results default to the language with the most keyword records in the specified location;
refer to the `available_languages.keywords` field of the [Locations and Languages endpoint](https://docs.dataforseo.com/v3/dataforseo_labs/locations_and_languages) to determine the default language | |
| `closely_variants` | boolean | _search mode_
optional field
if set to `true` the results will be based on the phrase-match search algorithm
if set to `false` the results will be based on the broad-match search algorithm
default value: `false` | |
| `ignore_synonyms` | boolean | _ignore highly similar keywords_
optional field
if set to `true` only core keywords will be returned, all highly similar keywords will be excluded;
default value: `false` | |
| `include_serp_info` | boolean | _include data from SERP for each keyword_
optional field
if set to `true`, we will return a `serp_info` array containing SERP data (number of search results, relevant URL, and SERP features) for every keyword in the response
default value: `false` | |
| `include_clickstream_data` | boolean | _include or exclude data from clickstream-based metrics in the result_
optional field
if the parameter is set to `true`, you will receive `clickstream_keyword_info`, `keyword_info_normalized_with_clickstream`, and `keyword_info_normalized_with_bing` fields in the response
default value: `false`
with this parameter enabled, you will be charged double the price for the request
learn more about how clickstream-based metrics are calculated in this [help center article](https://dataforseo.com/help-center/what-are-clickstream-based-metrics-and-how-do-we-calculate-them) | |
| `limit` | integer | _the maximum number of keywords in the results array_
optional field
default value: `700`
maximum value: `1000` | |
| `offset` | integer | _offset in the results array of returned keywords_
optional field
default value: `0`
if you specify the `10` value, the first ten keywords in the results array will be omitted and the data will be provided for the successive keywords | |
| `offset_token` | string | _offset token for subsequent requests_
optional field
provided in the identical filed of the response to each request;
use this parameter to avoid timeouts while trying to obtain over 10,000 results in a single request;
by specifying the unique `offset_token` value from the response array, you will get the subsequent results of the initial task;
`offset_token` values are unique for each subsequent task
**Note:** if the `offset_token` is specified in the request, all other parameters except `limit` will not be taken into account when processing a task. | |
| `filters` | array | _array of results filtering parameters_
optional field
**you can add several filters at once (8 filters maximum)**
you should set a logical operator `and`, `or` between the conditions
the following operators are supported:
`regex`, `not_regex`, `<`, `<=`, `>`, `>=`, `=`, `<>`, `in`, `not_in`, `match`, `not_match`, `ilike`, `not_ilike`, `like`, `not_like`
you can use the `%` operator with `like` and `not_like`,as well as `ilike`, `not_ilike` to match any string of zero or more characters
**note that you can not filter the results by `relevance`**
example:
`["keyword_info.search_volume",">",0]`
`[["keyword_info.search_volume","in",[0,1000]],
"and",
["keyword_info.competition_level","=","LOW"]]`
`[["keyword_info.search_volume",">",100],
"and",
[["keyword_info.cpc","<",0.5],
"or",
["keyword_info.high_top_of_page_bid","<=",0.5]]]`
for more information about filters, please refer to [Dataforseo Labs – Filters](https://docs.dataforseo.com/v3/dataforseo_labs/filters) or this [help center guide](https://dataforseo.com/help-center/how-to-use-filters-in-dataforseo-labs-api) | |
| `order_by` | array | _results sorting rules_
optional field
you can use the same values as in the `filters` array to sort the results
possible sorting types:
`asc` – results will be sorted in the ascending order
`desc` – results will be sorted in the descending order
you should use a comma to set up a sorting parameter
default rule:
`["relevance,desc"]`
relevance is used as the default sorting rule to provide you with the closest keyword ideas. We recommend using this sorting rule to get highly-relevant search terms. **Note** that `relevance` is only our internal system identifier, so **it can not be used as a filter**, and you will not find this field in the `result` array. The relevance score is based on a similar principle as used in [the Keywords For Keywords](https://docs.dataforseo.com/v3/keywords_data/google/keywords_for_keywords/live/?php) endpoint.
**note that you can set no more than three sorting rules in a single request**
you should use a comma to separate several sorting rules
example:
`["relevance,desc","keyword_info.search_volume,desc"]` | |
| `tag` | string | _user-defined task identifier_
optional field
_the character limit is 255_
you can use this parameter to identify the task and match it with the result
you will find the specified `tag` value in the `data` object of the response | |
‌
As a response of the API server, you will receive [JSON](https://en.wikipedia.org/wiki/JSON)-encoded data containing a `tasks` array with the information specific to the set tasks.
**Description of the fields in the results array:**
| Field name | Type | Description |
| --- | --- | --- |
| `version` | string | _the current version of the API_ | |
| `status_code` | integer | _general status code_
you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors)
**Note:** we strongly recommend designing a necessary system for handling related exceptional or error conditions | |
| `status_message` | string | _general informational message_
you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix/errors) | |
| `time` | string | _execution time, seconds_ | |
| `cost` | float | _total tasks cost, USD_ | |
| `tasks_count` | integer | _the number of tasks in the **`tasks`** array_ | |
| `tasks_error` | integer | _the number of tasks in the **`tasks`** array returned with an error_ | |
| **`tasks`** | array | _array of tasks_ | |
| `id` | string | _task identifier_
**unique task identifier in our system in the [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier) format** | |
| `status_code` | integer | _status code of the task_
generated by DataForSEO; can be within the following range: 10000-60000
you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors) | |
| `status_message` | string | _informational message of the task_
you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix-errors/) | |
| `time` | string | _execution time, seconds_ | |
| `cost` | float | _cost of the task, USD_ | |
| `result_count` | integer | _number of elements in the `result` array_ | |
| `path` | array | _URL path_ | |
| `data` | object | _contains the same parameters that you specified in the POST request_ | |
| **`result`** | array | _array of results_ | |
| `se_type` | string | _search engine type_ | |
| `seed_keywords` | array | _keywords in a POST array_
**keywords are returned with decoded %## (plus character ‘+’ will be decoded to a space character)** | |
| `location_code` | integer | _location code in a POST array_ | |
| `language_code` | string | _language code in a POST array_ | |
| `total_count` | integer | _total number of results relevant to your request in our database_ | |
| `items_count` | integer | _number of results returned in the `items` array_ | |
| `offset` | integer | _current offset value_ | |
| `offset_token` | string | _offset token for subsequent requests_
you can use the string provided in this field to get the subsequent results of the initial task;
**note:** `offset_token` values are unique for each subsequent task | |
| `items` | array | _contains keyword ideas and related data_ | |
| `se_type` | string | _search engine type_ | |
| `keyword` | string | _returned keyword idea_ | |
| `location_code` | integer | _location code in a POST array_ | |
| `language_code` | string | _language code in a POST array_ | |
| `keyword_info` | object | _keyword data for the returned keyword idea_ | |
| `se_type` | string | _search engine type_ | |
| `last_updated_time` | string | _date and time when keyword data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `competition` | float | _competition_
represents the relative amount of competition associated with the given keyword;
the value is based on Google Ads data and can be between 0 and 1 (inclusive) | |
| `competition_level` | string | _competition level_
represents the relative level of competition associated with the given keyword in paid SERP only;
possible values: `LOW`, `MEDIUM`, `HIGH`
if competition level is unknown, the value is `null`;
learn more about the metric in [this help center article](https://dataforseo.com/help-center/what-is-competition) | |
| `cpc` | float | _cost-per-click_
represents the average cost per click (USD) historically paid for the keyword | |
| `search_volume` | integer | _average monthly search volume rate_
represents the (approximate) number of searches for the given keyword idea on google.com | |
| `low_top_of_page_bid` | float | _minimum bid for the ad to be displayed at the top of the first page_
indicates the value greater than about 20% of the lowest bids for which ads were displayed (based on Google Ads statistics for advertisers)
the value may differ depending on the location specified in a POST request | |
| `high_top_of_page_bid` | float | _maximum bid for the ad to be displayed at the top of the first page_
indicates the value greater than about 80% of the lowest bids for which ads were displayed (based on Google Ads statistics for advertisers)
the value may differ depending on the location specified in a POST request | |
| `categories` | array | _product and service categories_
you can download the [full list of possible categories](https://cdn.dataforseo.com/v3/categories/categories_dataforseo_labs_2023_10_25.csv) | |
| `monthly_searches` | array | _monthly searches_
represents the (approximate) number of searches on this keyword idea (as available for the past twelve months), targeted to the specified geographic locations | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _monthly average search volume rate_ | |
| `search_volume_trend` | object | _search volume trend changes_
represents search volume change in percent compared to the previous period | |
| `monthly` | integer | _search volume change in percent compared to the previous month_ | |
| `quarterly` | integer | _search volume change in percent compared to the previous quarter_ | |
| `yearly` | integer | _search volume change in percent compared to the previous year_ | |
| `clickstream_keyword_info` | object | _clickstream data for the returned keyword_
to retrieve results for this field, the parameter `include_clickstream_data` must be set to `true` | |
| `search_volume` | integer | _monthly average clickstream search volume rate_ | |
| `last_updated_time` | string | _date and time when the clickstream dataset was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00” | |
| `gender_distribution` | object | _distribution of estimated clickstream-based metrics by gender_
learn more about how the metric is calculated in this [help center article](https://dataforseo.com/help-center/what-are-clickstream-based-metrics-and-how-do-we-calculate-them) | |
| `female` | integer | _number of female users in the relevant clickstream dataset_ | |
| `male` | integer | _number of male users in the relevant clickstream dataset_ | |
| `age_distribution` | object | _distribution of clickstream-based metrics by age_
learn more about how the metric is calculated in this [help center article](https://dataforseo.com/help-center/what-are-clickstream-based-metrics-and-how-do-we-calculate-them) | |
| `18-24` | integer | _number of users in the relevant clickstream dataset that fall within the 18-24 age range_ | |
| `25-34` | integer | _number of users in the relevant clickstream dataset that fall within the 25-34 age range_ | |
| `35-44` | integer | _number of users in the relevant clickstream dataset that fall within the 35-44 age range_ | |
| `45-54` | integer | _number of users in the relevant clickstream dataset that fall within the 45-54 age range_ | |
| `55-64` | integer | _number of users in the relevant clickstream dataset that fall within the 55-64 age range_ | |
| `monthly_searches` | array | _monthly clickstream search volume rates_
array of objects with clickstream search volume rates in a certain month of a year | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _clickstream-based search volume rate in a certain month of a year_ | |
| `keyword_properties` | object | _additional information about the keyword_ | |
| `se_type` | string | _search engine type_ | |
| `core_keyword` | string | _main keyword in a group_
contains the main keyword in a group determined by the synonym clustering algorithm
if the value is `null`, our database does not contain any keywords the corresponding algorithm could identify as synonymous with `keyword` | |
| `synonym_clustering_algorithm` | string | _the algorithm used to identify synonyms_
possible values:
`keyword_metrics` – indicates the algorithm based on `keyword_info` parameters
`text_processing` – indicates the text-based algorithm
if the value is `null`, our database does not contain any keywords the corresponding algorithm could identify as synonymous with `keyword` | |
| `keyword_difficulty` | integer | _difficulty of ranking in the first top-10 organic results for a keyword_
indicates the chance of getting in top-10 organic results for a keyword on a logarithmic scale from 0 to 100;
calculated by analysing, among other parameters, link profiles of the first 10 pages in SERP;
learn more about the metric in [this help center guide](https://dataforseo.com/help-center/what-is-keyword-difficulty-and-how-is-it-calculated) | |
| `detected_language` | string | _detected language of the keyword_
indicates the language of the keyword as identified by our system | |
| `is_another_language` | boolean | _detected language of the keyword is different from the set language_
if `true`, the language set in the request does not match the language determined by our system for a given keyword | |
| `serp_info` | object | _SERP data_
the value will be `null` if you didn’t set the field `include_serp_info` to `true` in the POST array or if there is no SERP data for this keyword in our database | |
| `se_type` | string | _search engine type_ | |
| `check_url` | string | _direct URL to search engine results_
you can use it to make sure that we provided accurate results | |
| 

| `se_results_count` | string | _number of search results for the returned keyword_ | |
| `last_updated_time` | string | _date and time when SERP data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `previous_updated_time` | string | _previous to the most recent date and time when SERP data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-10-15 12:57:46 +00:00` | |
| `avg_backlinks_info` | object | _backlink data for the returned keyword_
this object provides the average number of backlinks, referring pages and domains, as well as the average rank values among the top-10 webpages ranking organically for the keyword | |
| `se_type` | string | _search engine type_ | |
| `backlinks` | float | _average number of backlinks_ | |
| `dofollow` | float | _average number of dofollow links_ | |
| `referring_pages` | float | _average number of referring pages_ | |
| `referring_domains` | float | _average number of referring domains_ | |
| `referring_main_domains` | float | _average number of referring main domains_ | |
| `rank` | float | _average rank_
learn more about the metric and its calculation formula in [this help center article](https://dataforseo.com/help-center/what_is_rank_in_backlinks_api) | |
| `main_domain_rank` | float | _average main domain rank_
learn more about the metric and its calculation formula in [this help center article](https://dataforseo.com/help-center/what_is_rank_in_backlinks_api) | |
| `last_updated_time` | string | _date and time when backlink data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `search_intent_info` | object | _search intent info for the returned keyword_
learn about search intent in this [help center article](https://dataforseo.com/help-center/search-intent-and-its-types) | |
| `se_type` | string | _search engine type_
possible values: `google` | |
| `main_intent` | string | _main search intent_
possible values: `informational`, `navigational`, `commercial`, `transactional` | |
| `foreign_intent` | array | _supplementary search intents_
possible values: `informational`, `navigational`, `commercial`, `transactional` | |
| `last_updated_time` | string | _date and time when search intent data was last updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| **`keyword_info_normalized_with_bing`** | object | _contains keyword search volume normalized with Bing search volume_ | |
| `last_updated_time` | string | _date and time when the dataset was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `search_volume` | integer | _current search volume rate of a keyword_ | |
| `is_normalized` | boolean | _keyword info is normalized_
if `true`, values are normalized with Bing data | |
| `monthly_searches` | integer | _monthly search volume rates_
array of objects with search volume rates in a certain month of a year | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _search volume rate in a certain month of a year_ | |
| **`keyword_info_normalized_with_clickstream`** | object | _contains keyword search volume normalized with clickstream data_ | |
| `last_updated_time` | string | _date and time when the dataset was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `search_volume` | integer | _current search volume rate of a keyword_ | |
| `is_normalized` | boolean | _keyword info is normalized_
if `true`, values are normalized with clickstream data | |
| `monthly_searches` | integer | _monthly search volume rates_
array of objects with search volume rates in a certain month of a year | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _search volume rate in a certain month of a year_ | |
‌‌
[cURL](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live/?bash#) [php](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live/?bash#) [NodeJs](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live/?bash#) [Python](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live/?bash#) [cSharp](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_ideas/live/?bash#)
## Keyword Suggestions
‌‌
The Keyword Suggestions endpoint provides search queries that include the specified seed keyword.
The algorithm is based on the full-text search for the specified keyword and therefore returns only those search terms that contain the keyword you set in the POST array with additional words before, after, or within the specified key phrase. Returned keyword suggestions can contain the words from the specified key phrase in a sequence different from the one you specify.
As a result, you will get a list of long-tail keywords with each keyword in the list matching the specified search term.
Along with each suggested keyword, you will get its search volume rate for the last month, search volume trend for the previous 12 months, as well as current cost-per-click and competition values.
**Datasource:** DataForSEO Keyword Database
**Search algorithm:** full-text search for terms that match the specified seed keyword with additional words included before, after, or within the seed key phrase.
**Examples**
Specified seed keyword:
_“keyword research”_
Resulting suggestions:
_•”google research keyword”,_
_•”how to do keyword research”,_
_•”keyword competitor research”,_
_•”how to do keyword research for content marketing”_
> Instead of ‘login’ and ‘password’ use your credentials from https://app.dataforseo.com/api-access
```
# Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access
login="login"
password="password"
cred="$(printf ${login}:${password} | base64)"
curl --location --request POST "https://api.dataforseo.com/v3/dataforseo_labs/google/keyword_suggestions/live" \
--header "Authorization: Basic ${cred}" \
--header "Content-Type: application/json" \
--data-raw '[\
{\
"keyword": "phone",\
"location_code": 2840,\
"language_code": "en",\
"include_serp_info": true,\
"include_seed_keyword": true,\
"limit": 1\
}\
]'
```
```
// You can download this file from here https://cdn.dataforseo.com/v3/examples/php/php_RestClient.zip
require('RestClient.php');
$api_url = 'https://api.dataforseo.com/';
// Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access
$client = new RestClient($api_url, null, 'login', 'password');
$post_array = array();
// simple way to set a task
$post_array[] = array(
"keyword" => "phone",
"language_name" => "English",
"location_code" => 2840,
"include_serp_info"=> true,
"include_seed_keyword"=> true,
"limit" => 1
);
try {
// POST /v3/dataforseo_labs/google/keyword_suggestions/live
$result = $client->post('/v3/dataforseo_labs/google/keyword_suggestions/live', $post_array);
print_r($result);
// do something with post result
} catch (RestClientException $e) {
echo "n";
print "HTTP code: {$e->getHttpCode()}n";
print "Error code: {$e->getCode()}n";
print "Message: {$e->getMessage()}n";
print $e->getTraceAsString();
echo "n";
}
$client = null;
?>
```
```
from client import RestClient
# You can download this file from here https://cdn.dataforseo.com/v3/examples/python/python_Client.zip
client = RestClient("login", "password")
post_data = dict()
# simple way to set a task
post_data[len(post_data)] = dict(
keyword="phone",
location_name="United States",
language_name="English",
include_serp_info=True,
include_seed_keyword=True,
limit=1
)
# POST /v3/dataforseo_labs/google/keyword_suggestions/live
response = client.post("/v3/dataforseo_labs/google/keyword_suggestions/live", post_data)
# you can find the full list of the response codes here https://docs.dataforseo.com/v3/appendix/errors
if response["status_code"] == 20000:
print(response)
# do something with result
else:
print("error. Code: %d Message: %s" % (response["status_code"], response["status_message"]))
```
```
const post_array = [];
post_array.push({
"keyword": "phone",
"location_code": 2840,
"language_name": "English",
"include_serp_info": true,
"include_seed_keyword": true,
"limit": 1
});
const axios = require('axios');
axios({
method: 'post',
url: 'https://api.dataforseo.com/v3/dataforseo_labs/keyword_suggestions/live',
auth: {
username: 'login',
password: 'password'
},
data: post_array,
headers: {
'content-type': 'application/json'
}
}).then(function (response) {
var result = response['data']['tasks'];
// Result data
console.log(result);
}).catch(function (error) {
console.log(error);
});
```
```
using Newtonsoft.Json;
using System;
using System.Collections.Generic;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Text;
using System.Threading.Tasks;
namespace DataForSeoDemos
{
public static partial class Demos
{
public static async Task dataforseo_labs_google_keyword_suggestions_live()
{
var httpClient = new HttpClient
{
BaseAddress = new Uri("https://api.dataforseo.com/"),
// Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access
DefaultRequestHeaders = { Authorization = new AuthenticationHeaderValue("Basic", Convert.ToBase64String(Encoding.ASCII.GetBytes("login:password"))) }
};
var postData = new List();
postData.Add(new
{
keyword = "phone",
location_name = "United States",
language_name = "English",
include_serp_info = true,
include_seed_keyword = true,
limit = 1
});
// POST /v3/dataforseo_labs/google/keyword_suggestions/live
// the full list of possible parameters is available in documentation
var taskPostResponse = await httpClient.PostAsync("/v3/dataforseo_labs/google/keyword_suggestions/live", new StringContent(JsonConvert.SerializeObject(postData)));
var result = JsonConvert.DeserializeObject(await taskPostResponse.Content.ReadAsStringAsync());
// you can find the full list of the response codes here https://docs.dataforseo.com/v3/appendix/errors
if (result.status_code == 20000)
{
// do something with result
Console.WriteLine(result);
}
else
Console.WriteLine($"error. Code: {result.status_code} Message: {result.status_message}");
}
}
}
```
> The above command returns JSON structured like this:
```
{
"version": "0.1.20240801",
"status_code": 20000,
"status_message": "Ok.",
"time": "0.2704 sec.",
"cost": 0.0101,
"tasks_count": 1,
"tasks_error": 0,
"tasks": [\
{\
"id": "08221704-1535-0399-0000-0acd15b387ff",\
"status_code": 20000,\
"status_message": "Ok.",\
"time": "0.2019 sec.",\
"cost": 0.0101,\
"result_count": 1,\
"path": [\
"v3",\
"dataforseo_labs",\
"google",\
"keyword_suggestions",\
"live"\
],\
"data": {\
"api": "dataforseo_labs",\
"function": "keyword_suggestions",\
"se_type": "google",\
"keyword": "phone",\
"location_code": 2840,\
"language_code": "en",\
"include_serp_info": true,\
"include_seed_keyword": true,\
"limit": 1\
},\
"result": [\
{\
"se_type": "google",\
"seed_keyword": "phone",\
"seed_keyword_data": {\
"se_type": "google",\
"keyword": "phone",\
"location_code": 2840,\
"language_code": "en",\
"keyword_info": {\
"se_type": "google",\
"last_updated_time": "2024-08-11 13:24:34 +00:00",\
"competition": 1,\
"competition_level": "HIGH",\
"cpc": 5.98,\
"search_volume": 368000,\
"low_top_of_page_bid": 3.08,\
"high_top_of_page_bid": 10.5,\
"categories": [\
10007,\
10878,\
12133,\
13381\
],\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 450000\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 368000\
}\
],\
"search_volume_trend": {\
"monthly": 22,\
"quarterly": 22,\
"yearly": 0\
}\
}\
},\
"clickstream_keyword_info": null,\
"serp_info": {\
"se_type": "google",\
"check_url": "https://www.google.com/search?q=phone&num=100&hl=en&gl=US&gws_rd=cr&ie=UTF-8&oe=UTF-8&glp=1&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",\
"serp_item_types": [\
"popular_products",\
"images",\
"organic",\
"product_considerations",\
"refine_products",\
"top_stories",\
"related_searches"\
],\
"se_results_count": 19880000000,\
"last_updated_time": "2024-07-15 00:43:34 +00:00",\
"previous_updated_time": "2024-05-18 22:29:28 +00:00"\
},\
"keyword_properties": {\
"se_type": "google",\
"core_keyword": null,\
"synonym_clustering_algorithm": "text_processing",\
"keyword_difficulty": 83,\
"detected_language": "en",\
"is_another_language": false\
},\
"search_intent_info": {\
"se_type": "google",\
"main_intent": "navigational",\
"foreign_intent": [\
"commercial"\
],\
"last_updated_time": "2023-03-02 03:54:21 +00:00"\
},\
"avg_backlinks_info": {\
"se_type": "google",\
"backlinks": 6835.7,\
"dofollow": 3775.6,\
"referring_pages": 5352.2,\
"referring_domains": 1100.3,\
"referring_main_domains": 955.1,\
"rank": 369.3,\
"main_domain_rank": 681.2,\
"last_updated_time": "2024-07-14 21:43:39 +00:00"\
},\
"keyword_info_normalized_with_bing": {\
"last_updated_time": "2024-08-17 01:41:37 +00:00",\
"search_volume": 324416,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 396705\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 324416\
}\
]\
},\
"keyword_info_normalized_with_clickstream": {\
"last_updated_time": "2024-08-11 13:24:34 +00:00",\
"search_volume": 368000,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 450000\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 368000\
}\
]\
}\
},\
{\
"location_code": 2840,\
"language_code": "en",\
"total_count": 3488300,\
"items_count": 1,\
"offset": 0,\
"offset_token": "eyJDdXJyZW50T2Zmc2V0IjoxLCJSZXF1ZXN0RGF0YSI6eyJrZXl3b3JkIjoicGhvbmUiLCJpbmNsdWRlX3NlZWRfa2V5d29yZCI6dHJ1ZSwiZnVsbF9tYXRjaCI6ZmFsc2UsImxvYWRfc2VycF9pbmZvIjp0cnVlLCJzZWFyY2hfYWZ0ZXJfdG9rZW4iOm51bGwsImlnbm9yZV9zeW5vbnltcyI6ZmFsc2UsImxhbmd1YWdlIjoiZW4iLCJzZWFyY2hfZW5naW5lIjoiZ29vZ2xlIiwibG9jYXRpb24iOjI4NDAsInVzZV9uZXdfY2F0ZWdvcmllcyI6dHJ1ZSwib3JkZXJfYnkiOnsib3JkZXJfZmllbGQiOiJrZXl3b3JkX2luZm8uc2VhcmNoX3ZvbHVtZSIsIm9yZGVyX3R5cGUiOiJEZXNjIiwibmV4dCI6bnVsbH0sImxpbWl0IjoxLCJvZmZzZXQiOjAsImFpZCI6MTUzNX0sIlJhd1F1ZXJ5IjpudWxsLCJJZCI6Ijc0MjcwOGQwLWZjMjgtNDMwZi04NzA3LTRhZmVjYmJkNDgwZCIsIlNlYXJjaEFmdGVyRGF0YSI6WzE4MzAwMDAsIjkwYzI4YjVjLWVmNWQtNGUwMi04MGU2LTBkYThkZjQyZDY0NyJdfQ==",\
"items": [\
{\
"se_type": "google",\
"keyword": "boost cell phone",\
"location_code": 2840,\
"language_code": "en",\
"keyword_info": {\
"se_type": "google",\
"last_updated_time": "2024-08-12 23:31:45 +00:00",\
"competition": 0.96,\
"competition_level": "HIGH",\
"cpc": 1.47,\
"search_volume": 1830000,\
"low_top_of_page_bid": 0.85,\
"high_top_of_page_bid": 10.44,\
"categories": [\
10007,\
10878,\
12161,\
13381\
],\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 2240000\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 2240000\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 1830000\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 1830000\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 2240000\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 1830000\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 1830000\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 2240000\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 2240000\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 1830000\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 1830000\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 1830000\
}\
],\
"search_volume_trend": {\
"monthly": 22,\
"quarterly": 22,\
"yearly": 0\
}\
},\
"clickstream_keyword_info": null,\
"keyword_properties": {\
"se_type": "google",\
"core_keyword": null,\
"synonym_clustering_algorithm": "text_processing",\
"keyword_difficulty": 0,\
"detected_language": "en",\
"is_another_language": false\
},\
"serp_info": {\
"se_type": "google",\
"check_url": "https://www.google.com/search?q=boost%20cell%20phone&num=100&hl=en&gl=US&gws_rd=cr&ie=UTF-8&oe=UTF-8&glp=1&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",\
"serp_item_types": [\
"organic",\
"people_also_ask",\
"local_pack",\
"popular_products",\
"top_sights",\
"video",\
"images",\
"related_searches"\
],\
"se_results_count": 115000000,\
"last_updated_time": "2024-08-04 08:25:36 +00:00",\
"previous_updated_time": "2024-06-22 17:54:36 +00:00"\
},\
"avg_backlinks_info": {\
"se_type": "google",\
"backlinks": 4739.3,\
"dofollow": 2334.9,\
"referring_pages": 4121.3,\
"referring_domains": 210.3,\
"referring_main_domains": 184.2,\
"rank": 113.1,\
"main_domain_rank": 512.4,\
"last_updated_time": "2024-08-04 08:25:38 +00:00"\
},\
"search_intent_info": {\
"se_type": "google",\
"main_intent": "transactional",\
"foreign_intent": null,\
"last_updated_time": "2023-12-14 04:27:21 +00:00"\
},\
"keyword_info_normalized_with_bing": {\
"last_updated_time": "2024-08-17 06:05:32 +00:00",\
"search_volume": 2893,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 3541\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 3541\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 2893\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 2893\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 3541\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 2893\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 2893\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 3541\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 3541\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 2893\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 2893\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 2893\
},\
{\
"year": 2023,\
"month": 7,\
"search_volume": 2778\
}\
]\
},\
"keyword_info_normalized_with_clickstream": {\
"last_updated_time": "2024-08-17 06:05:32 +00:00",\
"search_volume": 197,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 242\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 242\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 197\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 197\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 242\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 197\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 197\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 242\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 242\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 197\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 197\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 197\
},\
{\
"year": 2023,\
"month": 7,\
"search_volume": 190\
}\
]\
}\
}\
]\
}\
]\
}\
]
}
```
**`POST https://api.dataforseo.com/v3/dataforseo_labs/google/keyword_suggestions/live`**
Your account will be charged for each request.
The cost can be calculated on the [Pricing](https://dataforseo.com/pricing/dataforseo-labs/dataforseo-google-api "Pricing") page.
All POST data should be sent in the [JSON](https://en.wikipedia.org/wiki/JSON) format (UTF-8 encoding). The task setting is done using the POST method. When setting a task, you should send all task parameters in the task array of the generic POST array. You can send up to 2000 API calls per minute. The maximum number of requests that can be sent simultaneously is limited to 30.
You can specify the number of results you want to retrieve, filter and sort them.
Below you will find a detailed description of the fields you can use for setting a task.
**Description of the fields for setting a task:**
| Field name | Type | Description |
| --- | --- | --- |
| `keyword` | string | _keyword_
**required field**
UTF-8 encoding
the keywords will be converted to lowercase format;
learn more about rules and limitations of `keyword` and `keywords` fields in DataForSEO APIs in this [Help Center article](https://dataforseo.com/help-center/rules-and-limitations-of-keyword-and-keywords-fields-in-dataforseo-apis) | |
| `location_name` | string | _full name of the location_
optional field
if you use this field, you don’t need to specify `location_code`
you can receive the list of available locations with their `location_name` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
ignore this field to get the results for all available locations
example:
`United Kingdom` | |
| `location_code` | integer | _location code_
optional field
if you use this field, you don’t need to specify `location_name`
you can receive the list of available locations with their `location_code` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
ignore this field to get the results for all available locations
example:
`2840` | |
| `language_name` | string | _full name of the language_
optional field
if you use this field, you don’t need to specify `language_code`
you can receive the list of available languages with their `language_name` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
example:
`English`
**Note:** if omitted, results default to the language with the most keyword records in the specified location;
refer to the `available_languages.keywords` field of the [Locations and Languages endpoint](https://docs.dataforseo.com/v3/dataforseo_labs/locations_and_languages) to determine the default language | |
| `language_code` | string | _language code_
optional field
if you use this field, you don’t need to specify `language_name`
you can receive the list of available languages with their `language_code` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
example:
`en`
**Note:** if omitted, results default to the language with the most keyword records in the specified location;
refer to the `available_languages.keywords` field of the [Locations and Languages endpoint](https://docs.dataforseo.com/v3/dataforseo_labs/locations_and_languages) to determine the default language | |
| `include_seed_keyword` | boolean | _include data for the seed keyword_
optional field
if set to `true`, data for the seed keyword specified in the `keyword` field will be provided in the `seed_keyword_data` array of the response
default value: `false` | |
| `include_serp_info` | boolean | _include data from SERP for each keyword_
optional field
if set to `true`, we will return a `serp_info` array containing SERP data (number of search results, relevant URL, and SERP features) for every keyword in the response
default value: `false` | |
| `include_clickstream_data` | boolean | _include or exclude data from clickstream-based metrics in the result_
optional field
if the parameter is set to `true`, you will receive `clickstream_keyword_info`, `keyword_info_normalized_with_clickstream`, and `keyword_info_normalized_with_bing` fields in the response
default value: `false`
with this parameter enabled, you will be charged double the price for the request
learn more about how clickstream-based metrics are calculated in this [help center article](https://dataforseo.com/help-center/what-are-clickstream-based-metrics-and-how-do-we-calculate-them) | |
| `exact_match` | boolean | _search for the exact phrase_
optional field
if set to `true`, the returned keywords will include the exact keyword phrase you specified, with potentially other words before or after that phrase
default value: `false` | |
| `ignore_synonyms` | boolean | _ignore highly similar keywords_
optional field
if set to `true` only core keywords will be returned, all highly similar keywords will be excluded;
default value: `false` | |
| `filters` | array | _array of results filtering parameters_
optional field
**you can add several filters at once (8 filters maximum)**
you should set a logical operator `and`, `or` between the conditions
the following operators are supported:
`regex`, `not_regex`, `<`, `<=`, `>`, `>=`, `=`, `<>`, `in`, `not_in`, `match`, `not_match`, `ilike`, `not_ilike`, `like`, `not_like`
you can use the `%` operator with `like` and `not_like`, as well as `ilike` and `not_ilike` to match any string of zero or more characters
example:
`["keyword_info.search_volume",">",0]`
`[["keyword_info.search_volume","in",[0,1000]],
"and",
["keyword_info.competition_level","=","LOW"]]` `[["keyword_info.search_volume",">",100],
"and",
[["keyword_info.cpc","<",0.5],
"or",
["keyword_info.high_top_of_page_bid","<=",0.5]]]`
for more information about filters, please refer to [Dataforseo Labs – Filters](https://docs.dataforseo.com/v3/dataforseo_labs/filters) or this [help center guide](https://dataforseo.com/help-center/how-to-use-filters-in-dataforseo-labs-api) | |
| `order_by` | array | _results sorting rules_
optional field
you can use the same values as in the `filters` array to sort the results
possible sorting types:
`asc` – results will be sorted in the ascending order
`desc` – results will be sorted in the descending order
a comma is used as a separator
example:
`["keyword_info.competition,desc"]`
default rule:
`["keyword_info.search_volume,desc"]`
**note that you can set no more than three sorting rules in a single request**
you should use a comma to separate several sorting rules
example:
`["keyword_info.search_volume,desc","keyword_info.cpc,desc"]` | |
| `limit` | integer | _the maximum number of returned keywords_
optional field
default value: `100`
maximum value: `1000` | |
| `offset` | integer | _offset in the results array of returned keywords_
optional field
default value: `0`
if you specify the `10` value, the first ten keywords in the results array will be omitted and the data will be provided for the successive keywords | |
| `offset_token` | string | _offset token for subsequent requests_
optional field
provided in the identical filed of the response to each request;
use this parameter to avoid timeouts while trying to obtain over 10,000 results in a single request;
by specifying the unique `offset_token` value from the response array, you will get the subsequent results of the initial task;
`offset_token` values are unique for each subsequent task
**Note:** if the `offset_token` is specified in the request, all other parameters except `limit` will not be taken into account when processing a task. | |
| `tag` | string | _user-defined task identifier_
optional field
_the character limit is 255_
you can use this parameter to identify the task and match it with the result
you will find the specified `tag` value in the `data` object of the response | |
‌
As a response of the API server, you will receive [JSON](https://en.wikipedia.org/wiki/JSON)-encoded data containing a `tasks` array with the information specific to the set tasks.
**Description of the fields in the results array:**
| Field name | Type | Description |
| --- | --- | --- |
| `version` | string | _the current version of the API_ | |
| `status_code` | integer | _general status code_
you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors)
**Note:** we strongly recommend designing a necessary system for handling related exceptional or error conditions | |
| `status_message` | string | _general informational message_
you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix/errors) | |
| `time` | string | _execution time, seconds_ | |
| `cost` | float | _total tasks cost, USD_ | |
| `tasks_count` | integer | _the number of tasks in the **`tasks`** array_ | |
| `tasks_error` | integer | _the number of tasks in the **`tasks`** array returned with an error_ | |
| **`tasks`** | array | _array of tasks_ | |
| `id` | string | _task identifier_
**unique task identifier in our system in the [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier) format** | |
| `status_code` | integer | _status code of the task_
generated by DataForSEO; can be within the following range: 10000-60000
you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors) | |
| `status_message` | string | _informational message of the task_
you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix-errors/) | |
| `time` | string | _execution time, seconds_ | |
| `cost` | float | _cost of the task, USD_ | |
| `result_count` | integer | _number of elements in the `result` array_ | |
| `path` | array | _URL path_ | |
| `data` | object | _contains the same parameters that you specified in the POST request_ | |
| **`result`** | array | _array of results_ | |
| `se_type` | string | _search engine type_ | |
| `seed_keyword` | string | _keyword in a POST array_ | |
| **`seed_keyword_data`** | object | _keyword data for the seed keyword_
fields in this object are identical to those of the `items` array | |
| `location_code` | integer | _location code in a POST array_
if there is no data, then the value is `null` | |
| `language_code` | string | _language code in a POST array_
if there is no data, then the value is `null` | |
| `total_count` | integer | _total amount of results in our database relevant to your request_ | |
| `items_count` | integer | _the number of results returned in the `items` array_ | |
| `offset` | integer | _current offset value_ | |
| `offset_token` | string | _offset token for subsequent requests_
you can use the string provided in this field to get the subsequent results of the initial task;
**note:** `offset_token` values are unique for each subsequent task | |
| **`items`** | array | _contains keywords and related data_ | |
| `se_type` | string | _search engine type_ | |
| `keyword` | string | _keyword suggestion_ | |
| `location_code` | integer | _location code in a POST array_ | |
| `language_code` | string | _language code in a POST array_ | |
| **`keyword_info`** | object | _keyword data for the returned keyword_ | |
| `se_type` | string | _search engine type_ | |
| `last_updated_time` | string | _date and time when keyword data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `competition` | float | _competition_
represents the relative amount of competition associated with the given keyword;
the value is based on Google Ads data and can be between 0 and 1 (inclusive) | |
| `competition_level` | string | _competition level_
represents the relative level of competition associated with the given keyword in paid SERP only;
possible values: `LOW`, `MEDIUM`, `HIGH`
if competition level is unknown, the value is `null`;
learn more about the metric in [this help center article](https://dataforseo.com/help-center/what-is-competition) | |
| `cpc` | float | _cost-per-click_
represents the average cost per click (USD) historically paid for the keyword | |
| `search_volume` | integer | _average monthly search volume rate_
represents the (approximate) number of searches for the given keyword idea on google.com | |
| `low_top_of_page_bid` | float | _minimum bid for the ad to be displayed at the top of the first page_
indicates the value greater than about 20% of the lowest bids for which ads were displayed (based on Google Ads statistics for advertisers)
the value may differ depending on the location specified in a POST request | |
| `high_top_of_page_bid` | float | _maximum bid for the ad to be displayed at the top of the first page_
indicates the value greater than about 80% of the lowest bids for which ads were displayed (based on Google Ads statistics for advertisers)
the value may differ depending on the location specified in a POST request | |
| `categories` | array | _product and service categories_
you can download the [full list of possible categories](https://cdn.dataforseo.com/v3/categories/categories_dataforseo_labs_2023_10_25.csv) | |
| `monthly_searches` | array | _monthly searches_
represents the (approximate) number of searches for this keyword idea (as available for the past twelve months), targeted to the specified geographic locations | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _monthly average search volume rate_ | |
| `search_volume_trend` | object | _search volume trend changes_
represents search volume change in percent compared to the previous period | |
| `monthly` | integer | _search volume change in percent compared to the previous month_ | |
| `quarterly` | integer | _search volume change in percent compared to the previous quarter_ | |
| `yearly` | integer | _search volume change in percent compared to the previous year_ | |
| `clickstream_keyword_info` | object | _clickstream data for the returned keyword_
to retrieve results for this field, the parameter `include_clickstream_data` must be set to `true` | |
| `search_volume` | integer | _monthly average clickstream search volume rate_ | |
| `last_updated_time` | string | _date and time when the clickstream dataset was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00” | |
| `gender_distribution` | object | _distribution of estimated clickstream-based metrics by gender_
learn more about how the metric is calculated in this [help center article](https://dataforseo.com/help-center/what-are-clickstream-based-metrics-and-how-do-we-calculate-them) | |
| `female` | integer | _number of female users in the relevant clickstream dataset_ | |
| `male` | integer | _number of male users in the relevant clickstream dataset_ | |
| `age_distribution` | object | _distribution of clickstream-based metrics by age_
learn more about how the metric is calculated in this [help center article](https://dataforseo.com/help-center/what-are-clickstream-based-metrics-and-how-do-we-calculate-them) | |
| `18-24` | integer | _number of users in the relevant clickstream dataset that fall within the 18-24 age range_ | |
| `25-34` | integer | _number of users in the relevant clickstream dataset that fall within the 25-34 age range_ | |
| `35-44` | integer | _number of users in the relevant clickstream dataset that fall within the 35-44 age range_ | |
| `45-54` | integer | _number of users in the relevant clickstream dataset that fall within the 45-54 age range_ | |
| `55-64` | integer | _number of users in the relevant clickstream dataset that fall within the 55-64 age range_ | |
| `monthly_searches` | array | _monthly clickstream search volume rates_
array of objects with clickstream search volume rates in a certain month of a year | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _clickstream-based search volume rate in a certain month of a year_ | |
| **`keyword_properties`** | object | _additional information about the keyword_ | |
| `se_type` | string | _search engine type_ | |
| `core_keyword` | string | _main keyword in a group_
contains the main keyword in a group determined by the synonym clustering algorithm
if the value is `null`, our database does not contain any keywords the corresponding algorithm could identify as synonymous with `keyword` | |
| `synonym_clustering_algorithm` | string | _the algorithm used to identify synonyms_
possible values:
`keyword_metrics` – indicates the algorithm based on `keyword_info` parameters
`text_processing` – indicates the text-based algorithm
if the value is `null`, our database does not contain any keywords the corresponding algorithm could identify as synonymous with `keyword` | |
| `keyword_difficulty` | integer | _difficulty of ranking in the first top-10 organic results for a keyword_
indicates the chance of getting in top-10 organic results for a keyword on a logarithmic scale from 0 to 100;
calculated by analysing, among other parameters, link profiles of the first 10 pages in SERP;
learn more about the metric in [this help center guide](https://dataforseo.com/help-center/what-is-keyword-difficulty-and-how-is-it-calculated) | |
| `detected_language` | string | _detected language of the keyword_
indicates the language of the keyword as identified by our system | |
| `is_another_language` | boolean | _detected language of the keyword is different from the set language_
if `true`, the language set in the request does not match the language determined by our system for a given keyword | |
| **`serp_info`** | object | _SERP data_
the value will be `null` if you didn’t set the field `include_serp_info` to `true` in the POST array or if there is no SERP data for this keyword in our database | |
| `se_type` | string | _search engine type_ | |
| `check_url` | string | _direct URL to search engine results_
you can use it to make sure that we provided accurate results | |
| `serp_item_types` | array | _types of search results in SERP_
contains types of search results (items) found in SERP
possible item types:
`answer_box`, `app`, `carousel`, `multi_carousel`, `featured_snippet`, `google_flights`, `google_reviews`, `third_party_reviews`, `google_posts`, `images`, `jobs`, `knowledge_graph`, `local_pack`, `hotels_pack`, `map`, `organic`, `paid`, `people_also_ask`, `related_searches`, `people_also_search`, `shopping`, `top_stories`, `twitter`, `video`, `events`, `mention_carousel`, `recipes`, `top_sights`, `scholarly_articles`, `popular_products`, `podcasts`, `questions_and_answers`, `find_results_on`, `stocks_box`, `visual_stories`, `commercial_units`, `local_services`, `google_hotels`, `math_solver`, `currency_box`, `product_considerations`, `found_on_web`, `short_videos`, `refine_products`, `explore_brands`, `perspectives`, `discussions_and_forums`, `compare_sites`, `courses`, `ai_overview`;
**note** that the actual results will be returned only for `organic`, `paid`, `featured_snippet`, and `local_pack` elements | |
| `se_results_count` | string | _number of search results for the returned keyword_ | |
| `last_updated_time` | string | _date and time when SERP data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `previous_updated_time` | string | _previous to the most recent date and time when SERP data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-10-15 12:57:46 +00:00` | |
| **`avg_backlinks_info`** | object | _backlink data for the returned keyword_
this object provides the average number of backlinks, referring pages and domains, as well as the average rank values among the top-10 webpages ranking organically for the keyword | |
| `se_type` | string | _search engine type_ | |
| `backlinks` | float | _average number of backlinks_ | |
| `dofollow` | float | _average number of dofollow links_ | |
| `referring_pages` | float | _average number of referring pages_ | |
| `referring_domains` | float | _average number of referring domains_ | |
| `referring_main_domains` | float | _average number of referring main domains_ | |
| `rank` | float | _average rank_
learn more about the metric and its calculation formula in [this help center article](https://dataforseo.com/help-center/what_is_rank_in_backlinks_api) | |
| `main_domain_rank` | float | _average main domain rank_
learn more about the metric and its calculation formula in [this help center article](https://dataforseo.com/help-center/what_is_rank_in_backlinks_api) | |
| `last_updated_time` | string | _date and time when backlink data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| **`search_intent_info`** | object | _search intent info for the returned keyword_
learn about search intent in this [help center article](https://dataforseo.com/help-center/search-intent-and-its-types) | |
| `se_type` | string | _search engine type_
possible values: `google` | |
| `main_intent` | string | _main search intent_
possible values: `informational`, `navigational`, `commercial`, `transactional` | |
| `foreign_intent` | array | _supplementary search intents_
possible values: `informational`, `navigational`, `commercial`, `transactional` | |
| `last_updated_time` | string | _date and time when search intent data was last updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| **`keyword_info_normalized_with_bing`** | object | _contains keyword search volume normalized with Bing search volume_ | |
| `last_updated_time` | string | _date and time when the dataset was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `search_volume` | integer | _current search volume rate of a keyword_ | |
| `is_normalized` | boolean | _keyword info is normalized_
if `true`, values are normalized with Bing data | |
| `monthly_searches` | integer | _monthly search volume rates_
array of objects with search volume rates in a certain month of a year | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _search volume rate in a certain month of a year_ | |
| **`keyword_info_normalized_with_clickstream`** | object | _contains keyword search volume normalized with clickstream data_ | |
| `last_updated_time` | string | _date and time when the dataset was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `search_volume` | integer | _current search volume rate of a keyword_ | |
| `is_normalized` | boolean | _keyword info is normalized_
if `true`, values are normalized with clickstream data | |
| `monthly_searches` | integer | _monthly search volume rates_
array of objects with search volume rates in a certain month of a year | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _search volume rate in a certain month of a year_ | |
‌‌
[cURL](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_suggestions/live/?bash#) [php](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_suggestions/live/?bash#) [NodeJs](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_suggestions/live/?bash#) [Python](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_suggestions/live/?bash#) [cSharp](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_suggestions/live/?bash#)
## Related Keywords
The Related Keywords endpoint provides keywords appearing in the
["searches related to" SERP element![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-related-searches.png)](https://docs.dataforseo.com/v3/dataforseo_labs/google/related_keywords/live/?bash#)
You can get up to 4680 keyword ideas by specifying the search depth. Each related keyword comes with the list of relevant product categories, search volume rate for the last month, search volume trend for the previous 12 months, as well as current cost-per-click and competition values.
**Datasource:** DataForSEO SERPs Database
**Search algorithm:** depth-first search for queries appearing in the “search related to” element of SERP for the specified seed keyword.
**Examples:**
Note: the `depth` parameter is set to `1`
Specified seed keyword:
_“keyword research”_
Resulting related keywords:
_•”free keyword research”_,
_•”keyword research tools”_,
_•”best free keyword research tool”_,
_•”keyword research tips”_,
_•”seo keyword research tool”_,
_•”keyword research step by step”_,
_•”how to do keyword research 2019″_,
_•”keyword research google ads”_
> Instead of ‘login’ and ‘password’ use your credentials from https://app.dataforseo.com/api-access
```
# Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access
login="login"
password="password"
cred="$(printf ${login}:${password} | base64)"
curl --location --request POST "https://api.dataforseo.com/v3/dataforseo_labs/google/related_keywords/live" \
--header "Authorization: Basic ${cred}" \
--header "Content-Type: application/json" \
--data-raw '[\
{\
"keyword": "phone",\
"language_name": "English",\
"location_code": 2840,\
"limit": 3\
}\
]'
```
```
// You can download this file from here https://cdn.dataforseo.com/v3/examples/php/php_RestClient.zip
require('RestClient.php');
$api_url = 'https://api.dataforseo.com/';
// Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access
$client = new RestClient($api_url, null, 'login', 'password');
$post_array = array();
// simple way to set a task
$post_array[] = array(
"keyword" => "phone",
"language_name" => "English",
"location_code" => 2840,
"filters" => [\
["keyword_data.keyword_info.search_volume", ">", 10]\
],
"limit": 3
);
try {
// POST /v3/dataforseo_labs/google/related_keywords/live
$result = $client->post('/v3/dataforseo_labs/google/related_keywords/live', $post_array);
print_r($result);
// do something with post result
} catch (RestClientException $e) {
echo "n";
print "HTTP code: {$e->getHttpCode()}n";
print "Error code: {$e->getCode()}n";
print "Message: {$e->getMessage()}n";
print $e->getTraceAsString();
echo "n";
}
$client = null;
?>
```
```
from client import RestClient
# You can download this file from here https://cdn.dataforseo.com/v3/examples/python/python_Client.zip
client = RestClient("login", "password")
post_data = dict()
# simple way to set a task
post_data[len(post_data)] = dict(
keyword="phone",
location_name="United States",
language_name="English",
filters=[\
["keyword_data.keyword_info.search_volume", ">", 10]\
],
limit=3
)
# POST /v3/dataforseo_labs/google/related_keywords/live
response = client.post("/v3/dataforseo_labs/google/related_keywords/live", post_data)
# you can find the full list of the response codes here https://docs.dataforseo.com/v3/appendix/errors
if response["status_code"] == 20000:
print(response)
# do something with result
else:
print("error. Code: %d Message: %s" % (response["status_code"], response["status_message"]))
```
```
const post_array = [];
post_array.push({
"keyword": "phone",
"language_name": "English",
"location_code": 2840,
"filters": [\
["keyword_data.keyword_info.search_volume", ">", 10]\
],
"limit": 3
});
const axios = require('axios');
axios({
method: 'post',
url: 'https://api.dataforseo.com/v3/dataforseo_labs/google/related_keywords/live',
auth: {
username: 'login',
password: 'password'
},
data: post_array,
headers: {
'content-type': 'application/json'
}
}).then(function (response) {
var result = response['data']['tasks'];
// Result data
console.log(result);
}).catch(function (error) {
console.log(error);
});
```
```
using Newtonsoft.Json;
using System;
using System.Collections.Generic;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Text;
using System.Threading.Tasks;
namespace DataForSeoDemos
{
public static partial class Demos
{
public static async Task dataforseo_labs_google_related_keywords_live()
{
var httpClient = new HttpClient
{
BaseAddress = new Uri("https://api.dataforseo.com/"),
// Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access
DefaultRequestHeaders = { Authorization = new AuthenticationHeaderValue("Basic", Convert.ToBase64String(Encoding.ASCII.GetBytes("login:password"))) }
};
var postData = new List();
postData.Add(new
{
keyword = "phone",
location_name = "United States",
language_name = "English",
filters = new object[]
{
new object[] { "keyword_data.keyword_info.search_volume", ">", 10 }
},
limit = 3
});
// POST /v3/dataforseo_labs/google/related_keywords/live
// the full list of possible parameters is available in documentation
var taskPostResponse = await httpClient.PostAsync("/v3/dataforseo_labs/google/related_keywords/live", new StringContent(JsonConvert.SerializeObject(postData)));
var result = JsonConvert.DeserializeObject(await taskPostResponse.Content.ReadAsStringAsync());
// you can find the full list of the response codes here https://docs.dataforseo.com/v3/appendix/errors
if (result.status_code == 20000)
{
// do something with result
Console.WriteLine(result);
}
else
Console.WriteLine($"error. Code: {result.status_code} Message: {result.status_message}");
}
}
}
```
> The above command returns JSON structured like this:
```
{
"version": "0.1.20240801",
"status_code": 20000,
"status_message": "Ok.",
"time": "0.0995 sec.",
"cost": 0.0103,
"tasks_count": 1,
"tasks_error": 0,
"tasks": [\
{\
"id": "08221812-1535-0387-0000-53d53d3e60c5",\
"status_code": 20000,\
"status_message": "Ok.",\
"time": "0.0326 sec.",\
"cost": 0.0103,\
"result_count": 1,\
"path": [\
"v3",\
"dataforseo_labs",\
"google",\
"related_keywords",\
"live"\
],\
"data": {\
"api": "dataforseo_labs",\
"function": "related_keywords",\
"se_type": "google",\
"keyword": "phone",\
"language_name": "English",\
"location_code": 2840,\
"limit": 3\
},\
"result": [\
{\
"se_type": "google",\
"seed_keyword": "phone",\
"seed_keyword_data": null,\
"location_code": 2840,\
"language_code": "en",\
"total_count": 9,\
"items_count": 3,\
"items": [\
{\
"se_type": "google",\
"keyword_data": {\
"se_type": "google",\
"keyword": "phone",\
"location_code": 2840,\
"language_code": "en",\
"keyword_info": {\
"se_type": "google",\
"last_updated_time": "2024-08-11 13:24:34 +00:00",\
"competition": 1,\
"competition_level": "HIGH",\
"cpc": 5.98,\
"search_volume": 368000,\
"low_top_of_page_bid": 3.08,\
"high_top_of_page_bid": 10.5,\
"categories": [\
10007,\
10878,\
12133,\
13381\
],\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 450000\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 368000\
}\
],\
"search_volume_trend": {\
"monthly": 22,\
"quarterly": 22,\
"yearly": 0\
}\
},\
"clickstream_keyword_info": null,\
"keyword_properties": {\
"se_type": "google",\
"core_keyword": null,\
"synonym_clustering_algorithm": "text_processing",\
"keyword_difficulty": 83,\
"detected_language": "en",\
"is_another_language": false\
},\
"serp_info": {\
"se_type": "google",\
"check_url": "https://www.google.com/search?q=phone&num=100&hl=en&gl=US&gws_rd=cr&ie=UTF-8&oe=UTF-8&glp=1&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",\
"serp_item_types": [\
"popular_products",\
"images",\
"organic",\
"product_considerations",\
"refine_products",\
"top_stories",\
"related_searches"\
],\
"se_results_count": 19880000000,\
"last_updated_time": "2024-07-15 00:43:34 +00:00",\
"previous_updated_time": "2024-05-18 22:29:28 +00:00"\
},\
"avg_backlinks_info": {\
"se_type": "google",\
"backlinks": 6835.7,\
"dofollow": 3775.6,\
"referring_pages": 5352.2,\
"referring_domains": 1100.3,\
"referring_main_domains": 955.1,\
"rank": 369.3,\
"main_domain_rank": 681.2,\
"last_updated_time": "2024-07-14 21:43:39 +00:00"\
},\
"search_intent_info": {\
"se_type": "google",\
"main_intent": "navigational",\
"foreign_intent": [\
"commercial"\
],\
"last_updated_time": "2023-03-02 03:54:21 +00:00"\
},\
"keyword_info_normalized_with_bing": {\
"last_updated_time": "2024-08-17 01:41:37 +00:00",\
"search_volume": 324416,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 396705\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 324416\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 324416\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 324416\
}\
]\
},\
"keyword_info_normalized_with_clickstream": {\
"last_updated_time": "2024-08-11 13:24:34 +00:00",\
"search_volume": 368000,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 450000\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 368000\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 368000\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 368000\
}\
]\
}\
},\
"depth": 0,\
"related_keywords": [\
"phone app",\
"phone call",\
"phone app download",\
"phone call app",\
"phone samsung",\
"phone definition",\
"phone app on android",\
"my phone app"\
]\
},\
{\
"se_type": "google",\
"keyword_data": {\
"se_type": "google",\
"keyword": "phone call",\
"location_code": 2840,\
"language_code": "en",\
"keyword_info": {\
"se_type": "google",\
"last_updated_time": "2024-08-08 14:16:10 +00:00",\
"competition": 0.07,\
"competition_level": "LOW",\
"cpc": 4.23,\
"search_volume": 27100,\
"low_top_of_page_bid": 0.92,\
"high_top_of_page_bid": 7.55,\
"categories": [\
10007,\
10019,\
10167,\
10878,\
11506,\
11510,\
12762,\
13419\
],\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 60500\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 27100\
}\
],\
"search_volume_trend": {\
"monthly": 22,\
"quarterly": 22,\
"yearly": 0\
}\
},\
"clickstream_keyword_info": null,\
"keyword_properties": {\
"se_type": "google",\
"core_keyword": "phone calling",\
"synonym_clustering_algorithm": "text_processing",\
"keyword_difficulty": 57,\
"detected_language": "en",\
"is_another_language": false\
},\
"serp_info": {\
"se_type": "google",\
"check_url": "https://www.google.com/search?q=phone%20call&num=100&hl=en&gl=US&gws_rd=cr&ie=UTF-8&oe=UTF-8&glp=1&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",\
"serp_item_types": [\
"organic",\
"people_also_ask",\
"related_searches"\
],\
"se_results_count": 25270000000,\
"last_updated_time": "2024-08-04 13:21:19 +00:00",\
"previous_updated_time": "2024-06-22 22:50:34 +00:00"\
},\
"avg_backlinks_info": {\
"se_type": "google",\
"backlinks": 11475.1,\
"dofollow": 7174.8,\
"referring_pages": 10754.1,\
"referring_domains": 884.2,\
"referring_main_domains": 765,\
"rank": 329.4,\
"main_domain_rank": 787.5,\
"last_updated_time": "2024-08-04 10:21:19 +00:00"\
},\
"search_intent_info": {\
"se_type": "google",\
"main_intent": "commercial",\
"foreign_intent": null,\
"last_updated_time": "2023-03-02 03:54:30 +00:00"\
},\
"keyword_info_normalized_with_bing": {\
"last_updated_time": "2024-08-16 12:35:38 +00:00",\
"search_volume": 19134,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 19134\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 42717\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 19134\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 19134\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 19134\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 19134\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 19134\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 19134\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 19134\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 19134\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 19134\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 19134\
}\
]\
},\
"keyword_info_normalized_with_clickstream": {\
"last_updated_time": "2024-08-08 14:16:10 +00:00",\
"search_volume": 27100,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 60500\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 27100\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 27100\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 27100\
}\
]\
}\
},\
"depth": 1,\
"related_keywords": [\
"phone call online",\
"phone call app",\
"free phone call",\
"phone app",\
"phone call app download",\
"i want to make a phone call on my phone",\
"phone by google",\
"make a phone call to someone"\
]\
},\
{\
"se_type": "google",\
"keyword_data": {\
"se_type": "google",\
"keyword": "phone app",\
"location_code": 2840,\
"language_code": "en",\
"keyword_info": {\
"se_type": "google",\
"last_updated_time": "2024-08-11 20:06:52 +00:00",\
"competition": 0.15,\
"competition_level": "LOW",\
"cpc": 2.14,\
"search_volume": 22200,\
"low_top_of_page_bid": 0.46,\
"high_top_of_page_bid": 3.16,\
"categories": [\
10007,\
10019,\
10168,\
10878,\
10885,\
13378,\
13381\
],\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 22200\
}\
],\
"search_volume_trend": {\
"monthly": 22,\
"quarterly": 22,\
"yearly": 0\
}\
},\
"clickstream_keyword_info": null,\
"keyword_properties": {\
"se_type": "google",\
"core_keyword": "phone for apps",\
"synonym_clustering_algorithm": "text_processing",\
"keyword_difficulty": 66,\
"detected_language": "en",\
"is_another_language": false\
},\
"serp_info": {\
"se_type": "google",\
"check_url": "https://www.google.com/search?q=phone%20app&num=100&hl=en&gl=US&gws_rd=cr&ie=UTF-8&oe=UTF-8&glp=1&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",\
"serp_item_types": [\
"organic",\
"people_also_ask",\
"images",\
"related_searches"\
],\
"se_results_count": 25270000000,\
"last_updated_time": "2024-08-04 13:51:15 +00:00",\
"previous_updated_time": "2024-06-22 23:19:03 +00:00"\
},\
"avg_backlinks_info": {\
"se_type": "google",\
"backlinks": 4502.4,\
"dofollow": 2513.9,\
"referring_pages": 3667.1,\
"referring_domains": 984.5,\
"referring_main_domains": 855.8,\
"rank": 322.6,\
"main_domain_rank": 814.6,\
"last_updated_time": "2024-08-04 10:51:22 +00:00"\
},\
"search_intent_info": {\
"se_type": "google",\
"main_intent": "commercial",\
"foreign_intent": [\
"navigational"\
],\
"last_updated_time": "2023-03-02 03:54:24 +00:00"\
},\
"keyword_info_normalized_with_bing": {\
"last_updated_time": "2024-08-16 23:26:54 +00:00",\
"search_volume": 4979,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 4979\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 4979\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 4979\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 4979\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 4979\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 4979\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 4979\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 4979\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 4979\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 4979\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 4979\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 4979\
}\
]\
},\
"keyword_info_normalized_with_clickstream": {\
"last_updated_time": "2024-08-11 20:06:52 +00:00",\
"search_volume": 22200,\
"is_normalized": true,\
"monthly_searches": [\
{\
"year": 2024,\
"month": 7,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 6,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 5,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 4,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 3,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 2,\
"search_volume": 22200\
},\
{\
"year": 2024,\
"month": 1,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 12,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 11,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 10,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 9,\
"search_volume": 22200\
},\
{\
"year": 2023,\
"month": 8,\
"search_volume": 22200\
}\
]\
}\
},\
"depth": 1,\
"related_keywords": [\
"phone app download",\
"phone app on android",\
"my phone app",\
"phone app free",\
"google phone app",\
"phone call",\
"phone app download free",\
"phone by google"\
]\
}\
]\
}\
]\
}\
]
}
```
**`POST https://api.dataforseo.com/v3/dataforseo_labs/google/related_keywords/live`**
Your account will be charged for each request.
The cost can be calculated on the [Pricing](https://dataforseo.com/pricing/dataforseo-labs/dataforseo-google-api "Pricing") page.
All POST data should be sent in the [JSON](https://en.wikipedia.org/wiki/JSON) format (UTF-8 encoding). The task setting is done using the POST method. When setting a task, you should send all task parameters in the task array of the generic POST array. You can send up to 2000 API calls per minute. The maximum number of requests that can be sent simultaneously is limited to 30.
You can specify the number of results you want to retrieve, filter and sort them.
Below you will find a detailed description of the fields you can use for setting a task.
**Description of the fields for setting a task:**
| Field name | Type | Description |
| --- | --- | --- |
| `keyword` | string | _keyword_
**required field**
UTF-8 encoding
the keywords will be converted to lowercase format
learn more about rules and limitations of `keyword` and `keywords` fields in DataForSEO APIs in this [Help Center article](https://dataforseo.com/help-center/rules-and-limitations-of-keyword-and-keywords-fields-in-dataforseo-apis) | |
| `location_name` | string | _full name of the location_
**required field if you don’t specify** `location_code`
**Note:** it is required to specify either `location_name` or `location_code`
you can receive the list of available locations with their `location_name` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
example:
`United Kingdom` | |
| `location_code` | integer | _location code_
**required field if you don’t specify** `location_name`
**Note:** it is required to specify either `location_name` or `location_code`
you can receive the list of available locations with their `location_code` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
example:
`2840` | |
| `language_name` | string | _full name of the language_
**required field if you don’t specify** `language_code`
**Note:** it is required to specify either `language_name` or `language_code`
you can receive the list of available locations with their `language_name` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
example:
`English` | |
| `language_code` | string | _language code_
**required field if you don’t specify** `language_name`
**Note:** it is required to specify either `language_name` or `language_code`
you can receive the list of available locations with their `language_code` by making a separate request to the
`https://api.dataforseo.com/v3/dataforseo_labs/locations_and_languages`
example:
`en` | |
| `depth` | integer | _keyword search depth_
optional field
default value: `1`
number of the returned results depends on the value you set in this field
you can specify a level from 0 to 4
estimated number of keywords for each level (maximum):
0 – the keyword set in the `keyword` field
1 – 8 keywords
2 – 72 keywords
3 – 584 keywords
4 – 4680 keywords | |
| `include_seed_keyword` | boolean | _include data for the seed keyword_
optional field
if set to `true`, data for the seed keyword specified in the `keyword` field will be provided in the `seed_keyword_data` array of the response
default value: `false` | |
| `include_serp_info` | boolean | _include data from SERP for each keyword_
optional field
if set to `true`, we will return a `serp_info` array containing SERP data (number of search results, relevant URL, and SERP features) for every keyword in the response
default value: `false` | |
| `include_clickstream_data` | boolean | _include or exclude data from clickstream-based metrics in the result_
optional field
if the parameter is set to `true`, you will receive `clickstream_keyword_info`, `keyword_info_normalized_with_clickstream`, and `keyword_info_normalized_with_bing` fields in the response
default value: `false`
with this parameter enabled, you will be charged double the price for the request
learn more about how clickstream-based metrics are calculated in this [help center article](https://dataforseo.com/help-center/what-are-clickstream-based-metrics-and-how-do-we-calculate-them) | |
| `ignore_synonyms` | boolean | _ignore highly similar keywords_
optional field
if set to `true` only core keywords will be returned, all highly similar keywords will be excluded;
default value: `false` | |
| `replace_with_core_keyword` | boolean | _return data for core keyword_
optional field
if `true`, `serp_info` and `related_keywords` will be returned for the main keyword in the group that the specified `keyword` belongs to;
if `false`, `serp_info` and `related_keywords` will be returned for the specified `keyword` (if available);
refer to [this help center article](https://dataforseo.com/help-center/replace_with_core_keyword) for more details;
default value: `false` | |
| `filters` | array | _array of results filtering parameters_
optional field
**you can add several filters at once (8 filters maximum)**
you should set a logical operator `and`, `or` between the conditions
the following operators are supported:
`regex`, `not_regex`, `<`, `<=`, `>`, `>=`, `=`, `<>`, `in`, `not_in`, `match`, `not_match`, `ilike`, `not_ilike`, `like`, `not_like`
you can use the `%` operator with `like` and `not_like`, as well as `ilike` and `not_ilike` to match any string of zero or more characters
example:
`["keyword_data.keyword_info.search_volume",">",0]`
`[["keyword_info.search_volume","in",[0,1000]],
"and",
["keyword_data.keyword_info.competition_level","=","LOW"]]`
`[["keyword_data.keyword_info.search_volume",">",100],
"and",
[["keyword_data.keyword_info.cpc","<",0.5],
"or",
["keyword_info.high_top_of_page_bid","<=",0.5]]]`
for more information about filters, please refer to [Dataforseo Labs – Filters](https://docs.dataforseo.com/v3/dataforseo_labs/filters) or this [help center guide](https://dataforseo.com/help-center/how-to-use-filters-in-dataforseo-labs-api) | |
| `order_by` | array | _results sorting rules_
optional field
you can use the same values as in the `filters` array to sort the results
possible sorting types:
`asc` – results will be sorted in the ascending order
`desc` – results will be sorted in the descending order
you should use a comma to set up a sorting type
example:
`["keyword_data.keyword_info.competition,desc"]`
default rule:
`["keyword_data.keyword_info.search_volume,desc"]`
**note that you can set no more than three sorting rules in a single request**
you should use a comma to separate several sorting rules
example:
`["keyword_data.keyword_info.search_volume,desc","keyword_data.keyword_info.cpc,desc"]` | |
| `limit` | integer | _the maximum number of returned keywords_
optional field
default value: `100`
maximum value: `1000` | |
| `offset` | integer | _offset in the results array of returned keywords_
optional field
default value: `0`
if you specify the `10` value, the first ten keywords in the results array will be omitted and the data will be provided for the successive keywords | |
| `tag` | string | _user-defined task identifier_
optional field
_the character limit is 255_
you can use this parameter to identify the task and match it with the result
you will find the specified `tag` value in the `data` object of the response | |
‌
As a response of the API server, you will receive [JSON](https://en.wikipedia.org/wiki/JSON)-encoded data containing a `tasks` array with the information specific to the set tasks.
**Description of the fields in the results array:**
| Field name | Type | Description |
| --- | --- | --- |
| `version` | string | _the current version of the API_ | |
| `status_code` | integer | _general status code_
you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors)
**Note:** we strongly recommend designing a necessary system for handling related exceptional or error conditions | |
| `status_message` | string | _general informational message_
you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix/errors) | |
| `time` | string | _execution time, seconds_ | |
| `cost` | float | _total tasks cost, USD_ | |
| `tasks_count` | integer | _the number of tasks in the **`tasks`** array_ | |
| `tasks_error` | integer | _the number of tasks in the **`tasks`** array returned with an error_ | |
| **`tasks`** | array | _array of tasks_ | |
| `id` | string | _task identifier_
**unique task identifier in our system in the [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier) format** | |
| `status_code` | integer | _status code of the task_
generated by DataForSEO; can be within the following range: 10000-60000
you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors) | |
| `status_message` | string | _informational message of the task_
you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix-errors/) | |
| `time` | string | _execution time, seconds_ | |
| `cost` | float | _cost of the task, USD_ | |
| `result_count` | integer | _number of elements in the `result` array_ | |
| `path` | array | _URL path_ | |
| `data` | object | _contains the same parameters that you specified in the POST request_ | |
| **`result`** | array | _array of results_ | |
| `se_type` | string | _search engine type_ | |
| `seed_keyword` | string | _keyword in a POST array_ | |
| **`seed_keyword_data`** | array | _keyword data for the seed keyword_
fields in the array are identical to that of `keyword_data` | |
| `location_code` | integer | _location code in a POST array_ | |
| `language_code` | string | _language code in a POST array_ | |
| `total_count` | integer | _total amount of results in our database relevant to your request_ | |
| `items_count` | integer | _the number of results returned in the `items` array_ | |
| `items` | array | _contains keywords and related data_ | |
| `se_type` | string | _search engine type_ | |
| `keyword_data` | object | _keyword data for the returned keyword_ | |
| `se_type` | string | _search engine type_ | |
| `keyword` | string | _related keyword_ | |
| `location_code` | integer | _location code in a POST array_ | |
| `language_code` | string | _language code in a POST array_ | |
| `keyword_info` | object | _keyword data for the returned keyword_ | |
| `se_type` | string | _search engine type_ | |
| `last_updated_time` | string | _date and time when keyword data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `competition` | float | _competition_
represents the relative amount of competition associated with the given keyword;
the value is based on Google Ads data and can be between 0 and 1 (inclusive) | |
| `competition_level` | string | _competition level_
represents the relative level of competition associated with the given keyword in paid SERP only;
possible values: `LOW`, `MEDIUM`, `HIGH`
if competition level is unknown, the value is `null`;
learn more about the metric in [this help center article](https://dataforseo.com/help-center/what-is-competition) | |
| `cpc` | float | _cost-per-click_
represents the average cost per click (USD) historically paid for the keyword | |
| `search_volume` | integer | _average monthly search volume rate_
represents the (approximate) number of searches for the given keyword idea on google.com | |
| `low_top_of_page_bid` | float | _minimum bid for the ad to be displayed at the top of the first page_
indicates the value greater than about 20% of the lowest bids for which ads were displayed (based on Google Ads statistics for advertisers)
the value may differ depending on the location specified in a POST request | |
| `high_top_of_page_bid` | float | _maximum bid for the ad to be displayed at the top of the first page_
indicates the value greater than about 80% of the lowest bids for which ads were displayed (based on Google Ads statistics for advertisers)
the value may differ depending on the location specified in a POST request | |
| `categories` | array | _product and service categories_
you can download the [full list of possible categories](https://cdn.dataforseo.com/v3/categories/categories_dataforseo_labs_2023_10_25.csv) | |
| `monthly_searches` | array | _monthly searches_
represents the (approximate) number of searches on this keyword idea (as available for the past twelve months), targeted to the specified geographic locations | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _monthly average search volume rate_ | |
| `search_volume_trend` | object | _search volume trend changes_
represents search volume change in percent compared to the previous period | |
| `monthly` | integer | _search volume change in percent compared to the previous month_ | |
| `quarterly` | integer | _search volume change in percent compared to the previous quarter_ | |
| `yearly` | integer | _search volume change in percent compared to the previous year_ | |
| `clickstream_keyword_info` | object | _clickstream data for the returned keyword_
to retrieve results for this field, the parameter `include_clickstream_data` must be set to `true` | |
| `search_volume` | integer | _monthly average clickstream search volume rate_ | |
| `last_updated_time` | string | _date and time when the clickstream dataset was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00” | |
| `gender_distribution` | object | _distribution of estimated clickstream-based metrics by gender_
learn more about how the metric is calculated in this [help center article](https://dataforseo.com/help-center/what-are-clickstream-based-metrics-and-how-do-we-calculate-them) | |
| `female` | integer | _number of female users in the relevant clickstream dataset_ | |
| `male` | integer | _number of male users in the relevant clickstream dataset_ | |
| `age_distribution` | object | _distribution of clickstream-based metrics by age_
learn more about how the metric is calculated in this [help center article](https://dataforseo.com/help-center/what-are-clickstream-based-metrics-and-how-do-we-calculate-them) | |
| `18-24` | integer | _number of users in the relevant clickstream dataset that fall within the 18-24 age range_ | |
| `25-34` | integer | _number of users in the relevant clickstream dataset that fall within the 25-34 age range_ | |
| `35-44` | integer | _number of users in the relevant clickstream dataset that fall within the 35-44 age range_ | |
| `45-54` | integer | _number of users in the relevant clickstream dataset that fall within the 45-54 age range_ | |
| `55-64` | integer | _number of users in the relevant clickstream dataset that fall within the 55-64 age range_ | |
| `monthly_searches` | array | _monthly clickstream search volume rates_
array of objects with clickstream search volume rates in a certain month of a year | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _clickstream-based search volume rate in a certain month of a year_ | |
| `keyword_properties` | object | _additional information about the keyword_ | |
| `se_type` | string | _search engine type_ | |
| `core_keyword` | string | _main keyword in a group_
contains the main keyword in a group determined by the synonym clustering algorithm
if the value is `null`, our database does not contain any keywords the corresponding algorithm could identify as synonymous with `keyword` | |
| `synonym_clustering_algorithm` | string | _the algorithm used to identify synonyms_
possible values:
`keyword_metrics` – indicates the algorithm based on `keyword_info` parameters
`text_processing` – indicates the text-based algorithm
if the value is `null`, our database does not contain any keywords the corresponding algorithm could identify as synonymous with `keyword` | |
| `keyword_difficulty` | integer | _difficulty of ranking in the first top-10 organic results for a keyword_
indicates the chance of getting in top-10 organic results for a keyword on a logarithmic scale from 0 to 100;
calculated by analysing, among other parameters, link profiles of the first 10 pages in SERP;
learn more about the metric in [this help center guide](https://dataforseo.com/help-center/what-is-keyword-difficulty-and-how-is-it-calculated) | |
| `detected_language` | string | _detected language of the keyword_
indicates the language of the keyword as identified by our system | |
| `is_another_language` | boolean | _detected language of the keyword is different from the set language_
if `true`, the language set in the request does not match the language determined by our system for a given keyword | |
| `serp_info` | object | _SERP data_
the value will be `null` if you didn’t set the field `include_serp_info` to `true` in the POST array or if there is no SERP data for this keyword in our database | |
| `se_type` | string | _search engine type_ | |
| `check_url` | string | _direct URL to search engine results_
you can use it to make sure that we provided accurate results | |
| `serp_item_types` | array | _types of search results in SERP_
contains types of search results (items) found in SERP
possible item types:
`answer_box`, `app`, `carousel`, `multi_carousel`, `featured_snippet`, `google_flights`, `google_reviews`, `third_party_reviews`, `google_posts`, `images`, `jobs`, `knowledge_graph`, `local_pack`, `hotels_pack`, `map`, `organic`, `paid`, `people_also_ask`, `related_searches`, `people_also_search`, `shopping`, `top_stories`, `twitter`, `video`, `events`, `mention_carousel`, `recipes`, `top_sights`, `scholarly_articles`, `popular_products`, `podcasts`, `questions_and_answers`, `find_results_on`, `stocks_box`, `visual_stories`, `commercial_units`, `local_services`, `google_hotels`, `math_solver`, `currency_box`, `product_considerations`, `found_on_web`, `short_videos`, `refine_products`, `explore_brands`, `perspectives`, `discussions_and_forums`, `compare_sites`, `courses`, `ai_overview`;
**note** that the actual results will be returned only for `organic`, `paid`, `featured_snippet`, and `local_pack` elements | |
| `se_results_count` | integer | _number of search results for the returned keyword_ | |
| `last_updated_time` | string | _date and time when SERP data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `previous_updated_time` | string | _previous to the most recent date and time when SERP data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-10-15 12:57:46 +00:00` | |
| `avg_backlinks_info` | object | _backlink data for the returned keyword_
this object provides the average number of backlinks, referring pages and domains, as well as the average rank values among the top-10 webpages ranking organically for the keyword | |
| `se_type` | string | _search engine type_ | |
| `backlinks` | float | _average number of backlinks_ | |
| `dofollow` | float | _average number of dofollow links_ | |
| `referring_pages` | float | _average number of referring pages_ | |
| `referring_domains` | float | _average number of referring domains_ | |
| `referring_main_domains` | float | _average number of referring main domains_ | |
| `rank` | float | _average rank_
learn more about the metric and its calculation formula in [this help center article](https://dataforseo.com/help-center/what_is_rank_in_backlinks_api) | |
| `main_domain_rank` | float | _average main domain rank_
learn more about the metric and its calculation formula in [this help center article](https://dataforseo.com/help-center/what_is_rank_in_backlinks_api) | |
| `last_updated_time` | string | _date and time when backlink data was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `search_intent_info` | object | _search intent info for the returned keyword_
learn about search intent in this [help center article](https://dataforseo.com/help-center/search-intent-and-its-types) | |
| `se_type` | string | _search engine type_
possible values: `google` | |
| `main_intent` | string | _main search intent_
possible values: `informational`, `navigational`, `commercial`, `transactional` | |
| `foreign_intent` | array | _supplementary search intents_
possible values: `informational`, `navigational`, `commercial`, `transactional` | |
| `last_updated_time` | string | _date and time when search intent data was last updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| **`keyword_info_normalized_with_bing`** | object | _contains keyword search volume normalized with Bing search volume_ | |
| `last_updated_time` | string | _date and time when the dataset was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `search_volume` | integer | _current search volume rate of a keyword_ | |
| `is_normalized` | boolean | _keyword info is normalized_
if `true`, values are normalized with Bing data | |
| `monthly_searches` | integer | _monthly search volume rates_
array of objects with search volume rates in a certain month of a year | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _search volume rate in a certain month of a year_ | |
| **`keyword_info_normalized_with_clickstream`** | object | _contains keyword search volume normalized with clickstream data_ | |
| `last_updated_time` | string | _date and time when the dataset was updated_
in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”
example:
`2019-11-15 12:57:46 +00:00` | |
| `search_volume` | integer | _current search volume rate of a keyword_ | |
| `is_normalized` | boolean | _keyword info is normalized_
if `true`, values are normalized with clickstream data | |
| `monthly_searches` | integer | _monthly search volume rates_
array of objects with search volume rates in a certain month of a year | |
| `year` | integer | _year_ | |
| `month` | integer | _month_ | |
| `search_volume` | integer | _search volume rate in a certain month of a year_ | |
| `depth` | integer | _keyword search depth_ | |
| `related_keywords` | array | _list of related keywords_
represents the list of search queries which are related to the keyword returned in the array above | |
SERP TYPES:
| `serp_item_types` | array | _types of search results in SERP_
contains types of search results (items) found in SERP
possible item types:
`answer_box`, `app`, `carousel`, `multi_carousel`, `featured_snippet`, `google_flights`, `google_reviews`, `third_party_reviews`, `google_posts`, `images`, `jobs`, `knowledge_graph`, `local_pack`, `hotels_pack`, `map`, `organic`, `paid`, `people_also_ask`, `related_searches`, `people_also_search`, `shopping`, `top_stories`, `twitter`, `video`, `events`, `mention_carousel`, `recipes`, `top_sights`, `scholarly_articles`, `popular_products`, `podcasts`, `questions_and_answers`, `find_results_on`, `stocks_box`, `visual_stories`, `commercial_units`, `local_services`, `google_hotels`, `math_solver`, `currency_box`, `product_considerations`, `found_on_web`, `short_videos`, `refine_products`, `explore_brands`, `perspectives`, `discussions_and_forums`, `compare_sites`, `courses`, `ai_overview`


[Tap into Google’s AI Mode with SERP API >>](https://dataforseo.com/apis/serp-api/google-ai-mode-serp-api)

[Tap into Google’s AI Mode with SERP API >>](https://dataforseo.com/apis/serp-api/google-ai-mode-serp-api)

- [API Docs](https://docs.dataforseo.com/v3)
- [Free Tools](https://dataforseo.com/free-seo-stats)
  - [Top 1000 Websites By Ranking Keywords](https://dataforseo.com/free-seo-stats/top-1000-websites)
  - [Top 1000 Keywords](https://dataforseo.com/free-seo-stats/top-1000-keywords)
  - [Top 1000 Websites By Domain Rank](https://dataforseo.com/free-seo-stats/top-1000-websites-by-domain-rank)
  - [Top 1000 Websites By Traffic Trends](https://dataforseo.com/free-seo-stats/top-1000-websites-by-traffic-trends)
  - [SERP Volatility Index](https://dataforseo.com/free-seo-stats/serp-volatility-index)
  - [DataForSEO Trends](https://trends.dataforseo.com/)
- [About us](https://dataforseo.com/help-center/using-filters#)
  - [Mission and vision](https://dataforseo.com/about-us)
  - [Our data](https://dataforseo.com/our-data)
  - [Ukrainian Bravery](https://dataforseo.com/bravery)
  - [Status Page](https://status.dataforseo.com/)
- [AI Assistant](https://chat.dataforseo.com/)
- [Contact us](https://dataforseo.com/contact)
- [Login](https://app.dataforseo.com/signin)

[![logo](https://dataforseo.com/wp-content/uploads/2022/04/ukr-logo_429.png)![dark logo](https://dataforseo.com/wp-content/uploads/2022/04/ukr-logo_429.png)![light logo](https://dataforseo.com/wp-content/uploads/2022/04/ukr-logo-light_429.png)](https://dataforseo.com/)

- [Products](https://dataforseo.com/help-center/using-filters#)




- [APIs](https://dataforseo.com/apis)
  - [SERP API](https://dataforseo.com/apis/serp-api)
  - [Keywords Data APIs](https://dataforseo.com/apis/keyword-data-api)
    - [Google Ads API](https://dataforseo.com/apis/google-ads-api)
    - [Google Trends API](https://dataforseo.com/apis/google-trends-api)
    - [Bing Ads API](https://dataforseo.com/apis/bing-ads-api)
    - [Clickstream Data API](https://dataforseo.com/apis/clickstream-data-api)
    - [DataForSEO Trends API](https://dataforseo.com/apis/dataforseo-trends-api)
  - [Backlinks API](https://dataforseo.com/apis/backlinks-api)
  - [On-Page API](https://dataforseo.com/apis/on-page-api)
  - [DataForSEO Labs API](https://dataforseo.com/apis/dataforseo-labs-api)
  - [AI Optimization Data API](https://dataforseo.com/apis/ai-optimization-api)
  - [Reviews API](https://dataforseo.com/apis/reviews-api)
    - [Google Reviews API](https://dataforseo.com/apis/reviews-api/google-reviews-api)
    - [Google Shopping Reviews API](https://dataforseo.com/apis/reviews-api/google-shopping-api)
    - [Trustpilot Reviews API](https://dataforseo.com/apis/reviews-api/trustpilot-reviews-api)
    - [Amazon Reviews API](https://dataforseo.com/apis/reviews-api/amazon-reviews-api)
    - [Tripadvisor Reviews API](https://dataforseo.com/apis/reviews-api/tripadvisor-reviews-api)
    - [Google Play Reviews API](https://dataforseo.com/apis/reviews-api/google-play-store-reviews-api)
    - [App Store Reviews API](https://dataforseo.com/apis/reviews-api/app-store-reviews-api)
  - [Social Media API](https://dataforseo.com/apis/social-media-api)
    - [Pinterest API](https://dataforseo.com/apis/social-media-api/pinterest-api)
    - [Facebook API](https://dataforseo.com/apis/social-media-api/facebook-api)
    - [Reddit API](https://dataforseo.com/apis/social-media-api/reddit-api)
  - [App Data API](https://dataforseo.com/apis/app-data-api)
    - [Google Play Store API](https://dataforseo.com/apis/app-data-api/google-play-store-api)
    - [App Store API](https://dataforseo.com/apis/app-data-api/app-store)
  - [Business Data API](https://dataforseo.com/apis/business-data-api)
    - [Google My Business API](https://dataforseo.com/apis/business-data-api/google-my-business)
    - [Google Hotels API](https://dataforseo.com/apis/business-data-api/google-hotels-api)
    - [Trustpilot API](https://dataforseo.com/apis/business-data-api/trustpilot-api)
    - [Tripadvisor API](https://dataforseo.com/apis/business-data-api/tripadvisor-api)
    - [Business Listings API](https://dataforseo.com/apis/business-data-api/business-listings-api)
  - [Merchant API](https://dataforseo.com/apis/merchant-api)
    - [Amazon](https://dataforseo.com/apis/merchant-api-amazon)
    - [Google Shopping](https://dataforseo.com/apis/merchant-api-google-shopping)
  - [Domain Analytics API](https://dataforseo.com/apis/domain-analytics-api)
  - [Content Analysis API](https://dataforseo.com/apis/content-analysis-api)
  - [Content Generation API](https://dataforseo.com/apis/content-generation-api)
- [Databases](https://dataforseo.com/databases)
  - [Google Databases](https://dataforseo.com/databases/google-databases)
  - [Historical Google Databases](https://dataforseo.com/databases/historical-google-databases)
  - [Bing Databases](https://dataforseo.com/databases/bing-databases)
  - [Amazon Database](https://dataforseo.com/databases/amazon-database)
  - [App Store Database](https://dataforseo.com/databases/app-store-database)
  - [Google Play Database](https://dataforseo.com/databases/google-play-database)
  - [Business Listings Database](https://dataforseo.com/databases/business-listings-database)
  - [Domains’ Whois & HTML Databases](https://dataforseo.com/databases/whois-database)
  - [Domains’ Backlink Stats Database](https://dataforseo.com/databases/backlink-database)

- [Integrations](https://dataforseo.com/integrations)




- [Automation Platforms](https://dataforseo.com/help-center/using-filters#)
  - [Google Sheets Connector](https://dataforseo.com/google-sheets-connector)
  - [n8n Integration](https://dataforseo.com/n8n-integration)
  - [Make Integration](https://dataforseo.com/make-integration)
  - [Airtable Integration](https://dataforseo.com/airtable-integration)
  - [Postman Integration](https://dataforseo.com/postman-integrations)
  - [Zapier Integration](https://dataforseo.com/zapier-integration)
  - [Pabbly integration](https://dataforseo.com/pabbly-integration)
  - [Lindy integration](https://dataforseo.com/lindy-ai-integration)
- [AI / LLM Agents](https://dataforseo.com/help-center/using-filters#)
  - [DataForSEO MCP](https://dataforseo.com/model-context-protocol)
  - [Custom GPT](https://dataforseo.com/custom-gpt)

- [Use Cases](https://dataforseo.com/solutions)




- By industry















[Media](https://dataforseo.com/solutions/media)



[Ecommerce](https://dataforseo.com/solutions/ecommerce-apis)



[Digital Marketing](https://dataforseo.com/solutions/digital-marketing-apis)



[Marketing Tech](https://dataforseo.com/solutions/marketing-tech-apis)



[AI Development](https://dataforseo.com/solutions/data-for-ai-training)























By company type















[SEO Agency](https://dataforseo.com/solutions/seo-agency)



[SEO Software](https://dataforseo.com/solutions/seo-software)



[Enterprise](https://dataforseo.com/solutions/enterprise)























By application















[Backlink App](https://dataforseo.com/solutions/api-driven-backlinksapp)



[Reputation Management](https://dataforseo.com/solutions/api-driven-reputation-management)



[Rank Tracking App](https://dataforseo.com/solutions/rank-tracking-app)



[Keyword Research App](https://dataforseo.com/solutions/keyword-research-app)



[Keyword Trends Tool](https://dataforseo.com/solutions/keyword-trends-tool)


- [Pricing](https://dataforseo.com/pricing)
- [Knowledge base](https://dataforseo.com/knowledgebase)




- [Blog](https://dataforseo.com/blog)
- [API Documentation](https://docs.dataforseo.com/v3)
- [Help Center](https://dataforseo.com/help-center)
- [Product updates](https://dataforseo.com/updates)
- [FAQ](https://dataforseo.com/faq)
- [SERP features](https://dataforseo.com/serp-features)
- [White papers](https://dataforseo.com/whitepapers)

- [Try for free](https://app.dataforseo.com/register)

[![logo](https://dataforseo.com/wp-content/uploads/2022/04/ukr-logo_429.png)![dark logo](https://dataforseo.com/wp-content/uploads/2022/04/ukr-logo_429.png)![light logo](https://dataforseo.com/wp-content/uploads/2022/04/ukr-logo-light_429.png)](https://dataforseo.com/)

- [Products](https://dataforseo.com/help-center/using-filters#)




- [APIs](https://dataforseo.com/apis)
  - [SERP API](https://dataforseo.com/apis/serp-api)
  - [Keywords Data APIs](https://dataforseo.com/apis/keyword-data-api)
    - [Google Ads API](https://dataforseo.com/apis/google-ads-api)
    - [Google Trends API](https://dataforseo.com/apis/google-trends-api)
    - [Bing Ads API](https://dataforseo.com/apis/bing-ads-api)
    - [Clickstream Data API](https://dataforseo.com/apis/clickstream-data-api)
    - [DataForSEO Trends API](https://dataforseo.com/apis/dataforseo-trends-api)
  - [Backlinks API](https://dataforseo.com/apis/backlinks-api)
  - [On-Page API](https://dataforseo.com/apis/on-page-api)
  - [DataForSEO Labs API](https://dataforseo.com/apis/dataforseo-labs-api)
  - [AI Optimization Data API](https://dataforseo.com/apis/ai-optimization-api)
  - [Reviews API](https://dataforseo.com/apis/reviews-api)
    - [Google Reviews API](https://dataforseo.com/apis/reviews-api/google-reviews-api)
    - [Google Shopping Reviews API](https://dataforseo.com/apis/reviews-api/google-shopping-api)
    - [Trustpilot Reviews API](https://dataforseo.com/apis/reviews-api/trustpilot-reviews-api)
    - [Amazon Reviews API](https://dataforseo.com/apis/reviews-api/amazon-reviews-api)
    - [Tripadvisor Reviews API](https://dataforseo.com/apis/reviews-api/tripadvisor-reviews-api)
    - [Google Play Reviews API](https://dataforseo.com/apis/reviews-api/google-play-store-reviews-api)
    - [App Store Reviews API](https://dataforseo.com/apis/reviews-api/app-store-reviews-api)
  - [Social Media API](https://dataforseo.com/apis/social-media-api)
    - [Pinterest API](https://dataforseo.com/apis/social-media-api/pinterest-api)
    - [Facebook API](https://dataforseo.com/apis/social-media-api/facebook-api)
    - [Reddit API](https://dataforseo.com/apis/social-media-api/reddit-api)
  - [App Data API](https://dataforseo.com/apis/app-data-api)
    - [Google Play Store API](https://dataforseo.com/apis/app-data-api/google-play-store-api)
    - [App Store API](https://dataforseo.com/apis/app-data-api/app-store)
  - [Business Data API](https://dataforseo.com/apis/business-data-api)
    - [Google My Business API](https://dataforseo.com/apis/business-data-api/google-my-business)
    - [Google Hotels API](https://dataforseo.com/apis/business-data-api/google-hotels-api)
    - [Trustpilot API](https://dataforseo.com/apis/business-data-api/trustpilot-api)
    - [Tripadvisor API](https://dataforseo.com/apis/business-data-api/tripadvisor-api)
    - [Business Listings API](https://dataforseo.com/apis/business-data-api/business-listings-api)
  - [Merchant API](https://dataforseo.com/apis/merchant-api)
    - [Amazon](https://dataforseo.com/apis/merchant-api-amazon)
    - [Google Shopping](https://dataforseo.com/apis/merchant-api-google-shopping)
  - [Domain Analytics API](https://dataforseo.com/apis/domain-analytics-api)
  - [Content Analysis API](https://dataforseo.com/apis/content-analysis-api)
  - [Content Generation API](https://dataforseo.com/apis/content-generation-api)
- [Databases](https://dataforseo.com/databases)
  - [Google Databases](https://dataforseo.com/databases/google-databases)
  - [Historical Google Databases](https://dataforseo.com/databases/historical-google-databases)
  - [Bing Databases](https://dataforseo.com/databases/bing-databases)
  - [Amazon Database](https://dataforseo.com/databases/amazon-database)
  - [App Store Database](https://dataforseo.com/databases/app-store-database)
  - [Google Play Database](https://dataforseo.com/databases/google-play-database)
  - [Business Listings Database](https://dataforseo.com/databases/business-listings-database)
  - [Domains’ Whois & HTML Databases](https://dataforseo.com/databases/whois-database)
  - [Domains’ Backlink Stats Database](https://dataforseo.com/databases/backlink-database)

- [Integrations](https://dataforseo.com/integrations)




- [Automation Platforms](https://dataforseo.com/help-center/using-filters#)
  - [Google Sheets Connector](https://dataforseo.com/google-sheets-connector)
  - [n8n Integration](https://dataforseo.com/n8n-integration)
  - [Make Integration](https://dataforseo.com/make-integration)
  - [Airtable Integration](https://dataforseo.com/airtable-integration)
  - [Postman Integration](https://dataforseo.com/postman-integrations)
  - [Zapier Integration](https://dataforseo.com/zapier-integration)
  - [Pabbly integration](https://dataforseo.com/pabbly-integration)
  - [Lindy integration](https://dataforseo.com/lindy-ai-integration)
- [AI / LLM Agents](https://dataforseo.com/help-center/using-filters#)
  - [DataForSEO MCP](https://dataforseo.com/model-context-protocol)
  - [Custom GPT](https://dataforseo.com/custom-gpt)

- [Use Cases](https://dataforseo.com/solutions)




- By industry















[Media](https://dataforseo.com/solutions/media)



[Ecommerce](https://dataforseo.com/solutions/ecommerce-apis)



[Digital Marketing](https://dataforseo.com/solutions/digital-marketing-apis)



[Marketing Tech](https://dataforseo.com/solutions/marketing-tech-apis)



[AI Development](https://dataforseo.com/solutions/data-for-ai-training)























By company type















[SEO Agency](https://dataforseo.com/solutions/seo-agency)



[SEO Software](https://dataforseo.com/solutions/seo-software)



[Enterprise](https://dataforseo.com/solutions/enterprise)























By application















[Backlink App](https://dataforseo.com/solutions/api-driven-backlinksapp)



[Reputation Management](https://dataforseo.com/solutions/api-driven-reputation-management)



[Rank Tracking App](https://dataforseo.com/solutions/rank-tracking-app)



[Keyword Research App](https://dataforseo.com/solutions/keyword-research-app)



[Keyword Trends Tool](https://dataforseo.com/solutions/keyword-trends-tool)


- [Pricing](https://dataforseo.com/pricing)
- [Knowledge base](https://dataforseo.com/knowledgebase)




- [Blog](https://dataforseo.com/blog)
- [API Documentation](https://docs.dataforseo.com/v3)
- [Help Center](https://dataforseo.com/help-center)
- [Product updates](https://dataforseo.com/updates)
- [FAQ](https://dataforseo.com/faq)
- [SERP features](https://dataforseo.com/serp-features)
- [White papers](https://dataforseo.com/whitepapers)

- [Try for free](https://app.dataforseo.com/register)

[![mobile-logo](https://dataforseo.com/wp-content/uploads/2022/04/ukr-logo_429.png)](https://dataforseo.com/)

- #### Products


  - [APIs](https://dataforseo.com/apis)
    - [SERP API](https://dataforseo.com/apis/serp-api)
    - [Keywords Data APIs](https://dataforseo.com/apis/keyword-data-api)
      - [Google Ads API](https://dataforseo.com/apis/google-ads-api)
      - [Google Trends API](https://dataforseo.com/apis/google-trends-api)
      - [Bing Ads API](https://dataforseo.com/apis/bing-ads-api)
      - [Clickstream Data API](https://dataforseo.com/apis/clickstream-data-api)
      - [DataForSEO Trends API](https://dataforseo.com/apis/dataforseo-trends-api)
    - [Backlinks API](https://dataforseo.com/apis/backlinks-api)
    - [On-Page API](https://dataforseo.com/apis/on-page-api)
    - [DataForSEO Labs API](https://dataforseo.com/apis/dataforseo-labs-api)
    - [AI Optimization Data API](https://dataforseo.com/apis/ai-optimization-api)
    - [Reviews API](https://dataforseo.com/apis/reviews-api)
      - [Google Reviews API](https://dataforseo.com/apis/reviews-api/google-reviews-api)
      - [Google Shopping Reviews API](https://dataforseo.com/apis/reviews-api/google-shopping-api)
      - [Trustpilot Reviews API](https://dataforseo.com/apis/reviews-api/trustpilot-reviews-api)
      - [Amazon Reviews API](https://dataforseo.com/apis/reviews-api/amazon-reviews-api)
      - [Tripadvisor Reviews API](https://dataforseo.com/apis/reviews-api/tripadvisor-reviews-api)
      - [Google Play Reviews API](https://dataforseo.com/apis/reviews-api/google-play-store-reviews-api)
      - [App Store Reviews API](https://dataforseo.com/apis/reviews-api/app-store-reviews-api)
    - [Social Media API](https://dataforseo.com/apis/social-media-api)
      - [Pinterest API](https://dataforseo.com/apis/social-media-api/pinterest-api)
      - [Facebook API](https://dataforseo.com/apis/social-media-api/facebook-api)
      - [Reddit API](https://dataforseo.com/apis/social-media-api/reddit-api)
    - [App Data API](https://dataforseo.com/apis/app-data-api)
      - [Google Play Store API](https://dataforseo.com/apis/app-data-api/google-play-store-api)
      - [App Store API](https://dataforseo.com/apis/app-data-api/app-store)
    - [Business Data API](https://dataforseo.com/apis/business-data-api)
      - [Google My Business API](https://dataforseo.com/apis/business-data-api/google-my-business)
      - [Google Hotels API](https://dataforseo.com/apis/business-data-api/google-hotels-api)
      - [Trustpilot API](https://dataforseo.com/apis/business-data-api/trustpilot-api)
      - [Tripadvisor API](https://dataforseo.com/apis/business-data-api/tripadvisor-api)
      - [Business Listings API](https://dataforseo.com/apis/business-data-api/business-listings-api)
    - [Merchant API](https://dataforseo.com/apis/merchant-api)
      - [Amazon](https://dataforseo.com/apis/merchant-api-amazon)
      - [Google Shopping](https://dataforseo.com/apis/merchant-api-google-shopping)
    - [Domain Analytics API](https://dataforseo.com/apis/domain-analytics-api)
    - [Content Analysis API](https://dataforseo.com/apis/content-analysis-api)
    - [Content Generation API](https://dataforseo.com/apis/content-generation-api)
  - [Databases](https://dataforseo.com/databases)
    - [Google Databases](https://dataforseo.com/databases/google-databases)
    - [Historical Google Databases](https://dataforseo.com/databases/historical-google-databases)
    - [Bing Databases](https://dataforseo.com/databases/bing-databases)
    - [Amazon Database](https://dataforseo.com/databases/amazon-database)
    - [App Store Database](https://dataforseo.com/databases/app-store-database)
    - [Google Play Database](https://dataforseo.com/databases/google-play-database)
    - [Business Listings Database](https://dataforseo.com/databases/business-listings-database)
    - [Domains’ Whois & HTML Databases](https://dataforseo.com/databases/whois-database)
    - [Domains’ Backlink Stats Database](https://dataforseo.com/databases/backlink-database)
- [Integrations](https://dataforseo.com/integrations)
  - #### Automation Platforms


    - [Google Sheets Connector](https://dataforseo.com/google-sheets-connector)
    - [n8n Integration](https://dataforseo.com/n8n-integration)
    - [Make Integration](https://dataforseo.com/make-integration)
    - [Airtable Integration](https://dataforseo.com/airtable-integration)
    - [Postman Integration](https://dataforseo.com/postman-integrations)
    - [Zapier Integration](https://dataforseo.com/zapier-integration)
    - [Pabbly integration](https://dataforseo.com/pabbly-integration)
    - [Lindy integration](https://dataforseo.com/lindy-ai-integration)
  - #### AI / LLM Agents


    - [DataForSEO MCP](https://dataforseo.com/model-context-protocol)
    - [Custom GPT](https://dataforseo.com/custom-gpt)
- [Use Cases](https://dataforseo.com/solutions)
  - By industry















    [Media](https://dataforseo.com/solutions/media)



    [Ecommerce](https://dataforseo.com/solutions/ecommerce-apis)



    [Digital Marketing](https://dataforseo.com/solutions/digital-marketing-apis)



    [Marketing Tech](https://dataforseo.com/solutions/marketing-tech-apis)



    [AI Development](https://dataforseo.com/solutions/data-for-ai-training)























    By company type















    [SEO Agency](https://dataforseo.com/solutions/seo-agency)



    [SEO Software](https://dataforseo.com/solutions/seo-software)



    [Enterprise](https://dataforseo.com/solutions/enterprise)























    By application















    [Backlink App](https://dataforseo.com/solutions/api-driven-backlinksapp)



    [Reputation Management](https://dataforseo.com/solutions/api-driven-reputation-management)



    [Rank Tracking App](https://dataforseo.com/solutions/rank-tracking-app)



    [Keyword Research App](https://dataforseo.com/solutions/keyword-research-app)



    [Keyword Trends Tool](https://dataforseo.com/solutions/keyword-trends-tool)
- [Pricing](https://dataforseo.com/pricing)
- [Knowledge base](https://dataforseo.com/knowledgebase)
  - [Blog](https://dataforseo.com/blog)
  - [API Documentation](https://docs.dataforseo.com/v3)
  - [Help Center](https://dataforseo.com/help-center)
  - [Product updates](https://dataforseo.com/updates)
  - [FAQ](https://dataforseo.com/faq)
  - [SERP features](https://dataforseo.com/serp-features)
  - [White papers](https://dataforseo.com/whitepapers)
- [Try for free](https://app.dataforseo.com/register)

- [API Docs](https://docs.dataforseo.com/v3)
- [Free Tools](https://dataforseo.com/free-seo-stats)
  - [Top 1000 Websites By Ranking Keywords](https://dataforseo.com/free-seo-stats/top-1000-websites)
  - [Top 1000 Keywords](https://dataforseo.com/free-seo-stats/top-1000-keywords)
  - [Top 1000 Websites By Domain Rank](https://dataforseo.com/free-seo-stats/top-1000-websites-by-domain-rank)
  - [Top 1000 Websites By Traffic Trends](https://dataforseo.com/free-seo-stats/top-1000-websites-by-traffic-trends)
  - [SERP Volatility Index](https://dataforseo.com/free-seo-stats/serp-volatility-index)
  - [DataForSEO Trends](https://trends.dataforseo.com/)
- #### About us


  - [Mission and vision](https://dataforseo.com/about-us)
  - [Our data](https://dataforseo.com/our-data)
  - [Ukrainian Bravery](https://dataforseo.com/bravery)
  - [Status Page](https://status.dataforseo.com/)
- [AI Assistant](https://chat.dataforseo.com/)
- [Contact us](https://dataforseo.com/contact)
- [Login](https://app.dataforseo.com/signin)

Help Center


✕


Search for:

- [Getting Started](https://dataforseo.com/help-center/category/getting-started)
- [Account & Billing](https://dataforseo.com/help-center/category/account-and-billing)
- [Databases](https://dataforseo.com/help-center/category/databases)
- [SERP API](https://dataforseo.com/help-center/category/serp-api)
- [Keyword Data API](https://dataforseo.com/help-center/category/keyword-data-api)
- [Domain Analytics API](https://dataforseo.com/help-center/category/domain-analytics-api)
- [On-Page API](https://dataforseo.com/help-center/category/onpage-api)
- [DataForSEO Labs API](https://dataforseo.com/help-center/category/dataforseo-labs-api)
- [Business Data API](https://dataforseo.com/help-center/category/business-data-api)
- [Merchant API](https://dataforseo.com/help-center/category/merchant-api)
- [Backlinks API](https://dataforseo.com/help-center/category/backlinks-api)
- [App Data API](https://dataforseo.com/help-center/category/app-data-api)
- [Content Analysis API](https://dataforseo.com/help-center/category/content-analysis-api)
- [Content Generation API](https://dataforseo.com/help-center/category/content-generation-api)
- [Third-Party Integrations](https://dataforseo.com/help-center/category/third-party-integrations)
- [AI Optimization API](https://dataforseo.com/help-center/category/ai-optimization-api)

HELP CENTER

Search for:

[Home](https://dataforseo.com/help-center) / [Backlinks API](https://dataforseo.com/help-center/category/backlinks-api)

HELP CENTER

- [Getting Started](https://dataforseo.com/help-center/category/getting-started)
- [Account & Billing](https://dataforseo.com/help-center/category/account-and-billing)
- [Databases](https://dataforseo.com/help-center/category/databases)
- [SERP API](https://dataforseo.com/help-center/category/serp-api)
- [Keyword Data API](https://dataforseo.com/help-center/category/keyword-data-api)
- [Domain Analytics API](https://dataforseo.com/help-center/category/domain-analytics-api)
- [On-Page API](https://dataforseo.com/help-center/category/onpage-api)
- [DataForSEO Labs API](https://dataforseo.com/help-center/category/dataforseo-labs-api)
- [Business Data API](https://dataforseo.com/help-center/category/business-data-api)
- [Merchant API](https://dataforseo.com/help-center/category/merchant-api)
- [Backlinks API](https://dataforseo.com/help-center/category/backlinks-api)
- [App Data API](https://dataforseo.com/help-center/category/app-data-api)
- [Content Analysis API](https://dataforseo.com/help-center/category/content-analysis-api)
- [Content Generation API](https://dataforseo.com/help-center/category/content-generation-api)
- [Third-Party Integrations](https://dataforseo.com/help-center/category/third-party-integrations)
- [AI Optimization API](https://dataforseo.com/help-center/category/ai-optimization-api)

**Subscribe To Our Newsletter**

Name **\***

Email **\***

Subscribe

# Using filters in DataForSEO APIs

in

With DataForSEO APIs, you can get very specific in your research, competitor analysis, and market study. By using the filters available for our APIs, you will always get the most relevant data in the response, which will enable you to reduce the number of requests and save your budget accordingly.

You can use filters when making a request to the following APIs:

[**DataForSEO Labs API**](https://dataforseo.com/help-center/using-filters#dataforseo_labs)

[**OnPage API**](https://dataforseo.com/help-center/using-filters#on_page)

[**Backlinks API**](https://dataforseo.com/help-center/using-filters#backlinks)

[**Content Analysis API**](https://dataforseo.com/help-center/using-filters#content_analysis)

**_You can set a maximum of eight filters in a single API request._**

Filters are associated with certain parameters from the API response. Each endpoint provides different data, and thus the filter structure and values differ depending on the endpoint.

Generally the structure of the filter includes three parts that are separated with a `,` symbol:

- filtered parameter – target parameter of the endpoint you want to filter;
- filter operator – operator of the filter;
- filter value – filtering value.

You can find the exact filter structure in the description of each DataForSEO API below.

In addition, filters support different data types. If you look at the list of supported filters, you will find the supported data type after the colon in each filter.

For example, `"keyword_info.search_volume": "num"` where `num` is the supported data type.

Each data type, in turn, supports different filter operators.

For example, for the `["domain_rank": "num"]` we can use these operators `<`, `<=`, `>`, `>=`, `=`, `<>`, `in`, `not_in`.

### How to use operators?

1When you use the `in` or **`not_in`** operator, the filter value should be specified as an array.

_Example:_`["keyword_info.search_volume","in",[10,1000]]`

2The **`like`** and **`not_like`** operators require adding the `%` symbol to get accurate results.

_Example:_

The `["keyword_data.keyword", "like", "%seo%"]` filter returns `keyword_data` items that contain “seo” anywhere in the keyword string and the `["keyword_data.keyword", "not_like", "%seo%"]` filter returns `keyword_data` items that don’t contain “seo” anywhere in the keyword string.

3The **`match`** and **`not_match`** are full-text search operators that only work with `string` values.

_Example:_

The `["keyword", "match", "phone"]` will return data for keywords that contain the “phone” word anywhere in the string.

4As for the **`has`** and `has_not` operators, they can be used only for `array.str` and `array.num`.

For example, the `serp_item_types` parameter in the [Keyword Suggestions endpoint](https://docs.dataforseo.com/v3/dataforseo_labs/google/keyword_suggestions/live/?bash) is the array of strings that displays SERP features available for the specified keyword (`featured_snippet`, `answer_box`, `knowledge_graph`, and others).

By applying a filter with the `has` or `has_not` operator to this parameter, you can receive the keywords for which a certain element is present or not present in SERP.

_Example:_

`["serp_info.serp_item_types", "has", "featured_snippet"]`

In this case, the API response will return only the keywords that have featured snippets in SERP.

And after the filter operator comes the filtering value, which may take the form of a string, array string, number, or boolean.

_**To not get confused with filter structure, we recommend calling the Available Filters endpoint and checking the examples.**_

### Using Regex Operator

The `regex` operator is supported in DataForSEO Labs API, OnPage API, Backlinks API, Content Analysis API and works only with `string` values.

**Note:** the maximum number of symbols you can specify in `regex` and `not_regex` is 1000;

**`.`** – Matches any character. For example:

`ab.` # matches ‘aba’, ‘abb’, ‘abz’, etc.

**`?`** – Repeat the preceding character zero or one times. Often used to make the preceding character optional. For example:

`abc?` # matches ‘ab’ and ‘abc’

**`+`** – Repeat the preceding character one or more times. For example:

`ab+` # matches ‘ab’, ‘abb’, ‘abbb’, etc.

**`*`** – Repeat the preceding character zero or more times. For example:

`ab*` # matches ‘a’, ‘ab’, ‘abb’, ‘abbb’, etc.

**`{}`** – Minimum and maximum number of times the preceding character can repeat. For example:

`a{2}` # matches ‘aa’

`a{2,4}` # matches ‘aa’, ‘aaa’, and ‘aaaa’

`a{2,}` # matches ‘a\` repeated two or more times

**`|`** – OR operator. The match will succeed if the longest pattern on either the left side OR the right side matches. For example:

`abc|xyz` # matches ‘abc’ and ‘xyz’

**`( … )`** – Forms a group. You can use a group to treat part of the expression as a single character. For example:

abc(def)? # matches ‘abc’ and ‘abcdef’ but not ‘abcd’

**`[ … ]`** – Match one of the characters in the brackets. For example:

`[abc]` # matches ‘a’, ‘b’, ‘c’

Inside the brackets, `-` indicates a range unless – is the first character or escaped. For example:

`[a-c]` # matches ‘a’, ‘b’, or ‘c’

`[-abc]` # ‘-‘ is first character. Matches ‘-‘, ‘a’, ‘b’, or ‘c’

`[abc\-]` # Escapes ‘-‘. Matches ‘a’, ‘b’, ‘c’, or ‘-‘

A `^` before a character in the brackets negates the character or range. For example:

`[^abc]` # matches any character except ‘a’, ‘b’, or ‘c’

`[^a-c]` # matches any character except ‘a’, ‘b’, or ‘c’

`[^-abc]` # matches any character except ‘-‘, ‘a’, ‘b’, or ‘c’

`[^abc\-]` # matches any character except ‘a’, ‘b’, ‘c’, or ‘-‘

**Note:** backslash symbols (`"\"`) in DataForSEO Regex can have different interpretations:

- `\b` is interpreted as a backspace symbol (not the same as in standard regex);
- `\\b` asserts that the current position in the string is a word boundary (same as `"\b"` in standard regex);
- `\\\\b` means 2 separate signs: `"\"` and `"b"` (for example, if you need to find `"a\b"` – you’d have to pass it as `"\\\\b"`;

If you specify `"\\"` without the expected symbol after it (for example `\\c` instead of `\\\\` or `\\c`), API will return a 4xx error.

APIs with filtration parameters also support the negative regex operator called `not_regex`, which allows for the exclusion of a certain string from API results.

_Example:_

Return string that contains “how”, “what”, or “when” keywords: `["keyword_data.keyword", "regex", "(how|what|when)"]`

Return string that does not contain “how”, “what”, or “when” keywords: `["keyword_data.keyword", "not_regex", "(how|what|when)"]`

You can test parameters with regex operator [via this link](https://regex101.com/).

### DataForSEO Labs API

You can get the full list of filters available for all DataForSEO Labs endpoints by calling the following endpoint:

`GET https://api.dataforseo.com/v3/dataforseo_labs/available_filters`

DataForSEO Labs API filters have the following structure:

`[$item_array.$results_array.$parameter_field, $filter_operator, $filter_value]`

`.` and `,` symbols are used as separators.

_Example:_

`["keyword_data.keyword_info.search_volume", "=", 1000]` where `keyword_data` is the `$item_array`, `keyword_info` is the `$results_array`, `search_volume` is the `$parameter_field`, `=` is the `$filter_operator`, and `1000` is the `$filter_value`.

DataForSEO Labs API `$item_array` and `$results_array` can be omitted depending on the endpoint.

_Example:_

The `[”depth”, “>”, 500]` filter in the [Related Keywords](https://docs.dataforseo.com/v3/dataforseo_labs/google/related_keywords/live/?bash) endpoint.

In this case, `depth` is both `$item_array` and `$parameter_field`.

Page Intersection, Ranked Keywords, Subdomains, Relevant Pages, Competitors Domain, Categories For Domain, Domain Intersection endpoints also support an alternative structure:

**`[$item_array.$results_array.$parameter_field, $filter_operator, $item->$item_array.$results_array.$parameter_field]`**

If you use this structure, you need to attach `$item->` to the right part of the condition.

**Note:** the `$parameter_field` variables in the right part and in the left part of the condition should have identical type: `bool`, `num`, `str`, `time`.

_Example:_

`["metrics.organic.pos_1", ">", "$item->metrics.organic.pos_2_3"]`

DataForSEO Labs API supports the following types and operators:

- `bool` supports `=`, and `<>`;
- `num` supports `<`, `<=`, `>`, `>=`, `=`, `<>`, `in`, and `not_in`;
- `str` supports `regex`, `match`, `not_match`, `like`, `not_like`, `ilike`, `not_ilike`, `in`, `not_in`, `=`, and `<>`;
- `array.str` supports `has`, and `has_not`;
- `array.num` supports `has`, and `has_not`;
- `time` supports `<` , `>` (time should be specified in the “yyyy-mm-dd hh-mm-ss +00:00” format. Example: `2021-01-29 15:02:37 +00:00)`

For example, for the `["keyword_info.search_volume": "num"]` filter, we can use one of the following operators: `<`, `<=`, `>`, `>=`, `=`, `<>`, `in`, `not_in`.

_**Example request:**_

Suppose you’re performing keyword research and want to find keyword suggestions for the _buy lava lamp_ keyword. Your site is new, so you know that high search volume keywords aren’t for you — you want to focus on less popular terms instead. To find them, all you have to do is send a request to the Keyword Suggestions endpoint with the following filter: `"keyword_info.search_volume": "num"`.

Let’s find search terms with a search volume of less than 500.

The filter will be structured as in the following example:

`["keyword_info.search_volume", "<", 500]`

Your POST request should be structured as in the example below:

``

```
[\
    {\
    	"keyword": "buy lava lamp",\
        "language_name": "English",\
        "location_code": 2840,\
        "filters": [\
	        ["keyword_info.search_volume", "<", 500]\
	    ],\
        "limit": 10\
    }\
]
```

In the API response, you will receive keyword suggestions with current search volumes of less than 500.

``

```
{
    "version": "0.1.20220216",
    "status_code": 20000,
    "status_message": "Ok.",
    "time": "1.2705 sec.",
    "cost": 0.011,
    "tasks_count": 1,
    "tasks_error": 0,
    "tasks": [\
        {\
            "id": "03211218-2806-0399-0000-23f54f988271",\
            "status_code": 20000,\
            "status_message": "Ok.",\
            "time": "1.2158 sec.",\
            "cost": 0.011,\
            "result_count": 1,\
            "path": [\
                "v3",\
                "dataforseo_labs",\
                "google",\
                "keyword_suggestions",\
                "live"\
            ],\
            "data": {\
                "api": "dataforseo_labs",\
                "function": "keyword_suggestions",\
                "se_type": "google",\
                "keyword": "buy lava lamp",\
                "language_name": "English",\
                "location_code": 2840,\
                "filters": [\
                    [\
                        "keyword_info.search_volume",\
                        "<",\
                        500\
                    ]\
                ],\
                "limit": 10\
            },\
            "result": [\
                {\
                    "se_type": "google",\
                    "seed_keyword": "buy lava lamp",\
                    "seed_keyword_data": null,\
                    "location_code": 2840,\
                    "language_code": "en",\
                    "total_count": 33,\
                    "items_count": 10,\
                    "offset": 0,\
                    "offset_token": "eyJDdXJyZW50T2Zmc2V0IjoxMCwiUmVxdWVzdERhdGEiOnsia2V5d29yZCI6ImJ1eSBsYXZhIGxhbXAiLCJpbmNsdWRlX3NlZWRfa2V5d29yZCI6ZmFsc2UsImZ1bGxfbWF0Y2giOmZhbHNlLCJsb2FkX3NlcnBfaW5mbyI6ZmFsc2UsInNlYXJjaF9hZnRlcl90b2tlbiI6bnVsbCwibGFuZ3VhZ2UiOiJlbiIsInNlYXJjaF9lbmdpbmUiOiJnb29nbGUiLCJsb2NhdGlvbiI6Mjg0MCwib3JkZXJfYnkiOnsib3JkZXJfZmllbGQiOiJrZXl3b3JkX2luZm8uc2VhcmNoX3ZvbHVtZSIsIm9yZGVyX3R5cGUiOiJEZXNjIiwibmV4dCI6bnVsbH0sImxpbWl0IjoxMCwib2Zmc2V0IjowLCJUb2tlbiI6eyJJc0NhbmNlbGxhdGlvblJlcXVlc3RlZCI6ZmFsc2UsIkNhbkJlQ2FuY2VsZWQiOnRydWUsIldhaXRIYW5kbGUiOnsiSGFuZGxlIjp7InZhbHVlIjoyODY4fSwiU2FmZVdhaXRIYW5kbGUiOnsiSXNJbnZhbGlkIjpmYWxzZSwiSXNDbG9zZWQiOmZhbHNlfX19fSwiUmF3UXVlcnkiOnsiZmllbGQiOiJrZXl3b3JkX2luZm8uc2VhcmNoX3ZvbHVtZSIsInR5cGUiOiJsdCIsInZhbHVlIjo1MDB9LCJTZWFyY2hBZnRlckRhdGEiOls0MCwiNjNmOWY4ZTQtYmYxMi0wNTBmLTA5NDUtZjA2ZTdjN2RmMTgyIl19",\
                    "items": [\
                        {\
                            "se_type": "google",\
                            "keyword": "lava lamp buy",\
                            "location_code": 2840,\
                            "language_code": "en",\
                            "keyword_info": {\
                                "se_type": "google",\
                                "last_updated_time": "2022-03-04 01:54:48 +00:00",\
                                "competition": 1,\
                                "cpc": 0.404441,\
                                "search_volume": 320,\
                                "categories": [\
                                    10009,\
                                    10010,\
                                    10081,\
                                    10419,\
                                    11587,\
                                    11588,\
                                    12820\
                                ],\
                                "monthly_searches": [\
                                    {\
                                        "year": 2022,\
                                        "month": 1,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 12,\
                                        "search_volume": 880\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 11,\
                                        "search_volume": 480\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 10,\
                                        "search_volume": 320\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 9,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 8,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 7,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 6,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 5,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 4,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 3,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 2,\
                                        "search_volume": 260\
                                    }\
                                ]\
                            },\
                            "keyword_properties": {\
                                "se_type": "google",\
                                "core_keyword": "buy lava lamp",\
                                "keyword_difficulty": 49\
                            },\
                            "serp_info": null\
                        },\
                        {\
                            "se_type": "google",\
                            "keyword": "buy lava lamp",\
                            "location_code": 2840,\
                            "language_code": "en",\
                            "keyword_info": {\
                                "se_type": "google",\
                                "last_updated_time": "2022-02-11 04:39:55 +00:00",\
                                "competition": 1,\
                                "cpc": 0.404441,\
                                "search_volume": 320,\
                                "categories": [\
                                    10009,\
                                    10010,\
                                    10081,\
                                    10419,\
                                    11587,\
                                    11588,\
                                    12820\
                                ],\
                                "monthly_searches": [\
                                    {\
                                        "year": 2022,\
                                        "month": 1,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 12,\
                                        "search_volume": 880\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 11,\
                                        "search_volume": 480\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 10,\
                                        "search_volume": 320\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 9,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 8,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 7,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 6,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 5,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 4,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 3,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 2,\
                                        "search_volume": 260\
                                    }\
                                ]\
                            },\
                            "keyword_properties": {\
                                "se_type": "google",\
                                "core_keyword": null,\
                                "keyword_difficulty": 52\
                            },\
                            "serp_info": null\
                        },\
                        {\
                            "se_type": "google",\
                            "keyword": "lava lamp where to buy",\
                            "location_code": 2840,\
                            "language_code": "en",\
                            "keyword_info": {\
                                "se_type": "google",\
                                "last_updated_time": "2022-02-13 08:44:03 +00:00",\
                                "competition": 1,\
                                "cpc": 0.404441,\
                                "search_volume": 320,\
                                "categories": [\
                                    10009,\
                                    10010,\
                                    10081,\
                                    10419,\
                                    11587,\
                                    11588,\
                                    12820\
                                ],\
                                "monthly_searches": [\
                                    {\
                                        "year": 2022,\
                                        "month": 1,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 12,\
                                        "search_volume": 880\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 11,\
                                        "search_volume": 480\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 10,\
                                        "search_volume": 320\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 9,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 8,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 7,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 6,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 5,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 4,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 3,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 2,\
                                        "search_volume": 260\
                                    }\
                                ]\
                            },\
                            "keyword_properties": {\
                                "se_type": "google",\
                                "core_keyword": "buy lava lamp",\
                                "keyword_difficulty": 52\
                            },\
                            "serp_info": null\
                        },\
                        {\
                            "se_type": "google",\
                            "keyword": "where to buy a lava lamp",\
                            "location_code": 2840,\
                            "language_code": "en",\
                            "keyword_info": {\
                                "se_type": "google",\
                                "last_updated_time": "2022-03-19 12:43:13 +00:00",\
                                "competition": 1,\
                                "cpc": 0.358656,\
                                "search_volume": 320,\
                                "categories": [\
                                    10009,\
                                    10010,\
                                    10081,\
                                    10419,\
                                    11587,\
                                    11588,\
                                    12820\
                                ],\
                                "monthly_searches": [\
                                    {\
                                        "year": 2022,\
                                        "month": 2,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2022,\
                                        "month": 1,\
                                        "search_volume": 390\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 12,\
                                        "search_volume": 1300\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 11,\
                                        "search_volume": 590\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 10,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 9,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 8,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 7,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 6,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 5,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 4,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 3,\
                                        "search_volume": 170\
                                    }\
                                ]\
                            },\
                            "keyword_properties": {\
                                "se_type": "google",\
                                "core_keyword": "where to buy lava lamp",\
                                "keyword_difficulty": 52\
                            },\
                            "serp_info": null\
                        },\
                        {\
                            "se_type": "google",\
                            "keyword": "where to buy lava lamp",\
                            "location_code": 2840,\
                            "language_code": "en",\
                            "keyword_info": {\
                                "se_type": "google",\
                                "last_updated_time": "2022-02-14 09:40:38 +00:00",\
                                "competition": 1,\
                                "cpc": 0.329418,\
                                "search_volume": 320,\
                                "categories": [\
                                    10009,\
                                    10010,\
                                    10081,\
                                    10419,\
                                    11587,\
                                    11588,\
                                    12820\
                                ],\
                                "monthly_searches": [\
                                    {\
                                        "year": 2022,\
                                        "month": 1,\
                                        "search_volume": 390\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 12,\
                                        "search_volume": 1300\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 11,\
                                        "search_volume": 590\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 10,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 9,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 8,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 7,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 6,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 5,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 4,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 3,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 2,\
                                        "search_volume": 210\
                                    }\
                                ]\
                            },\
                            "keyword_properties": {\
                                "se_type": "google",\
                                "core_keyword": "where to buy a lava lamp",\
                                "keyword_difficulty": 52\
                            },\
                            "serp_info": null\
                        },\
                        {\
                            "se_type": "google",\
                            "keyword": "where can i buy a lava lamp",\
                            "location_code": 2840,\
                            "language_code": "en",\
                            "keyword_info": {\
                                "se_type": "google",\
                                "last_updated_time": "2022-03-17 07:23:19 +00:00",\
                                "competition": 1,\
                                "cpc": 0.390699,\
                                "search_volume": 260,\
                                "categories": [\
                                    10009,\
                                    10405,\
                                    10419,\
                                    11587,\
                                    12820\
                                ],\
                                "monthly_searches": [\
                                    {\
                                        "year": 2022,\
                                        "month": 2,\
                                        "search_volume": 320\
                                    },\
                                    {\
                                        "year": 2022,\
                                        "month": 1,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 12,\
                                        "search_volume": 880\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 11,\
                                        "search_volume": 320\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 10,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 9,\
                                        "search_volume": 140\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 8,\
                                        "search_volume": 140\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 7,\
                                        "search_volume": 140\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 6,\
                                        "search_volume": 110\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 5,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 4,\
                                        "search_volume": 140\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 3,\
                                        "search_volume": 170\
                                    }\
                                ]\
                            },\
                            "keyword_properties": {\
                                "se_type": "google",\
                                "core_keyword": null,\
                                "keyword_difficulty": 52\
                            },\
                            "serp_info": null\
                        },\
                        {\
                            "se_type": "google",\
                            "keyword": "where can i buy lava lamp",\
                            "location_code": 2840,\
                            "language_code": "en",\
                            "keyword_info": {\
                                "se_type": "google",\
                                "last_updated_time": "2022-02-10 20:22:33 +00:00",\
                                "competition": 1,\
                                "cpc": 0.444673,\
                                "search_volume": 260,\
                                "categories": [\
                                    10005,\
                                    10011,\
                                    10089,\
                                    11772\
                                ],\
                                "monthly_searches": [\
                                    {\
                                        "year": 2022,\
                                        "month": 1,\
                                        "search_volume": 260\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 12,\
                                        "search_volume": 880\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 11,\
                                        "search_volume": 320\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 10,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 9,\
                                        "search_volume": 140\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 8,\
                                        "search_volume": 140\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 7,\
                                        "search_volume": 140\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 6,\
                                        "search_volume": 110\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 5,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 4,\
                                        "search_volume": 140\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 3,\
                                        "search_volume": 170\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 2,\
                                        "search_volume": 170\
                                    }\
                                ]\
                            },\
                            "keyword_properties": {\
                                "se_type": "google",\
                                "core_keyword": null,\
                                "keyword_difficulty": 50\
                            },\
                            "serp_info": null\
                        },\
                        {\
                            "se_type": "google",\
                            "keyword": "best buy lava lamp",\
                            "location_code": 2840,\
                            "language_code": "en",\
                            "keyword_info": {\
                                "se_type": "google",\
                                "last_updated_time": "2022-03-18 01:21:28 +00:00",\
                                "competition": 1,\
                                "cpc": 0.268335,\
                                "search_volume": 70,\
                                "categories": [\
                                    10009,\
                                    10010,\
                                    10081,\
                                    10419,\
                                    11587,\
                                    11588,\
                                    12820\
                                ],\
                                "monthly_searches": [\
                                    {\
                                        "year": 2022,\
                                        "month": 2,\
                                        "search_volume": 30\
                                    },\
                                    {\
                                        "year": 2022,\
                                        "month": 1,\
                                        "search_volume": 90\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 12,\
                                        "search_volume": 320\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 11,\
                                        "search_volume": 70\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 10,\
                                        "search_volume": 30\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 9,\
                                        "search_volume": 30\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 8,\
                                        "search_volume": 50\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 7,\
                                        "search_volume": 40\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 6,\
                                        "search_volume": 30\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 5,\
                                        "search_volume": 30\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 4,\
                                        "search_volume": 50\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 3,\
                                        "search_volume": 20\
                                    }\
                                ]\
                            },\
                            "keyword_properties": {\
                                "se_type": "google",\
                                "core_keyword": null,\
                                "keyword_difficulty": 47\
                            },\
                            "serp_info": null\
                        },\
                        {\
                            "se_type": "google",\
                            "keyword": "where to buy lava lamp near me",\
                            "location_code": 2840,\
                            "language_code": "en",\
                            "keyword_info": {\
                                "se_type": "google",\
                                "last_updated_time": "2022-03-02 10:28:46 +00:00",\
                                "competition": 1,\
                                "cpc": 0.255234,\
                                "search_volume": 50,\
                                "categories": null,\
                                "monthly_searches": [\
                                    {\
                                        "year": 2022,\
                                        "month": 1,\
                                        "search_volume": 50\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 12,\
                                        "search_volume": 210\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 11,\
                                        "search_volume": 70\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 10,\
                                        "search_volume": 30\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 9,\
                                        "search_volume": 40\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 8,\
                                        "search_volume": 40\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 7,\
                                        "search_volume": 40\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 6,\
                                        "search_volume": 30\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 5,\
                                        "search_volume": 30\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 4,\
                                        "search_volume": 20\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 3,\
                                        "search_volume": 20\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 2,\
                                        "search_volume": 30\
                                    }\
                                ]\
                            },\
                            "keyword_properties": {\
                                "se_type": "google",\
                                "core_keyword": null,\
                                "keyword_difficulty": 53\
                            },\
                            "serp_info": null\
                        },\
                        {\
                            "se_type": "google",\
                            "keyword": "what is the biggest lava lamp you can buy",\
                            "location_code": 2840,\
                            "language_code": "en",\
                            "keyword_info": {\
                                "se_type": "google",\
                                "last_updated_time": "2022-03-21 04:44:35 +00:00",\
                                "competition": 1,\
                                "cpc": 0.524548,\
                                "search_volume": 40,\
                                "categories": null,\
                                "monthly_searches": [\
                                    {\
                                        "year": 2022,\
                                        "month": 2,\
                                        "search_volume": 40\
                                    },\
                                    {\
                                        "year": 2022,\
                                        "month": 1,\
                                        "search_volume": 50\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 12,\
                                        "search_volume": 70\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 11,\
                                        "search_volume": 50\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 10,\
                                        "search_volume": 20\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 9,\
                                        "search_volume": 30\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 8,\
                                        "search_volume": 40\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 7,\
                                        "search_volume": 40\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 6,\
                                        "search_volume": 30\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 5,\
                                        "search_volume": 40\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 4,\
                                        "search_volume": 50\
                                    },\
                                    {\
                                        "year": 2021,\
                                        "month": 3,\
                                        "search_volume": 50\
                                    }\
                                ]\
                            },\
                            "keyword_properties": {\
                                "se_type": "google",\
                                "core_keyword": null,\
                                "keyword_difficulty": 44\
                            },\
                            "serp_info": null\
                        }\
                    ]\
                }\
            ]\
        }\
    ]
}
```

### OnPage API

You can get the full list of filters available for OnPage endpoints by calling the following endpoint:

`GET https://api.dataforseo.com/v3/on_page/available_filters`

DataForSEO OnPage API filters have the following structure:

**`[$item_array.$parameter_field, $filter_operator, $filter_value]`**

`.` and `,` symbols are used as separators.

_Example:_

`["fetch_timing.duration_time",">",1]` where f`etch_timing` is the `$item_array`, `duration_time` is the `$parameter_field`, `>` is the `$filter_operator`, and `1` is the `$filter_value`.

DataForSEO OnPage API supports the following types and operators:

- `bool` supports `=,` and `<>`;
- `num` supports `<, <=, >, >=, =, <>, in,` and `not_in`;
- `str` supports `regex, match, not_match, like, not_like, in, not_in, =,` and `<>`;
- `array.str` supports `has`, and `has_not`.

For example, for the `["resource_type": "str"]` filter, we can use one of the following operators: `regex`, `like`, `not_like`, `in`, `not_in`, `=`, `<>`.

_**Example request:**_

If you have a website you might want to establish clear paths for your site visitors and create a smooth user experience. This can be done by minimizing the click depth across a website.

Firstly [set a task](https://docs.dataforseo.com/v3/on_page/task_post/?bash&_ga=2.224695130.1330823059.1631439795-1025978533.1626870654) to On-Page API and specify the necessary target site. Then configure your request to the [Pages endpoint](https://docs.dataforseo.com/v3/on_page/pages/?bash&_ga=2.224695130.1330823059.1631439795-1025978533.1626870654) with the following filter:

``

```
[\
  {\
    "id": "07281559-0695-0216-0000-c269be8b7592",\
    "filters": [\
      ["resource_type", "=", "html"],\
      "and",\
      ["click_depth", ">", 4]\
    ]\
  }\
]
```

The response from the API server will contain a list of pages located further than within 4 clicks from the homepage. You will also get detailed performance metrics, including meta tags checks, page timing, and over 60 other check-ups for each listed page.

### Backlinks API

You can get the full list of filters available for [Backlinks endpoints](https://docs.dataforseo.com/v3/backlinks/overview/?bash) by calling the following endpoint:

`GET https://api.dataforseo.com/v3/backlinks/available_filters`

[DataForSEO Backlinks API filters](https://docs.dataforseo.com/v3/backlinks/filters/?bash) have the following structure:

`[filered_field, filter_operator, filter_value]`

`.` and `,` symbols are used as separators.

_Example:_

`["domain_from","=", "forbes.com"]` where `domain_from` is the `filered_field`, `=` is the `filter_operator`, and `forbes.com` is the `filter_value`.

DataForSEO Backlinks API supports the following data types and operators:

- `num: <, <=, >, >=, =, <>, in, not_in`
- `str: match, not_match, like, not_like, ilike, not_ilike, =, <>`
- `array.str: has, has_not`
- `array.num: has, has_not`
- `time: <, >` (Example: `2021-01-29 15:02:37 +00:00`)

For example, for the `["main_domain": "str"]` filter, we can use one of the following operators: `<`, `<=`, `>`, `>=`, `=`, `<>`, `in`, `not_in`.

**Note:** in [Domain Intersection](https://docs.dataforseo.com/v3/backlinks/domain_intersection/live/?bash) and [Page Intersection](https://docs.dataforseo.com/v3/backlinks/page_intersection/live/?bash) endpoints the `$key` parameter refers to the variable denoting the sequence number of the page indicated in the targets array of the POST request;

For example, if you specify the pages in the POST request as follows:

`"targets":[\
\
"https://football.com/news",\
\
"https://fifa.com/updates",\
\
"https://uefa.com/updates"]`

The specified pages will be assigned with sequence numbers and the filter by the `“https://fifa.com/updates”` page will be assigned with a `“2”` and the filter will look as follows:

`"filters": ["2.domain_from","=","news24.com"]`

**Note #2:** in [Backlinks Anchors](https://docs.dataforseo.com/v3/backlinks/anchors/live/?bash), [Domain Pages](https://docs.dataforseo.com/v3/backlinks/domain_pages/live/?bash), and [Domain Intersection](https://docs.dataforseo.com/v3/backlinks/domain_intersection/live/?bash) endpoints you may come across empty-titled fields in the `referring_links_semantic_locations` array, which, however, are also available in filters. These empty fields are denoted as `$empty` and have the `num` type.

_Example:_

`referring_links_semantic_locations.$empty`

**Note #3:** the `ilike` and `not_ilike` operators represent the **case-insesntive** alternative to the `like` and `not_like` operators accordingly.

_Example:_

`"filters": ["page_from_title", "ilike", "%health%"]`

**Note #4:** in the following endpoints, you can filter the results by any top-level domain (TLD) from the `referring_links_tld` object of the response, but the structure of the filtered field is different:

• in [Backlinks Anchors](https://dataforseo.com/help-center/docs.dataforseo.com/v3/backlinks/anchors/live), [Referring Domains](https://docs.dataforseo.com/v3/backlinks/referring_domains/live), [Referring Networks](https://dataforseo.com/v3/backlinks/referring_networks/live), and [Domain Pages Summary](https://docs.dataforseo.com/v3/backlinks/domain_pages_summary/live) endpoints, use `referring_links_tld.$tld`;

• in the [Domain Pages](https://docs.dataforseo.com/v3/backlinks/domain_pages/live) endpoint, use `page_summary.referring_links_tld.$tld`;

• in the [Domain Intersection](https://docs.dataforseo.com/v3/backlinks/domain_intersection/live) endpoint, use `$key.referring_links_tld.$tld`.

In all cases, replace the `$tld` variable with the necessary TLD returned in the `referring_links_tld` object of the endpoint’s response. Example filter:

`"filters": ["1.referring_links_tld.com",">",0]`

**Note #5:** in the [Backlinks](https://docs.dataforseo.com/v3/backlinks/backlinks/live/?bash) and [Page Intersection](https://docs.dataforseo.com/v3/backlinks/page_intersection/live/?bash) endpoints, you can filter results by links that have specific attributes such as `dofollow`, `nofollow`, `noopener`, `noreferrer`, `external`, `ugc` and `sponsored`.

_Example filters:_

`"filters": ["attributes","has","ugc"]

"filters": ["attributes","has","noopener"]

"filters": ["dofollow", "=", true]

"filters": ["dofollow", "=", false]`
## Filters for DataForSEO Labs API

Add to FavoritesCopy URLEmailShare

‌‌

Here you will find all the necessary information about filters that can be used with DataForSEO Labs API endpoints.

Please, keep in mind that filters are associated with a certain object in the `result` array, and should be specified accordingly.

We recommend learning more about how to use filters in [this Help Center article](https://dataforseo.com/help-center/using-filters).

**Note that it is not possible to use the following types of fields as sorting rules in `order_by`: `array.str`, `array.num`.**

Pricing

Your account will not be charged for using this API

![checked](https://docs.dataforseo.com/v3/wp-content/themes/dataforseo/assets/img/icons/checked-circle.svg)

GET
https://api.dataforseo.com/v3/dataforseo\_labs/available\_filters

You will receive the full list of filters by calling this API. You can also download the full list of possible filters [by this link.](https://cdn.dataforseo.com/v3/available_filters.php?api=dataforseo_labs)

‌‌As a response of the API server, you will receive [JSON](https://en.wikipedia.org/wiki/JSON)-encoded data containing a `tasks` array with the information specific to the set tasks.

| Field name | Type | Description |
| --- | --- | --- |
| `version` | string | _the current version of the API_ |  |
| `status_code` | integer | _general status code_<br>you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors) |  |
| `status_message` | string | _general informational message_<br>you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix/errors) |  |
| `time` | string | _execution time, seconds_ |  |
| `cost` | float | _total tasks cost, USD_ |  |
| `tasks_count` | integer | _the number of tasks in the **`tasks`** array_ |  |
| `tasks_error` | integer | _the number of tasks in the **`tasks`** array returned with an error_ |  |
| **`tasks`** | array | _array of tasks_ |  |
| `id` | string | _task identifier_<br>**unique task identifier in our system in the [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier) format** |  |
| `status_code` | integer | _status code of the task_<br>generated by DataForSEO; can be within the following range: 10000-60000<br>you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors) |  |
| `status_message` | string | _informational message of the task_<br>you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix/errors) |  |
| `time` | string | _execution time, seconds_ |  |
| `cost` | float | _cost of the task, USD_ |  |
| `result_count` | integer | _number of elements in the `result` array_ |  |
| `path` | array | _URL path_ |  |
| `data` | object | _contains the parameters passed in the URL of the GET request_ |  |
| **`result`** | array | _array of results_<br>contains the full list of available parameters that can be used for data filtration<br>the parameters are grouped by the endpoint they can be used with |  |

Below you will find a detailed description of the structure that should be used to specify `filters` when setting tasks with DataForSEO Labs API. You will also find the types of parameters that can be used with each endpoint, and examples of pre-made filters.

**Description of the fields:**

| Field name | Type | Description |
| --- | --- | --- |
| `filters` | array | _array of results filtering parameters_<br>optional field<br>**you can add several filters at once (8 filters maximum)**<br>you should set a logical operator `and`, `or` between the conditions<br>filters have the following structure:<br>`[``$item_array``.``$results_array``.``$parameter_field``,``$filter_operator``,``$filter_value``]`<br>you should use the `.` and `,` symbols as separators<br>example:<br>`["keyword_data.keyword_info.search_volume", ">=", 50]`<br>Page Intersection, Ranked Keywords, Subdomains, Relevant Pages, Competitors Domain, Categories For Domain, Domain Intersection endpoints also support an alternative structure:<br>`[``$item_array``.``$results_array``.``$parameter_field``,``$filter_operator``,``$item->``$item_array``.``$results_array``.``$parameter_field``]`<br>if you use this structure, you need to attach `$item->` to the right part of the condition<br>**note** that the `$parameter_field` variables in the right part and in the left part of the condition should have identical type: `bool`, `num`, `str`, `time`<br>example:<br>`["metrics.organic.pos_1", ">", "$item->metrics.organic.pos_2_3"]` |  |
| `$item_array` | str | _item name in the filter_<br>optional field<br>possible values:<br>`keyword_data`, `ranked_serp_element` |  |
| `$results_array` | str | _results array in the filter_<br>optional field<br>possible values:<br>`keyword`, `keyword_info`, `check_url`, `se_results_count`, `serp_item`, `metrics` |  |
| `$parameter_field` | str | _parameter field in the filter_<br>optional field<br>**required field if the filter is applied**<br>the parameter in the superordinate `$results_array` or `item_array`<br>represents the field you want to filter the results by |  |
| `$filter_operator` | str | _operator in the filter_<br>optional field<br>**required field if the filter is applied**<br>available filter operators:<br>• if **`bool`**: `=`, `<>`<br>• if **`num`**: `<`, `<=`, `>`, `>=`, `=`, `<>`, `in`, `not_in`<br>• if **`str`**: `match`, `not_match`, `like`, `not_like`, `ilike`, `not_ilike`, `in`, `not_in`, `=`, `<>`, `regex`, `not_regex`<br>• if **`array.str`**: `has`, `has_not`<br>• if **`array.num`**: `has`, `has_not`<br>• if **`time`**: `<`, `>`<br>note: `time` should be specified in the format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2021-01-29 15:02:37 +00:00`<br>if you specify `in` or `not_in` operator, the `$filter_value` should be specified as an array<br>example:<br>`["keyword_info.search_volume","in",[10,1000]]`<br>`regex` and `not_regex` operators can be specified with `string` values using the [RE2 regex](https://github.com/google/re2/wiki/Syntax) syntax;<br>**Note:** the maximum limit for the number of characters you can specify in `regex` and `not_regex` is **1000**;<br>example:<br>string contains keywords: ` ["keyword_data.keyword", "regex", "(how|what|when)"]`<br>string does not contain keywords: ` ["keyword_data.keyword", "not_regex", "(how|what|when)"]`<br>`like` and `not_like` operators require adding `%` symbol to get accurate results<br>example:<br>`["keyword_data.keyword", "like", "%seo%"]` return `keyword_data` items that contain “seo” in the `keyword` field<br>`["keyword_data.keyword", "not_like", "%seo%"]` do not return `keyword_data` items that contain “seo” in the `keyword` field<br>`match` and `not_match` are full-text search operators that work with `string` values<br>example:<br>`["keyword", "not_match", "camera"]` return keywords that do not contain the “camera” word<br>`["keyword", "match", "phone"]` return keywords that contain the “phone” word |  |
| `$filter_value` | num<br>str<br>bool<br>time | _filtering value_<br>optional field<br>**required field if the filter is applied** |  |

‌‌

cURL

php

Node.js

Python

cSharp


![](https://docs.dataforseo.com/v3/wp-content/themes/dataforseo/assets/img/icons/coding.svg)

Looking for an example?

Choose a specific API endpoint from the sidebar to see ready-to-use code samples in your preferred programming language.


The list of available filtration parameters:

1

2

{

"version": "0.1.20241203",

הההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההההה

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

Copy

AI Assistant - DataForSEO

![Logo](https://chat.dataforseo.com/Sofia-ai-64.png)Sofia AI Assistant

Private

# 👋 Hello there! I'm Sofia, your brand-new DataForSEO AI assistant.

I'm here to help you make the most of DataForSEO and transform your SEO automation challenges into data-driven solutions. From beginner questions to advanced, enterprise-grade builds, let me show how I can assist you.

I can answer your questions & provide guidance.

Ask me anything about DataForSEO's services, APIs, databases, pricing, documentation, or technical details.

"What's the difference between Keyword Data API and DataForSEO Labs API?""How does DataForSEO Backlinks API pricing work?"

I will give API recommendations for your use case.

Describe your use case and I'll suggest the best APIs and endpoints, plus help you build the actual API requests.

"I want to track my website's keyword rankings daily""I need to find competitor backlinks for 50 domains"

I will help you with development roadmaps

Need to build an SEO tool? I'll create a step-by-step development guide custom-tailored to your project using DataForSEO APIs.

"Help me build a keyword research tool like SEMrush""I want to create a rank tracking dashboard for my agency"

I can do tool analysis for you

Share a mockup or screenshot of any SEO tool (could be yours or your competitor's) and I'll show you how to build it using DataForSEO APIs.

Ready to dive in? Just tell me what you're working on or what you'd like to know about DataForSEO!

DataForSEO AI can make mistakes. Consider double-checking important information with support and documentation.

{
  "version": "0.1.20241203",
  "status_code": 20000,
  "status_message": "Ok.",
  "time": "0.0580 sec.",
  "cost": 0,
  "tasks_count": 1,
  "tasks_error": 0,
  "tasks": [
    {
      "id": "12051450-1535-0329-0000-023286aad228",
      "status_code": 20000,
      "status_message": "Ok.",
      "time": "0 sec.",
      "cost": 0,
      "result_count": 1,
      "path": [
        "v3",
        "dataforseo_labs",
        "available_filters"
      ],
      "data": {
        "api": "dataforseo_labs",
        "function": "available_filters"
      },
      "result": [
        {
          "related_keywords": {
            "google": {
              "depth": "num",
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.competition": "num",
              "keyword_data.keyword_info.competition_level": "str",
              "keyword_data.keyword_info.cpc": "num",
              "keyword_data.keyword_info.search_volume": "num",
              "keyword_data.keyword_info.low_top_of_page_bid": "num",
              "keyword_data.keyword_info.high_top_of_page_bid": "num",
              "keyword_data.keyword_info.categories": "array.num",
              "keyword_data.keyword_info.search_volume_trend.monthly": "num",
              "keyword_data.keyword_info.search_volume_trend.quarterly": "num",
              "keyword_data.keyword_info.search_volume_trend.yearly": "num",
              "keyword_data.clickstream_keyword_info.search_volume": "num",
              "keyword_data.clickstream_keyword_info.last_updated_time": "time",
              "keyword_data.clickstream_keyword_info.gender_distribution.female": "num",
              "keyword_data.clickstream_keyword_info.gender_distribution.male": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.18-24": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.25-34": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.35-44": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.45-54": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.55-64": "num",
              "keyword_data.keyword_properties.core_keyword": "str",
              "keyword_data.keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_data.keyword_properties.keyword_difficulty": "num",
              "keyword_data.keyword_properties.detected_language": "str",
              "keyword_data.keyword_properties.is_another_language": "bool",
              "keyword_data.serp_info.check_url": "str",
              "keyword_data.serp_info.serp_item_types": "array.str",
              "keyword_data.serp_info.se_results_count": "num",
              "keyword_data.serp_info.last_updated_time": "time",
              "keyword_data.serp_info.previous_updated_time": "time",
              "keyword_data.avg_backlinks_info.backlinks": "num",
              "keyword_data.avg_backlinks_info.dofollow": "num",
              "keyword_data.avg_backlinks_info.referring_pages": "num",
              "keyword_data.avg_backlinks_info.referring_domains": "num",
              "keyword_data.avg_backlinks_info.referring_main_domains": "num",
              "keyword_data.avg_backlinks_info.rank": "num",
              "keyword_data.avg_backlinks_info.main_domain_rank": "num",
              "keyword_data.avg_backlinks_info.last_updated_time": "time",
              "keyword_data.search_intent_info.main_intent": "str",
              "keyword_data.search_intent_info.foreign_intent": "array.str",
              "keyword_data.search_intent_info.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_bing.search_volume": "num",
              "keyword_data.keyword_info_normalized_with_bing.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_bing.is_normalized": "bool",
              "keyword_data.keyword_info_normalized_with_clickstream.search_volume": "num",
              "keyword_data.keyword_info_normalized_with_clickstream.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_clickstream.is_normalized": "bool"
            },
            "bing": {
              "depth": "num",
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.competition": "num",
              "keyword_data.keyword_info.cpc": "num",
              "keyword_data.keyword_info.search_volume": "num",
              "keyword_data.keyword_properties.core_keyword": "str",
              "keyword_data.keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_data.keyword_properties.keyword_difficulty": "num",
              "keyword_data.keyword_properties.detected_language": "str",
              "keyword_data.keyword_properties.is_another_language": "bool",
              "keyword_data.serp_info.check_url": "str",
              "keyword_data.serp_info.se_results_count": "num",
              "keyword_data.serp_info.last_updated_time": "time",
              "keyword_data.serp_info.previous_updated_time": "time",
              "keyword_data.serp_info.serp_item_types": "array.str"
            },
            "amazon": {
              "depth": "num",
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.search_volume": "num"
            }
          },
          "keyword_suggestions": {
            "google": {
              "keyword": "str",
              "keyword_info.last_updated_time": "time",
              "keyword_info.competition": "num",
              "keyword_info.competition_level": "str",
              "keyword_info.cpc": "num",
              "keyword_info.search_volume": "num",
              "keyword_info.low_top_of_page_bid": "num",
              "keyword_info.high_top_of_page_bid": "num",
              "keyword_info.categories": "array.num",
              "keyword_info.search_volume_trend.monthly": "num",
              "keyword_info.search_volume_trend.quarterly": "num",
              "keyword_info.search_volume_trend.yearly": "num",
              "clickstream_keyword_info.search_volume": "num",
              "clickstream_keyword_info.last_updated_time": "time",
              "clickstream_keyword_info.gender_distribution.female": "num",
              "clickstream_keyword_info.gender_distribution.male": "num",
              "clickstream_keyword_info.age_distribution.18-24": "num",
              "clickstream_keyword_info.age_distribution.25-34": "num",
              "clickstream_keyword_info.age_distribution.35-44": "num",
              "clickstream_keyword_info.age_distribution.45-54": "num",
              "clickstream_keyword_info.age_distribution.55-64": "num",
              "keyword_properties.core_keyword": "str",
              "keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_properties.keyword_difficulty": "num",
              "keyword_properties.detected_language": "str",
              "keyword_properties.is_another_language": "bool",
              "serp_info.check_url": "str",
              "serp_info.se_results_count": "num",
              "serp_info.last_updated_time": "time",
              "serp_info.serp_item_types": "array.str",
              "avg_backlinks_info.backlinks": "num",
              "avg_backlinks_info.dofollow": "num",
              "avg_backlinks_info.referring_pages": "num",
              "avg_backlinks_info.referring_domains": "num",
              "avg_backlinks_info.referring_main_domains": "num",
              "avg_backlinks_info.rank": "num",
              "avg_backlinks_info.main_domain_rank": "num",
              "avg_backlinks_info.last_updated_time": "time",
              "search_intent_info.main_intent": "str",
              "search_intent_info.foreign_intent": "array.str",
              "search_intent_info.last_updated_time": "time",
              "keyword_info_normalized_with_bing.search_volume": "num",
              "keyword_info_normalized_with_bing.last_updated_time": "time",
              "keyword_info_normalized_with_bing.is_normalized": "bool",
              "keyword_info_normalized_with_clickstream.search_volume": "num",
              "keyword_info_normalized_with_clickstream.last_updated_time": "time",
              "keyword_info_normalized_with_clickstream.is_normalized": "bool"
            }
          },
          "ranked_keywords": {
            "google": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.competition": "num",
              "keyword_data.keyword_info.competition_level": "str",
              "keyword_data.keyword_info.cpc": "num",
              "keyword_data.keyword_info.search_volume": "num",
              "keyword_data.keyword_info.low_top_of_page_bid": "num",
              "keyword_data.keyword_info.high_top_of_page_bid": "num",
              "keyword_data.keyword_info.categories": "array.num",
              "keyword_data.keyword_info.search_volume_trend.monthly": "num",
              "keyword_data.keyword_info.search_volume_trend.quarterly": "num",
              "keyword_data.keyword_info.search_volume_trend.yearly": "num",
              "keyword_data.clickstream_keyword_info.search_volume": "num",
              "keyword_data.clickstream_keyword_info.last_updated_time": "time",
              "keyword_data.clickstream_keyword_info.gender_distribution.female": "num",
              "keyword_data.clickstream_keyword_info.gender_distribution.male": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.18-24": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.25-34": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.35-44": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.45-54": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.55-64": "num",
              "keyword_data.keyword_properties.core_keyword": "str",
              "keyword_data.keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_data.keyword_properties.keyword_difficulty": "num",
              "keyword_data.keyword_properties.detected_language": "str",
              "keyword_data.keyword_properties.is_another_language": "bool",
              "keyword_data.serp_info.check_url": "str",
              "keyword_data.serp_info.serp_item_types": "array.str",
              "keyword_data.serp_info.se_results_count": "num",
              "keyword_data.serp_info.last_updated_time": "time",
              "keyword_data.serp_info.previous_updated_time": "time",
              "keyword_data.avg_backlinks_info.backlinks": "num",
              "keyword_data.avg_backlinks_info.dofollow": "num",
              "keyword_data.avg_backlinks_info.referring_pages": "num",
              "keyword_data.avg_backlinks_info.referring_domains": "num",
              "keyword_data.avg_backlinks_info.referring_main_domains": "num",
              "keyword_data.avg_backlinks_info.rank": "num",
              "keyword_data.avg_backlinks_info.main_domain_rank": "num",
              "keyword_data.avg_backlinks_info.last_updated_time": "time",
              "keyword_data.search_intent_info.main_intent": "str",
              "keyword_data.search_intent_info.foreign_intent": "array.str",
              "keyword_data.search_intent_info.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_bing.search_volume": "num",
              "keyword_data.keyword_info_normalized_with_bing.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_bing.is_normalized": "bool",
              "keyword_data.keyword_info_normalized_with_clickstream.search_volume": "num",
              "keyword_data.keyword_info_normalized_with_clickstream.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_clickstream.is_normalized": "bool",
              "ranked_serp_element.serp_item.type": "str",
              "ranked_serp_element.serp_item.rank_group": "num",
              "ranked_serp_element.serp_item.rank_absolute": "num",
              "ranked_serp_element.serp_item.position": "str",
              "ranked_serp_element.serp_item.xpath": "str",
              "ranked_serp_element.serp_item.domain": "str",
              "ranked_serp_element.serp_item.title": "str",
              "ranked_serp_element.serp_item.url": "str",
              "ranked_serp_element.serp_item.breadcrumb": "str",
              "ranked_serp_element.serp_item.website_name": "str",
              "ranked_serp_element.serp_item.is_image": "bool",
              "ranked_serp_element.serp_item.is_video": "bool",
              "ranked_serp_element.serp_item.is_featured_snippet": "bool",
              "ranked_serp_element.serp_item.is_malicious": "bool",
              "ranked_serp_element.serp_item.description": "str",
              "ranked_serp_element.serp_item.pre_snippet": "str",
              "ranked_serp_element.serp_item.extended_snippet": "str",
              "ranked_serp_element.serp_item.amp_version": "bool",
              "ranked_serp_element.serp_item.rating.rating_type": "str",
              "ranked_serp_element.serp_item.rating.value": "num",
              "ranked_serp_element.serp_item.rating.votes_count": "num",
              "ranked_serp_element.serp_item.rating.rating_max": "num",
              "ranked_serp_element.serp_item.about_this_result": "str",
              "ranked_serp_element.serp_item.main_domain": "str",
              "ranked_serp_element.serp_item.relative_url": "str",
              "ranked_serp_element.serp_item.etv": "num",
              "ranked_serp_element.serp_item.estimated_paid_traffic_cost": "num",
              "ranked_serp_element.serp_item.clickstream_etv": "num",
              "ranked_serp_element.serp_item.rank_changes.previous_rank_absolute": "num",
              "ranked_serp_element.serp_item.rank_changes.is_new": "bool",
              "ranked_serp_element.serp_item.rank_changes.is_up": "bool",
              "ranked_serp_element.serp_item.rank_changes.is_down": "bool",
              "ranked_serp_element.serp_item.backlinks_info.referring_domains": "num",
              "ranked_serp_element.serp_item.backlinks_info.referring_main_domains": "num",
              "ranked_serp_element.serp_item.backlinks_info.referring_pages": "num",
              "ranked_serp_element.serp_item.backlinks_info.dofollow": "num",
              "ranked_serp_element.serp_item.backlinks_info.backlinks": "num",
              "ranked_serp_element.serp_item.backlinks_info.time_update": "time",
              "ranked_serp_element.serp_item.rank_info.page_rank": "num",
              "ranked_serp_element.serp_item.rank_info.main_domain_rank": "num",
              "ranked_serp_element.check_url": "str",
              "ranked_serp_element.serp_item_types": "array.str",
              "ranked_serp_element.se_results_count": "num",
              "ranked_serp_element.keyword_difficulty": "num",
              "ranked_serp_element.is_lost": "bool",
              "ranked_serp_element.last_updated_time": "time",
              "ranked_serp_element.previous_updated_time": "time",
              "ranked_serp_element.serp_item.extra.ad_aclk": "str",
              "ranked_serp_element.serp_item.description_rows": "array.str",
              "ranked_serp_element.serp_item.phone": "str",
              "ranked_serp_element.serp_item.is_paid": "bool",
              "ranked_serp_element.serp_item.featured_title": "str"
            },
            "bing": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.competition": "num",
              "keyword_data.keyword_info.cpc": "num",
              "keyword_data.keyword_info.search_volume": "num",
              "keyword_data.keyword_properties.core_keyword": "str",
              "keyword_data.keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_data.keyword_properties.keyword_difficulty": "num",
              "keyword_data.keyword_properties.detected_language": "str",
              "keyword_data.keyword_properties.is_another_language": "bool",
              "keyword_data.serp_info.check_url": "str",
              "keyword_data.serp_info.serp_item_types": "array.str",
              "keyword_data.serp_info.se_results_count": "num",
              "keyword_data.serp_info.last_updated_time": "time",
              "keyword_data.serp_info.previous_updated_time": "time",
              "ranked_serp_element.serp_item.type": "str",
              "ranked_serp_element.serp_item.rank_group": "num",
              "ranked_serp_element.serp_item.rank_absolute": "num",
              "ranked_serp_element.serp_item.position": "str",
              "ranked_serp_element.serp_item.xpath": "str",
              "ranked_serp_element.serp_item.title": "str",
              "ranked_serp_element.serp_item.domain": "str",
              "ranked_serp_element.serp_item.description": "str",
              "ranked_serp_element.serp_item.breadcrumb": "str",
              "ranked_serp_element.serp_item.url": "str",
              "ranked_serp_element.serp_item.main_domain": "str",
              "ranked_serp_element.serp_item.relative_url": "str",
              "ranked_serp_element.serp_item.etv": "num",
              "ranked_serp_element.serp_item.estimated_paid_traffic_cost": "num",
              "ranked_serp_element.serp_item.rank_changes.previous_rank_absolute": "num",
              "ranked_serp_element.serp_item.rank_changes.is_new": "bool",
              "ranked_serp_element.serp_item.rank_changes.is_up": "bool",
              "ranked_serp_element.serp_item.rank_changes.is_down": "bool",
              "ranked_serp_element.serp_item.backlinks_info.referring_domains": "num",
              "ranked_serp_element.serp_item.backlinks_info.referring_main_domains": "num",
              "ranked_serp_element.serp_item.backlinks_info.referring_pages": "num",
              "ranked_serp_element.serp_item.backlinks_info.dofollow": "num",
              "ranked_serp_element.serp_item.backlinks_info.backlinks": "num",
              "ranked_serp_element.serp_item.backlinks_info.time_update": "time",
              "ranked_serp_element.serp_item.rank_info.page_rank": "num",
              "ranked_serp_element.serp_item.rank_info.main_domain_rank": "num",
              "ranked_serp_element.serp_item.is_image": "bool",
              "ranked_serp_element.serp_item.is_video": "bool",
              "ranked_serp_element.check_url": "str",
              "ranked_serp_element.serp_item_types": "array.str",
              "ranked_serp_element.se_results_count": "num",
              "ranked_serp_element.keyword_difficulty": "num",
              "ranked_serp_element.is_lost": "bool",
              "ranked_serp_element.last_updated_time": "time",
              "ranked_serp_element.previous_updated_time": "time",
              "ranked_serp_element.serp_item.extra.ad_aclk": "str",
              "ranked_serp_element.serp_item.description_rows": "array.str",
              "ranked_serp_element.serp_item.phone": "str",
              "ranked_serp_element.serp_item.is_paid": "bool",
              "ranked_serp_element.serp_item.featured_title": "str"
            },
            "amazon": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.search_volume": "num",
              "ranked_serp_element.serp_item.type": "str",
              "ranked_serp_element.serp_item.rank_group": "num",
              "ranked_serp_element.serp_item.rank_absolute": "num",
              "ranked_serp_element.serp_item.xpath": "str",
              "ranked_serp_element.serp_item.domain": "str",
              "ranked_serp_element.serp_item.title": "str",
              "ranked_serp_element.serp_item.url": "str",
              "ranked_serp_element.serp_item.asin": "str",
              "ranked_serp_element.serp_item.image_url": "str",
              "ranked_serp_element.serp_item.price_from": "num",
              "ranked_serp_element.serp_item.price_to": "num",
              "ranked_serp_element.serp_item.currency": "str",
              "ranked_serp_element.serp_item.is_best_seller": "bool",
              "ranked_serp_element.serp_item.is_amazon_choice": "bool",
              "ranked_serp_element.serp_item.rating.rating_type": "str",
              "ranked_serp_element.serp_item.rating.value": "num",
              "ranked_serp_element.serp_item.rating.votes_count": "num",
              "ranked_serp_element.serp_item.rating.rating_max": "num",
              "ranked_serp_element.serp_item.delivery_info.delivery_date_from": "time",
              "ranked_serp_element.serp_item.delivery_info.delivery_date_to": "time",
              "ranked_serp_element.serp_item.delivery_info.fastest_delivery_date_from": "time",
              "ranked_serp_element.serp_item.delivery_info.fastest_delivery_date_to": "time",
              "ranked_serp_element.serp_item.delivery_info.delivery_message": "str",
              "ranked_serp_element.serp_item.delivery_info.delivery_price.current": "num",
              "ranked_serp_element.serp_item.delivery_info.delivery_price.regular": "num",
              "ranked_serp_element.serp_item.delivery_info.delivery_price.max_value": "num",
              "ranked_serp_element.serp_item.delivery_info.delivery_price.currency": "str",
              "ranked_serp_element.serp_item.delivery_info.delivery_price.is_price_range": "bool",
              "ranked_serp_element.serp_item.delivery_info.delivery_price.displayed_price": "str",
              "ranked_serp_element.check_url": "str",
              "ranked_serp_element.serp_item_types": "array.str",
              "ranked_serp_element.se_results_count": "num",
              "ranked_serp_element.last_updated_time": "time",
              "ranked_serp_element.previous_updated_time": "time"
            }
          },
          "keyword_ideas": {
            "google": {
              "keyword": "str",
              "keyword_info.last_updated_time": "time",
              "keyword_info.competition": "num",
              "keyword_info.competition_level": "str",
              "keyword_info.cpc": "num",
              "keyword_info.search_volume": "num",
              "keyword_info.low_top_of_page_bid": "num",
              "keyword_info.high_top_of_page_bid": "num",
              "keyword_info.categories": "array.num",
              "keyword_info.search_volume_trend.monthly": "num",
              "keyword_info.search_volume_trend.quarterly": "num",
              "keyword_info.search_volume_trend.yearly": "num",
              "clickstream_keyword_info.search_volume": "num",
              "clickstream_keyword_info.last_updated_time": "time",
              "clickstream_keyword_info.gender_distribution.female": "num",
              "clickstream_keyword_info.gender_distribution.male": "num",
              "clickstream_keyword_info.age_distribution.18-24": "num",
              "clickstream_keyword_info.age_distribution.25-34": "num",
              "clickstream_keyword_info.age_distribution.35-44": "num",
              "clickstream_keyword_info.age_distribution.45-54": "num",
              "clickstream_keyword_info.age_distribution.55-64": "num",
              "keyword_properties.core_keyword": "str",
              "keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_properties.keyword_difficulty": "num",
              "keyword_properties.detected_language": "str",
              "keyword_properties.is_another_language": "bool",
              "serp_info.check_url": "str",
              "serp_info.serp_item_types": "array.str",
              "serp_info.se_results_count": "num",
              "serp_info.last_updated_time": "time",
              "avg_backlinks_info.backlinks": "num",
              "avg_backlinks_info.dofollow": "num",
              "avg_backlinks_info.referring_pages": "num",
              "avg_backlinks_info.referring_domains": "num",
              "avg_backlinks_info.referring_main_domains": "num",
              "avg_backlinks_info.rank": "num",
              "avg_backlinks_info.main_domain_rank": "num",
              "avg_backlinks_info.last_updated_time": "time",
              "search_intent_info.main_intent": "str",
              "search_intent_info.foreign_intent": "array.str",
              "search_intent_info.last_updated_time": "time",
              "keyword_info_normalized_with_bing.search_volume": "num",
              "keyword_info_normalized_with_bing.last_updated_time": "time",
              "keyword_info_normalized_with_bing.is_normalized": "bool",
              "keyword_info_normalized_with_clickstream.search_volume": "num",
              "keyword_info_normalized_with_clickstream.last_updated_time": "time",
              "keyword_info_normalized_with_clickstream.is_normalized": "bool"
            }
          },
          "serp_competitors": {
            "google": {
              "domain": "str",
              "avg_position": "num",
              "median_position": "num",
              "rating": "num",
              "etv": "num",
              "keywords_count": "num",
              "visibility": "num",
              "relevant_serp_items": "num"
            },
            "bing": {
              "domain": "str",
              "avg_position": "num",
              "median_position": "num",
              "rating": "num",
              "etv": "num",
              "keywords_count": "num",
              "visibility": "num",
              "relevant_serp_items": "num"
            }
          },
          "relevant_pages": {
            "google": {
              "page_address": "str",
              "metrics.$item_type.pos_1": "num",
              "metrics.$item_type.pos_2_3": "num",
              "metrics.$item_type.pos_4_10": "num",
              "metrics.$item_type.pos_11_20": "num",
              "metrics.$item_type.pos_21_30": "num",
              "metrics.$item_type.pos_31_40": "num",
              "metrics.$item_type.pos_41_50": "num",
              "metrics.$item_type.pos_51_60": "num",
              "metrics.$item_type.pos_61_70": "num",
              "metrics.$item_type.pos_71_80": "num",
              "metrics.$item_type.pos_81_90": "num",
              "metrics.$item_type.pos_91_100": "num",
              "metrics.$item_type.etv": "num",
              "metrics.$item_type.count": "num",
              "metrics.$item_type.estimated_paid_traffic_cost": "num",
              "metrics.$item_type.is_up": "num",
              "metrics.$item_type.is_new": "num",
              "metrics.$item_type.is_down": "num",
              "metrics.$item_type.is_lost": "num",
              "metrics.$item_type.clickstream_etv": "num",
              "metrics.$item_type.clickstream_gender_distribution.female": "num",
              "metrics.$item_type.clickstream_gender_distribution.male": "num",
              "metrics.$item_type.clickstream_age_distribution.18-24": "num",
              "metrics.$item_type.clickstream_age_distribution.25-34": "num",
              "metrics.$item_type.clickstream_age_distribution.35-44": "num",
              "metrics.$item_type.clickstream_age_distribution.45-54": "num",
              "metrics.$item_type.clickstream_age_distribution.55-64": "num"
            },
            "bing": {
              "page_address": "str",
              "metrics.$item_type.pos_1": "num",
              "metrics.$item_type.pos_2_3": "num",
              "metrics.$item_type.pos_4_10": "num",
              "metrics.$item_type.pos_11_20": "num",
              "metrics.$item_type.pos_21_30": "num",
              "metrics.$item_type.pos_31_40": "num",
              "metrics.$item_type.pos_41_50": "num",
              "metrics.$item_type.pos_51_60": "num",
              "metrics.$item_type.pos_61_70": "num",
              "metrics.$item_type.pos_71_80": "num",
              "metrics.$item_type.pos_81_90": "num",
              "metrics.$item_type.pos_91_100": "num",
              "metrics.$item_type.etv": "num",
              "metrics.$item_type.count": "num",
              "metrics.$item_type.estimated_paid_traffic_cost": "num",
              "metrics.$item_type.is_up": "num",
              "metrics.$item_type.is_new": "num",
              "metrics.$item_type.is_down": "num",
              "metrics.$item_type.is_lost": "num"
            }
          },
          "subdomains": {
            "google": {
              "subdomain": "str",
              "metrics.$item_type.pos_1": "num",
              "metrics.$item_type.pos_2_3": "num",
              "metrics.$item_type.pos_4_10": "num",
              "metrics.$item_type.pos_11_20": "num",
              "metrics.$item_type.pos_21_30": "num",
              "metrics.$item_type.pos_31_40": "num",
              "metrics.$item_type.pos_41_50": "num",
              "metrics.$item_type.pos_51_60": "num",
              "metrics.$item_type.pos_61_70": "num",
              "metrics.$item_type.pos_71_80": "num",
              "metrics.$item_type.pos_81_90": "num",
              "metrics.$item_type.pos_91_100": "num",
              "metrics.$item_type.etv": "num",
              "metrics.$item_type.count": "num",
              "metrics.$item_type.estimated_paid_traffic_cost": "num",
              "metrics.$item_type.is_up": "num",
              "metrics.$item_type.is_new": "num",
              "metrics.$item_type.is_down": "num",
              "metrics.$item_type.is_lost": "num",
              "metrics.$item_type.clickstream_etv": "num",
              "metrics.$item_type.clickstream_gender_distribution.female": "num",
              "metrics.$item_type.clickstream_gender_distribution.male": "num",
              "metrics.$item_type.clickstream_age_distribution.18-24": "num",
              "metrics.$item_type.clickstream_age_distribution.25-34": "num",
              "metrics.$item_type.clickstream_age_distribution.35-44": "num",
              "metrics.$item_type.clickstream_age_distribution.45-54": "num",
              "metrics.$item_type.clickstream_age_distribution.55-64": "num"
            },
            "bing": {
              "subdomain": "str",
              "metrics.$item_type.pos_1": "num",
              "metrics.$item_type.pos_2_3": "num",
              "metrics.$item_type.pos_4_10": "num",
              "metrics.$item_type.pos_11_20": "num",
              "metrics.$item_type.pos_21_30": "num",
              "metrics.$item_type.pos_31_40": "num",
              "metrics.$item_type.pos_41_50": "num",
              "metrics.$item_type.pos_51_60": "num",
              "metrics.$item_type.pos_61_70": "num",
              "metrics.$item_type.pos_71_80": "num",
              "metrics.$item_type.pos_81_90": "num",
              "metrics.$item_type.pos_91_100": "num",
              "metrics.$item_type.etv": "num",
              "metrics.$item_type.count": "num",
              "metrics.$item_type.estimated_paid_traffic_cost": "num",
              "metrics.$item_type.is_up": "num",
              "metrics.$item_type.is_new": "num",
              "metrics.$item_type.is_down": "num",
              "metrics.$item_type.is_lost": "num"
            }
          },
          "competitors_domain": {
            "google": {
              "domain": "str",
              "avg_position": "num",
              "sum_position": "num",
              "intersections": "num",
              "metrics.$item_type.pos_1": "num",
              "metrics.$item_type.pos_2_3": "num",
              "metrics.$item_type.pos_4_10": "num",
              "metrics.$item_type.pos_11_20": "num",
              "metrics.$item_type.pos_21_30": "num",
              "metrics.$item_type.pos_31_40": "num",
              "metrics.$item_type.pos_41_50": "num",
              "metrics.$item_type.pos_51_60": "num",
              "metrics.$item_type.pos_61_70": "num",
              "metrics.$item_type.pos_71_80": "num",
              "metrics.$item_type.pos_81_90": "num",
              "metrics.$item_type.pos_91_100": "num",
              "metrics.$item_type.etv": "num",
              "metrics.$item_type.count": "num",
              "metrics.$item_type.estimated_paid_traffic_cost": "num",
              "metrics.$item_type.is_up": "num",
              "metrics.$item_type.is_new": "num",
              "metrics.$item_type.is_down": "num",
              "metrics.$item_type.is_lost": "num",
              "metrics.$item_type.clickstream_etv": "num",
              "metrics.$item_type.clickstream_gender_distribution.female": "num",
              "metrics.$item_type.clickstream_gender_distribution.male": "num",
              "metrics.$item_type.clickstream_age_distribution.18-24": "num",
              "metrics.$item_type.clickstream_age_distribution.25-34": "num",
              "metrics.$item_type.clickstream_age_distribution.35-44": "num",
              "metrics.$item_type.clickstream_age_distribution.45-54": "num",
              "metrics.$item_type.clickstream_age_distribution.55-64": "num",
              "competitor_metrics.$item_type.pos_1": "num",
              "competitor_metrics.$item_type.pos_2_3": "num",
              "competitor_metrics.$item_type.pos_4_10": "num",
              "competitor_metrics.$item_type.pos_11_20": "num",
              "competitor_metrics.$item_type.pos_21_30": "num",
              "competitor_metrics.$item_type.pos_31_40": "num",
              "competitor_metrics.$item_type.pos_41_50": "num",
              "competitor_metrics.$item_type.pos_51_60": "num",
              "competitor_metrics.$item_type.pos_61_70": "num",
              "competitor_metrics.$item_type.pos_71_80": "num",
              "competitor_metrics.$item_type.pos_81_90": "num",
              "competitor_metrics.$item_type.pos_91_100": "num",
              "competitor_metrics.$item_type.etv": "num",
              "competitor_metrics.$item_type.count": "num",
              "competitor_metrics.$item_type.estimated_paid_traffic_cost": "num",
              "competitor_metrics.$item_type.is_up": "num",
              "competitor_metrics.$item_type.is_new": "num",
              "competitor_metrics.$item_type.is_down": "num",
              "competitor_metrics.$item_type.is_lost": "num",
              "competitor_metrics.$item_type.clickstream_etv": "num",
              "competitor_metrics.$item_type.clickstream_gender_distribution.female": "num",
              "competitor_metrics.$item_type.clickstream_gender_distribution.male": "num",
              "competitor_metrics.$item_type.clickstream_age_distribution.18-24": "num",
              "competitor_metrics.$item_type.clickstream_age_distribution.25-34": "num",
              "competitor_metrics.$item_type.clickstream_age_distribution.35-44": "num",
              "competitor_metrics.$item_type.clickstream_age_distribution.45-54": "num",
              "competitor_metrics.$item_type.clickstream_age_distribution.55-64": "num"
            },
            "bing": {
              "domain": "str",
              "avg_position": "num",
              "sum_position": "num",
              "intersections": "num",
              "metrics.$item_type.pos_1": "num",
              "metrics.$item_type.pos_2_3": "num",
              "metrics.$item_type.pos_4_10": "num",
              "metrics.$item_type.pos_11_20": "num",
              "metrics.$item_type.pos_21_30": "num",
              "metrics.$item_type.pos_31_40": "num",
              "metrics.$item_type.pos_41_50": "num",
              "metrics.$item_type.pos_51_60": "num",
              "metrics.$item_type.pos_61_70": "num",
              "metrics.$item_type.pos_71_80": "num",
              "metrics.$item_type.pos_81_90": "num",
              "metrics.$item_type.pos_91_100": "num",
              "metrics.$item_type.etv": "num",
              "metrics.$item_type.count": "num",
              "metrics.$item_type.estimated_paid_traffic_cost": "num",
              "metrics.$item_type.is_up": "num",
              "metrics.$item_type.is_new": "num",
              "metrics.$item_type.is_down": "num",
              "metrics.$item_type.is_lost": "num",
              "competitor_metrics.$item_type.pos_1": "num",
              "competitor_metrics.$item_type.pos_2_3": "num",
              "competitor_metrics.$item_type.pos_4_10": "num",
              "competitor_metrics.$item_type.pos_11_20": "num",
              "competitor_metrics.$item_type.pos_21_30": "num",
              "competitor_metrics.$item_type.pos_31_40": "num",
              "competitor_metrics.$item_type.pos_41_50": "num",
              "competitor_metrics.$item_type.pos_51_60": "num",
              "competitor_metrics.$item_type.pos_61_70": "num",
              "competitor_metrics.$item_type.pos_71_80": "num",
              "competitor_metrics.$item_type.pos_81_90": "num",
              "competitor_metrics.$item_type.pos_91_100": "num",
              "competitor_metrics.$item_type.etv": "num",
              "competitor_metrics.$item_type.count": "num",
              "competitor_metrics.$item_type.estimated_paid_traffic_cost": "num",
              "competitor_metrics.$item_type.is_up": "num",
              "competitor_metrics.$item_type.is_new": "num",
              "competitor_metrics.$item_type.is_down": "num",
              "competitor_metrics.$item_type.is_lost": "num"
            }
          },
          "categories_for_domain": {
            "google": {
              "categories": "array.num",
              "metrics.$item_type.pos_1": "num",
              "metrics.$item_type.pos_2_3": "num",
              "metrics.$item_type.pos_4_10": "num",
              "metrics.$item_type.pos_11_20": "num",
              "metrics.$item_type.pos_21_30": "num",
              "metrics.$item_type.pos_31_40": "num",
              "metrics.$item_type.pos_41_50": "num",
              "metrics.$item_type.pos_51_60": "num",
              "metrics.$item_type.pos_61_70": "num",
              "metrics.$item_type.pos_71_80": "num",
              "metrics.$item_type.pos_81_90": "num",
              "metrics.$item_type.pos_91_100": "num",
              "metrics.$item_type.etv": "num",
              "metrics.$item_type.count": "num",
              "metrics.$item_type.estimated_paid_traffic_cost": "num",
              "metrics.$item_type.is_up": "num",
              "metrics.$item_type.is_new": "num",
              "metrics.$item_type.is_down": "num",
              "metrics.$item_type.is_lost": "num",
              "metrics.$item_type.clickstream_etv": "num",
              "metrics.$item_type.clickstream_gender_distribution.female": "num",
              "metrics.$item_type.clickstream_gender_distribution.male": "num",
              "metrics.$item_type.clickstream_age_distribution.18-24": "num",
              "metrics.$item_type.clickstream_age_distribution.25-34": "num",
              "metrics.$item_type.clickstream_age_distribution.35-44": "num",
              "metrics.$item_type.clickstream_age_distribution.45-54": "num",
              "metrics.$item_type.clickstream_age_distribution.55-64": "num"
            }
          },
          "keywords_for_categories": {
            "google": {
              "keyword": "str",
              "keyword_info.last_updated_time": "time",
              "keyword_info.competition": "num",
              "keyword_info.competition_level": "str",
              "keyword_info.cpc": "num",
              "keyword_info.search_volume": "num",
              "keyword_info.low_top_of_page_bid": "num",
              "keyword_info.high_top_of_page_bid": "num",
              "keyword_info.categories": "array.num",
              "keyword_info.search_volume_trend.monthly": "num",
              "keyword_info.search_volume_trend.quarterly": "num",
              "keyword_info.search_volume_trend.yearly": "num",
              "clickstream_keyword_info.search_volume": "num",
              "clickstream_keyword_info.last_updated_time": "time",
              "clickstream_keyword_info.gender_distribution.female": "num",
              "clickstream_keyword_info.gender_distribution.male": "num",
              "clickstream_keyword_info.age_distribution.18-24": "num",
              "clickstream_keyword_info.age_distribution.25-34": "num",
              "clickstream_keyword_info.age_distribution.35-44": "num",
              "clickstream_keyword_info.age_distribution.45-54": "num",
              "clickstream_keyword_info.age_distribution.55-64": "num",
              "keyword_properties.core_keyword": "str",
              "keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_properties.keyword_difficulty": "num",
              "keyword_properties.detected_language": "str",
              "keyword_properties.is_another_language": "bool",
              "serp_info.check_url": "str",
              "serp_info.serp_item_types": "array.str",
              "serp_info.se_results_count": "num",
              "serp_info.last_updated_time": "time",
              "avg_backlinks_info.backlinks": "num",
              "avg_backlinks_info.dofollow": "num",
              "avg_backlinks_info.referring_pages": "num",
              "avg_backlinks_info.referring_domains": "num",
              "avg_backlinks_info.referring_main_domains": "num",
              "avg_backlinks_info.rank": "num",
              "avg_backlinks_info.main_domain_rank": "num",
              "avg_backlinks_info.last_updated_time": "time",
              "search_intent_info.main_intent": "str",
              "search_intent_info.foreign_intent": "array.str",
              "search_intent_info.last_updated_time": "time",
              "keyword_info_normalized_with_bing.search_volume": "num",
              "keyword_info_normalized_with_bing.last_updated_time": "time",
              "keyword_info_normalized_with_bing.is_normalized": "bool",
              "keyword_info_normalized_with_clickstream.search_volume": "num",
              "keyword_info_normalized_with_clickstream.last_updated_time": "time",
              "keyword_info_normalized_with_clickstream.is_normalized": "bool"
            }
          },
          "domain_intersection": {
            "google": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.competition": "num",
              "keyword_data.keyword_info.competition_level": "str",
              "keyword_data.keyword_info.cpc": "num",
              "keyword_data.keyword_info.search_volume": "num",
              "keyword_data.keyword_info.low_top_of_page_bid": "num",
              "keyword_data.keyword_info.high_top_of_page_bid": "num",
              "keyword_data.keyword_info.categories": "array.num",
              "keyword_data.keyword_info.search_volume_trend.monthly": "num",
              "keyword_data.keyword_info.search_volume_trend.quarterly": "num",
              "keyword_data.keyword_info.search_volume_trend.yearly": "num",
              "keyword_data.clickstream_keyword_info.search_volume": "num",
              "keyword_data.clickstream_keyword_info.last_updated_time": "time",
              "keyword_data.clickstream_keyword_info.gender_distribution.female": "num",
              "keyword_data.clickstream_keyword_info.gender_distribution.male": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.18-24": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.25-34": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.35-44": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.45-54": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.55-64": "num",
              "keyword_data.keyword_properties.core_keyword": "str",
              "keyword_data.keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_data.keyword_properties.keyword_difficulty": "num",
              "keyword_data.keyword_properties.detected_language": "str",
              "keyword_data.keyword_properties.is_another_language": "bool",
              "keyword_data.serp_info.check_url": "str",
              "keyword_data.serp_info.serp_item_types": "array.str",
              "keyword_data.serp_info.se_results_count": "num",
              "keyword_data.serp_info.last_updated_time": "time",
              "keyword_data.serp_info.previous_updated_time": "time",
              "keyword_data.avg_backlinks_info.backlinks": "num",
              "keyword_data.avg_backlinks_info.dofollow": "num",
              "keyword_data.avg_backlinks_info.referring_pages": "num",
              "keyword_data.avg_backlinks_info.referring_domains": "num",
              "keyword_data.avg_backlinks_info.referring_main_domains": "num",
              "keyword_data.avg_backlinks_info.rank": "num",
              "keyword_data.avg_backlinks_info.main_domain_rank": "num",
              "keyword_data.avg_backlinks_info.last_updated_time": "time",
              "keyword_data.search_intent_info.main_intent": "str",
              "keyword_data.search_intent_info.foreign_intent": "array.str",
              "keyword_data.search_intent_info.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_bing.search_volume": "num",
              "keyword_data.keyword_info_normalized_with_bing.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_bing.is_normalized": "bool",
              "keyword_data.keyword_info_normalized_with_clickstream.search_volume": "num",
              "keyword_data.keyword_info_normalized_with_clickstream.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_clickstream.is_normalized": "bool",
              "first_domain_serp_element.type": "str",
              "first_domain_serp_element.rank_group": "num",
              "first_domain_serp_element.rank_absolute": "num",
              "first_domain_serp_element.position": "str",
              "first_domain_serp_element.xpath": "str",
              "first_domain_serp_element.title": "str",
              "first_domain_serp_element.pre_snippet": "str",
              "first_domain_serp_element.description": "str",
              "first_domain_serp_element.breadcrumb": "str",
              "first_domain_serp_element.is_image": "bool",
              "first_domain_serp_element.is_video": "bool",
              "first_domain_serp_element.is_featured_snippet": "bool",
              "first_domain_serp_element.amp_version": "bool",
              "first_domain_serp_element.is_malicious": "bool",
              "first_domain_serp_element.extended_snippet": "str",
              "first_domain_serp_element.domain": "str",
              "first_domain_serp_element.main_domain": "str",
              "first_domain_serp_element.url": "str",
              "first_domain_serp_element.relative_url": "str",
              "first_domain_serp_element.etv": "num",
              "first_domain_serp_element.clickstream_etv": "num",
              "first_domain_serp_element.rank_changes.previous_rank_absolute": "num",
              "first_domain_serp_element.rank_changes.is_new": "bool",
              "first_domain_serp_element.rank_changes.is_up": "bool",
              "first_domain_serp_element.rank_changes.is_down": "bool",
              "first_domain_serp_element.extra.ad_aclk": "str",
              "first_domain_serp_element.description_rows": "array.str",
              "first_domain_serp_element.phone": "str",
              "first_domain_serp_element.is_paid": "bool",
              "first_domain_serp_element.featured_title": "str",
              "second_domain_serp_element.type": "str",
              "second_domain_serp_element.rank_group": "num",
              "second_domain_serp_element.rank_absolute": "num",
              "second_domain_serp_element.position": "str",
              "second_domain_serp_element.xpath": "str",
              "second_domain_serp_element.title": "str",
              "second_domain_serp_element.pre_snippet": "str",
              "second_domain_serp_element.description": "str",
              "second_domain_serp_element.breadcrumb": "str",
              "second_domain_serp_element.is_image": "bool",
              "second_domain_serp_element.is_video": "bool",
              "second_domain_serp_element.is_featured_snippet": "bool",
              "second_domain_serp_element.amp_version": "bool",
              "second_domain_serp_element.is_malicious": "bool",
              "second_domain_serp_element.extended_snippet": "str",
              "second_domain_serp_element.domain": "str",
              "second_domain_serp_element.main_domain": "str",
              "second_domain_serp_element.url": "str",
              "second_domain_serp_element.relative_url": "str",
              "second_domain_serp_element.etv": "num",
              "second_domain_serp_element.clickstream_etv": "num",
              "second_domain_serp_element.estimated_paid_traffic_cost": "num",
              "second_domain_serp_element.rank_changes.previous_rank_absolute": "num",
              "second_domain_serp_element.rank_changes.is_new": "bool",
              "second_domain_serp_element.rank_changes.is_up": "bool",
              "second_domain_serp_element.rank_changes.is_down": "bool",
              "second_domain_serp_element.extra.ad_aclk": "str",
              "second_domain_serp_element.description_rows": "array.str",
              "second_domain_serp_element.phone": "str",
              "second_domain_serp_element.is_paid": "bool",
              "second_domain_serp_element.featured_title": "str"
            },
            "bing": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.competition": "num",
              "keyword_data.keyword_info.cpc": "num",
              "keyword_data.keyword_info.search_volume": "num",
              "keyword_data.keyword_properties.core_keyword": "str",
              "keyword_data.keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_data.keyword_properties.keyword_difficulty": "num",
              "keyword_data.keyword_properties.detected_language": "str",
              "keyword_data.keyword_properties.is_another_language": "bool",
              "keyword_data.serp_info.check_url": "str",
              "keyword_data.serp_info.serp_item_types": "array.str",
              "keyword_data.serp_info.se_results_count": "num",
              "keyword_data.serp_info.last_updated_time": "time",
              "keyword_data.serp_info.previous_updated_time": "time",
              "first_domain_serp_element.type": "str",
              "first_domain_serp_element.rank_group": "num",
              "first_domain_serp_element.rank_absolute": "num",
              "first_domain_serp_element.position": "str",
              "first_domain_serp_element.xpath": "str",
              "first_domain_serp_element.title": "str",
              "first_domain_serp_element.pre_snippet": "str",
              "first_domain_serp_element.description": "str",
              "first_domain_serp_element.breadcrumb": "str",
              "first_domain_serp_element.is_image": "bool",
              "first_domain_serp_element.is_video": "bool",
              "first_domain_serp_element.is_featured_snippet": "bool",
              "first_domain_serp_element.amp_version": "bool",
              "first_domain_serp_element.is_malicious": "bool",
              "first_domain_serp_element.extended_snippet": "str",
              "first_domain_serp_element.domain": "str",
              "first_domain_serp_element.main_domain": "str",
              "first_domain_serp_element.url": "str",
              "first_domain_serp_element.relative_url": "str",
              "first_domain_serp_element.etv": "num",
              "first_domain_serp_element.rank_changes.previous_rank_absolute": "num",
              "first_domain_serp_element.rank_changes.is_new": "bool",
              "first_domain_serp_element.rank_changes.is_up": "bool",
              "first_domain_serp_element.rank_changes.is_down": "bool",
              "first_domain_serp_element.extra.ad_aclk": "str",
              "first_domain_serp_element.description_rows": "array.str",
              "first_domain_serp_element.phone": "str",
              "first_domain_serp_element.is_paid": "bool",
              "first_domain_serp_element.featured_title": "str",
              "second_domain_serp_element.type": "str",
              "second_domain_serp_element.rank_group": "num",
              "second_domain_serp_element.rank_absolute": "num",
              "second_domain_serp_element.position": "str",
              "second_domain_serp_element.xpath": "str",
              "second_domain_serp_element.title": "str",
              "second_domain_serp_element.pre_snippet": "str",
              "second_domain_serp_element.description": "str",
              "second_domain_serp_element.breadcrumb": "str",
              "second_domain_serp_element.is_image": "bool",
              "second_domain_serp_element.is_video": "bool",
              "second_domain_serp_element.is_featured_snippet": "bool",
              "second_domain_serp_element.amp_version": "bool",
              "second_domain_serp_element.is_malicious": "bool",
              "second_domain_serp_element.extended_snippet": "str",
              "second_domain_serp_element.domain": "str",
              "second_domain_serp_element.main_domain": "str",
              "second_domain_serp_element.url": "str",
              "second_domain_serp_element.relative_url": "str",
              "second_domain_serp_element.etv": "num",
              "second_domain_serp_element.estimated_paid_traffic_cost": "num",
              "second_domain_serp_element.rank_changes.previous_rank_absolute": "num",
              "second_domain_serp_element.rank_changes.is_new": "bool",
              "second_domain_serp_element.rank_changes.is_up": "bool",
              "second_domain_serp_element.rank_changes.is_down": "bool",
              "second_domain_serp_element.extra.ad_aclk": "str",
              "second_domain_serp_element.description_rows": "array.str",
              "second_domain_serp_element.phone": "str",
              "second_domain_serp_element.is_paid": "bool",
              "second_domain_serp_element.featured_title": "str"
            }
          },
          "page_intersection": {
            "google": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.competition": "num",
              "keyword_data.keyword_info.competition_level": "str",
              "keyword_data.keyword_info.cpc": "num",
              "keyword_data.keyword_info.search_volume": "num",
              "keyword_data.keyword_info.low_top_of_page_bid": "num",
              "keyword_data.keyword_info.high_top_of_page_bid": "num",
              "keyword_data.keyword_info.categories": "array.num",
              "keyword_data.keyword_info.search_volume_trend.monthly": "num",
              "keyword_data.keyword_info.search_volume_trend.quarterly": "num",
              "keyword_data.keyword_info.search_volume_trend.yearly": "num",
              "keyword_data.clickstream_keyword_info.search_volume": "num",
              "keyword_data.clickstream_keyword_info.last_updated_time": "time",
              "keyword_data.clickstream_keyword_info.gender_distribution.female": "num",
              "keyword_data.clickstream_keyword_info.gender_distribution.male": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.18-24": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.25-34": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.35-44": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.45-54": "num",
              "keyword_data.clickstream_keyword_info.age_distribution.55-64": "num",
              "keyword_data.keyword_properties.core_keyword": "str",
              "keyword_data.keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_data.keyword_properties.keyword_difficulty": "num",
              "keyword_data.keyword_properties.detected_language": "str",
              "keyword_data.keyword_properties.is_another_language": "bool",
              "keyword_data.serp_info.check_url": "str",
              "keyword_data.serp_info.se_results_count": "num",
              "keyword_data.serp_info.last_updated_time": "time",
              "keyword_data.serp_info.previous_updated_time": "time",
              "keyword_data.serp_info.serp_item_types": "array.str",
              "keyword_data.avg_backlinks_info.backlinks": "num",
              "keyword_data.avg_backlinks_info.dofollow": "num",
              "keyword_data.avg_backlinks_info.referring_pages": "num",
              "keyword_data.avg_backlinks_info.referring_domains": "num",
              "keyword_data.avg_backlinks_info.referring_main_domains": "num",
              "keyword_data.avg_backlinks_info.rank": "num",
              "keyword_data.avg_backlinks_info.main_domain_rank": "num",
              "keyword_data.avg_backlinks_info.last_updated_time": "time",
              "keyword_data.search_intent_info.main_intent": "str",
              "keyword_data.search_intent_info.foreign_intent": "array.str",
              "keyword_data.search_intent_info.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_bing.search_volume": "num",
              "keyword_data.keyword_info_normalized_with_bing.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_bing.is_normalized": "bool",
              "keyword_data.keyword_info_normalized_with_clickstream.search_volume": "num",
              "keyword_data.keyword_info_normalized_with_clickstream.last_updated_time": "time",
              "keyword_data.keyword_info_normalized_with_clickstream.is_normalized": "bool",
              "intersection_result.$page.type": "str",
              "intersection_result.$page.rank_group": "num",
              "intersection_result.$page.rank_absolute": "num",
              "intersection_result.$page.position": "str",
              "intersection_result.$page.xpath": "str",
              "intersection_result.$page.title": "str",
              "intersection_result.$page.pre_snippet": "str",
              "intersection_result.$page.description": "str",
              "intersection_result.$page.breadcrumb": "str",
              "intersection_result.$page.is_image": "bool",
              "intersection_result.$page.is_video": "bool",
              "intersection_result.$page.is_featured_snippet": "bool",
              "intersection_result.$page.amp_version": "bool",
              "intersection_result.$page.is_malicious": "bool",
              "intersection_result.$page.extended_snippet": "str",
              "intersection_result.$page.domain": "str",
              "intersection_result.$page.main_domain": "str",
              "intersection_result.$page.url": "str",
              "intersection_result.$page.relative_url": "str",
              "intersection_result.$page.etv": "num",
              "intersection_result.$page.clickstream_etv": "num",
              "intersection_result.$page.estimated_paid_traffic_cost": "num",
              "intersection_result.$page.rank_changes.previous_rank_absolute": "num",
              "intersection_result.$page.rank_changes.is_new": "bool",
              "intersection_result.$page.rank_changes.is_up": "bool",
              "intersection_result.$page.rank_changes.is_down": "bool",
              "intersection_result.$page.extra.ad_aclk": "str",
              "intersection_result.$page.description_rows": "array.str",
              "intersection_result.$page.phone": "str",
              "intersection_result.$page.is_paid": "bool",
              "intersection_result.$page.featured_title": "str"
            },
            "bing": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.competition": "num",
              "keyword_data.keyword_info.cpc": "num",
              "keyword_data.keyword_info.search_volume": "num",
              "keyword_data.keyword_properties.core_keyword": "str",
              "keyword_data.keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_data.keyword_properties.keyword_difficulty": "num",
              "keyword_data.keyword_properties.detected_language": "str",
              "keyword_data.keyword_properties.is_another_language": "bool",
              "keyword_data.serp_info.check_url": "str",
              "keyword_data.serp_info.serp_item_types": "array.str",
              "keyword_data.serp_info.se_results_count": "num",
              "keyword_data.serp_info.last_updated_time": "time",
              "keyword_data.serp_info.previous_updated_time": "time",
              "intersection_result.$page.type": "str",
              "intersection_result.$page.rank_group": "num",
              "intersection_result.$page.rank_absolute": "num",
              "intersection_result.$page.position": "str",
              "intersection_result.$page.xpath": "str",
              "intersection_result.$page.title": "str",
              "intersection_result.$page.pre_snippet": "str",
              "intersection_result.$page.description": "str",
              "intersection_result.$page.breadcrumb": "str",
              "intersection_result.$page.is_image": "bool",
              "intersection_result.$page.is_video": "bool",
              "intersection_result.$page.is_featured_snippet": "bool",
              "intersection_result.$page.amp_version": "bool",
              "intersection_result.$page.is_malicious": "bool",
              "intersection_result.$page.extended_snippet": "str",
              "intersection_result.$page.domain": "str",
              "intersection_result.$page.main_domain": "str",
              "intersection_result.$page.url": "str",
              "intersection_result.$page.relative_url": "str",
              "intersection_result.$page.etv": "num",
              "intersection_result.$page.estimated_paid_traffic_cost": "num",
              "intersection_result.$page.rank_changes.previous_rank_absolute": "num",
              "intersection_result.$page.rank_changes.is_new": "bool",
              "intersection_result.$page.rank_changes.is_up": "bool",
              "intersection_result.$page.rank_changes.is_down": "bool",
              "intersection_result.$page.extra.ad_aclk": "str",
              "intersection_result.$page.description_rows": "array.str",
              "intersection_result.$page.phone": "str",
              "intersection_result.$page.is_paid": "bool",
              "intersection_result.$page.featured_title": "str"
            }
          },
          "domain_whois_overview": {
            "google": {
              "domain": "str",
              "created_datetime": "time",
              "changed_datetime": "time",
              "expiration_datetime": "time",
              "updated_datetime": "time",
              "first_seen": "time",
              "epp_status_codes": "array.str",
              "tld": "str",
              "registered": "bool",
              "registrar": "str",
              "metrics.paid.pos_1": "num",
              "metrics.paid.pos_2_3": "num",
              "metrics.paid.pos_4_10": "num",
              "metrics.paid.pos_11_20": "num",
              "metrics.paid.pos_21_30": "num",
              "metrics.paid.pos_31_40": "num",
              "metrics.paid.pos_41_50": "num",
              "metrics.paid.pos_51_60": "num",
              "metrics.paid.pos_61_70": "num",
              "metrics.paid.pos_71_80": "num",
              "metrics.paid.pos_81_90": "num",
              "metrics.paid.pos_91_100": "num",
              "metrics.paid.etv": "num",
              "metrics.paid.count": "num",
              "metrics.paid.estimated_paid_traffic_cost": "num",
              "metrics.organic.pos_1": "num",
              "metrics.organic.pos_2_3": "num",
              "metrics.organic.pos_4_10": "num",
              "metrics.organic.pos_11_20": "num",
              "metrics.organic.pos_21_30": "num",
              "metrics.organic.pos_31_40": "num",
              "metrics.organic.pos_41_50": "num",
              "metrics.organic.pos_51_60": "num",
              "metrics.organic.pos_61_70": "num",
              "metrics.organic.pos_71_80": "num",
              "metrics.organic.pos_81_90": "num",
              "metrics.organic.pos_91_100": "num",
              "metrics.organic.etv": "num",
              "metrics.organic.count": "num",
              "metrics.organic.estimated_paid_traffic_cost": "num",
              "backlinks_info.referring_domains": "num",
              "backlinks_info.referring_main_domains": "num",
              "backlinks_info.referring_pages": "num",
              "backlinks_info.dofollow": "num",
              "backlinks_info.backlinks": "num",
              "backlinks_info.time_update": "time"
            }
          },
          "top_searches": {
            "google": {
              "keyword": "str",
              "keyword_info.last_updated_time": "time",
              "keyword_info.competition": "num",
              "keyword_info.competition_level": "str",
              "keyword_info.cpc": "num",
              "keyword_info.search_volume": "num",
              "keyword_info.low_top_of_page_bid": "num",
              "keyword_info.high_top_of_page_bid": "num",
              "keyword_info.categories": "array.num",
              "keyword_info.search_volume_trend.monthly": "num",
              "keyword_info.search_volume_trend.quarterly": "num",
              "keyword_info.search_volume_trend.yearly": "num",
              "clickstream_keyword_info.search_volume": "num",
              "clickstream_keyword_info.last_updated_time": "time",
              "clickstream_keyword_info.gender_distribution.female": "num",
              "clickstream_keyword_info.gender_distribution.male": "num",
              "clickstream_keyword_info.age_distribution.18-24": "num",
              "clickstream_keyword_info.age_distribution.25-34": "num",
              "clickstream_keyword_info.age_distribution.35-44": "num",
              "clickstream_keyword_info.age_distribution.45-54": "num",
              "clickstream_keyword_info.age_distribution.55-64": "num",
              "keyword_properties.keyword_difficulty": "num",
              "keyword_properties.core_keyword": "str",
              "keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_properties.detected_language": "str",
              "keyword_properties.is_another_language": "bool",
              "serp_info.check_url": "str",
              "serp_info.se_results_count": "num",
              "serp_info.last_updated_time": "time",
              "serp_info.serp_item_types": "array.str",
              "avg_backlinks_info.backlinks": "num",
              "avg_backlinks_info.dofollow": "num",
              "avg_backlinks_info.referring_pages": "num",
              "avg_backlinks_info.referring_domains": "num",
              "avg_backlinks_info.referring_main_domains": "num",
              "avg_backlinks_info.rank": "num",
              "avg_backlinks_info.main_domain_rank": "num",
              "avg_backlinks_info.last_updated_time": "time",
              "search_intent_info.main_intent": "str",
              "search_intent_info.foreign_intent": "array.str",
              "search_intent_info.last_updated_time": "time",
              "keyword_info_normalized_with_bing.search_volume": "num",
              "keyword_info_normalized_with_bing.last_updated_time": "time",
              "keyword_info_normalized_with_bing.is_normalized": "bool",
              "keyword_info_normalized_with_clickstream.search_volume": "num",
              "keyword_info_normalized_with_clickstream.last_updated_time": "time",
              "keyword_info_normalized_with_clickstream.is_normalized": "bool"
            }
          },
          "domain_metrics_by_categories": {
            "google": {
              "top_categories": "array.num",
              "organic_etv": "num",
              "organic_count": "num",
              "organic_is_lost": "num",
              "organic_is_new": "num",
              "domain": "str",
              "metrics_history.$key.$item_type.pos_1": "num",
              "metrics_history.$key.$item_type.pos_2_3": "num",
              "metrics_history.$key.$item_type.pos_4_10": "num",
              "metrics_history.$key.$item_type.pos_11_20": "num",
              "metrics_history.$key.$item_type.pos_21_30": "num",
              "metrics_history.$key.$item_type.pos_31_40": "num",
              "metrics_history.$key.$item_type.pos_41_50": "num",
              "metrics_history.$key.$item_type.pos_51_60": "num",
              "metrics_history.$key.$item_type.pos_61_70": "num",
              "metrics_history.$key.$item_type.pos_71_80": "num",
              "metrics_history.$key.$item_type.pos_81_90": "num",
              "metrics_history.$key.$item_type.pos_91_100": "num",
              "metrics_history.$key.$item_type.etv": "num",
              "metrics_history.$key.$item_type.count": "num",
              "metrics_history.$key.$item_type.estimated_paid_traffic_cost": "num",
              "metrics_history.$key.$item_type.is_up": "num",
              "metrics_history.$key.$item_type.is_new": "num",
              "metrics_history.$key.$item_type.is_down": "num",
              "metrics_history.$key.$item_type.is_lost": "num",
              "metrics_difference.$item_type.pos_1": "num",
              "metrics_difference.$item_type.pos_2_3": "num",
              "metrics_difference.$item_type.pos_4_10": "num",
              "metrics_difference.$item_type.pos_11_20": "num",
              "metrics_difference.$item_type.pos_21_30": "num",
              "metrics_difference.$item_type.pos_31_40": "num",
              "metrics_difference.$item_type.pos_41_50": "num",
              "metrics_difference.$item_type.pos_51_60": "num",
              "metrics_difference.$item_type.pos_61_70": "num",
              "metrics_difference.$item_type.pos_71_80": "num",
              "metrics_difference.$item_type.pos_81_90": "num",
              "metrics_difference.$item_type.pos_91_100": "num",
              "metrics_difference.$item_type.etv": "num",
              "metrics_difference.$item_type.count": "num",
              "metrics_difference.$item_type.estimated_paid_traffic_cost": "num",
              "metrics_difference.$item_type.is_up": "num",
              "metrics_difference.$item_type.is_new": "num",
              "metrics_difference.$item_type.is_down": "num",
              "metrics_difference.$item_type.is_lost": "num"
            }
          },
          "keywords_for_site": {
            "google": {
              "keyword": "str",
              "keyword_info.last_updated_time": "time",
              "keyword_info.competition": "num",
              "keyword_info.competition_level": "str",
              "keyword_info.cpc": "num",
              "keyword_info.search_volume": "num",
              "keyword_info.low_top_of_page_bid": "num",
              "keyword_info.high_top_of_page_bid": "num",
              "keyword_info.categories": "array.num",
              "keyword_info.search_volume_trend.monthly": "num",
              "keyword_info.search_volume_trend.quarterly": "num",
              "keyword_info.search_volume_trend.yearly": "num",
              "clickstream_keyword_info.search_volume": "num",
              "clickstream_keyword_info.last_updated_time": "time",
              "clickstream_keyword_info.gender_distribution.female": "num",
              "clickstream_keyword_info.gender_distribution.male": "num",
              "clickstream_keyword_info.age_distribution.18-24": "num",
              "clickstream_keyword_info.age_distribution.25-34": "num",
              "clickstream_keyword_info.age_distribution.35-44": "num",
              "clickstream_keyword_info.age_distribution.45-54": "num",
              "clickstream_keyword_info.age_distribution.55-64": "num",
              "keyword_properties.core_keyword": "str",
              "keyword_properties.synonym_clustering_algorithm": "str",
              "keyword_properties.keyword_difficulty": "num",
              "keyword_properties.detected_language": "str",
              "keyword_properties.is_another_language": "bool",
              "serp_info.check_url": "str",
              "serp_info.serp_item_types": "array.str",
              "serp_info.se_results_count": "num",
              "serp_info.last_updated_time": "time",
              "avg_backlinks_info.backlinks": "num",
              "avg_backlinks_info.dofollow": "num",
              "avg_backlinks_info.referring_pages": "num",
              "avg_backlinks_info.referring_domains": "num",
              "avg_backlinks_info.referring_main_domains": "num",
              "avg_backlinks_info.rank": "num",
              "avg_backlinks_info.main_domain_rank": "num",
              "avg_backlinks_info.last_updated_time": "time",
              "search_intent_info.main_intent": "str",
              "search_intent_info.foreign_intent": "array.str",
              "search_intent_info.last_updated_time": "time",
              "keyword_info_normalized_with_bing.search_volume": "num",
              "keyword_info_normalized_with_bing.last_updated_time": "time",
              "keyword_info_normalized_with_bing.is_normalized": "bool",
              "keyword_info_normalized_with_clickstream.search_volume": "num",
              "keyword_info_normalized_with_clickstream.last_updated_time": "time",
              "keyword_info_normalized_with_clickstream.is_normalized": "bool"
            }
          },
          "product_competitors": {
            "amazon": {
              "asin": "str",
              "avg_position": "num",
              "sum_position": "num",
              "intersections": "num",
              "competitor_metrics.amazon_serp.pos_1": "num",
              "competitor_metrics.amazon_serp.pos_2_3": "num",
              "competitor_metrics.amazon_serp.pos_4_10": "num",
              "competitor_metrics.amazon_serp.pos_11_100": "num",
              "competitor_metrics.amazon_serp.count": "num",
              "competitor_metrics.amazon_serp.search_volume": "num",
              "competitor_metrics.amazon_paid.pos_1": "num",
              "competitor_metrics.amazon_paid.pos_2_3": "num",
              "competitor_metrics.amazon_paid.pos_4_10": "num",
              "competitor_metrics.amazon_paid.pos_11_100": "num",
              "competitor_metrics.amazon_paid.count": "num",
              "competitor_metrics.amazon_paid.search_volume": "num"
            }
          },
          "product_keyword_intersections": {
            "amazon": {
              "keyword_data.keyword": "str",
              "keyword_data.location_code": "num",
              "keyword_data.language_code": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.search_volume": "num",
              "intersection_result.$key.type": "str",
              "intersection_result.$key.rank_group": "num",
              "intersection_result.$key.rank_absolute": "num",
              "intersection_result.$key.xpath": "str",
              "intersection_result.$key.domain": "str",
              "intersection_result.$key.title": "str",
              "intersection_result.$key.url": "str",
              "intersection_result.$key.asin": "str",
              "intersection_result.$key.image_url": "str",
              "intersection_result.$key.price_from": "num",
              "intersection_result.$key.price_to": "num",
              "intersection_result.$key.currency": "str",
              "intersection_result.$key.is_best_seller": "bool",
              "intersection_result.$key.is_amazon_choice": "bool",
              "intersection_result.$key.rating.rating_type": "str",
              "intersection_result.$key.rating.value": "num",
              "intersection_result.$key.rating.votes_count": "num",
              "intersection_result.$key.rating.rating_max": "num",
              "intersection_result.$key.delivery_info.delivery_message": "str",
              "intersection_result.$key.delivery_info.delivery_price.current": "num",
              "intersection_result.$key.delivery_info.delivery_price.regular": "num",
              "intersection_result.$key.delivery_info.delivery_price.max_value": "num",
              "intersection_result.$key.delivery_info.delivery_price.currency": "str",
              "intersection_result.$key.delivery_info.delivery_price.is_price_range": "bool",
              "intersection_result.$key.delivery_info.delivery_price.displayed_price": "str"
            }
          },
          "app_intersection": {
            "google": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.search_volume": "num",
              "keyword_data.serp_info.check_url": "str",
              "keyword_data.serp_info.se_results_count": "num",
              "keyword_data.serp_info.last_updated_time": "time",
              "keyword_data.serp_info.previous_updated_time": "time",
              "intersection_result.$app_id.type": "str",
              "intersection_result.$app_id.rank_group": "num",
              "intersection_result.$app_id.rank_absolute": "num",
              "intersection_result.$app_id.position": "str",
              "intersection_result.$app_id.title": "str",
              "intersection_result.$app_id.url": "str",
              "intersection_result.$app_id.reviews_count": "num",
              "intersection_result.$app_id.is_free": "bool",
              "intersection_result.$app_id.developer": "str",
              "intersection_result.$app_id.developer_url": "str"
            },
            "apple": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.search_volume": "num",
              "keyword_data.serp_info.check_url": "str",
              "keyword_data.serp_info.se_results_count": "num",
              "keyword_data.serp_info.last_updated_time": "time",
              "keyword_data.serp_info.previous_updated_time": "time",
              "intersection_result.$app_id.type": "str",
              "intersection_result.$app_id.rank_group": "num",
              "intersection_result.$app_id.rank_absolute": "num",
              "intersection_result.$app_id.position": "str",
              "intersection_result.$app_id.title": "str",
              "intersection_result.$app_id.url": "str",
              "intersection_result.$app_id.reviews_count": "num",
              "intersection_result.$app_id.is_free": "bool"
            }
          },
          "app_competitors": {
            "google": {
              "app_id": "str",
              "avg_position": "num",
              "sum_position": "num",
              "intersections": "num",
              "competitor_metrics.google_play_search_organic.pos_1": "num",
              "competitor_metrics.google_play_search_organic.pos_2_3": "num",
              "competitor_metrics.google_play_search_organic.pos_4_10": "num",
              "competitor_metrics.google_play_search_organic.pos_11_100": "num",
              "competitor_metrics.google_play_search_organic.count": "num",
              "competitor_metrics.google_play_search_organic.search_volume": "num"
            },
            "apple": {
              "app_id": "str",
              "avg_position": "num",
              "sum_position": "num",
              "intersections": "num",
              "competitor_metrics.app_store_search_organic.pos_1": "num",
              "competitor_metrics.app_store_search_organic.pos_2_3": "num",
              "competitor_metrics.app_store_search_organic.pos_4_10": "num",
              "competitor_metrics.app_store_search_organic.pos_11_100": "num",
              "competitor_metrics.app_store_search_organic.count": "num",
              "competitor_metrics.app_store_search_organic.search_volume": "num"
            }
          },
          "keywords_for_app": {
            "google": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.search_volume": "num",
              "ranked_serp_element.check_url": "str",
              "ranked_serp_element.se_results_count": "num",
              "ranked_serp_element.last_updated_time": "time",
              "ranked_serp_element.previous_updated_time": "time",
              "ranked_serp_element.serp_item.type": "str",
              "ranked_serp_element.serp_item.rank_group": "num",
              "ranked_serp_element.serp_item.rank_absolute": "num",
              "ranked_serp_element.serp_item.position": "str",
              "ranked_serp_element.serp_item.title": "str",
              "ranked_serp_element.serp_item.url": "str",
              "ranked_serp_element.serp_item.reviews_count": "num",
              "ranked_serp_element.serp_item.is_free": "bool",
              "ranked_serp_element.serp_item.developer": "str",
              "ranked_serp_element.serp_item.developer_url": "str"
            },
            "apple": {
              "keyword_data.keyword": "str",
              "keyword_data.keyword_info.last_updated_time": "time",
              "keyword_data.keyword_info.search_volume": "num",
              "ranked_serp_element.check_url": "str",
              "ranked_serp_element.se_results_count": "num",
              "ranked_serp_element.last_updated_time": "time",
              "ranked_serp_element.previous_updated_time": "time",
              "ranked_serp_element.serp_item.type": "str",
              "ranked_serp_element.serp_item.rank_group": "num",
              "ranked_serp_element.serp_item.rank_absolute": "num",
              "ranked_serp_element.serp_item.position": "str",
              "ranked_serp_element.serp_item.title": "str",
              "ranked_serp_element.serp_item.url": "str",
              "ranked_serp_element.serp_item.reviews_count": "num",
              "ranked_serp_element.serp_item.is_free": "bool"
            }
          },
          "database_rows_count": {
            "keyword": "str",
            "keyword_info.last_updated_time": "time",
            "keyword_info.competition": "num",
            "keyword_info.competition_level": "str",
            "keyword_info.cpc": "num",
            "keyword_info.search_volume": "num",
            "keyword_info.categories": "array.num",
            "keyword_properties.core_keyword": "str",
            "keyword_properties.synonym_clustering_algorithm": "str",
            "bing_keyword_info.last_updated_time": "time",
            "bing_keyword_info.search_volume": "num",
            "serp_info.check_url": "str",
            "serp_info.se_results_count": "num",
            "serp_info.last_updated_time": "time",
            "serp_info.serp_item_types": "array.str",
            "serp_info.keyword_difficulty": "num",
            "keyword_properties.keyword_difficulty": "num",
            "keyword_properties.detected_language": "str",
            "keyword_properties.is_another_language": "bool"
          }
        }
      ]
    }
  ]
}


## Live Google Organic SERP Advanced

‌

### **Note:** the default value for the `depth` parameter has been updated from 100 to 10. Corresponding pricing changes are already in effect. [Full details >>](https://dataforseo.com/update/organic-serp-api-pricing-changes-now-in-effect)

Live SERP provides real-time data on top search engine results for the specified keyword, search engine, and location. This endpoint will supply a complete overview of featured snippets and other extra elements of SERPs.

> Instead of ‘login’ and ‘password’ use your credentials from https://app.dataforseo.com/api-access

```

Plain text

Copy to clipboard

Open code in new window

EnlighterJS 3 Syntax Highlighter

# Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access \

login="login"

password="password"

cred="$(printf ${login}:${password} | base64)"

curl --location --request POST "https://api.dataforseo.com/v3/serp/google/organic/live/advanced" \

--header "Authorization: Basic ${cred}"  \

--header "Content-Type: application/json" \

--data-raw '[\
\
  {\
\
      "language_code": "en",\
\
      "location_code": 2840,\
\
      "keyword": "albert einstein",\
\
      "calculate_rectangles": true\
\
  }\
\
]'
# Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access \
login="login"
password="password"
cred="$(printf ${login}:${password} | base64)"
curl --location --request POST "https://api.dataforseo.com/v3/serp/google/organic/live/advanced" \
--header "Authorization: Basic ${cred}"  \
--header "Content-Type: application/json" \
--data-raw '[\
  {\
      "language_code": "en",\
      "location_code": 2840,\
      "keyword": "albert einstein",\
      "calculate_rectangles": true\
  }\
]'
# Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access \
login="login"
password="password"
cred="$(printf ${login}:${password} | base64)"
curl --location --request POST "https://api.dataforseo.com/v3/serp/google/organic/live/advanced" \
--header "Authorization: Basic ${cred}"  \
--header "Content-Type: application/json" \
--data-raw '[\
  {\
      "language_code": "en",\
      "location_code": 2840,\
      "keyword": "albert einstein",\
      "calculate_rectangles": true\
  }\
]'

```

```
All POST data should be sent in the [JSON](https://en.wikipedia.org/wiki/JSON) format (UTF-8 encoding). When setting a task, you should send all task parameters in the task array of the generic POST array. You can send up to 2000 API calls per minute, each Live SERP API call can contain only one task.\
\
Below you will find a detailed description of the fields you can use for setting a task.\
\
**Description of the fields for setting a task:**\
\
| Field name | Type | Description |\
| --- | --- | --- |\
| `keyword` | string | _keyword_<br>**required field**<br>you can specify **up to 700 characters** in the `keyword` field<br>all %## will be decoded (plus character ‘+’ will be decoded to a space character)<br>if you need to use the “%” character for your `keyword`, please specify it as “%25”;<br>if you need to use the “+” character for your `keyword`, please specify it as “%2B”;<br>if this field contains such parameters as _‘allinanchor:’, ‘allintext:’, ‘allintitle:’, ‘allinurl:’, ‘define:’, ‘definition:’, ‘filetype:’, ‘id:’, ‘inanchor:’, ‘info:’, ‘intext:’, ‘intitle:’, ‘inurl:’, ‘link:’, ‘site:’_, **the charge per task will be multiplied by 5**<br>**Note:** queries containing the ‘cache:’ parameter are not supported and will return a validation error<br>learn more about rules and limitations of `keyword` and `keywords` fields in DataForSEO APIs in this [Help Center article](https://dataforseo.com/help-center/rules-and-limitations-of-keyword-and-keywords-fields-in-dataforseo-apis) |  |\
| `url` | string | _direct URL of the search query_<br>optional field<br>you can specify a direct URL and we will sort it out to the necessary fields. Note that this method is the most difficult for our API to process and also requires you to specify the exact language and location in the URL. In most cases, we wouldn’t recommend using this method.<br>example:<br>`https://www.google.co.uk/search?q=%20rank%20tracker%20api&hl=en&gl=GB&uule=w+CAIQIFISCXXeIa8LoNhHEZkq1d1aOpZS` |  |\
| `depth` | integer | _parsing depth_<br>optional field<br>number of results in SERP<br>**default value: `10`**<br>max value: `700`<br>**Note:** your account will be billed per each SERP containing up to 10 results;<br>thus, setting a depth above `10` may result in additional charges if the search engine returns more than 10 results;<br>if the specified depth is higher than the number of results in the response, the difference will be refunded automatically to your account balance |  |\
| `max_crawl_pages` | integer | _page crawl limit_<br>optional field<br>number of search results pages to crawl<br>max value: `100`<br>**Note:** you will be charged for each page crawled (10 organic results per page);<br>learn more about pricing on our [Pricing](https://dataforseo.com/pricing/serp/google-organic-serp-api) page;<br>**Note#2:** the `max_crawl_pages` and `depth` parameters complement each other;<br>learn more at [our help center](https://dataforseo.com/help-center/what-is-max-crawl-pages-and-how-does-it-work) |  |\
| `location_name` | string | _full name of search engine location_<br>**required field if you don’t specify** `location_code` or `location_coordinate`<br>**if you use this field, you don’t need to specify `location_code` or `location_coordinate`**<br>you can receive the list of available locations of the search engine with their `location_name` by making a separate request to the `https://api.dataforseo.com/v3/serp/google/locations`<br>example:<br>`London,England,United Kingdom` |  |\
| `location_code` | integer | _search engine location code_<br>**required field if you don’t specify** `location_name` or `location_coordinate`<br>**if you use this field, you don’t need to specify `location_name` or `location_coordinate`**<br>you can receive the list of available locations of the search engines with their `location_code` by making a separate request to the `https://api.dataforseo.com/v3/serp/google/locations`<br>example:<br>`2840` |  |\
| `location_coordinate` | string | _GPS coordinates of a location_<br>optional field if you specify `location_name` or `location_code`<br>**if you use this field, you don’t need to specify `location_name` or `location_code`**<br>`location_coordinate` parameter should be specified in the _“latitude,longitude,radius”_ format<br>the maximum number of decimal digits for _“latitude”_ and _“longitude”_: 7<br>the minimum value for _“radius”_: 199.9 (mm)<br>the maximum value for _“radius”_: 199999 (mm)<br>example:<br>`53.476225,-2.243572,200` |  |\
| `language_name` | string | _full name of search engine language_<br>optional field if you specify `language_code`<br>**if you use this field, you don’t need to specify `language_code`**<br>you can receive the list of available languages of the search engine with their `language_name` by making a separate request to the `https://api.dataforseo.com/v3/serp/google/languages`<br>example:<br>`English` |  |\
| `language_code` | string | _search engine language code_<br>optional field if you specify `language_name`<br>**if you use this field, you don’t need to specify `language_name`**<br>you can receive the list of available languages of the search engine with their `language_code` by making a separate request to the `https://api.dataforseo.com/v3/serp/google/languages` example: `en` |  |\
| `se_domain` | string | _search engine domain_<br>optional field<br>we choose the relevant search engine domain automatically according to the location and language you specify<br>however, you can set a custom search engine domain in this field<br>example:<br>_google.co.uk_, `google.com.au`, `google.de`, etc. |  |\
| `device` | string | _device type_<br>optional field<br>can take the values: `desktop`, `mobile`<br>default value: `desktop` |  |\
| `os` | string | _device operating system_<br>optional field<br>if you specify `desktop` in the `device` field, choose from the following values: `windows`, `macos`<br>default value: `windows`<br>if you specify `mobile` in the `device` field, choose from the following values: `android`, `ios`<br>default value: `android` |  |\
| `target` | string | _target domain, subdomain, or webpage to get results for_<br>optional field<br>a domain or a subdomain should be specified without `https://` and `www.`<br>note that the results of `target`-specific tasks will only include SERP elements that contain a `url` string;<br>you can also use a wildcard (‘\*’) character to specify the search pattern in SERP and narrow down the results;<br>examples:<br>**`example.com`** – returns results for the website’s home page with URLs, such as `https://example.com`, or `https://www.example.com/`, or `https://example.com/`;<br>**`example.com*`** – returns results for the domain, including all its pages;<br>**`*example.com*`** – returns results for the entire domain, including all its pages and subdomains;<br>**`*example.com`** – returns results for the home page regardless of the subdomain, such as `https://en.example.com`;<br>**`example.com/example-page`** – returns results for the exact URL;<br>**`example.com/example-page*`** – returns results for all domain’s URLs that start with the specified string |  |\
| `group_organic_results` | boolean | _display related results_<br>optional field<br>if set to `true`, the `related_result` element in the response will be provided as a snippet of its parent organic result;<br>if set to `false`, the `related_result` element will be provided as a separate organic result;<br>default value: `true` |  |\
| `calculate_rectangles` | boolean | _calcualte pixel rankings for SERP elements in advanced results_<br>optional field<br>pixel ranking refers to the distance between the result snippet and top left corner of the screen;<br>[Visit Help Center to learn more>>](https://dataforseo.com/help-center/pixel-ranking-in-serp-api)<br>by default, the parameter is set to `false`;<br>**Note:** you will be charged extra $0.002 for using this parameter |  |\
| `browser_screen_width` | integer | _browser screen width_<br>optional field<br>you can set a custom browser screen width to calculate pixel rankings for a particular device;<br>by default, the parameter is set to:<br>`1920` for `desktop`;<br>`360` for `mobile` on `android`;<br>`375` for `mobile` on `iOS`;<br>**Note:** to use this parameter, set `calculate_rectangles` to `true` |  |\
| `browser_screen_height` | integer | _browser screen height_<br>optional field<br>you can set a custom browser screen height to calculate pixel rankings for a particular device;<br>by default, the parameter is set to:<br>`1080` for `desktop`;<br>`640` for `mobile` on `android`;<br>`812` for `mobile` on `iOS`;<br>**Note:** to use this parameter, set `calculate_rectangles` to `true` |  |\
| `browser_screen_resolution_ratio` | integer | _browser screen resolution ratio_<br>optional field<br>you can set a custom browser screen resolution ratio to calculate pixel rankings for a particular device;<br>possible values: from `1` to `3`;<br>by default, the parameter is set to:<br>`1` for `desktop`;<br>`3` for `mobile` on `android`;<br>`3` for `mobile` on `iOS`;<br>**Note:** to use this parameter, set `calculate_rectangles` to `true` |  |\
| `people_also_ask_click_depth` | integer | _clicks on the corresponding element_<br>optional field<br>specify the click depth on the `people_also_ask` element to get additional `people_also_ask_element` items;<br>**Note** your account will be billed $0.00015 extra for each click;<br>if the element is absent or we perform fewer clicks than you specified, all extra charges will be returned to your account balance<br>possible values: from `1` to `4` |  |\
| `load_async_ai_overview` | boolean | _load asynchronous ai overview_<br>optional field<br>set to `true` to obtain `ai_overview` items is SERPs even if they are loaded asynchronically;<br>if set to `false`, you will only obtain `ai_overview` items from cache;<br>default value: `false`<br>**Note:** you will be charged extra $0.002 for using this parameter;<br>if the element is absent or contains `"asynchronous_ai_overview": false`, all extra charges will be returned to your account balance |  |\
| `search_param` | string | _additional parameters of the search query_<br>optional field<br>[get the list of available parameters and additional details here](https://dataforseo.com/help-center/google-search-engine-parameters-and-how-to-use-them) |  |\
| `remove_from_url` | array | _remove specific parameters from URLs_<br>optional field<br>using this field, you can specify up to 10 parameters to remove from URLs in the result<br>example:<br>`"remove_from_url": ["srsltid"]`<br>**Note:** if the `target` field is specified, the specified URL parameters will be removed before the search |  |\
| `tag` | string | _user-defined task identifier_<br>optional field<br>_the character limit is 255_<br>you can use this parameter to identify the task and match it with the result<br>you will find the specified `tag` value in the `data` object of the response |  |\
\
‌‌‌\
\
As a response of the API server, you will receive [JSON](https://en.wikipedia.org/wiki/JSON)-encoded data containing a `tasks` array with the information specific to the set tasks.\
\
**Description of the fields in the results array:**\
\
| Field name | Type | Description |\
| --- | --- | --- |\
| `version` | string | _the current version of the API_ |  |\
| `status_code` | integer | _general status code_<br>you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors)<br>**Note:** we strongly recommend designing a necessary system for handling related exceptional or error conditions |  |\
| `status_message` | string | _general informational message_<br>you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix/errors) |  |\
| `time` | string | _execution time, seconds_ |  |\
| `cost` | float | _total tasks cost, USD_ |  |\
| `tasks_count` | integer | _the number of tasks in the **`tasks`** array_ |  |\
| `tasks_error` | integer | _the number of tasks in the **`tasks`** array returned with an error_ |  |\
| **`tasks`** | array | _array of tasks_ |  |\
| `id` | string | _task identifier_<br>**unique task identifier in our system in the [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier) format** |  |\
| `status_code` | integer | _status code of the task_<br>generated by DataForSEO; can be within the following range: 10000-60000<br>you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors) |  |\
| `status_message` | string | _informational message of the task_<br>you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix/errors) |  |\
| `time` | string | _execution time, seconds_ |  |\
| `cost` | float | _cost of the task, USD_ |  |\
| `result_count` | integer | _number of elements in the `result` array_ |  |\
| `path` | array | _URL path_ |  |\
| `data` | object | _contains the same parameters that you specified in the POST request_ |  |\
| **`result`** | array | _array of results_ |  |\
| `keyword` | string | _keyword received in a POST array_ **the keyword is returned with decoded %## (plus character ‘+’ will be decoded to a space character)** |  |\
| `type` | string | _search engine type in a POST array_ |  |\
| `se_domain` | string | _search engine domain in a POST array_ |  |\
| `location_code` | integer | _location code in a POST array_ |  |\
| `language_code` | string | _language code in a POST array_ |  |\
| `check_url` | string | _direct URL to search engine results_<br>you can use it to make sure that we provided accurate results |  |\
| `datetime` | string | _date and time when the result was received_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `spell` | object | _autocorrection of the search engine_<br>if the search engine provided results for a keyword that was corrected, we will specify the keyword corrected by the search engine and the type of autocorrection |  |\
| `keyword` | string | _keyword obtained as a result of search engine autocorrection_<br>the results will be provided for the corrected keyword |  |\
| `type` | string | _type of autocorrection_<br>possible values:<br>`did_you_mean`, `showing_results_for`, `no_results_found_for`, `including_results_for` |  |\
| `refinement_chips` | object | _search refinement chips_ |  |\
| `type` | string | _type of element = **‘refinement\_chips’**_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _items of the element_ |  |\
| `type` | string | _type of element = **‘refinement\_chips\_element’**_ |  |\
| `title` | string | _title of the element_ |  |\
| `url` | string | _search URL with refinement parameters_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `options` | array | _further search refinement options_ |  |\
| `type` | string | _type of element = **‘refinement\_chips\_option’**_ |  |\
| `title` | string | _title of the element_ |  |\
| `url` | string | _search URL with refinement parameters_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `item_types` | array | _types of search results in SERP_<br>contains types of search results ( `items`) found in SERP.<br>possible item types:<br>[`answer_box`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#answer_box), [`app`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#app), [`carousel`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#carousel), [`multi_carousel`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#multi_carousel), [`featured_snippet`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#featured_snippet), [`google_flights`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#google_flights), [`google_reviews`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#google_reviews), [`third_party_reviews`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#third_party_reviews), [`google_posts`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#google_posts), [`images`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#images), [`jobs`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#jobs), [`knowledge_graph`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#knowledge_graph), [`local_pack`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#local_pack), [`hotels_pack`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#hotels_pack), [`map`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#map), [`organic`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#organic), [`paid`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#paid), [`people_also_ask`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#people_also_ask), [`related_searches`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#related_searches), [`people_also_search`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#people_also_search), [`shopping`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#shopping), [`top_stories`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#top_stories), [`twitter`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#twitter), [`video`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#video), [`events`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#events), [`mention_carousel`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#mention_carousel), [`recipes`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#recipes), [`top_sights`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#top_sights), [`scholarly_articles`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#scholarly_articles), [`popular_products`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#popular_products), [`podcasts`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#podcasts), [`questions_and_answers`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#questions_and_answers), [`find_results_on`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#find_results_on), [`stocks_box`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#stocks_box), [`visual_stories`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#visual_stories), [`commercial_units`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#commercial_units), [`local_services`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#local_services), [`google_hotels`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#google_hotels), [`math_solver`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#math_solver), [`currency_box`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#currency_box), [`product_considerations`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#product_considerations), [`found_on_web`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#found_on_web), [`short_videos`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#short_videos), [`refine_products`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#refine_products), [`explore_brands`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#explore_brands), [`perspectives`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#perspectives), [`discussions_and_forums`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#discussions_and_forums), [`compare_sites`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#compare_sites), [`courses`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#courses), [`ai_overview`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#ai_overview) |  |\
| `se_results_count` | integer | _total number of results in SERP_ |  |\
| `pages_count` | integer | _total search results pages retrieved_<br>total number of retrieved SERPs in the result |  |\
| `items_count` | integer | _the number of results returned in the **`items`** array_ |  |\
| **`items`** | array | _elements of search results found in SERP_ |  |\
| **[‘organic’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-organic-1.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘organic’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _relevant URL in SERP_ |  |\
| `cache_url` | string | _cached version of the page_ |  |\
| `related_search_url` | string | _URL to a similar search_<br>URL to a new search for the same keyword(s) [on related sites](https://support.google.com/websearch/answer/2466433?hl=en#:~:text=Search%20for%20related%20sites) |  |\
| `breadcrumb` | string | _breadcrumb in SERP_ |  |\
| `website_name` | string | _name of the website in SERP_ |  |\
| `is_image` | boolean | _indicates whether the element contains an `image`_ |  |\
| `is_video` | boolean | _indicates whether the element contains a `video`_ |  |\
| `is_featured_snippet` | boolean | _indicates whether the element is a `featured_snippet`_ |  |\
| `is_malicious` | boolean | _indicates whether the element is marked as malicious_ |  |\
| `is_web_story` | boolean | _indicates whether the element is marked as Google web story_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `pre_snippet` | string | _includes additional information appended before the result description in SERP_ |  |\
| `extended_snippet` | string | _includes additional information appended after the result description in SERP_ |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `amp_version` | boolean | _Accelerated Mobile Pages_<br>indicates whether an item has the Accelerated Mobile Page (AMP) version |  |\
| `rating` | object | _the item’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of_ _rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | float | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `price` | object | _pricing details_<br>contains the pricing details of the product or service featured in the result |  |\
| `current` | float | _current price_<br>indicates the current price of the product or service featured in the result |  |\
| `regular` | float | _regular price_<br>indicates the regular price of the product or service with no discounts applied |  |\
| `max_value` | float | _the maximum price_<br>the maximum price of the product or service as indicated in the result |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `highlighted` | array | _words highlighted in bold within the results `description`_ |  |\
| `links` | array | _sitelinks_<br>the links shown below some of Google’s search results<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `url` | string | _sitelink URL_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `faq` | object | _frequently asked questions_<br>questions and answers extension shown below some of Google’s search results<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **faq\_box**‘_ |  |\
| `items` | array | _items featured in the faq\_box_ |  |\
| `type` | string | _type of element = ‘ **faq\_box\_element**‘_ |  |\
| `title` | string | _question related to the result_ |  |\
| `description` | string | _answer provided in the drop-down block_ |  |\
| `links` | array | _links featured in the faq\_box\_element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _link anchor text_ |  |\
| `url` | string | _link URL_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `extended_people_also_search` | array | _extension of the organic element_<br>extension of the organic result containing related search queries<br>**Note:** extension appears in SERP upon clicking on the result and then bouncing back to search results |  |\
| `about_this_result` | object | _contains information from the ‘About this result’ panel_<br>[‘About this result’ panel](https://blog.google/products/search/learn-more-and-get-more-from-search/) provides additional context about why Google returned this result for the given query;<br>this feature appears after clicking on the three dots next to most results |  |\
| `type` | string | _type of element = ‘ **about\_this\_result\_element**‘_ |  |\
| `url` | string | _result’s URL_ |  |\
| `source` | string | _source of additional information about the result_ |  |\
| `source_info` | string | _additional information about the result_<br>description of the website from Wikipedia or another additional context |  |\
| `source_url` | string | _URL to full information from the `source`_ |  |\
| `language` | string | _the language of the result_ |  |\
| `location` | string | _location for which the result is relevant_ |  |\
| `search_terms` | array | _matching search terms that appear in the result_ |  |\
| `related_terms` | array | _related search terms that appear in the result_ |  |\
| `related_result` | array | _related result from the same domain_<br>related result from the same domain appears as a part of the main result snippet;<br>you can derive the `related_result` snippets as `"type": "organic"` results by setting the `group_organic_results` parameter to `false` in the POST request |  |\
| `type` | string | _type of element = ‘ **related\_result**‘_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `domain` | string | _relevant domain_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _relevant URL in SERP_ |  |\
| `cache_url` | string | _cached version of the page_ |  |\
| `related_search_url` | string | _URL to a similar search_<br>URL to a new search for the same keyword(s) [on related sites](https://support.google.com/websearch/answer/2466433?hl=en#:~:text=Search%20for%20related%20sites) |  |\
| `breadcrumb` | string | _breadcrumb in SERP_ |  |\
| `is_image` | boolean | _indicates whether the element contains an `image`_ |  |\
| `is_video` | boolean | _indicates whether the element contains a `video`_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `pre_snippet` | string | _includes additional information appended before the result description in SERP_ |  |\
| `extended_snippet` | string | _includes additional information appended after the result description in SERP_ |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `amp_version` | boolean | _Accelerated Mobile Pages_<br>indicates whether an item has the Accelerated Mobile Page (AMP) version |  |\
| `rating` | object | _the item’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of_ _rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | float | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `price` | object | _pricing details_<br>contains the pricing details of the product or service featured in the result |  |\
| `current` | float | _current price_<br>indicates the current price of the product or service featured in the result |  |\
| `regular` | float | _regular price_<br>indicates the regular price of the product or service with no discounts applied |  |\
| `max_value` | float | _the maximum price_<br>the maximum price of the product or service as indicated in the result |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `highlighted` | array | _words highlighted in bold within the results `description`_ |  |\
| `about_this_result` | object | _contains information from the ‘About this result’ panel_<br>[‘About this result’ panel](https://blog.google/products/search/learn-more-and-get-more-from-search/) provides additional context about why Google returned this result for the given query;<br>this feature appears after clicking on the three dots next to most results |  |\
| `type` | string | _type of element = ‘ **about\_this\_result\_element**‘_ |  |\
| `url` | string | _result’s URL_ |  |\
| `source` | string | _source of additional information about the result_ |  |\
| `source_info` | string | _additional information about the result_<br>description of the website from Wikipedia or another additional context |  |\
| `source_url` | string | _URL to full information from the `source`_ |  |\
| `language` | string | _the language of the result_ |  |\
| `location` | string | _location for which the result is relevant_ |  |\
| `search_terms` | array | _matching search terms that appear in the result_ |  |\
| `related_terms` | array | _related search terms that appear in the result_ |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘paid’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-paid-1.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘paid’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `domain` | string | _domain in SERP of the ad element_ |  |\
| `website_name` | string | _name of the website in the ad element_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `url` | string | _relevant URL of the Ad element in SERP_ |  |\
| `breadcrumb` | string | _breadcrumb of the Ad element in SERP_ |  |\
| `is_image` | boolean | _indicates whether the element contains an `image`_ |  |\
| `is_video` | boolean | _indicates whether the element contains a `video`_ |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `highlighted` | array | _words highlighted in bold within the results `description`_ |  |\
| `extra` | object | _additional information about the result_ |  |\
| `ad_aclk` | string | _the identifier of the ad_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `description_rows` | array | _extended description_<br>if there is none, equals `null` |  |\
| `links` | array | _sitelinks_<br>the links shown below some of Google’s search results<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of the link element_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `url` | string | _URL link_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `ad_aclk` | string | _the identifier of the ad_ |  |\
| `price` | object | _pricing details_<br>contains the pricing details of the product or service featured in the result |  |\
| `current` | float | _current price_<br>indicates the current price of the product or service featured in the result |  |\
| `regular` | float | _regular price_<br>indicates the regular price of the product or service with no discounts applied |  |\
| `max_value` | float | _the maximum price_<br>the maximum price of the product or service as indicated in the result |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `rating` | object | _the element’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of_ _rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | float | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘carousel’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-carousel-1.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘carousel’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **carousel\_element**‘_ |  |\
| `title` | string | _title of the item_ |  |\
| `subtitle` | string | _subtitle of the item_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘multi\_carousel’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/05/mobile-element-multi-carousel.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘multi\_carousel’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **multi\_carousel\_element**‘_ |  |\
| `title` | string | _title of the item_ |  |\
| `multi_carousel_snippets` | array | _`multi_carousel_snippet` results_ |  |\
| `type` | string | _type of element = ‘ **multi\_carousel\_snippet**‘_ |  |\
| `title` | string | _title of a particular item_ |  |\
| `subtitle` | string | _subtitle of the item_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘answer\_box’ element in SERP![](https://dataforseo.com/wp-content/uploads/2023/10/Answer_box-img.webp)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘answer\_box’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `text` | array | _text_<br>if there is none, equals `null` |  |\
| `links` | array | _sitelinks_<br>the links shown below some of Google’s search results<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of the link_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `url` | string | _URL link_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘related\_searches’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-related-searches.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘related\_searches’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘people\_also\_search’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-people-also-search.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘people\_also\_search’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘local\_pack’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-local-pack.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘local\_pack’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `phone` | string | _phone number_ |  |\
| `url` | string | _relevant URL_ |  |\
| `is_paid` | boolean | _indicates whether the element is an ad_ |  |\
| `rating` | object | _the item’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | float | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `cid` | string | _google-defined client id_<br>unique id of a local establishment;<br>can be used with [Google Reviews API](https://docs.dataforseo.com/v3/reviews/google/overview/?php) to get a full list of reviews |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘hotels\_pack’ element in SERP![](https://dataforseo.com/wp-content/uploads/2021/03/window-feature-hotel-1.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘hotels\_pack’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `date_from` | string | _starting date of stay_<br>in the format “year-month-date”<br>example:<br>2019-11-15 |  |\
| `date_to` | string | _ending date of stay_<br>in the format “year-month-date”<br>example:<br>2019-11-17 |  |\
| `items` | array | _contains results featured in the ‘hotels\_pack’ element of SERP_ |  |\
| `type` | string | _type of element = **‘hotels\_pack\_element’**_ |  |\
| `price` | object | _price of booking a place for the specified dates of stay_ |  |\
| `current` | float | _current price_<br>indicates the current price of booking a place for the specified dates of stay |  |\
| `regular` | float | _regular price_<br>indicates the regular price of booking a place for the specified dates of stay |  |\
| `max_value` | float | _the maximum price_<br>the maximum price of booking a place for the specified dates of stay |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `title` | string | _title of the place_ |  |\
| `desription` | string | _description of the place in SERP_ |  |\
| `hotel_identifier` | string | _unique hotel identifier_<br>unique hotel identifier assigned by Google;<br>example: `"CgoIjaeSlI6CnNpVEAE"` |  |\
| `domain` | string | _domain in SERP_ |  |\
| `url` | string | _relevant URL_ |  |\
| `is_paid` | boolean | _indicates whether the element is an ad_ |  |\
| `rating` | object | _the item’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | float | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘knowledge\_graph’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-Knowledge-Graph-2.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  | **note** that `knowledge_graph` items in `mobile` results may be separated by other elements;<br>in such cases, the API response returns several `knowledge_graph` elements, each containing distinct items, and each placed according to the items’ placement in SERP |  |\
| `type` | string | _type of element = **‘knowledge\_graph’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `subtitle` | string | _subtitle of the item_ |  |\
| `description` | string | _description_ |  |\
| `card_id` | string | _card id_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image from knowledge graph_ |  |\
| `logo_url` | string | _URL of the logo from knowledge graph_ |  |\
| `cid` | string | _google-defined client id_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_images\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `link` | object | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _contains arrays of specific images_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_images\_element**‘_ |  |\
| `url` | string | _image source URL_ |  |\
| `domain` | string | _website domain_ |  |\
| `alt` | string | _alt attribute_ |  |\
| `image_url` | string | _URL of a specific image_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_list\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the element_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`action:listen_artist` |  |\
| `link` | object | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _contains arrays of elements available in the list_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_list\_element**‘_ |  |\
| `title` | string | _title of the element_ |  |\
| `subtitle` | string | _subtitle of the element_ |  |\
| `url` | string | _URL of element_ |  |\
| `domain` | string | _website domain_ |  |\
| `image_url` | string | _URL of the image_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_ai\_overview\_item‘**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `asynchronous_ai_overview` | boolean | _indicates whether the element is loaded asynchronically_<br>if `true`, the `ai_overview` element is loaded asynchronically;<br>if `false`, the `ai_overview` element is loaded from cache; |  |\
| `items` | array | _items present in the element_ |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_element**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `title` | string | _title of the element_ |  |\
| `text` | string | _text or description of the element in SERP_ |  |\
| `markdown` | string | _content of the element in markdown format_ |  |\
| `links` | array | _website links featured in the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _link anchor text_ |  |\
| `description` | string | _link description_ |  |\
| `url` | string | _link URL_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `images` | array | _images of the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `references` | array | _references relevant to the element_<br>includes references to webpages that were used to generate the `ai_overview_element` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_reference**‘_ |  |\
| `source` | string | _reference source name or title_ |  |\
| `domain` | string | _domain name of the reference_ |  |\
| `url` | string | _reference page URL_ |  |\
| `title` | string | _reference page title_ |  |\
| `text` | string | _reference text_<br>text snippet from the page that was used to generate the `ai_overview_element` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_video\_element**‘_ |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `title` | string | _title of the element_ |  |\
| `snippet` | string | _additional information for the video_ |  |\
| `url` | string | _URL of the link to the video_ |  |\
| `domain` | string | _domain of the website hosting the video_ |  |\
| `image_url` | string | _URL to the image thumbnail of the video_ |  |\
| `source` | string | _name of the source of the video_ |  |\
| `date` | string | _date when the video was published or indexed_<br>example:<br>`Apr 26, 2024` |  |\
| `timestamp` | string | _date and time when the video was published or indexed_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_table\_element**‘_ |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `markdown` | string | _content of the element in markdown format_ |  |\
| `table` | object | _table present in the element_<br>the header and content of the table present in the element |  |\
| `table_header` | array | _content in the header of the table_ |  |\
| `table_content` | array | _array of contents of the table present in the element_<br>each array represents the table row |  |\
| `references` | array | _references relevant to the element_<br>includes references to webpages that were used to generate the `ai_overview_element` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_reference**‘_ |  |\
| `source` | string | _reference source name or title_ |  |\
| `domain` | string | _domain name of the reference_ |  |\
| `url` | string | _reference page URL_ |  |\
| `title` | string | _reference page title_ |  |\
| `text` | string | _reference text_<br>text snippet from the page that was used to generate the `ai_overview_element` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_expanded\_element**‘_ |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `title` | string | _title of the element in SERP_ |  |\
| `text` | string | _additional text of the element in SERP_ |  |\
| `components` | array | _array of components of the element_ |  |\
| `type` | string | _type of component = ‘ **ai\_overview\_expanded\_component**‘_ |  |\
| `title` | string | _title of the element in SERP_ |  |\
| `text` | string | _text of the component_ |  |\
| `markdown` | string | _text of the component in the markdwon format_ |  |\
| `images` | array | _images of the component_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `links` | array | _sitelinks_<br>the links shown below some of Google’s search results<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of the link_ |  |\
| `description` | string | _description of the link_ |  |\
| `url` | string | _URL in link_ |  |\
| `domain` | string | _domain in link_ |  |\
| `references` | array | _additional references relevant to the item_<br>includes references to webpages that may have been used to generate the `ai_overview` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_reference**‘_ |  |\
| `source` | string | _reference source name or title_ |  |\
| `domain` | string | _domain name of the reference_ |  |\
| `url` | string | _reference page URL_ |  |\
| `title` | string | _reference page title_ |  |\
| `text` | string | _reference text_<br>text snippet from the page that was used to generate the `ai_overview_element` |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_description\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `text` | string | _description content_ |  |\
| `links` | array | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_row\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the row_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`ss:/webfacts:net_worth` |  |\
| `text` | string | _row content_ |  |\
| `links` | array | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_carousel\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the carousel item_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`kc:/common/topic:social media presence` |  |\
| `link` | object | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _contains arrays of elements available in the list_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_carousel\_element**‘_ |  |\
| `title` | string | _title of the element_ |  |\
| `subtitle` | string | _subtitle of the element_ |  |\
| `url` | string | _URL of element_ |  |\
| `domain` | string | _website domain_ |  |\
| `image_url` | string | _URL of the image_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_part\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the item_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`kc:/local:place qa` |  |\
| `text` | string | _content within the item_ |  |\
| `links` | array | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_expanded\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the item_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`kc:/local:place qa` |  |\
| `expanded_element` | array | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_expanded\_element**‘_ |  |\
| `featured_title` | string | _title of a given element_ |  |\
| `url` | string | _source URL_ |  |\
| `domain` | string | _source domain_ |  |\
| `title` | string | _source title_ |  |\
| `snippet` | string | _text alongside the title_ |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `table` | object | _table element_ |  |\
| `table_element` | string | _name assigned to the table element_<br>possible values:<br>`table_element` |  |\
| `table_header` | array | _column names_ |  |\
| `table_content` | array | _the content of the table_<br>one line of the table in this element of the array |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_shopping\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the item_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`kc:/shopping/gpc:organic-offers` |  |\
| `items` | array | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_shopping\_element**‘_ |  |\
| `title` | string | _title of a given shopping element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain in url_ |  |\
| `price` | object | _price indicated in the element_ |  |\
| `current` | float | _current price_<br>refers to the current price indicated in the element |  |\
| `regular` | float | _regular price_<br>refers to the regular price indicated in the element |  |\
| `max_value` | float | _the maximum price_<br>refers to the maximum price indicated in the element |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `source` | string | _web source of the shopping element_<br>indicates the source of information included in the element |  |\
| `snippet` | string | _description of the shopping element_ |  |\
| `marketplace` | string | _merchant account provider_<br>ecommerce site that hosts products or websites of individual sellers under the same merchant account<br>example:<br>`by Google` |  |\
| `marketplace_url` | string | _URL to the merchant account provider_<br>ecommerce site that hosts products or websites of individual sellers under the same merchant account |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_hotels\_booking\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the element_ |  |\
| `date_from` | string | _starting date of stay_<br>in the format “year-month-date”<br>example:<br>2019-11-15 |  |\
| `date_to` | string | _ending date of stay_<br>in the format “year-month-date”<br>example:<br>2019-11-17 |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`kc:/local:hotel booking` |  |\
| `items` | array | _contains arrays of elements available in the list_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_hotels\_booking\_element**‘_ |  |\
| `source` | string | _web source of the hotel booking element_<br>indicates the source of information included in the element |  |\
| `description` | string | _description of the hotel booking element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain in the URL_ |  |\
| `price` | object | _price indicated in the element_ |  |\
| `current` | float | _current price_<br>refers to the current price indicated in the element |  |\
| `regular` | float | _regular price_<br>refers to the regular price indicated in the element |  |\
| `max_value` | float | _the maximum price_<br>refers to the maximum price indicated in the element |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `is_paid` | boolean | _indicates whether the element is an ad_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘featured\_snippet’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-featured_snippet.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘featured\_snippet’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `featured_title` | string | _the title of the featured snippets source page_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `url` | string | _relevant URL_ |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `table` | object | _results table_<br>if there are none, equals `null` |  |\
| `table_header` | array | _column names_ |  |\
| `table_content` | array | _the content of the table_<br>one line of the table in this element of the array |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘top\_stories’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-top-stories.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘top\_stories’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the element in SERP_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **top\_stories\_element**‘_ |  |\
| `source` | string | _source of the element_<br>indicates the source of information included in the `top_stories_element` |  |\
| `domain` | string | _domain in SERP_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `date` | string | _the date when the page source of the element was published_ |  |\
| `amp_version` | boolean | _Accelerated Mobile Pages_<br>indicates whether an item has the Accelerated Mobile Page (AMP) version |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `url` | string | _URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `badges` | array | _badges relevant to the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘twitter’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-twitters.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘twitter’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _URL_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **twitter** **\_element**‘_ |  |\
| `tweet` | string | _tweet message_ |  |\
| `date` | string | _the posting date_ |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `url` | string | _URL_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘map’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-map.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘map’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _URL_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘google\_flights’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-google-flights.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘google\_flights’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _URL_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **google\_flights\_element**‘_ |  |\
| `description` | string | _description_ |  |\
| `url` | string | _URL_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘google\_reviews’ element in SERP![](https://dataforseo.com/wp-content/uploads/2021/03/desktop-element-google_reviews.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘google\_reviews’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `reviews_count` | integer | _the number of reviews_ |  |\
| `rating` | object | _the element’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of_ _rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | integer | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `place_id` | string | _the identifier of a place_ |  |\
| `feature` | string | _the additional feature of the review_ |  |\
| `cid` | string | _google-defined client id_<br>unique id of a local establishment;<br>can be used with [Google Reviews API](https://docs.dataforseo.com/v3/reviews/google/overview/?php) to get a full list of reviews |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘third\_party\_reviews’ element in SERP![](https://docs.dataforseo.com/wp-content/uploads/2025/02/external_reviews.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘third\_party\_reviews’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `reviews_count` | integer | _the number of reviews_ |  |\
| `title` | string | _name of the third-party review source_ |  |\
| `url` | string | _URL of the third-party review source_ |  |\
| `rating` | object | _the element’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of_ _rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | integer | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘google\_posts’ element in SERP![](https://dataforseo.com/wp-content/uploads/2021/03/desktop-element-google_posts.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘google\_posts’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `posts_id` | string | _the identifier of the google\_posts feature_ |  |\
| `feature` | string | _the additional feature of the review_ |  |\
| `cid` | string | _google-defined client id_<br>unique id of a local establishment;<br>can be used with [Google Reviews API](https://docs.dataforseo.com/v3/reviews/google/overview/?php) to get a full list of reviews |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘video’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-video.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘video’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **video\_element**‘_ |  |\
| `source` | string | _source of the element_<br>indicates the source of the video |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `url` | string | _URL_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘app’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/05/mobile-element-app.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘app’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **app\_element**‘_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _URL_ |  |\
| `price` | object | _price of the app element_ |  |\
| `current` | float | _current price_<br>refers to the current price indicated in the app element |  |\
| `regular` | float | _regular price_<br>refers to the regular price indicated in the app element |  |\
| `max_value` | float | _the maximum price_<br>refers to the maximum price indicated in the app element |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘people\_also\_ask’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-paa.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘people\_also\_ask’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **people\_also\_ask\_element**‘_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `seed_question` | string | _question that triggered additional expanded elements_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `expanded_element` | array | _expanded element_ |  |\
| `type` | string | _type of element = ‘ **people\_also\_ask\_expanded\_element**‘_ |  |\
| `featured_title` | string | _title_ |  |\
| `url` | string | _relevant URL_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| code>title | string | _title of the result in SERP_ |\
| `description` | string | _description of the results element in SERP_ |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original |  |



## DataForSEO API v.3: OnPage API Instant Pages

This function allows you to retrieve page-specific data with detailed information on how well a particular page is optimized for organic search. This endpoint operates based on the **Live method**, meaning it does not require a separate GET request to obtain task results.

### API Endpoint

**POST `https://api.dataforseo.com/v3/on_page/instant_pages`**

Your account will be charged for each request made to this endpoint.

### Authentication

To authenticate, you should use your credentials from `https://app.dataforseo.com/api-access`.

You will need to encode your `login:password` combination using Base64 for the `Authorization` header.

**Example Authentication Header:**
`Authorization: Basic {base64_encoded_login:password}`

### Request Parameters

All POST data must be sent in **JSON format** with **UTF-8 encoding**. The task setting is done using the POST method, where all task parameters are sent within a `task` array of the generic POST array.

You can send up to **2000 API requests per minute**, with each request containing no more than **20 tasks**. The maximum number of simultaneous requests is limited to **30**. In a single request, you can set up to 20 tasks, each containing one URL, but these URLs cannot contain more than **5 identical domains**.

**Request Body Example:**

```json
[
{
  "url": "https://dataforseo.com/blog",
  "enable_javascript": true,
  "custom_js": "meta = {}; meta.url = document.URL; meta;"
}
]
```
_This example shows setting a task with additional parameters._

**Description of Fields for Setting a Task:**

| Field Name                   | Type      | Description                                                                                                                                                                                       |url|string|*target page url* **required field**
absolute URL of the target page;
**Note #1:** results will be returned for the specified URL only;
**Note #2:** to prevent denial-of-service events, tasks that contain a duplicate crawl host will be returned with a 40501 error;
to prevent this error from occurring, avoid setting tasks with the same domain if at least one of your previous tasks with this domain (including a page URL on the domain) is still in a crawling queue|
|`custom_user_agent`          |string     |*custom user agent* optional field
custom user agent for crawling a website
example: `Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36`
default value: `Mozilla/5.0 (compatible; RSiteAuditor)`|
|`browser_preset`             |string     |*preset for browser screen parameters* optional field
If you use this field, you do not need to indicate `browser_screen_width`, `browser_screen_height`, `browser_screen_scale_factor`.
Possible values: `desktop`, `mobile`, `tablet`
`desktop` preset will apply: `browser_screen_width: 1920`, `browser_screen_height: 1080`, `browser_screen_scale_factor: 1`
`mobile` preset will apply: `browser_screen_width: 390`, `browser_screen_height: 844`, `browser_screen_scale_factor: 3`
`tablet` preset will apply: `browser_screen_width: 1024`, `browser_screen_height: 1366`, `browser_screen_scale_factor: 2`
**Note:** To use this parameter, set `enable_javascript` or `enable_browser_rendering` to `true`|
|`browser_screen_width`       |integer    |*browser screen width* optional field
You can set a custom browser screen width to perform an audit for a particular device.
If you use this field, `browser_preset` will be ignored.
**Note:** To use this parameter, set `enable_javascript` or `enable_browser_rendering` to `true`.
Minimum value, in pixels: `240`
Maximum value, in pixels: `9999`|
|`browser_screen_height`      |integer    |*browser screen height* optional field
You can set a custom browser screen height to perform an audit for a particular device.
If you use this field, `browser_preset` will be ignored.
**Note:** To use this parameter, set `enable_javascript` or `enable_browser_rendering` to `true`.
Minimum value, in pixels: `240`
Maximum value, in pixels: `9999`|
|`browser_screen_scale_factor`|float      |*browser screen scale factor* optional field
You can set a custom browser screen resolution ratio to perform an audit for a particular device.
If you use this field, `browser_preset` will be ignored.
**Note:** To use this parameter, set `enable_javascript` or `enable_browser_rendering` to `true`.
Minimum value: `0.5`
Maximum value: `3`|
|`store_raw_html`             |boolean    |*store HTML of a crawled page* optional field
Set to `true` if you want to get the HTML of the page using the OnPage Raw HTML endpoint.
Default value: `false`|
|`accept_language`            |string     |*language header for accessing the website* optional field
All locale formats are supported (xx, xx-XX, xxx-XX, etc.).
**Note:** If you do not specify this parameter, some websites may deny access; in this case, pages will be returned with `"type":"broken"` in the response array|
|`load_resources`             |boolean    |*load resources* optional field
Set to `true` if you want to load images, stylesheets, scripts, and broken resources.
Default value: `false`
**Note:** If you use this parameter, additional charges will apply. The cost can be calculated on the Pricing Page|
|`enable_javascript`          |boolean    |*load javascript on a page* optional field
Set to `true` if you want to load the scripts available on a page.
Default value: `false`
**Note:** If you use this parameter, additional charges will apply. The cost can be calculated on the Pricing Page|
|`enable_browser_rendering`   |boolean    |*emulate browser rendering to measure Core Web Vitals* optional field
By using this parameter, you will be able to emulate a browser when loading a web page.
`enable_browser_rendering` loads styles, images, fonts, animations, videos, and other resources on a page.
Default value: `false`
Set to `true` to obtain Core Web Vitals (FID, CLS, LCP) metrics in the response.
**If you use this field, parameters `enable_javascript` and `load_resources` are enabled automatically.**
**Note:** If you use this parameter, additional charges will apply. The cost can be calculated on the Pricing Page|
|`disable_cookie_popup`       |boolean    |*disable the cookie popup* optional field
Set to `true` if you want to disable the popup requesting cookie consent from the user.
Default value: `false`|
|`return_despite_timeout`     |boolean    |*return data on pages despite the timeout error* optional field
If `true`, data will be provided on pages that failed to load within 120 seconds and responded with a timeout error.
Default value: `false`|
|`enable_xhr`                 |boolean    |*enable XMLHttpRequest on a page* optional field
Set to `true` if you want our crawler to request data from a web server using the XMLHttpRequest object.
Default value: `false`
**Note:** If you use this field, `enable_javascript` must be set to `true`|
|`custom_js`                  |string     |*custom javascript* optional field
The execution time for the script you enter here should be **700 ms maximum**.
For example, you can use a JS snippet to check if the website contains Google Tag Manager.
The returned value depends on what you specify. For instance, `meta = {}; meta.url = document.URL; meta.test = 'test'; meta;` will return `"custom_js_response": { "url": "https://dataforseo.com/", "test": "test" }`.
**Note:** If you use this parameter, additional charges will apply. The cost can be calculated on the Pricing Page|
|`validate_micromarkup`       |boolean    |*enable microdata validation* optional field
If set to `true`, you can use the OnPage API Microdata endpoint with the `id` of the task.
Default value: `false`|
|`check_spell`                |boolean    |*check spelling* optional field
Set to `true` to check spelling on a website using the Hunspell library.
Default value: `false`|
|`checks_threshold`           |array      |*custom threshold values for checks* optional field
You can specify custom threshold values for the parameters included in the `checks` array of OnPage API responses.
**Note:** Only integer threshold values can be modified|
|`switch_pool`                |boolean    |*switch proxy pool* optional field
If `true`, additional proxy pools will be used to obtain the requested data.
This parameter can be used if a multitude of tasks is set simultaneously, resulting in occasional `rate-limit` and/or `site_unreachable` errors|
|`ip_pool_for_scan`           |string     |*proxy pool* optional field
You can choose a location of the proxy pool that will be used to obtain the requested data.
This parameter can be used if page content is inaccessible in one of the locations, resulting in occasional `site_unreachable` errors.
Possible values: `us`, `de`|

### Response Parameters

As a response, the API server will return JSON-encoded data containing a `tasks` array with information specific to the set tasks.

**General Response Fields:**

| Field Name     | Type    | Description                                                                                                                                              |
| :------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `version`      | string  | **The current version of the API**.                                                                                                                 |
| `status_code`  | integer | **General status code** of the request. A full list of response codes is available.                                                            |
| `status_message` | string  | **General informational message**. A full list of general informational messages is available.                                                 |
| `time`         | string  | **Execution time**, in seconds.                                                                                                                     |
| `cost`         | float   | Total **cost of tasks**, in USD.                                                                                                                    |
| `tasks_count`  | integer | The number of tasks in the `tasks` array.                                                                                                           |
| `tasks_error`  | integer | The number of tasks in the `tasks` array that returned with an error.                                                                               |
| `tasks`        | array   | **Array of tasks**. Each element represents a task submitted in the POST request.                                                                   |

**Fields within the `tasks` Array (for each task):**

| Field Name     | Type    | Description                                                                                                                                              |
| :------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `id`           | string  | **Task identifier**, a unique UUID in our system.                                                                                                   |
| `status_code`  | integer | **Status code of the task**, generated by DataForSEO (range: 10000-60000).                                                                        |
| `status_message` | string  | **Informational message of the task**.                                                                                                            |
| `time`         | string  | **Execution time** for the task, in seconds.                                                                                                        |
| `cost`         | float   | **Cost of the task**, in USD.                                                                                                                       |
| `result_count` | integer | **Number of elements in the `result` array**.                                                                                                       |
| `path`         | array   | **URL path** for the API endpoint (e.g., `["v3", "on_page", "instant_pages"]`).                                                                 |
| `data`         | object  | **Contains the same parameters that you specified in the POST request**.                                                                        |
| `result`       | array   | **Array of results**.                                                                                                                           |

**Fields within the `result` Array:**

The `result` array contains items with different `resource_type` values, each having specific fields.

---

#### **`resource_type`: 'html' (for HTML pages)**

| Field Name                     | Type    | Description                                                                                                                                                                                                                                                                     |
| :----------------------------- | :------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `crawl_progress`               | string  | **Status of the crawling session**. Possible values: `in_progress`, `finished`.                                                                                                                                                                                |
| `crawl_status`                 | object  | **Details of the crawling session**. In this case, the value will be `null`.                                                                                                                                                                                   |
| `crawl_gateway_address`        | string  | **Crawler IP address**. Displays the IP address used by the crawler to initiate the current crawling session.                                                                                                                                                 |
| `items_count`                  | integer | **Number of items** in the `items` array.                                                                                                                                                                                                                    |
| `items`                        | array   | **Items array**, representing the 'html' page.                                                                                                                                                                                                                 |
| `items.resource_type`          | string  | **Type of the returned resource**. Equals `'html'`.                                                                                                                                                                                                                |
| `items.status_code`            | integer | **Status code of the page** (e.g., 200).                                                                                                                                                                                                                       |
| `items.location`               | string  | **Location header**. Indicates the URL to redirect a page to.                                                                                                                                                                                                  |
| `items.url`                    | string  | **Page URL**.                                                                                                                                                                                                                                                  |
| `items.meta`                   | object  | **Page properties**.                                                                                                                                                                                                                                           |
| `items.meta.title`             | string  | **Page title**.                                                                                                                                                                                                                                                |
| `items.meta.charset`           | integer | **Code page** (e.g., 65001).                                                                                                                                                                                                                                   |
| `items.meta.follow`            | boolean | **Indicates whether a page’s ‘meta robots’ allows crawlers to follow the links on the page**. If `false`, the page’s ‘meta robots’ tag contains “nofollow”.                                                                                                 |
| `items.meta.generator`         | string  | **Meta tag generator**.                                                                                                                                                                                                                                        |
| `items.meta.htags`             | object  | **HTML header tags** (e.g., `h4`, `h1`, `h2` and their content).                                                                                                                                                                                              |
| `items.meta.description`       | string  | **Content of the meta description tag**.                                                                                                                                                                                                                       |
| `items.meta.favicon`           | string  | **Favicon of the page**.                                                                                                                                                                                                                                       |
| `items.meta.meta_keywords`     | string  | **Content of the `keywords` meta tag**.                                                                                                                                                                                                                        |
| `items.meta.canonical`         | string  | **Canonical page URL**.                                                                                                                                                                                                                                        |
| `items.meta.internal_links_count` | integer | **Number of internal links** on the page.                                                                                                                                                                                                                      |
| `items.meta.external_links_count` | integer | **Number of external links** on the page.                                                                                                                                                                                                                      |
| `items.meta.inbound_links_count` | integer | **Number of internal links pointing at the page**.                                                                                                                                                                                                             |
| `items.meta.images_count`      | integer | **Number of images** on the page.                                                                                                                                                                                                                              |
| `items.meta.images_size`       | integer | **Total size of images** on the page measured in bytes.                                                                                                                                                                                                        |
| `items.meta.scripts_count`     | integer | **Number of scripts** on the page.                                                                                                                                                                                                                             |
| `items.meta.scripts_size`      | integer | **Total size of scripts** on the page measured in bytes.                                                                                                                                                                                                       |
| `items.meta.stylesheets_count` | integer | **Number of stylesheets** on the page.                                                                                                                                                                                                                         |
| `items.meta.stylesheets_size`  | integer | **Total size of stylesheets** on the page measured in bytes.                                                                                                                                                                                                   |
| `items.meta.title_length`      | integer | **Length of the `title` tag** in characters.                                                                                                                                                                                                                   |
| `items.meta.description_length` | integer | **Length of the `description` tag** in characters.                                                                                                                                                                                                             |
| `items.meta.render_blocking_scripts_count` | integer | **Number of scripts on the page that block page rendering**.                                                                                                                                                                                           |
| `items.meta.render_blocking_stylesheets_count` | integer | **Number of CSS styles on the page that block page rendering**.                                                                                                                                                                                  |
| `items.meta.cumulative_layout_shift` | float | **Core Web Vitals metric measuring the layout stability of the page**. Sum total of all individual layout shift scores for every unexpected layout shift during the page's lifespan.                                                                       |
| `items.meta.meta_title`        | string  | **Meta title of the page**. The meta tag in the head section of an HTML document that defines the title of a page.                                                                                                                                              |
| `items.meta.content`           | object  | **Overall information about content of the page**.                                                                                                                                                                                                             |
| `items.meta.content.plain_text_size` | integer | **Total size of the text** on the page measured in bytes.                                                                                                                                                                                                      |
| `items.meta.content.plain_text_rate` | integer | **Plaintext rate value**. `plain_text_size` to `size` ratio.                                                                                                                                                                                                   |
| `items.meta.content.plain_text_word_count` | float | **Number of words** on the page.                                                                                                                                                                                                                               |
| `items.meta.content.automated_readability_index` | float | **Automated Readability Index**.                                                                                                                                                                                                                             |
| `items.meta.content.coleman_liau_readability_index` | float | **Coleman–Liau Index**.                                                                                                                                                                                                                                        |
| `items.meta.content.dale_chall_readability_index` | float | **Dale–Chall Readability Index**.                                                                                                                                                                                                                              |
| `items.meta.content.flesch_kincaid_readability_index` | float | **Flesch–Kincaid Readability Index**.                                                                                                                                                                                                                          |
| `items.meta.content.smog_readability_index` | float | **SMOG Readability Index**.                                                                                                                                                                                                                                |
| `items.meta.content.description_to_content_consistency` | float | **Consistency of the meta `description` tag with the page content**, measured from 0 to 1.                                                                                                                                                                 |
| `items.meta.content.title_to_content_consistency` | float | **Consistency of the meta `title` tag with the page content**, measured from 0 to 1.                                                                                                                                                                       |
| `items.meta.content.meta_keywords_to_content_consistency` | float | **Consistency of meta `keywords` tag with the page content**, measured from 0 to 1.                                                                                                                                                                |
| `items.meta.deprecated_tags`   | array   | **Deprecated tags** on the page.                                                                                                                                                                                                                               |
| `items.meta.duplicate_meta_tags` | array   | **Duplicate meta tags** on the page (e.g., "generator").                                                                                                                                                                                                         |
| `items.meta.spell`             | object  | **Spellcheck** results, including Hunspell spellcheck errors.                                                                                                                                                                                                  |
| `items.meta.spell.hunspell_language_code` | string  | **Spellcheck language code**.                                                                                                                                                                                                                                  |
| `items.meta.spell.misspelled`  | array   | **Array of misspelled words**.                                                                                                                                                                                                                                   |
| `items.meta.spell.misspelled.word` | string  | **Misspelled word**.                                                                                                                                                                                                                                               |
| `items.meta.social_media_tags` | object  | **Object of social media tags** found on the page, containing tags and their content (e.g., Open Graph and Twitter card tags like `og:locale`, `og:type`, `twitter:card`, etc.).                                                                         |
| `page_timing`                  | object  | **Object of page load metrics**.                                                                                                                                                                                                                               |
| `page_timing.time_to_interactive` | integer | **Time To Interactive (TTI) metric**. The time until the user can interact with a page (in milliseconds).                                                                                                                                                      |
| `page_timing.dom_complete`     | integer | **Time to load resources**. The time until the page and all of its subresources are downloaded (in milliseconds).                                                                                                                                              |
| `page_timing.largest_contentful_paint` | float | **Core Web Vitals metric measuring how fast the largest above-the-fold content element is displayed** (in milliseconds).                                                                                                                                   |
| `page_timing.first_input_delay` | float   | **Core Web Vitals metric indicating the responsiveness of a page**. Time from user interaction to browser response (in milliseconds).                                                                                                                           |
| `page_timing.connection_time`  | integer | **Time to connect to a server** (in milliseconds).                                                                                                                                                                                                             |
| `page_timing.time_to_secure_connection` | integer | **Time to establish a secure connection** (in milliseconds).                                                                                                                                                                                                 |
| `page_timing.request_sent_time` | integer | **Time to send a request to a server** (in milliseconds).                                                                                                                                                                                                      |
| `page_timing.waiting_time`     | integer | **Time to first byte (TTFB)** in milliseconds.                                                                                                                                                                                                                 |
| `page_timing.download_time`    | integer | **Time it takes for a browser to receive a response** (in milliseconds).                                                                                                                                                                                       |
| `page_timing.duration_time`    | integer | **Total time it takes until a browser receives a complete response from a server** (in milliseconds).                                                                                                                                                          |
| `page_timing.fetch_start`      | integer | **Time to start downloading the HTML resource**. The amount of time the browser needs to start downloading a page.                                                                                                                                               |
| `page_timing.fetch_end`        | integer | **Time to complete downloading the HTML resource**. The amount of time the browser needs to complete downloading a page.                                                                                                                                           |
| `onpage_score`                 | float   | **Shows how page is optimized on a 100-point scale**. 100 is the highest possible score.                                                                                                                                                                       |
| `total_dom_size`               | integer | **Total DOM size of a page**.                                                                                                                                                                                                                                  |
| `custom_js_response`           | string/object/integer | **The result of executing a specified JS script**. The field type and value depend on the script specified in the `custom_js` field. Results can be filtered by this value.                                                                 |
| `custom_js_client_exception`   | string  | **Error when executing a custom JS**. If an error occurred, the error message will be displayed here.                                                                                                                                                          |
| `resource_errors`              | object  | **Resource errors and warnings**.                                                                                                                                                                                                                              |
| `resource_errors.errors`       | array   | **Resource errors**.                                                                                                                                                                                                                                           |
| `resource_errors.errors.line`  | integer | **Line where the error was found**.                                                                                                                                                                                                                            |
| `resource_errors.errors.column` | integer | **Column where the error was found**.                                                                                                                                                                                                                          |
| `resource_errors.errors.message` | string  | **Text message of the error**. Possible HTML errors are available.                                                                                                                                                                                      |
| `resource_errors.errors.status_code` | integer | **Status code of the error**. Possible values: `0` (Unidentified), `501` (Html Parse Error), `1501` (JS Parse Error), `2501` (CSS Parse Error), `3501` (Image Parse Error), `3502` (Image Scale Is Zero), `3503` (Image Size Is Zero), `3504` (Image Format Invalid). |
| `resource_errors.warnings`     | array   | **Resource warnings**.                                                                                                                                                                                                                                         |
| `resource_errors.warnings.line` | integer | **Line the warning relates to**. `0` means the warning relates to the whole page.                                                                                                                                                                             |
| `resource_errors.warnings.column` | integer | **Column the warning relates to**. `0` means the warning relates to the whole page.                                                                                                                                                                           |
| `resource_errors.warnings.message` | string  | **Text message of the warning**. Possible messages: "Has node with more than 60 childs.", "Has more that 1500 nodes.", "HTML depth more than 32 tags.".                                                                                                    |
| `resource_errors.warnings.status_code` | integer | **Status code of the warning**. Possible values: `0` (Unidentified), `1` (Has node with more than 60 childs), `2` (Has more that 1500 nodes), `3` (HTML depth more than 32 tags).                                                                     |
| `broken_resources`             | boolean | **Indicates whether a page contains broken resources**.                                                                                                                                                                                                        |
| `broken_links`                 | boolean | **Indicates whether a page contains broken links**.                                                                                                                                                                                                            |
| `duplicate_title`              | boolean | **Indicates whether a page has duplicate `title` tags**.                                                                                                                                                                                                       |
| `duplicate_description`        | boolean | **Indicates whether a page has a duplicate description**.                                                                                                                                                                                                      |
| `duplicate_content`            | boolean | **Indicates whether a page has duplicate content**.                                                                                                                                                                                                            |
| `click_depth`                  | integer | **Number of clicks it takes to get to the page** from the homepage.                                                                                                                                                                                            |
| `size`                         | integer | **Resource size** in bytes.                                                                                                                                                                                                                                    |
| `encoded_size`                 | integer | **Page size after encoding** in bytes.                                                                                                                                                                                                                         |
| `total_transfer_size`          | integer | **Compressed page size** in bytes.                                                                                                                                                                                                                             |
| `fetch_time`                   | string  | **Date and time when a resource was fetched** in UTC format: “yyyy-mm-dd hh-mm-ss +00:00”.                                                                                                                                                              |
| `cache_control`                | object  | **Instructions for caching**.                                                                                                                                                                                                                                  |
| `cache_control.cachable`       | boolean | **Indicates whether the page is cacheable**.                                                                                                                                                                                                                   |
| `cache_control.ttl`            | integer | **Time to live**. The amount of time the browser caches a resource.                                                                                                                                                                                            |
| `checks`                       | object  | **Website checks** related to the page.                                                                                                                                                                                                                        |
| `checks.no_content_encoding`   | boolean | **Page with no content encoding**. Indicates whether a page has no compression algorithm of the content.                                                                                                                                                       |
| `checks.high_loading_time`     | boolean | **Page with high loading time**. Indicates whether a page loading time exceeds 3 seconds.                                                                                                                                                                      |
| `checks.is_redirect`           | boolean | **Page with redirects**. Indicates whether a page has `3XX` redirects to other pages.                                                                                                                                                                          |
| `checks.is_4xx_code`           | boolean | **Page with `4xx` status codes**. Indicates whether a page has a `4xx` response code.                                                                                                                                                                        |
| `checks.is_5xx_code`           | boolean | **Page with `5xx` status codes**. Indicates whether a page has a `5xx` response code.                                                                                                                                                                        |
| `checks.is_broken`             | boolean | **Broken page**. Indicates whether a page returns a response code less than `200` or greater than `400`.                                                                                                                                                   |
| `checks.is_www`                | boolean | **Page with www**. Indicates whether a page is on a `www` subdomain.                                                                                                                                                                                           |
| `checks.is_https`              | boolean | **Page with the https protocol**.                                                                                                                                                                                                                              |
| `checks.is_http`               | boolean | **Page with the http protocol**.                                                                                                                                                                                                                               |
| `checks.high_waiting_time`     | boolean | **Page with high waiting time**. Indicates whether a page waiting time (Time to First Byte) exceeds 1.5 seconds.                                                                                                                                              |
| `checks.has_micromarkup`       | boolean | **Page contains microdata markup**.                                                                                                                                                                                                                                |
| `checks.has_micromarkup_errors` | boolean | **Page contains microdata markup errors**.                                                                                                                                                                                                                         |
| `checks.no_doctype`            | boolean | **Page with no doctype**. Indicates whether a page is without the `<!DOCTYPE HTML>` declaration.                                                                                                                                                            |
| `checks.has_html_doctype`      | boolean | **Page with HTML doctype declaration**. `true` if the page has HTML `DOCTYPE` declaration.                                                                                                                                                                   |
| `checks.canonical`             | boolean | **Page is canonical**.                                                                                                                                                                                                                                         |
| `checks.no_encoding_meta_tag`  | boolean | **Page with no meta tag encoding**. Indicates whether a page is without `Content-Type`. Available for pages with `canonical` check set to `true`.                                                                                                       |
| `checks.no_h1_tag`             | boolean | **Page with empty or absent h1 tags**. Available for pages with `canonical` check set to `true`.                                                                                                                                                            |
| `checks.https_to_http_links`   | boolean | **HTTPS page has links to HTTP pages**. `true` if this HTTPS page has links to HTTP pages. Available for pages with `canonical` check set to `true`.                                                                                                        |
| `checks.size_greater_than_3mb` | boolean | **Page with size larger than 3 MB**. `true` if the page size is exceeding 3 MB. Available for pages with `canonical` check set to `true`.                                                                                                                   |
| `checks.meta_charset_consistency` | boolean | **Consistency between charset encoding and page charset**. `true` if the page’s charset encoding doesn’t match the actual charset of the page. Available for pages with `canonical` check set to `true`.                                                      |
| `checks.has_meta_refresh_redirect` | boolean | **Pages with meta refresh redirect**. `true` if the page has `<meta http-equiv=”refresh”>` tag. Available for pages with `canonical` check set to `true`.                                                                                                 |
| `checks.has_render_blocking_resources` | boolean | **Page with render-blocking resources**. `true` if the page has render-blocking scripts or stylesheets. Available for pages with `canonical` check set to `true`.                                                                                      |
| `checks.low_content_rate`      | boolean | **Page with low content rate**. Indicates whether a page has the `plaintext size` to `page size` ratio of less than 0.1. Available for pages with `canonical` check set to `true`.                                                                            |
| `checks.high_content_rate`     | boolean | **Page with high content rate**. Indicates whether a page has the `plaintext size` to `page size` ratio of more than 0.9. Available for pages with `canonical` check set to `true`.                                                                           |
| `checks.low_character_count`   | boolean | **Indicates whether the page has less than 1024 characters**. Available for pages with `canonical` check set to `true`.                                                                                                                                      |
| `checks.high_character_count`  | boolean | **Indicates whether the page has more than 256,000 characters**. Available for pages with `canonical` check set to `true`.                                                                                                                                     |
| `checks.small_page_size`       | boolean | **Indicates whether a page is too small**. `true` if a page size is smaller than 1024 bytes. Available for pages with `canonical` check set to `true`.                                                                                                       |
| `checks.large_page_size`       | boolean | **Indicates whether a page is too heavy**. `true` if a page size exceeds 1 megabyte. Available for pages with `canonical` check set to `true`.                                                                                                               |
| `checks.low_readability_rate`  | boolean | **Page with a low readability rate**. Indicates whether a page is scored less than 15 points on the Flesch–Kincaid readability test. Available for pages with `canonical` check set to `true`.                                                                |
| `checks.irrelevant_description` | boolean | **Page with irrelevant description**. Indicates whether a page `description` tag is irrelevant to the content of a page (relevance threshold is 0.2). Available for pages with `canonical` check set to `true`.                                                 |
| `checks.irrelevant_title`      | boolean | **Page with irrelevant title**. Indicates whether a page `title` tag is irrelevant to the content of the page (relevance threshold is 0.3). Available for pages with `canonical` check set to `true`.                                                         |
| `checks.irrelevant_meta_keywords` | boolean | **Page with irrelevant meta keywords**. Indicates whether a page `keywords` tags are irrelevant to the content of a page (relevance threshold is 0.6). Available for pages with `canonical` check set to `true`.                                              |
| `checks.title_too_long`        | boolean | **Page with a long title**. Indicates whether the content of the `title` tag exceeds 65 characters. Available for pages with `canonical` check set to `true`.                                                                                                 |
| `checks.has_meta_title`        | boolean | **Page has a meta title**. Indicates whether the HTML of a page contains the `meta_title` tag. Available for pages with `canonical` check set to `true`.                                                                                                   |
| `checks.title_too_short`       | boolean | **Page with short titles**. Indicates whether the content of `title` tag is shorter than 30 characters. Available for pages with `canonical` check set to `true`.                                                                                             |
| `checks.deprecated_html_tags`  | boolean | **Page with deprecated tags**. Indicates whether a page has deprecated HTML tags. Available for pages with `canonical` check set to `true`.                                                                                                                 |
| `checks.duplicate_meta_tags`   | boolean | **Page with duplicate meta tags**. Indicates whether a page has more than one meta tag of the same type. Available for pages with `canonical` check set to `true`.                                                                                          |
| `checks.duplicate_title_tag`   | boolean | **Page with more than one title tag**. Indicates whether a page has more than one `title` tag. Available for pages with `canonical` check set to `true`.                                                                                                    |
| `checks.no_image_alt`          | boolean | **Images without `alt` tags**. Available for pages with `canonical` check set to `true`.                                                                                                                                                                     |
| `checks.no_image_title`        | boolean | **Images without `title` tags**. Available for pages with `canonical` check set to `true`.                                                                                                                                                                   |
| `checks.no_description`        | boolean | **Pages with no description**. Indicates whether a page has an empty or absent `description` meta tag. Available for pages with `canonical` check set to `true`.                                                                                             |
| `checks.no_title`              | boolean | **Page with no title**. Indicates whether a page has an empty or absent `title` tag. Available for pages with `canonical` check set to `true`.                                                                                                             |
| `checks.no_favicon`            | boolean | **Page with no favicon**. Available for pages with `canonical` check set to `true`.                                                                                                                                                                          |
| `checks.seo_friendly_url`      | boolean | **Page with seo-friendly URL**. Checked by four parameters: relative path length < 120 chars, no special characters, no dynamic parameters, URL relevance to the page. If any fail, URL is not SEO-friendly. Available for pages with `canonical` check set to `true`. |
| `checks.flash`                 | boolean | **Page with flash**. Indicates whether a page has flash elements.                                                                                                                                                                                              |
| `checks.frame`                 | boolean | **Page with frames**. Indicates whether a page contains `frame`, `iframe`, `frameset` tags.                                                                                                                                                                  |
| `checks.lorem_ipsum`           | boolean | **Page with lorem ipsum**. Indicates whether a page has *lorem ipsum* content. Available for pages with `canonical` check set to `true`.                                                                                                                 |
| `checks.has_misspelling`       | boolean | **Page with misspelling**. Indicates whether a page has spelling mistakes. Informative if `check_spell` was set to `true` in the POST array.                                                                                                                  |
| `checks.seo_friendly_url_characters_check` | boolean | **URL characters check-up**. Indicates whether a page URL contains only uppercase/lowercase Latin characters, digits, and dashes.                                                                                                                  |
| `checks.seo_friendly_url_dynamic_check` | boolean | **URL dynamic check-up**. `true` if a page has no dynamic parameters in the URL.                                                                                                                                                                     |
| `checks.seo_friendly_url_keywords_check` | boolean | **URL keyword check-up**. Indicates whether a page URL is consistent with the `title` meta tag.                                                                                                                                                  |
| `checks.seo_friendly_url_relative_length_check` | boolean | **URL length check-up**. `true` if a page URL is no longer than 120 characters.                                                                                                                                                              |
| `content_encoding`             | string  | **Type of encoding** (e.g., "br").                                                                                                                                                                                                                             |
| `media_type`                   | string  | **Types of media** used to display a page (e.g., "text/html").                                                                                                                                                                                                 |
| `server`                       | string  | **Server version** (e.g., "cloudflare").                                                                                                                                                                                                                       |
| `is_resource`                  | boolean | **Indicates whether a page is a single resource**.                                                                                                                                                                                                             |
| `url_length`                   | integer | **Page URL length** in characters.                                                                                                                                                                                                                                 |
| `relative_url_length`          | integer | **Relative URL length** in characters.                                                                                                                                                                                                                             |
| `last_modified`                | object  | **Contains data on changes related to the resource**. `null` if no data.                                                                                                                                                                                       |
| `last_modified.header`         | string  | **Date and time when the header was last modified** in UTC format. `null` if no data.                                                                                                                                                                     |
| `last_modified.sitemap`        | string  | **Date and time when the sitemap was last modified** in UTC format. `null` if no data.                                                                                                                                                                    |
| `last_modified.meta_tag`       | string  | **Date and time when the meta tag was last modified** in UTC format. `null` if no data.                                                                                                                                                                   |

---

#### **`resource_type`: 'broken' (for broken pages)**

| Field Name                   | Type    | Description                                                                                                                                                                                  |
| :--------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `resource_type`              | string  | Type of the returned resource. Equals `'broken'`.                                                                                                                                       |
| `status_code`                | integer | **Status code of the page**.                                                                                                                                                            |
| `location`                   | string  | **Location header**. Indicates the URL to redirect a page to.                                                                                                                           |
| `url`                        | string  | **Page URL**.                                                                                                                                                                           |
| `size`                       | integer | **Resource size** in bytes.                                                                                                                                                             |
| `encoded_size`               | integer | **Page size after encoding** in bytes.                                                                                                                                                  |
| `total_transfer_size`        | integer | **Compressed page size** in bytes.                                                                                                                                                      |
| `fetch_time`                 | string  | **Date and time when a resource was fetched** in UTC format.                                                                                                                            |
| `fetch_timing`               | object  | **Time range within which a result was fetched**.                                                                                                                                       |
| `fetch_timing.duration_time` | integer | **Indicates how many seconds it took to download a page**.                                                                                                                              |
| `fetch_timing.fetch_start`   | integer | **Time to start downloading the HTML resource**.                                                                                                                                        |
| `fetch_timing.fetch_end`     | integer | **Time to complete downloading the HTML resource**.                                                                                                                                     |
| `cache_control`              | object  | **Instructions for caching**.                                                                                                                                                           |
| `cache_control.cachable`     | boolean | **Indicates whether the page is cacheable**.                                                                                                                                            |
| `cache_control.ttl`          | integer | **Time to live**. The amount of time the browser caches a resource.                                                                                                                     |
| `checks`                     | object  | **On-page check-ups**.                                                                                                                                                                  |
| `checks.no_content_encoding` | boolean | **Page with no content encoding**.                                                                                                                                                      |
| `checks.high_loading_time`   | boolean | **Page with high loading time**.                                                                                                                                                        |
| `checks.is_redirect`         | boolean | **Page with redirects**.                                                                                                                                                                |
| `checks.is_4xx_code`         | boolean | **Page with `4xx` status codes**.                                                                                                                                                       |
| `checks.is_5xx_code`         | boolean | **Page with `5xx` status codes**.                                                                                                                                                       |
| `checks.is_broken`           | boolean | **Broken page**.                                                                                                                                                                        |
| `checks.is_www`              | boolean | **Page with www**.                                                                                                                                                                      |
| `checks.is_https`            | boolean | **Page with the https protocol**.                                                                                                                                                       |
| `checks.is_http`             | boolean | **Page with the http protocol**.                                                                                                                                                        |
| `resource_errors`            | object  | **Resource errors and warnings**. (Same sub-fields as for 'html' resource type: `errors`, `errors.line`, `errors.column`, `errors.message`, `errors.status_code`, `warnings`, `warnings.line`, `warnings.column`, `warnings.message`, `warnings.status_code`). |
| `content_encoding`           | string  | **Type of encoding**.                                                                                                                                                                   |
| `media_type`                 | string  | **Types of media** used to display a page (e.g., "text/html").                                                                                                                          |
| `server`                     | string  | **Server version**.                                                                                                                                                                     |
| `is_resource`                | boolean | **Indicates whether a page is a single resource**.                                                                                                                                      |
| `last_modified`              | object  | **Contains data on changes related to the resource**. `null` if no data. (Same sub-fields as for 'html' resource type: `header`, `sitemap`, `meta_tag`).                           |

---

#### **`resource_type`: 'redirect' (for redirect pages)**

| Field Name                   | Type    | Description                                                                                                                                                                                  |
| :--------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `resource_type`              | string  | Type of the returned resource. Equals `'redirect'`.                                                                                                                                     |
| `status_code`                | integer | **Status code of the page**.                                                                                                                                                            |
| `location`                   | string  | **Target URL** for "redirect" resources.                                                                                                                                                |
| `url`                        | string  | **Source URL** for "redirect" resources.                                                                                                                                                |
| `size`                       | integer | **Resource size** in bytes. Equals `0` for "redirect" resources.                                                                                                                        |
| `encoded_size`               | integer | **Page size after encoding**. Equals `0` for "redirect" resources.                                                                                                                      |
| `total_transfer_size`        | integer | **Compressed page size** in bytes.                                                                                                                                                      |
| `fetch_time`                 | string  | **Date and time when a resource was fetched** in UTC format.                                                                                                                            |
| `fetch_timing`               | object  | **Time range within which a result was fetched**.                                                                                                                                       |
| `fetch_timing.duration_time` | integer | **Indicates how many seconds it took to download a page**.                                                                                                                              |
| `fetch_timing.fetch_start`   | integer | **Time to start downloading the HTML resource**.                                                                                                                                        |
| `fetch_timing.fetch_end`     | integer | **Time to complete downloading the HTML resource**.                                                                                                                                     |
| `resource_errors`            | object  | **Resource errors and warnings**. (Same sub-fields as for 'html' resource type: `errors`, `errors.line`, `errors.column`, `errors.message`, `errors.status_code`, `warnings`, `warnings.line`, `warnings.column`, `warnings.message`, `warnings.status_code`). |
| `cache_control`              | object  | **Instructions for caching**. (Same sub-fields as for 'html' resource type: `cachable`, `ttl`).                                                                                   |
| `checks`                     | object  | **On-page check-ups**. (Same sub-fields as for 'html' resource type regarding loading/status codes/protocols: `no_content_encoding`, `high_loading_time`, `is_redirect`, `is_4xx_code`, `is_5xx_code`, `is_broken`, `is_www`, `is_https`, `is_http`). |
| `content_encoding`           | string  | **Type of encoding**.                                                                                                                                                                   |
| `media_type`                 | string  | **Types of media** used to display a page (e.g., "text/html").                                                                                                                          |
| `server`                     | string  | **Server version**.                                                                                                                                                                     |
| `is_resource`                | boolean | **Indicates whether a page is a single resource**.                                                                                                                                      |
| `last_modified`              | object  | **Contains data on changes related to the resource**. `null` if no data. (Same sub-fields as for 'html' resource type: `header`, `sitemap`, `meta_tag`).                           |

---

#### **`resource_type`: 'script', 'image', 'stylesheet' (for resources)**
(Note: These types of resources are displayed only if the first URL to crawl is a script, image, or stylesheet.)

| Field Name                   | Type    | Description                                                                                                                                                                                  |
| :--------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `resource_type`              | string  | **Type of the returned resource**. Possible types: `script`, `image`, `stylesheet`.                                                                                                     |
| `meta`                       | object  | **Resource properties**. Available only for items with `resource_type: 'image'`.                                                                                                        |
| `meta.alternative_text`      | string  | **Content of the image `alt` attribute**.                                                                                                                                               |
| `meta.title`                 | string  | **Title**.                                                                                                                                                                              |
| `meta.original_width`        | integer | **Original image width** in px.                                                                                                                                                         |
| `meta.original_height`       | integer | **Original image height** in px.                                                                                                                                                        |
| `meta.width`                 | integer | **Image width** in px.                                                                                                                                                                  |
| `meta.height`                | integer | **Image height** in px.                                                                                                                                                                 |
| `status_code`                | integer | **Status code of the page** where a given resource is located.                                                                                                                          |
| `location`                   | string  | **Location header**. Indicates the URL to redirect a page to.                                                                                                                           |
| `url`                        | string  | **Resource URL**.                                                                                                                                                                       |
| `size`                       | integer | **Resource size** in bytes.                                                                                                                                                             |
| `encoded_size`               | integer | **Resource size after encoding** in bytes.                                                                                                                                              |
| `total_transfer_size`        | integer | **Compressed resource size** in bytes.                                                                                                                                                  |
| `fetch_time`                 | string  | **Date and time when a resource was fetched** in UTC format.                                                                                                                            |
| `fetch_timing`               | object  | **Resource fetching time range**.                                                                                                                                                       |
| `fetch_timing.duration_time` | integer | **Indicates how many milliseconds it took to fetch a resource**.                                                                                                                        |
| `fetch_timing.fetch_start`   | integer | **Time to start downloading the resource**.                                                                                                                                             |
| `fetch_timing.fetch_end`     | integer | **Time to complete downloading the resource**.                                                                                                                                          |
| `cache_control`              | object  | **Instructions for caching**. (Same sub-fields as for 'html' resource type: `cachable`, `ttl`).                                                                                   |
| `checks`                     | object  | **Resource check-ups**. Contents depend on the `resource_type`.                                                                                                                         |
| `checks.no_content_encoding` | boolean | **Resource with no content encoding**.                                                                                                                                                  |
| `checks.high_loading_time`   | boolean | **Resource with high loading time**.                                                                                                                                                    |
| `checks.is_redirect`         | boolean | **Resource with redirects**.                                                                                                                                                            |
| `checks.is_4xx_code`         | boolean | **Resource with `4xx` status codes**.                                                                                                                                                   |
| `checks.is_5xx_code`         | boolean | **Resource with `5xx` status codes**.                                                                                                                                                   |
| `checks.is_broken`           | boolean | **Broken resource**.                                                                                                                                                                    |
| `checks.is_www`              | boolean | **Page with www**.                                                                                                                                                                      |
| `checks.is_https`            | boolean | **Page with the https protocol**.                                                                                                                                                       |
| `checks.is_http`             | boolean | **Page with the http protocol**.                                                                                                                                                        |
| `checks.is_minified`         | boolean | **Resource is minified**. Indicates whether the content of a stylesheet or script is minified. Available for `stylesheet`, `script`.                                                      |
| `checks.has_redirect`        | boolean | **Resource has a redirect**. Available for `script`, `image`. Indicates redirects pointing at the resource or if the script contains a redirect.                                        |
| `checks.has_subrequests`     | boolean | **Resource contains subrequests**. Indicates whether the content of a stylesheet or script contain additional requests. Available for `stylesheet`, `script`.                           |
| `checks.original_size_displayed` | boolean | **Image displayed in its original size**. Available only for `image`.                                                                                                                   |
| `resource_errors`            | object  | **Resource errors and warnings**. (Same sub-fields as for 'html' resource type: `errors`, `errors.line`, `errors.column`, `errors.message`, `errors.status_code`, `warnings`, `warnings.line`, `warnings.column`, `warnings.message`, `warnings.status_code`). |
| `content_encoding`           | string  | **Type of encoding**.                                                                                                                                                                   |
| `media_type`                 | string  | **Types of media** used to display a resource.                                                                                                                                          |
| `accept_type`                | string  | **Indicates the expected type of resource**. For a broken resource, indicates its original type. Possible values: `any`, `none`, `image`, `sitemap`, `robots`, `script`, `stylesheet`, `redirect`, `html`, `text`, `other`, `font`. |
| `server`                     | string  | **Server version**.                                                                                                                                                                     |
| `last_modified`              | object  | **Contains data on changes related to the resource**. `null` if no data. (Same sub-fields as for 'html' resource type: `header`, `sitemap`, `meta_tag`).                           |

--- 

Based on the sources provided, this documentation covers two distinct endpoints within the DataForSEO Labs API: `dataforseo_labs/google/keyword_ideas/live` and `dataforseo_labs/google/serp_competitors/live`.

***


Structured model outputs
========================

Ensure text responses from the model adhere to a JSON schema you define.

JSON is one of the most widely used formats in the world for applications to exchange data.

Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied [JSON Schema](https://json-schema.org/overview/what-is-jsonschema), so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value.

Some benefits of Structured Outputs include:

1.  **Reliable type-safety:** No need to validate or retry incorrectly formatted responses
2.  **Explicit refusals:** Safety-based model refusals are now programmatically detectable
3.  **Simpler prompting:** No need for strongly worded prompts to achieve consistent formatting

In addition to supporting JSON Schema in the REST API, the OpenAI SDKs for [Python](https://github.com/openai/openai-python/blob/main/helpers.md#structured-outputs-parsing-helpers) and [JavaScript](https://github.com/openai/openai-node/blob/master/helpers.md#structured-outputs-parsing-helpers) also make it easy to define object schemas using [Pydantic](https://docs.pydantic.dev/latest/) and [Zod](https://zod.dev/) respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.

Getting a structured response

```javascript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI();

const CalendarEvent = z.object({
  name: z.string(),
  date: z.string(),
  participants: z.array(z.string()),
});

const response = await openai.responses.parse({
  model: "gpt-4o-2024-08-06",
  input: [
    { role: "system", content: "Extract the event information." },
    {
      role: "user",
      content: "Alice and Bob are going to a science fair on Friday.",
    },
  ],
  text: {
    format: zodTextFormat(CalendarEvent, "event"),
  },
});

const event = response.output_parsed;
```

```python
from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[
        {"role": "system", "content": "Extract the event information."},
        {
            "role": "user",
            "content": "Alice and Bob are going to a science fair on Friday.",
        },
    ],
    text_format=CalendarEvent,
)

event = response.output_parsed
```

### Supported models

Structured Outputs is available in our [latest large language models](/docs/models), starting with GPT-4o. Older models like `gpt-4-turbo` and earlier may use [JSON mode](/docs/guides/structured-outputs#json-mode) instead.

When to use Structured Outputs via function calling vs via text.format

--------------------------------------------------------------------------

Structured Outputs is available in two forms in the OpenAI API:

1.  When using [function calling](/docs/guides/function-calling)
2.  When using a `json_schema` response format

Function calling is useful when you are building an application that bridges the models and functionality of your application.

For example, you can give the model access to functions that query a database in order to build an AI assistant that can help users with their orders, or functions that can interact with the UI.

Conversely, Structured Outputs via `response_format` are more suitable when you want to indicate a structured schema for use when the model responds to the user, rather than when the model calls a tool.

For example, if you are building a math tutoring application, you might want the assistant to respond to your user using a specific JSON Schema so that you can generate a UI that displays different parts of the model's output in distinct ways.

Put simply:

*   If you are connecting the model to tools, functions, data, etc. in your system, then you should use function calling - If you want to structure the model's output when it responds to the user, then you should use a structured `text.format`

The remainder of this guide will focus on non-function calling use cases in the Responses API. To learn more about how to use Structured Outputs with function calling, check out the

[

Function Calling

](/docs/guides/function-calling#function-calling-with-structured-outputs)

guide.

### Structured Outputs vs JSON mode

Structured Outputs is the evolution of [JSON mode](/docs/guides/structured-outputs#json-mode). While both ensure valid JSON is produced, only Structured Outputs ensure schema adherence. Both Structured Outputs and JSON mode are supported in the Responses API, Chat Completions API, Assistants API, Fine-tuning API and Batch API.

We recommend always using Structured Outputs instead of JSON mode when possible.

However, Structured Outputs with `response_format: {type: "json_schema", ...}` is only supported with the `gpt-4o-mini`, `gpt-4o-mini-2024-07-18`, and `gpt-4o-2024-08-06` model snapshots and later.

||Structured Outputs|JSON Mode|
|---|---|---|
|Outputs valid JSON|Yes|Yes|
|Adheres to schema|Yes (see supported schemas)|No|
|Compatible models|gpt-4o-mini, gpt-4o-2024-08-06, and later|gpt-3.5-turbo, gpt-4-* and gpt-4o-* models|
|Enabling|text: { format: { type: "json_schema", "strict": true, "schema": ... } }|text: { format: { type: "json_object" } }|

Examples
--------

Chain of thought

### Chain of thought

You can ask the model to output an answer in a structured, step-by-step way, to guide the user through the solution.

Structured Outputs for chain-of-thought math tutoring

```javascript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI();

const Step = z.object({
  explanation: z.string(),
  output: z.string(),
});

const MathReasoning = z.object({
  steps: z.array(Step),
  final_answer: z.string(),
});

const response = await openai.responses.parse({
  model: "gpt-4o-2024-08-06",
  input: [
    {
      role: "system",
      content:
        "You are a helpful math tutor. Guide the user through the solution step by step.",
    },
    { role: "user", content: "how can I solve 8x + 7 = -23" },
  ],
  text: {
    format: zodTextFormat(MathReasoning, "math_reasoning"),
  },
});

const math_reasoning = response.output_parsed;
```

```python
from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class Step(BaseModel):
    explanation: str
    output: str

class MathReasoning(BaseModel):
    steps: list[Step]
    final_answer: str

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[
        {
            "role": "system",
            "content": "You are a helpful math tutor. Guide the user through the solution step by step.",
        },
        {"role": "user", "content": "how can I solve 8x + 7 = -23"},
    ],
    text_format=MathReasoning,
)

math_reasoning = response.output_parsed
```

```bash
curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-2024-08-06",
    "input": [
      {
        "role": "system",
        "content": "You are a helpful math tutor. Guide the user through the solution step by step."
      },
      {
        "role": "user",
        "content": "how can I solve 8x + 7 = -23"
      }
    ],
    "text": {
      "format": {
        "type": "json_schema",
        "name": "math_reasoning",
        "schema": {
          "type": "object",
          "properties": {
            "steps": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "explanation": { "type": "string" },
                  "output": { "type": "string" }
                },
                "required": ["explanation", "output"],
                "additionalProperties": false
              }
            },
            "final_answer": { "type": "string" }
          },
          "required": ["steps", "final_answer"],
          "additionalProperties": false
        },
        "strict": true
      }
    }
  }'
```

#### Example response

```json
{
  "steps": [
    {
      "explanation": "Start with the equation 8x + 7 = -23.",
      "output": "8x + 7 = -23"
    },
    {
      "explanation": "Subtract 7 from both sides to isolate the term with the variable.",
      "output": "8x = -23 - 7"
    },
    {
      "explanation": "Simplify the right side of the equation.",
      "output": "8x = -30"
    },
    {
      "explanation": "Divide both sides by 8 to solve for x.",
      "output": "x = -30 / 8"
    },
    {
      "explanation": "Simplify the fraction.",
      "output": "x = -15 / 4"
    }
  ],
  "final_answer": "x = -15 / 4"
}
```

Structured data extraction

### Structured data extraction

You can define structured fields to extract from unstructured input data, such as research papers.

Extracting data from research papers using Structured Outputs

```javascript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI();

const ResearchPaperExtraction = z.object({
  title: z.string(),
  authors: z.array(z.string()),
  abstract: z.string(),
  keywords: z.array(z.string()),
});

const response = await openai.responses.parse({
  model: "gpt-4o-2024-08-06",
  input: [
    {
      role: "system",
      content:
        "You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.",
    },
    { role: "user", content: "..." },
  ],
  text: {
    format: zodTextFormat(ResearchPaperExtraction, "research_paper_extraction"),
  },
});

const research_paper = response.output_parsed;
```

```python
from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class ResearchPaperExtraction(BaseModel):
    title: str
    authors: list[str]
    abstract: str
    keywords: list[str]

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[
        {
            "role": "system",
            "content": "You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure.",
        },
        {"role": "user", "content": "..."},
    ],
    text_format=ResearchPaperExtraction,
)

research_paper = response.output_parsed
```

```bash
curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-2024-08-06",
    "input": [
      {
        "role": "system",
        "content": "You are an expert at structured data extraction. You will be given unstructured text from a research paper and should convert it into the given structure."
      },
      {
        "role": "user",
        "content": "..."
      }
    ],
    "text": {
      "format": {
        "type": "json_schema",
        "name": "research_paper_extraction",
        "schema": {
          "type": "object",
          "properties": {
            "title": { "type": "string" },
            "authors": {
              "type": "array",
              "items": { "type": "string" }
            },
            "abstract": { "type": "string" },
            "keywords": {
              "type": "array",
              "items": { "type": "string" }
            }
          },
          "required": ["title", "authors", "abstract", "keywords"],
          "additionalProperties": false
        },
        "strict": true
      }
    }
  }'
```

#### Example response

```json
{
  "title": "Application of Quantum Algorithms in Interstellar Navigation: A New Frontier",
  "authors": [
    "Dr. Stella Voyager",
    "Dr. Nova Star",
    "Dr. Lyra Hunter"
  ],
  "abstract": "This paper investigates the utilization of quantum algorithms to improve interstellar navigation systems. By leveraging quantum superposition and entanglement, our proposed navigation system can calculate optimal travel paths through space-time anomalies more efficiently than classical methods. Experimental simulations suggest a significant reduction in travel time and fuel consumption for interstellar missions.",
  "keywords": [
    "Quantum algorithms",
    "interstellar navigation",
    "space-time anomalies",
    "quantum superposition",
    "quantum entanglement",
    "space travel"
  ]
}
```

UI generation

### UI Generation

You can generate valid HTML by representing it as recursive data structures with constraints, like enums.

Generating HTML using Structured Outputs

```javascript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI();

const UI = z.lazy(() =>
  z.object({
    type: z.enum(["div", "button", "header", "section", "field", "form"]),
    label: z.string(),
    children: z.array(UI),
    attributes: z.array(
      z.object({
        name: z.string(),
        value: z.string(),
      })
    ),
  })
);

const response = await openai.responses.parse({
  model: "gpt-4o-2024-08-06",
  input: [
    {
      role: "system",
      content: "You are a UI generator AI. Convert the user input into a UI.",
    },
    {
      role: "user",
      content: "Make a User Profile Form",
    },
  ],
  text: {
    format: zodTextFormat(UI, "ui"),
  },
});

const ui = response.output_parsed;
```

```python
from enum import Enum
from typing import List

from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class UIType(str, Enum):
    div = "div"
    button = "button"
    header = "header"
    section = "section"
    field = "field"
    form = "form"

class Attribute(BaseModel):
    name: str
    value: str

class UI(BaseModel):
    type: UIType
    label: str
    children: List["UI"]
    attributes: List[Attribute]

UI.model_rebuild()  # This is required to enable recursive types

class Response(BaseModel):
    ui: UI

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[
        {
            "role": "system",
            "content": "You are a UI generator AI. Convert the user input into a UI.",
        },
        {"role": "user", "content": "Make a User Profile Form"},
    ],
    text_format=Response,
)

ui = response.output_parsed
```

```bash
curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-2024-08-06",
    "input": [
      {
        "role": "system",
        "content": "You are a UI generator AI. Convert the user input into a UI."
      },
      {
        "role": "user",
        "content": "Make a User Profile Form"
      }
    ],
    "text": {
      "format": {
        "type": "json_schema",
        "name": "ui",
        "description": "Dynamically generated UI",
        "schema": {
          "type": "object",
          "properties": {
            "type": {
              "type": "string",
              "description": "The type of the UI component",
              "enum": ["div", "button", "header", "section", "field", "form"]
            },
            "label": {
              "type": "string",
              "description": "The label of the UI component, used for buttons or form fields"
            },
            "children": {
              "type": "array",
              "description": "Nested UI components",
              "items": {"$ref": "#"}
            },
            "attributes": {
              "type": "array",
              "description": "Arbitrary attributes for the UI component, suitable for any element",
              "items": {
                "type": "object",
                "properties": {
                  "name": {
                    "type": "string",
                    "description": "The name of the attribute, for example onClick or className"
                  },
                  "value": {
                    "type": "string",
                    "description": "The value of the attribute"
                  }
                },
                "required": ["name", "value"],
                "additionalProperties": false
              }
            }
          },
          "required": ["type", "label", "children", "attributes"],
          "additionalProperties": false
        },
        "strict": true
      }
    }
  }'
```

#### Example response

```json
{
  "type": "form",
  "label": "User Profile Form",
  "children": [
    {
      "type": "div",
      "label": "",
      "children": [
        {
          "type": "field",
          "label": "First Name",
          "children": [],
          "attributes": [
            {
              "name": "type",
              "value": "text"
            },
            {
              "name": "name",
              "value": "firstName"
            },
            {
              "name": "placeholder",
              "value": "Enter your first name"
            }
          ]
        },
        {
          "type": "field",
          "label": "Last Name",
          "children": [],
          "attributes": [
            {
              "name": "type",
              "value": "text"
            },
            {
              "name": "name",
              "value": "lastName"
            },
            {
              "name": "placeholder",
              "value": "Enter your last name"
            }
          ]
        }
      ],
      "attributes": []
    },
    {
      "type": "button",
      "label": "Submit",
      "children": [],
      "attributes": [
        {
          "name": "type",
          "value": "submit"
        }
      ]
    }
  ],
  "attributes": [
    {
      "name": "method",
      "value": "post"
    },
    {
      "name": "action",
      "value": "/submit-profile"
    }
  ]
}
```

Moderation

### Moderation

You can classify inputs on multiple categories, which is a common way of doing moderation.

Moderation using Structured Outputs

```javascript
import OpenAI from "openai";
import { zodTextFormat } from "openai/helpers/zod";
import { z } from "zod";

const openai = new OpenAI();

const ContentCompliance = z.object({
  is_violating: z.boolean(),
  category: z.enum(["violence", "sexual", "self_harm"]).nullable(),
  explanation_if_violating: z.string().nullable(),
});

const response = await openai.responses.parse({
    model: "gpt-4o-2024-08-06",
    input: [
      {
        "role": "system",
        "content": "Determine if the user input violates specific guidelines and explain if they do."
      },
      {
        "role": "user",
        "content": "How do I prepare for a job interview?"
      }
    ],
    text: {
        format: zodTextFormat(ContentCompliance, "content_compliance"),
    },
});

const compliance = response.output_parsed;
```

```python
from enum import Enum
from typing import Optional

from openai import OpenAI
from pydantic import BaseModel

client = OpenAI()

class Category(str, Enum):
    violence = "violence"
    sexual = "sexual"
    self_harm = "self_harm"

class ContentCompliance(BaseModel):
    is_violating: bool
    category: Optional[Category]
    explanation_if_violating: Optional[str]

response = client.responses.parse(
    model="gpt-4o-2024-08-06",
    input=[
        {
            "role": "system",
            "content": "Determine if the user input violates specific guidelines and explain if they do.",
        },
        {"role": "user", "content": "How do I prepare for a job interview?"},
    ],
    text_format=ContentCompliance,
)

compliance = response.output_parsed
```

```bash
curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-2024-08-06",
    "input": [
      {
        "role": "system",
        "content": "Determine if the user input violates specific guidelines and explain if they do."
      },
      {
        "role": "user",
        "content": "How do I prepare for a job interview?"
      }
    ],
    "text": {
      "format": {
        "type": "json_schema",
        "name": "content_compliance",
        "description": "Determines if content is violating specific moderation rules",
        "schema": {
          "type": "object",
          "properties": {
            "is_violating": {
              "type": "boolean",
              "description": "Indicates if the content is violating guidelines"
            },
            "category": {
              "type": ["string", "null"],
              "description": "Type of violation, if the content is violating guidelines. Null otherwise.",
              "enum": ["violence", "sexual", "self_harm"]
            },
            "explanation_if_violating": {
              "type": ["string", "null"],
              "description": "Explanation of why the content is violating"
            }
          },
          "required": ["is_violating", "category", "explanation_if_violating"],
          "additionalProperties": false
        },
        "strict": true
      }
    }
  }'
```

#### Example response

```json
{
  "is_violating": false,
  "category": null,
  "explanation_if_violating": null
}
```

How to use Structured Outputs with text.format
----------------------------------------------

Step 1: Define your schema

First you must design the JSON Schema that the model should be constrained to follow. See the [examples](/docs/guides/structured-outputs#examples) at the top of this guide for reference.

While Structured Outputs supports much of JSON Schema, some features are unavailable either for performance or technical reasons. See [here](/docs/guides/structured-outputs#supported-schemas) for more details.

#### Tips for your JSON Schema

To maximize the quality of model generations, we recommend the following:

*   Name keys clearly and intuitively
*   Create clear titles and descriptions for important keys in your structure
*   Create and use evals to determine the structure that works best for your use case

Step 2: Supply your schema in the API call

To use Structured Outputs, simply specify

```json
text: { format: { type: "json_schema", "strict": true, "schema": … } }
```

For example:

```python
response = client.responses.create(
    model="gpt-4o-2024-08-06",
    input=[
        {"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},
        {"role": "user", "content": "how can I solve 8x + 7 = -23"}
    ],
    text={
        "format": {
            "type": "json_schema",
            "name": "math_response",
            "schema": {
                "type": "object",
                "properties": {
                    "steps": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "explanation": {"type": "string"},
                                "output": {"type": "string"}
                            },
                            "required": ["explanation", "output"],
                            "additionalProperties": False
                        }
                    },
                    "final_answer": {"type": "string"}
                },
                "required": ["steps", "final_answer"],
                "additionalProperties": False
            },
            "strict": True
        }
    }
)

print(response.output_text)
```

```javascript
const response = await openai.responses.create({
    model: "gpt-4o-2024-08-06",
    input: [
        { role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },
        { role: "user", content: "how can I solve 8x + 7 = -23" }
    ],
    text: {
        format: {
            type: "json_schema",
            name: "math_response",
            schema: {
                type: "object",
                properties: {
                    steps: {
                        type: "array",
                        items: {
                            type: "object",
                            properties: {
                                explanation: { type: "string" },
                                output: { type: "string" }
                            },
                            required: ["explanation", "output"],
                            additionalProperties: false
                        }
                    },
                    final_answer: { type: "string" }
                },
                required: ["steps", "final_answer"],
                additionalProperties: false
            },
            strict: true
        }
    }
});

console.log(response.output_text);
```

```bash
curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-2024-08-06",
    "input": [
      {
        "role": "system",
        "content": "You are a helpful math tutor. Guide the user through the solution step by step."
      },
      {
        "role": "user",
        "content": "how can I solve 8x + 7 = -23"
      }
    ],
    "text": {
      "format": {
        "type": "json_schema",
        "name": "math_response",
        "schema": {
          "type": "object",
          "properties": {
            "steps": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "explanation": { "type": "string" },
                  "output": { "type": "string" }
                },
                "required": ["explanation", "output"],
                "additionalProperties": false
              }
            },
            "final_answer": { "type": "string" }
          },
          "required": ["steps", "final_answer"],
          "additionalProperties": false
        },
        "strict": true
      }
    }
  }'
```

**Note:** the first request you make with any schema will have additional latency as our API processes the schema, but subsequent requests with the same schema will not have additional latency.

Step 3: Handle edge cases

In some cases, the model might not generate a valid response that matches the provided JSON schema.

This can happen in the case of a refusal, if the model refuses to answer for safety reasons, or if for example you reach a max tokens limit and the response is incomplete.

```javascript
try {
  const response = await openai.responses.create({
    model: "gpt-4o-2024-08-06",
    input: [{
        role: "system",
        content: "You are a helpful math tutor. Guide the user through the solution step by step.",
      },
      {
        role: "user",
        content: "how can I solve 8x + 7 = -23"
      },
    ],
    max_output_tokens: 50,
    text: {
      format: {
        type: "json_schema",
        name: "math_response",
        schema: {
          type: "object",
          properties: {
            steps: {
              type: "array",
              items: {
                type: "object",
                properties: {
                  explanation: {
                    type: "string"
                  },
                  output: {
                    type: "string"
                  },
                },
                required: ["explanation", "output"],
                additionalProperties: false,
              },
            },
            final_answer: {
              type: "string"
            },
          },
          required: ["steps", "final_answer"],
          additionalProperties: false,
        },
        strict: true,
      },
    }
  });

  if (response.status === "incomplete" && response.incomplete_details.reason === "max_output_tokens") {
    // Handle the case where the model did not return a complete response
    throw new Error("Incomplete response");
  }

  const math_response = response.output[0].content[0];

  if (math_response.type === "refusal") {
    // handle refusal
    console.log(math_response.refusal);
  } else if (math_response.type === "output_text") {
    console.log(math_response.text);
  } else {
    throw new Error("No response content");
  }
} catch (e) {
  // Handle edge cases
  console.error(e);
}
```

```python
try:
    response = client.responses.create(
        model="gpt-4o-2024-08-06",
        input=[
            {
                "role": "system",
                "content": "You are a helpful math tutor. Guide the user through the solution step by step.",
            },
            {"role": "user", "content": "how can I solve 8x + 7 = -23"},
        ],
        text={
            "format": {
                "type": "json_schema",
                "name": "math_response",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "steps": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "explanation": {"type": "string"},
                                    "output": {"type": "string"},
                                },
                                "required": ["explanation", "output"],
                                "additionalProperties": False,
                            },
                        },
                        "final_answer": {"type": "string"},
                    },
                    "required": ["steps", "final_answer"],
                    "additionalProperties": False,
                },
                "strict": True,
            },
        },
    )
except Exception as e:
    # handle errors like finish_reason, refusal, content_filter, etc.
    pass
```

### 

Refusals with Structured Outputs

When using Structured Outputs with user-generated input, OpenAI models may occasionally refuse to fulfill the request for safety reasons. Since a refusal does not necessarily follow the schema you have supplied in `response_format`, the API response will include a new field called `refusal` to indicate that the model refused to fulfill the request.

When the `refusal` property appears in your output object, you might present the refusal in your UI, or include conditional logic in code that consumes the response to handle the case of a refused request.

```python
class Step(BaseModel):
    explanation: str
    output: str

class MathReasoning(BaseModel):
steps: list[Step]
final_answer: str

completion = client.chat.completions.parse(
model="gpt-4o-2024-08-06",
messages=[
{"role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step."},
{"role": "user", "content": "how can I solve 8x + 7 = -23"}
],
response_format=MathReasoning,
)

math_reasoning = completion.choices[0].message

# If the model refuses to respond, you will get a refusal message

if (math_reasoning.refusal):
print(math_reasoning.refusal)
else:
print(math_reasoning.parsed)
```

```javascript
const Step = z.object({
explanation: z.string(),
output: z.string(),
});

const MathReasoning = z.object({
steps: z.array(Step),
final_answer: z.string(),
});

const completion = await openai.chat.completions.parse({
model: "gpt-4o-2024-08-06",
messages: [
{ role: "system", content: "You are a helpful math tutor. Guide the user through the solution step by step." },
{ role: "user", content: "how can I solve 8x + 7 = -23" },
],
response_format: zodResponseFormat(MathReasoning, "math_reasoning"),
});

const math_reasoning = completion.choices[0].message

// If the model refuses to respond, you will get a refusal message
if (math_reasoning.refusal) {
console.log(math_reasoning.refusal);
} else {
console.log(math_reasoning.parsed);
}
```

The API response from a refusal will look something like this:

```json
{
  "id": "resp_1234567890",
  "object": "response",
  "created_at": 1721596428,
  "status": "completed",
  "error": null,
  "incomplete_details": null,
  "input": [],
  "instructions": null,
  "max_output_tokens": null,
  "model": "gpt-4o-2024-08-06",
  "output": [{
    "id": "msg_1234567890",
    "type": "message",
    "role": "assistant",
    "content": [
      {
        "type": "refusal",
        "refusal": "I'm sorry, I cannot assist with that request."
      }
    ]
  }],
  "usage": {
    "input_tokens": 81,
    "output_tokens": 11,
    "total_tokens": 92,
    "output_tokens_details": {
      "reasoning_tokens": 0,
    }
  },
}
```

### 

Tips and best practices

#### Handling user-generated input

If your application is using user-generated input, make sure your prompt includes instructions on how to handle situations where the input cannot result in a valid response.

The model will always try to adhere to the provided schema, which can result in hallucinations if the input is completely unrelated to the schema.

You could include language in your prompt to specify that you want to return empty parameters, or a specific sentence, if the model detects that the input is incompatible with the task.

#### Handling mistakes

Structured Outputs can still contain mistakes. If you see mistakes, try adjusting your instructions, providing examples in the system instructions, or splitting tasks into simpler subtasks. Refer to the [prompt engineering guide](/docs/guides/prompt-engineering) for more guidance on how to tweak your inputs.

#### Avoid JSON schema divergence

To prevent your JSON Schema and corresponding types in your programming language from diverging, we strongly recommend using the native Pydantic/zod sdk support.

If you prefer to specify the JSON schema directly, you could add CI rules that flag when either the JSON schema or underlying data objects are edited, or add a CI step that auto-generates the JSON Schema from type definitions (or vice-versa).


## Pexels API Technical Documentation

The Pexels API is a **RESTful JSON API** that enables programmatic access to the entire Pexels library of photos and videos, all available free of charge.

### Integration Basics

#### Base URLs
For historical reasons, endpoints are split across two base URLs:

*   **Photos and Collections:** `https://api.pexels.com/v1/`
*   **Videos:** `https://api.pexels.com/videos/`

#### Authorization
Authorization is **required** for all Pexels API requests.

*   **Requirement:** An API key, which can be instantly requested by anyone with a Pexels account.
*   **Method:** Include the API key by adding an **`Authorization` header** to every request.
*   **Example:** `curl -H "Authorization: YOUR_API_KEY" \ "https://api.pexels.com/v1/search?query=people"`

#### Usage Guidelines and Rate Limits
*   **Attribution:** Always show a **prominent link back to Pexels** (e.g., using a text link like "Photos provided by Pexels" or the Pexels logo).
*   **Credit:** Always credit the photographers when possible (e.g., "Photo by John Doe on Pexels" with a link to the photo page on Pexels).
*   **Abuse Policy:** Do not copy or replicate core Pexels functionality (e.g., creating a wallpaper app). Abuse, including attempting to bypass the rate limit, will lead to termination of API access.
*   **Default Rate Limit:** The API is rate-limited to **200 requests per hour** and **20,000 requests per month**. Higher limits may be requested by contacting Pexels and providing examples/demos demonstrating proper attribution.

#### Rate Limit Response Headers
Successful HTTP responses (2xx) from the Pexels API include three headers that help manage your quota:

| Response Header | Meaning |
| :--- | :--- |
| `X-Ratelimit-Limit` | Your total request limit for the monthly period. |
| `X-Ratelimit-Remaining` | How many of these requests remain. |
| `X-Ratelimit-Reset` | UNIX timestamp of when the current monthly period will roll over. |

*(Note: These headers are not included in non-successful responses, such as `429 Too Many Requests` which indicates the rate limit has been exceeded).*

### General Pagination Parameters

Most endpoints return multiple records and are paginated, supporting a maximum of **80** results per request.

| Parameter | Type | Required | Description | Default | Maximum |
| :--- | :--- | :--- | :--- | :--- | :--- |
| `page` | integer | optional | The page number being requested. | 1 | |
| `per_page` | integer | optional | The number of results requested per page. | 15 | 80 |

#### Common Pagination Response Attributes

| Attribute | Type | Description |
| :--- | :--- | :--- |
| `page` | integer | The current page number. |
| `per_page` | integer | The number of results returned with each page. |
| `total_results` | integer | The total number of results for the request. |
| `next_page` | string | URL for the next page of results (only returned if applicable). |
| `prev_page` | string | URL for the previous page of results (only returned if applicable). |

***

CODE:

This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-11-04T12:08:12.713Z

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Info

# Directory Structure
```
agents/
  __init__.py
  article_generator.py
  brief_assembler.py
  content_auditor.py
  html_formatter.py
  image_generator.py
  internal_linking_suggester.py
  prompt_assembler.py
  social_media_crafter.py
  summary_generator.py
api/
  routers/
    auth.py
    client_settings.py
    clients.py
    discovery.py
    jobs.py
    opportunities.py
    orchestrator.py
    qualification_settings.py
    qualification_strategies.py
    settings.py
  dependencies.py
  globals.py
  main.py
  models.py
app_config/
  __init__.py
  manager.py
  settings.ini
core/
  serp_analyzers/
    disqualification_analyzer.py
    featured_snippet_analyzer.py
    pixel_ranking_analyzer.py
    video_analyzer.py
  __init__.py
  blueprint_factory.py
  page_classifier.py
  serp_analyzer.py
  utils.py
data_access/
  migrations/
    001_add_new_tables.sql
    001_initial_schema.sql
    002_add_keywords_table.sql
    002_remove_json_columns.sql
    003_add_indexes.sql
    003_add_total_api_cost.sql
    004_add_qualification_settings_table.sql
    005_add_qualification_columns.sql
    006_add_intent_weights.sql
    007_add_competitor_strength_weight.sql
    008_add_serp_features_weight.sql
    009_add_trend_weight.sql
    010_add_history_columns.sql
    012_add_seasonality_weight.sql
    013_add_serp_volatility_weight.sql
    014_add_disqualification_rules.sql
    015_add_brand_keywords.sql
    016_add_review_threshold.sql
    017_add_strategies_table.sql
    018_add_job_id_and_cluster_name.sql
    019_add_core_keyword_and_competitor_metrics_to_opportunities.sql
    020_backfill_core_keyword_metrics.sql
    021_add_unique_keyword_constraint.sql
    023_add_run_id_to_opportunities.sql
    024_add_total_api_cost_to_opportunities.sql
    025_add_cost_to_discovery_runs.sql
    026_add_client_id_to_jobs.sql
    027_promote_json_fields.sql
  __init__.py
  database_manager.py
  initialize.py
  models.py
  queries.py
data_mappers/
  dataforseo_mapper.py
  keyword_data_mapper.py
  serp_overview_mapper.py
external_apis/
  dataforseo_client_v2.py
  on-page-api.py
  openai_client.py
  pexels_client.py
pipeline/
  orchestrator/
    __init__.py
    analysis_orchestrator.py
    content_orchestrator.py
    cost_estimator.py
    discovery_orchestrator.py
    image_orchestrator.py
    main.py
    social_orchestrator.py
    state_validator.py
    validation_orchestrator.py
    workflow_orchestrator.py
  step_01_discovery/
    keyword_discovery/
      expander.py
      filters.py
    __init__.py
    blog_content_qualifier.py
    cannibalization_checker.py
    disqualification_rules.py
    keyword_expander.py
    run_discovery.py
  step_02_qualification/
    __init__.py
    competitor_analyzer.py
    serp_analyzer.py
  step_03_prioritization/
    scoring_components/
      __init__.py
      commercial_intent.py
      competitor_performance.py
      competitor_weakness.py
      ease_of_ranking.py
      growth_trend.py
      keyword_structure.py
      serp_crowding.py
      serp_features.py
      serp_freshness.py
      serp_threat.py
      serp_volatility.py
      traffic_potential.py
      volume_volatility.py
    __init__.py
    run_prioritization.py
    scoring_engine.py
  step_04_analysis/
    content_analysis_modules/
      ai_intelligence_caller.py
      heading_analyzer.py
      metric_analyzer.py
    __init__.py
    competitor_analyzer.py
    content_analyzer.py
    run_analysis.py
  step_05_strategy/
    decision_engine.py
  step_06_content_creation/
    __init__.py
  __init__.py
  orchestrator.py
services/
  discovery_service.py
  disqualification_service.py
  keyword_data_aggregator.py
  opportunities_service.py
  qualification_service.py
  scoring_service.py
  serp_analysis_service.py
tests/
  test_content_generation.py
  test_filter_transformation.py
  test_onpage_instant_pages.py
  test_scoring_engine.py
Dockerfile
export_db.py
jobs.py
requirements.txt
```

# Files

## File: agents/__init__.py
```python
# agents/__init__.py
```

## File: agents/article_generator.py
```python
import logging
from typing import Dict, Any, Tuple, Optional, List
from concurrent.futures import ThreadPoolExecutor, as_completed

from external_apis.openai_client import OpenAIClientWrapper
from agents.prompt_assembler import DynamicPromptAssembler


class SectionalArticleGenerator:
    """
    An agentic generator that creates content for specific sections of an article
    based on a highly contextual prompt.
    """

    def __init__(
        self, openai_client: OpenAIClientWrapper, config: Dict[str, Any], db_manager
    ):
        self.openai_client = openai_client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.prompt_assembler = DynamicPromptAssembler(db_manager)

    def _generate_component(
        self, messages: List[Dict[str, str]], model: str, temperature: float
    ) -> Tuple[Optional[str], float]:
        """Internal helper to call the LLM and get a raw HTML string."""
        schema = {
            "name": "generate_html_content",
            "type": "object",
            "properties": {
                "content_html": {
                    "type": "string",
                    "description": "The generated content as a clean, well-structured HTML block. Do not include the main heading tag itself.",
                }
            },
            "required": ["content_html"],
            "additionalProperties": False,
        }
        response, error = self.openai_client.call_chat_completion(
            messages=messages,
            schema=schema,
            model=model,
            max_completion_tokens=self.config.get(
                "max_completion_tokens_for_generation", 4096
            ),
        )
        cost = self.openai_client.latest_cost
        if error or not response:
            self.logger.error(f"Failed to generate content component: {error}")
            return None, cost
        return response.get("content_html"), cost

    def generate_full_article(self, opportunity: Dict[str, Any]) -> Tuple[Dict[str, Any], float]:
        """
        Generates a full article by generating its sections in parallel and then assembling them.
        """
        total_cost = 0.0
        blueprint = opportunity.get("blueprint", {})
        brief = blueprint.get("ai_content_brief", {})
        outline_h2s = brief.get("mandatory_sections", [])
        
        generated_content = {}

        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_section = {}

            # 1. Submit Introduction task
            intro_future = executor.submit(self.generate_introduction, opportunity)
            future_to_section[intro_future] = "introduction"

            # 2. Submit Body Section tasks
            for i, section_title in enumerate(outline_h2s):
                previous_title = outline_h2s[i-1] if i > 0 else "Introduction"
                next_title = outline_h2s[i+1] if i < len(outline_h2s) - 1 else "Conclusion"
                
                # Assuming sub_points are not explicitly defined per section in the brief for now
                section_future = executor.submit(
                    self._generate_parallel_section,
                    opportunity,
                    section_title,
                    [], 
                    previous_title,
                    next_title
                )
                future_to_section[section_future] = section_title

            # 3. Collect results as they complete
            for future in as_completed(future_to_section):
                section_identifier = future_to_section[future]
                try:
                    content, cost = future.result()
                    total_cost += cost
                    generated_content[section_identifier] = content or f"<!-- Error generating section: {section_identifier} -->"
                except Exception as exc:
                    self.logger.error(f"Section '{section_identifier}' generated an exception: {exc}", exc_info=True)
                    generated_content[section_identifier] = f"<!-- Exception generating section: {section_identifier}. Error: {exc} -->"

        # 4. Assemble the article body in the correct order
        article_body_html = generated_content.get("introduction", "")
        for section_title in outline_h2s:
            article_body_html += f"\n<h2>{section_title}</h2>\n"
            article_body_html += generated_content.get(section_title, "")

        # 5. Generate Conclusion sequentially after the main body is assembled
        conclusion_html, cost = self.generate_conclusion(opportunity, article_body_html)
        total_cost += cost
        article_body_html += f"\n<h2>Conclusion</h2>\n"
        article_body_html += conclusion_html or "<!-- Error generating conclusion -->"

        # The method returns the assembled HTML content and the total cost.
        # The orchestrator will be responsible for packaging this into the final ai_content_json blob.
        return {"article_body_html": article_body_html}, total_cost

    def _generate_parallel_section(
        self,
        opportunity: Dict[str, Any],
        section_title: str,
        section_sub_points: List[str],
        previous_section_title: str,
        next_section_title: str,
    ) -> Tuple[Optional[str], float]:
        """Generates a single article section with context about surrounding sections."""
        brief = opportunity.get("blueprint", {}).get("ai_content_brief", {})
        prompt = f"""
        You are an expert SEO content writer and subject matter expert. Your task is to write a single, detailed section for a blog post about "{opportunity["keyword"]}".

        **Current Section to Write:** "{section_title}"
        **Key Sub-points to cover in this section:** {", ".join(section_sub_points) if section_sub_points else "N/A"}
        
        **Context of surrounding sections:**
        - The previous section was about: "{previous_section_title}"
        - The next section will be about: "{next_section_title}"

        **Instructions:**
        - Write a comprehensive, in-depth section covering the topic "{section_title}".
        - If provided, elaborate on all key sub-points, using them to structure the section's content.
        - Ensure your writing is aware of the surrounding topics to maintain a logical flow.
        - Incorporate relevant entities and demonstrate expertise by using practical examples or insights.
        - Persona: {brief.get("target_audience_persona")}
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}
        
        Return a JSON object with a single key "content_html" containing the HTML for this section (e.g., using <p>, <ul>, <h3> tags). Do NOT include the main <h2> tag for "{section_title}" itself.
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.7,
        )

    def generate_introduction(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[Optional[str], float]:
        brief = opportunity.get("blueprint", {}).get("ai_content_brief", {})
        prompt = f"""
        You are an expert copywriter. Write a compelling and hook-driven introduction for a blog post titled "{opportunity["keyword"]}".
        The introduction should be 2-3 paragraphs.
        - Immediately grab the reader's attention with a relatable problem or surprising statistic.
        - Briefly state the core problem or question the article will solve and why it matters.
        - End with a transition that clearly outlines what the reader will learn.
        - Target Audience: {brief.get("target_audience_persona")}
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}
        Return a JSON object with a single key "content_html" containing the HTML for the introduction (e.g., {{"content_html": "<p>...</p><p>...</p>"}}).
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.75,
        )

    def generate_conclusion(
        self, opportunity: Dict[str, Any], full_article_context: str
    ) -> Tuple[Optional[str], float]:
        cta_url = opportunity.get("client_cfg", {}).get("default_cta_url", "#")
        prompt = f"""
        You are an expert copywriter. Write a powerful conclusion for the following blog post.
        The conclusion should be 2 paragraphs.
        - Briefly summarize the most important takeaways from the article.
        - Provide a final, actionable thought or encouragement for the reader.
        - End with a compelling call-to-action that encourages the reader to visit {cta_url}.
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}

        **Full Article Context (for summary):**
        {full_article_context}

        Return a JSON object with a single key "content_html" containing the HTML for the conclusion (e.g., {{"content_html": "<p>...</p><p>...</p>"}}).
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.7,
        )

    def generate_section(
        self,
        opportunity: Dict[str, Any],
        section_title: str,
        section_sub_points: List[str],
        previous_section_content: str,
    ) -> Tuple[Optional[str], float]:
        brief = opportunity.get("blueprint", {}).get("ai_content_brief", {})
        prompt = f"""
        You are an expert SEO content writer and subject matter expert. Your task is to write a single, detailed section for a blog post about "{opportunity["keyword"]}".

        **Current Section to Write:** "{section_title}"
        **Key Sub-points to cover in this section:** {", ".join(section_sub_points) if section_sub_points else "N/A"}
        **Content from the Previous Section (for transition and context):**
        ...{previous_section_content[-1000:]}...

        **Instructions:**
        - Write a comprehensive, in-depth section covering the topic "{section_title}".
        - If provided, elaborate on all key sub-points, using them to structure the section's content.
        - Ensure a smooth, logical transition from the previous section's content.
        - Incorporate relevant entities and demonstrate expertise by using practical examples or insights.
        - Persona: {brief.get("target_audience_persona")}
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}
        
        Return a JSON object with a single key "content_html" containing the HTML for this section (e.g., using <p>, <ul>, <h3> tags). Do NOT include the main <h2> tag for "{section_title}" itself.
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.7,
        )
```

## File: agents/brief_assembler.py
```python
import logging
from typing import Dict, Any


class BriefAssembler:
    """
    Assembles the final AI content brief from the blueprint data.
    This agent acts as a transformation layer between the analysis blueprint
    and the actionable brief for the content generation AI.
    """

    def __init__(self, openai_client):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.openai_client = openai_client

    def _generate_dynamic_brief_attributes(self, blueprint: Dict[str, Any], client_cfg: Dict[str, Any]) -> Dict[str, Any]:
        """Uses an AI call to generate dynamic persona and goal."""
        try:
            target_keyword = blueprint.get("winning_keyword", {}).get("keyword", "the target topic")
            serp_overview = blueprint.get("serp_overview", {})
            
            prompt = f"""
            Based on the following SERP data for the keyword '{target_keyword}', define a target audience persona and a primary goal for a new piece of content.

            SERP Data:
            - Dominant Content Format: {serp_overview.get('dominant_content_format', 'N/A')}
            - People Also Ask: {', '.join(serp_overview.get('people_also_ask', []))}
            - Related Searches: {', '.join(serp_overview.get('related_searches', []))}

            Client's Brand Voice: "{client_cfg.get('brand_voice', 'expert and informative')}"

            Return a JSON object with two keys: 'target_audience_persona' and 'primary_goal'.
            The persona should be a brief, descriptive summary of the ideal reader.
            The goal should be a concise statement about what the content aims to achieve for that reader.
            """
            
            messages = [{"role": "user", "content": prompt}]
            schema = {
                "name": "generate_brief_attributes",
                "description": "Generates a target audience persona and primary goal for a piece of content.",
                "type": "object",
                "properties": {
                    "target_audience_persona": {
                        "type": "string",
                        "description": "A brief, descriptive summary of the ideal reader."
                    },
                    "primary_goal": {
                        "type": "string",
                        "description": "A concise statement about what the content aims to achieve for that reader."
                    }
                },
                "required": ["target_audience_persona", "primary_goal"],
                "additionalProperties": False
            }

            response_json, error = self.openai_client.call_chat_completion(
                messages=messages,
                model="gpt-5-nano",
                schema=schema
            )

            if error:
                raise Exception(f"AI call failed: {error}")

            return response_json

        except Exception as e:
            self.logger.error(f"Failed to generate dynamic brief attributes: {e}")
            # Fallback to static values
            return {
                "target_audience_persona": self._determine_persona("Blog Post", client_cfg),
                "primary_goal": f"To provide a comprehensive and helpful resource that ranks for '{target_keyword}'.",
            }

    def assemble_brief(
        self, blueprint: Dict[str, Any], client_id: str, client_cfg: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Constructs the AI content brief by extracting and structuring
        information from the blueprint.
        """
        winning_keyword_data = blueprint.get("winning_keyword", {})
        serp_overview = blueprint.get("serp_overview", {})
        content_intelligence = blueprint.get("content_intelligence", {})
        content_type = blueprint.get("recommended_strategy", {}).get(
            "content_format", "Blog Post"
        )
        
        dynamic_attrs = self._generate_dynamic_brief_attributes(blueprint, client_cfg)
        
        word_count_multiplier = self._get_word_count_multiplier(
            content_type, client_cfg
        )
        base_word_count = content_intelligence.get("recommended_word_count", 1500)
        target_word_count = int(base_word_count * word_count_multiplier)

        brief = {
            "target_keyword": winning_keyword_data.get("keyword"),
            "content_type": content_type,
            "target_audience_persona": dynamic_attrs.get("target_audience_persona"),
            "primary_goal": dynamic_attrs.get("primary_goal"),
            "target_word_count": target_word_count,
            "mandatory_sections": content_intelligence.get(
                "common_headings_to_cover", []
            ),
            "unique_angles_to_cover": content_intelligence.get(
                "unique_angles_to_include", []
            ),
            "questions_to_answer_directly": serp_overview.get("paa_questions", []),
            "key_entities_to_mention": content_intelligence.get(
                "key_entities_from_competitors", []
            ),
            "compelling_arguments_to_integrate": content_intelligence.get(
                "unique_arguments_from_competitors", []
            ),
            "core_questions_competitors_answer": content_intelligence.get(
                "core_questions_answered_by_competitors", []
            ),
            "related_topics_to_include": serp_overview.get("related_searches", []),
            "google_preferred_answers": serp_overview.get(
                "extracted_serp_features", []
            ),
            "dynamic_serp_instructions": self._get_dynamic_serp_instructions(
                serp_overview, content_intelligence, blueprint
            ),
            "source_and_inspiration_content": {
                "competitors_urls": [
                    comp["url"]
                    for comp in blueprint.get("competitor_analysis", [])
                    if "url" in comp
                ]
            },
            "client_id": client_id,
        }

        # --- START MODIFICATION ---
        # NEW: Add all the rich SERP data to the brief
        if serp_overview.get("knowledge_graph_facts"):
            brief["knowledge_graph_facts"] = serp_overview["knowledge_graph_facts"]
        if serp_overview.get("paid_ad_copy"):
            brief["paid_ad_copy"] = serp_overview["paid_ad_copy"]
        if serp_overview.get("ai_overview_sources"):
            brief["ai_overview_sources"] = serp_overview["ai_overview_sources"]
        if serp_overview.get("top_organic_faqs"):
            brief["top_organic_faqs"] = serp_overview["top_organic_faqs"]
        if serp_overview.get("top_organic_sitelinks"):
            brief["top_organic_sitelinks"] = serp_overview["top_organic_sitelinks"]
        if serp_overview.get("discussion_snippets"):
            brief["discussion_snippets"] = serp_overview["discussion_snippets"]

        all_about_search_terms = []
        all_about_related_terms = []
        for res in blueprint.get("serp_overview", {}).get("top_organic_results", []):
            if res.get("about_this_result_search_terms"):
                all_about_search_terms.extend(res["about_this_result_search_terms"])
            if res.get("about_this_result_related_terms"):
                all_about_related_terms.extend(res["about_this_result_related_terms"])

        if all_about_search_terms:
            brief["google_understanding_search_terms"] = list(
                set(all_about_search_terms)
            )
        if all_about_related_terms:
            brief["google_understanding_related_terms"] = list(
                set(all_about_related_terms)
            )
        # --- END MODIFICATION ---

        self.logger.info(f"Assembled AI content brief for '{brief['target_keyword']}'.")
        return brief

    def _get_dynamic_serp_instructions(
        self,
        serp_overview: Dict[str, Any],
        content_intelligence: Dict[str, Any],
        blueprint: Dict[str, Any],
    ) -> list[str]:
        """Generates specific instructions for the AI based on SERP features and content intelligence."""
        instructions = []
        if serp_overview.get("extracted_serp_features"):
            instructions.append(
                "Pay close attention to the 'google_preferred_answers' section. This contains content that Google has already featured in rich snippets, so it's a strong indication of what Google considers a good answer. Use it as a primary source of inspiration."
            )
        if serp_overview.get("serp_has_featured_snippet"):
            instructions.append(
                "Create a concise, clear paragraph early in the article that directly answers the main query to target the featured snippet."
            )
        if serp_overview.get("serp_has_ai_overview"):
            instructions.append(
                "The content must be exceptionally high-quality, original, and provide unique insights to stand out against Google's AI Overview."
            )
        if serp_overview.get("has_video_carousel"):
            instructions.append(
                "Structure content in a way that is easily adaptable into a video script, as video is a key format for this topic."
            )
        if serp_overview.get("knowledge_graph_facts"):
            instructions.append(
                f"Incorporate the key facts from the Knowledge Graph: {', '.join(serp_overview['knowledge_graph_facts'])}."
            )

        top_results_with_context = [
            r
            for r in serp_overview.get("top_organic_results", [])
            if r.get("about_this_result_source_info")
        ]
        if top_results_with_context:
            context_info = top_results_with_context[0]["about_this_result_source_info"]
            instructions.append(
                f"Google's 'About this Result' panel says the top result is relevant because: '{context_info}'. Use this to understand the core topic."
            )

        if serp_overview.get("discussions_and_forums_snippets"):
            instructions.append(
                f"Address user pain points from forum discussions: {' | '.join(serp_overview['discussions_and_forums_snippets'])}."
            )

        days_ago = serp_overview.get("serp_last_updated_days_ago")
        if days_ago is not None:
            if days_ago <= 30:
                instructions.append(
                    f"The SERP is fresh ({days_ago} days old). Ensure content reflects the latest information."
                )
            elif days_ago > 180:
                instructions.append(
                    f"The SERP is outdated ({days_ago} days old). This is an opportunity to publish a more current and comprehensive article."
                )

        # Instructions from content intelligence (including competitor weaknesses)
        if content_intelligence.get("key_entities_from_competitors"):
            instructions.append(
                f"Mention key entities identified from competitors: {', '.join(content_intelligence['key_entities_from_competitors'])}."
            )

            # Summarize competitor weaknesses based on new granular scores
            competitor_analysis = blueprint.get("competitor_analysis", [])
            if competitor_analysis and all(
                "overall_strength_score" in c
                for c in competitor_analysis
                if c.get("url")
            ):
                tech_scores = [
                    c.get("technical_strength_score", 100)
                    for c in competitor_analysis
                    if c.get("url")
                ]
                content_scores = [
                    c.get("content_quality_score", 100)
                    for c in competitor_analysis
                    if c.get("url")
                ]

                # NEW: Add specific technical weaknesses if identified
                all_tech_warnings = []
                for comp in competitor_analysis:
                    all_tech_warnings.extend(comp.get("technical_warnings", []))

                unique_issues_to_highlight = list(set(all_tech_warnings))[
                    :3
                ]  # Limit to top 3
                if unique_issues_to_highlight:
                    formatted_warnings = ", ".join(
                        [w.replace("_", " ") for w in unique_issues_to_highlight]
                    )
                    instructions.append(
                        f"EXPLOIT WEAKNESS: Top competitors exhibit specific technical flaws such as: {formatted_warnings}. Your content must be technically superior."
                    )
                elif tech_scores:  # Fallback to general if no specific warnings
                    avg_tech_score = sum(tech_scores) / len(tech_scores)
                    if avg_tech_score < 65:
                        instructions.append(
                            "EXPLOIT WEAKNESS: Top competitors are generally technically poor (e.g., slow page speed, render-blocking resources). A fast, well-built page has a strong advantage."
                        )

                if content_scores:
                    avg_content_score = sum(content_scores) / len(content_scores)
                    if avg_content_score < 65:
                        instructions.append(
                            "EXPLOIT WEAKNESS: Ranking content is low-quality, thin, or outdated. Win by providing significantly more depth and fresh information."
                        )

        return instructions

    def _determine_persona(self, content_type: str, client_cfg: Dict[str, Any]) -> str:
        """Determines the persona to use based on client config."""
        base_persona = client_cfg.get("expert_persona", "an expert writer")
        return f"{base_persona} who writes like a human"

    def _get_word_count_multiplier(
        self, content_type: str, client_cfg: Dict[str, Any]
    ) -> float:
        """
        Gets the word count multiplier for a given content type from the client config,
        falling back to a default if not found.
        """
        # Sanitize the content_type to match the keys in settings.ini
        # e.g., "Comprehensive Article" -> "comprehensive_article"
        sanitized_format = content_type.lower().replace(" ", "_")

        # Look up the specific multiplier, or use the default multiplier if not found
        return client_cfg.get(
            sanitized_format, client_cfg.get("default_multiplier", 1.2)
        )
```

## File: agents/content_auditor.py
```python
import logging
import textstat
from typing import Dict, Any, List, Optional  # ADD List
from bs4 import BeautifulSoup  # ADD this for HTML parsing
import re  # ADD this for regex checks
import requests


class ContentAuditor:
    """
    Audits the generated content for SEO and readability metrics,
    and checks for "publish-readiness" issues.
    """

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def _check_for_broken_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """Checks all <a> tags for 4xx or 5xx status codes."""
        issues = []
        links = soup.find_all("a", href=True)
        for link in links:
            href = link["href"]
            # Skip internal/anchor links and javascript links
            if (
                not href
                or href.startswith("#")
                or href.startswith("/")
                or href.startswith("javascript:")
            ):
                continue
            try:
                # Use a HEAD request for efficiency with a timeout
                response = requests.head(href, timeout=5, allow_redirects=True)
                if response.status_code >= 400:
                    issues.append(
                        {
                            "issue": "broken_link",
                            "context": f"URL '{href}' returned status {response.status_code}.",
                        }
                    )
            except requests.exceptions.Timeout:
                issues.append(
                    {
                        "issue": "link_timeout",
                        "context": f"Could not get response from '{href}' within 5 seconds.",
                    }
                )
            except requests.RequestException:
                issues.append(
                    {
                        "issue": "unreachable_link",
                        "context": f"Could not connect to URL '{href}'.",
                    }
                )
        return issues

    def audit_content(
        self,
        article_html: str,
        primary_keyword: str,
        blueprint: Dict[str, Any],
        client_cfg: Dict[str, Any],
        avg_competitor_readability: Optional[float] = None,
    ) -> Dict[str, Any]:
        """
        Audits the HTML content and returns a dictionary of metrics.
        """
        # Extract plain text for text-based analysis
        soup = BeautifulSoup(article_html, "html.parser")
        plain_text = soup.get_text(separator=" ", strip=True)
        html_issues = self._check_html_publish_readiness(article_html)
        if html_issues is None:
            html_issues = []

        # Add broken link check results
        broken_link_issues = self._check_for_broken_links(soup)
        html_issues.extend(broken_link_issues)

        word_count = len(plain_text.split())
        target_word_count = blueprint.get("ai_content_brief", {}).get(
            "target_word_count", 0
        )
        if target_word_count > 0:
            deviation = abs(word_count - target_word_count) / target_word_count
            if deviation > 0.20:
                html_issues.append(
                    {
                        "issue": "word_count_deviation",
                        "context": f"Actual count ({word_count}) deviates from target ({target_word_count}) by more than 20%.",
                    }
                )

        readability_score = textstat.flesch_kincaid_grade(plain_text)
        # Calculate additional metrics for comprehensive audit (Task 9.1)
        smog_score = textstat.smog_index(plain_text)
        coleman_liau_score = textstat.coleman_liau_index(plain_text)

        persona = blueprint.get("ai_content_brief", {}).get(
            "target_audience_persona", "General audience"
        )

        # W13 FIX: Determine Readability Mismatch and Required Refinement Command
        refinement_command = None
        readability_assessment = f"Flesch-Kincaid Grade Level: {readability_score:.1f}."

        if avg_competitor_readability is not None:
            readability_assessment += (
                f" (Avg. Competitor F-K: {avg_competitor_readability:.1f})."
            )
            if (
                abs(readability_score - avg_competitor_readability) > 3.0
            ):  # If our score is more than 3 grades off
                if readability_score < avg_competitor_readability:
                    readability_assessment += " Assessment: CRITICAL: Content is significantly simpler than competitors. Consider increasing complexity."
                    refinement_command = f"Increase the complexity to match competitor average of Flesch-Kincaid Grade Level {avg_competitor_readability:.1f}."
                else:
                    readability_assessment += " Assessment: CRITICAL: Content is significantly more complex than competitors. Consider simplifying."
                    refinement_command = f"Simplify the content to match competitor average of Flesch-Kincaid Grade Level {avg_competitor_readability:.1f}."
            else:
                readability_assessment += (
                    " Assessment: Readability is consistent with top competitors."
                )
        else:
            # Fallback to persona-based assessment if no competitor average is provided
            if "expert" in persona.lower() or "planner" in persona.lower():
                if readability_score < 9.5:
                    readability_assessment += " Assessment: CRITICAL: Content is likely oversimplified (Grade < 9.5)."
                    refinement_command = "Increase the complexity and authoritative tone of the writing to target a Flesch-Kincaid Grade Level of 10 or higher."
                else:
                    readability_assessment += (
                        " Assessment: Appropriate complexity for an expert audience."
                    )
            else:
                if readability_score > 12 or smog_score > 10.0:
                    readability_assessment += " Assessment: CRITICAL: Content is too academic or complex (Grade > 12 or SMOG > 10.0)."
                    refinement_command = "Simplify the complexity and reduce sentence length to target a Flesch-Kincaid Grade Level between 7 and 9, and a SMOG Index under 8."
                else:
                    readability_assessment += (
                        " Assessment: Appropriate complexity for a general audience."
                    )

        entity_metrics = self._check_entity_coverage(plain_text, blueprint)
        if entity_metrics.get("score", 100) < 75.0 and entity_metrics.get(
            "missing"
        ):  # Only flag if there are actually missing entities
            missing_entities_str = ", ".join(entity_metrics.get("missing", []))
            html_issues.append(
                {
                    "issue": "critical_entity_gap",
                    "context": f"Entity coverage is below 75%. Missing: {missing_entities_str}",
                }
            )

        # Ensure the final return object from audit_content includes all new data:
        return {
            "flesch_kincaid_grade": readability_score,
            "smog_index": smog_score,
            "coleman_liau_index": coleman_liau_score,
            "readability_assessment": readability_assessment,
            "refinement_command": refinement_command,
            "entity_coverage_score": entity_metrics.get("score", 0),
            "missing_entities": entity_metrics.get("missing", []),
            "covered_sections": None,
            "publish_readiness_issues": html_issues,  # This now includes broken links and critical entity gaps
        }

    def _check_entity_coverage(
        self, article_text: str, blueprint: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Checks if the key entities from the blueprint are present in the article text.
        """
        # ... (existing logic)
        entities = blueprint.get("ai_content_brief", {}).get(
            "key_entities_to_mention", []
        )
        if not entities:
            return {"score": 100, "missing": []}

        missing_entities = []
        for entity in entities:
            # Heuristic check: Look for exact match or simple pluralization (Task 11.1)
            if entity.endswith("s"):
                # If entity is already plural (e.g., 'tools'), check for exact match only
                pattern = r"\b" + re.escape(entity) + r"\b"
            else:
                # Check for singular or plural form (e.g., 'tool' or 'tools')
                pattern = r"\b" + re.escape(entity) + r"s?\b"
            if not re.search(pattern, article_text, re.IGNORECASE):
                missing_entities.append(entity)

        coverage_score = (
            100 - (len(missing_entities) / len(entities) * 100) if entities else 100
        )
        return {
            "score": coverage_score,
            "missing": missing_entities,
            "found_count": len(entities) - len(missing_entities),
            "total_expected": len(entities),
        }

    def _check_html_publish_readiness(self, article_html: str) -> List[Dict[str, Any]]:
        """
        Performs specific checks on the final HTML for publish-readiness, returning structured issues.
        """
        issues = []
        soup = BeautifulSoup(article_html, "html.parser")

        # Check for unresolved image placeholders
        placeholder_pattern = r"\[\[IMAGE_ID:\s*(.*?)\s*PROMPT:\s*(.*?)\s*\]\]"
        placeholders_found = re.findall(placeholder_pattern, article_html)
        if placeholders_found:
            issues.append(
                {
                    "issue": "unresolved_placeholder",
                    "context": f"{len(placeholders_found)} image placeholders remain in the text.",
                }
            )

        # Check for empty headings
        for h_tag in soup.find_all(re.compile(r"^h[1-6]$")):
            if not h_tag.get_text(strip=True):
                issues.append({"issue": "empty_heading", "context": str(h_tag)})

        # Check for extremely short paragraphs
        for p_tag in soup.find_all("p"):
            text = p_tag.get_text(strip=True)
            if 0 < len(text.split()) < 5:
                issues.append({"issue": "short_paragraph", "context": str(p_tag)})

        return issues
```

## File: agents/html_formatter.py
```python
import logging
from typing import Dict, Any, List, Optional
import os
import re
import markdown
from bs4 import BeautifulSoup
from datetime import datetime
from backend.core import utils


class HtmlFormatter:
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def _convert_markdown_tables_to_html(self, html_body: str) -> str:
        """Converts Markdown tables to HTML using markdown library directly."""
        return markdown.markdown(html_body, extensions=["tables", "fenced_code"])

    def _insert_internal_links(
        self, soup: BeautifulSoup, internal_links: List[Dict[str, str]]
    ) -> None:
        """
        Inserts internal links into the BeautifulSoup object based on specific contextual suggestions from an AI agent.
        """
        if not internal_links:
            return

        linked_anchors = set()

        for link_data in internal_links:
            anchor_text = link_data.get("anchor_text")
            url = link_data.get("url")
            context_paragraph = link_data.get("context_paragraph_text")

            if (
                not all([anchor_text, url, context_paragraph])
                or anchor_text.lower() in linked_anchors
            ):
                continue

            # Find all paragraphs that contain the exact context text
            potential_paragraphs = soup.find_all(
                "p", string=re.compile(re.escape(context_paragraph))
            )

            for p_tag in potential_paragraphs:
                # Find the text node within this paragraph that contains the anchor
                text_node = p_tag.find(
                    string=re.compile(re.escape(anchor_text), re.IGNORECASE)
                )

                if text_node and not text_node.find_parent(
                    "a"
                ):  # Ensure it's not already linked
                    match = re.search(
                        re.escape(anchor_text), str(text_node), re.IGNORECASE
                    )
                    if match:
                        before_text = str(text_node)[: match.start()]
                        matched_text = match.group(0)
                        after_text = str(text_node)[match.end() :]

                        link_tag = soup.new_tag("a", href=url)
                        link_tag.string = matched_text

                        new_content = []
                        if before_text:
                            new_content.append(before_text)
                        new_content.append(link_tag)
                        if after_text:
                            new_content.append(after_text)

                        text_node.replace_with(*new_content)
                        linked_anchors.add(anchor_text.lower())
                        break  # Move to the next link suggestion once placed

    def _generate_toc(self, soup: BeautifulSoup) -> None:
        """Generates and inserts a Table of Contents from H2 tags into the BeautifulSoup object."""
        toc_list = soup.new_tag("ul", **{"class": "toc-list"})
        h2_tags = soup.find_all("h2")

        if len(h2_tags) < 2:
            return  # No TOC needed for less than 2 headings

        # Add unique IDs to H2 tags and build TOC
        for i, h2 in enumerate(h2_tags):
            slug = utils.slugify(h2.text)
            if not slug:  # Fallback for empty/unsluggable H2s
                slug = f"section-{i + 1}"
            h2["id"] = slug  # Add ID to H2 for linking

            toc_item = soup.new_tag("li")
            toc_link = soup.new_tag("a", href=f"#{slug}")
            toc_link.string = h2.text
            toc_item.append(toc_link)
            toc_list.append(toc_item)

        toc_header = soup.new_tag("h2")
        toc_header.string = "Table of Contents"
        toc_header["id"] = "table-of-contents"  # Give TOC its own ID

        first_h2 = soup.find("h2")
        if first_h2:
            first_h2.insert_before(toc_list)
            first_h2.insert_before(toc_header)
        else:
            # Fallback if no H2s exist, place it after the first paragraph or at the start
            first_p = soup.find("p")
            if first_p:
                first_p.insert_after(toc_header)
                first_p.insert_after(toc_list)
            else:
                soup.insert(0, toc_list)
                soup.insert(0, toc_header)

    def _generate_schema_org(
        self, soup: BeautifulSoup, opportunity: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Dynamically generates Schema.org JSON-LD from the final HTML soup.
        """
        schema_graph: List[Dict[str, Any]] = []
        client_cfg = opportunity.get("client_cfg", {})
        slug = opportunity.get("blueprint", {}).get("slug", "default-slug")

        domain = client_cfg.get("target_domain", "")
        article_url = f"https://{domain}/{slug}" if domain else slug
        publisher_name = domain or "Publisher Name"

        h1_tag = soup.find("h1")
        article_headline = (
            h1_tag.get_text(strip=True)
            if h1_tag
            else opportunity.get("keyword", "Article")
        )

        article_schema = {
            "@type": "BlogPosting",
            "@id": f"{article_url}#article",
            "mainEntityOfPage": {"@id": article_url},
            "headline": article_headline,
            "author": {
                "@type": client_cfg.get("schema_author_type", "Organization"),
                "name": client_cfg.get("default_author_name", "Author"),
            },
            "publisher": {"@type": "Organization", "name": publisher_name},
            "datePublished": datetime.now().isoformat(),
            "dateModified": datetime.now().isoformat(),
            "additionalProperties": False,
        }
        schema_graph.append(article_schema)

        # Dynamic HowTo Schema
        how_to_headings = soup.find_all(
            ["h2", "h3"], string=re.compile(r"how to", re.IGNORECASE)
        )
        for heading in how_to_headings:
            ol = heading.find_next("ol")
            if ol:
                steps = [
                    {"@type": "HowToStep", "text": li.get_text(strip=True)}
                    for li in ol.find_all("li")
                ]
                if steps:
                    schema_graph.append(
                        {
                            "@type": "HowTo",
                            "name": heading.get_text(strip=True),
                            "step": steps,
                        }
                    )

        return {"@context": "https://schema.org", "@graph": schema_graph}

    def format_final_package(
        self,
        opportunity: Dict[str, Any],
        internal_linking_suggestions: Optional[List[Dict[str, str]]] = None,
        in_article_images_data: Optional[List[Dict[str, Any]]] = None,
    ) -> Dict[str, Any]:
        """
        Constructs the final content package, now including schema generation.
        """
        # ... (existing code for html_body_str, soup creation, internal linking, ToC, etc.) ...
        ai_content = opportunity.get("ai_content", {})
        client_cfg = opportunity.get("client_cfg", {})
        html_body_str = ai_content.get("article_body_html", "")
        soup = BeautifulSoup(f"<div>{html_body_str}</div>", "html.parser")

        if internal_linking_suggestions:
            self._insert_internal_links(soup, internal_linking_suggestions)

        if client_cfg.get("generate_toc", True):
            self._generate_toc(soup)

        # ... (image replacement logic) ...

        article_html_final = (
            str(soup.body.decode_contents()) if soup.body else str(soup)
        )

        # --- NEW: Call the schema generator ---
        schema_org_json = self._generate_schema_org(soup, opportunity)

        featured_image = opportunity.get("featured_image_data", {})
        featured_image_relative_path = (
            f"/api/images/{os.path.basename(featured_image.get('local_path'))}"
            if featured_image and featured_image.get("local_path")
            else None
        )

        return {
            "meta_title": ai_content.get("meta_title", "No Title"),
            "meta_description": ai_content.get("meta_description", ""),
            "article_html_final": article_html_final,
            "schema_org_json": schema_org_json,  # Add the generated schema to the final package
            "featured_image_path": featured_image.get("local_path"),
            "featured_image_relative_path": featured_image_relative_path,
            "social_media_posts": opportunity.get("social_media_posts_json", []),
        }
```

## File: agents/image_generator.py
```python
import logging
import os
from typing import Dict, Any, Tuple, Optional, List

from backend.external_apis.pexels_client import PexelsClient, download_image_from_url
from backend.core import utils

from PIL import Image, ImageDraw, ImageFont, ImageColor


class ImageGenerator:
    """
    Agent for finding featured and in-article images from Pexels.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

        self.pexels_client = None
        if self.config.get("pexels_api_key"):
            try:
                self.pexels_client = PexelsClient(self.config["pexels_api_key"])
            except ValueError as e:
                self.logger.warning(
                    f"Pexels client could not be initialized: {e}. Image generation will be skipped."
                )

    def _add_text_overlay(self, image_path: str, text: str) -> str:
        """Adds a text overlay to the image based on configured settings."""
        if not self.config.get("overlay_text_enabled", False):
            return image_path  # If disabled, return original path

        try:
            image = Image.open(image_path).convert(
                "RGBA"
            )  # Convert to RGBA for alpha channel in overlay background
            draw = ImageDraw.Draw(image)

            text_color = ImageColor.getrgb(
                self.config.get("overlay_text_color", "#FFFFFF")
            )
            bg_color_hex = self.config.get("overlay_background_color", "#00000080")
            # Extract RGB and alpha from RGBA hex
            bg_color = ImageColor.getrgb(
                bg_color_hex
            )  # This returns (R,G,B) for #RRGGBB or (R,G,B,A) for #RRGGBBAA
            # Ensure bg_color is (R,G,B,A) if alpha is specified
            if len(bg_color) == 3 and len(bg_color_hex) == 9:  # #RRGGBBAA format
                bg_alpha = int(bg_color_hex[7:9], 16)
                bg_color = bg_color + (bg_alpha,)
            elif len(bg_color) == 3:  # default to some alpha if only RGB is given
                bg_color = bg_color + (128,)  # Default 50% opacity

            font_size = self.config.get("overlay_font_size", 40)

            try:
                # Use a reliable path to a bundled font file.
                # Assumes a `resources/fonts` directory exists relative to the project root.
                font_path = os.path.join(
                    os.path.dirname(__file__),
                    "..",
                    "..",
                    "resources",
                    "fonts",
                    "DejaVuSans-Bold.ttf",
                )
                if not os.path.exists(font_path):
                    raise IOError("Bundled font file not found.")
                font = ImageFont.truetype(font_path, font_size)
            except IOError:
                self.logger.warning(
                    f"Could not load the bundled font at {font_path}. "
                    "Falling back to default bitmap font. Text quality will be poor. "
                    "Ensure the font file exists."
                )
                font = ImageFont.load_default()

            # Use textbbox (or textsize for older PIL versions)
            # draw.textbbox is preferred
            try:
                text_bbox = draw.textbbox((0, 0), text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_height = text_bbox[3] - text_bbox[1]
            except (
                AttributeError
            ):  # Fallback for older PIL where textbbox might not exist
                text_width, text_height = draw.textsize(text, font=font)

            # Position the text based on configuration
            position = self.config.get("overlay_position", "bottom_center")
            padding = 20  # Padding around text

            x, y = 0, 0
            if "center" in position:
                x = (image.width - text_width) / 2
            elif "left" in position:
                x = padding
            elif "right" in position:
                x = image.width - text_width - padding

            if "bottom" in position:
                y = image.height - text_height - padding
            elif "top" in position:
                y = padding
            elif "center" in position:  # Vertical center
                y = (image.height - text_height) / 2

            # Create a transparent layer for the background
            overlay = Image.new("RGBA", image.size, (255, 255, 255, 0))
            draw_overlay = ImageDraw.Draw(overlay)

            # Draw semi-transparent background
            draw_overlay.rectangle(
                (
                    x - padding / 2,
                    y - padding / 2,
                    x + text_width + padding / 2,
                    y + text_height + padding / 2,
                ),
                fill=bg_color,
            )

            image = Image.alpha_composite(image, overlay)  # Composite the background
            draw = ImageDraw.Draw(image)  # Re-get draw object for updated image

            draw.text((x, y), text, font=font, fill=text_color)

            # Save the modified image
            new_image_path = image_path.replace(".jpeg", "-overlay.jpeg")
            image.convert("RGB").save(new_image_path)
            return new_image_path
        except Exception as e:
            self.logger.error(f"Failed to add text overlay to image: {e}")
            return image_path

    def generate_featured_image(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Finds and saves the featured image for the article from Pexels.
        """
        if not self.pexels_client:
            self.logger.warning(
                "Pexels client not initialized. Cannot generate featured image."
            )
            return None, 0.0

        search_query = opportunity["keyword"]
        self.logger.info(f"Searching Pexels for featured image for '{search_query}'...")

        pexels_photos, cost = self.pexels_client.search_photos(
            query=search_query, orientation="landscape", size="large", per_page=1
        )

        if not pexels_photos:
            self.logger.warning(
                f"No suitable featured image found on Pexels for '{search_query}'."
            )
            return None, cost

        best_photo = pexels_photos[0]
        best_photo_url = (
            best_photo["src"].get("landscape")
            or best_photo["src"].get("large2x")
            or best_photo["src"].get("original")
        )

        if not best_photo_url:
            self.logger.warning(
                f"Pexels photo found, but no usable URL for '{search_query}'."
            )
            return None, cost

        image_dir = "generated_images"
        os.makedirs(image_dir, exist_ok=True)
        file_path = os.path.join(
            image_dir,
            f"pexels-featured-{utils.slugify(search_query)}-{best_photo['id']}.jpeg",
        )

        local_path = download_image_from_url(best_photo_url, file_path)

        if not local_path:
            self.logger.error(
                f"Failed to download featured image from Pexels: {best_photo_url}"
            )
            return None, cost

        # Add text overlay
        meta_title = opportunity.get("ai_content", {}).get(
            "meta_title", opportunity["keyword"]
        )
        local_path = self._add_text_overlay(local_path, meta_title)

        self.logger.info(
            f"Successfully sourced featured image from Pexels: {local_path}"
        )
        return {
            "type": "featured",
            "search_query": search_query,
            "local_path": local_path,
            "remote_url": best_photo_url,  # Store Pexels URL directly
            "alt_text": best_photo["alt"],
            "source_id": best_photo["id"],
            "source": "Pexels",
        }, cost

    def _simplify_prompt_for_pexels(self, descriptive_prompt: str) -> str:
        """
        Uses an LLM to extract 3-5 high-impact keywords suitable for a stock photo search
        from a more descriptive AI image prompt.
        """
        if not descriptive_prompt or not self.openai_client:
            return descriptive_prompt  # Fallback to original if no client or prompt

        self.logger.info(
            f"Refining image prompt for Pexels search: '{descriptive_prompt}'"
        )

        prompt_messages = [
            {
                "role": "system",
                "content": "You are a concise keyword extractor for stock photo sites. Extract 3-5 key nouns, adjectives, or short phrases from the user's descriptive image prompt that would be most effective for searching a stock photo library like Pexels. Return only a comma-separated list of keywords.",
            },
            {"role": "user", "content": f"Descriptive prompt: '{descriptive_prompt}'"},
        ]

        # Use a low temperature for predictable, factual output
        extracted_keywords_str, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
                            model=self.config.get("default_model", "gpt-5-nano"),  # Use a cost-effective model for this            temperature=0.1,
            max_completion_tokens=50,  # Keep output very short
            schema={
                "name": "extract_keywords",
                "type": "object",
                "properties": {
                    "keywords": {
                        "type": "string",
                        "description": "Comma-separated keywords.",
                    }
                },
                "required": ["keywords"],
                "additionalProperties": False
            },
        )

        if error or not extracted_keywords_str:
            self.logger.warning(
                f"Failed to extract keywords for Pexels. Falling back to original prompt. Error: {error}"
            )
            return descriptive_prompt  # Fallback to original prompt

        # The AI should return a dictionary with a 'keywords' key
        if (
            isinstance(extracted_keywords_str, dict)
            and "keywords" in extracted_keywords_str
        ):
            return extracted_keywords_str["keywords"]
        elif isinstance(
            extracted_keywords_str, str
        ):  # Fallback if AI doesn't follow schema perfectly
            return extracted_keywords_str

        return descriptive_prompt  # Final fallback

    # In class ImageGenerator, replace the generate_images_from_prompts method
    def generate_images_from_prompts(
        self, prompts: List[str]
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Finds and saves in-article images from Pexels based on a list of specific prompts.
        """
        if not self.pexels_client:
            self.logger.warning(
                "Pexels client not initialized. Cannot generate images from prompts."
            )
            return [], 0.0

        images_data = []
        total_cost = 0.0

        for i, prompt in enumerate(prompts):
            search_query = self._simplify_prompt_for_pexels(prompt)
            self.logger.info(
                f"Searching Pexels for in-article image with simplified query: '{search_query}' (from prompt: '{prompt}')..."
            )

            pexels_photos, cost = self.pexels_client.search_photos(
                query=search_query, orientation="landscape", size="large", per_page=1
            )
            total_cost += cost

            if pexels_photos:
                photo = pexels_photos[0]
                photo_url = photo["src"].get("large") or photo["src"].get("original")

                if photo_url:
                    image_dir = "generated_images"
                    os.makedirs(image_dir, exist_ok=True)
                    file_path = os.path.join(
                        image_dir,
                        f"pexels-in-article-{utils.slugify(search_query)}-{photo['id']}.jpeg",
                    )
                    local_path = download_image_from_url(photo_url, file_path)

                    if local_path:
                        images_data.append(
                            {
                                "type": f"in_article_{i + 1}",
                                "search_query": search_query,
                                "original_prompt": prompt,
                                "local_path": local_path,
                                "remote_url": photo_url,
                                "alt_text": photo.get("alt") or prompt,
                                "source_id": photo["id"],
                                "source": "Pexels",
                            }
                        )
                        self.logger.info(
                            f"Successfully sourced in-article image from Pexels: {local_path}"
                        )
                        continue

            self.logger.warning(
                f"Could not find a suitable Pexels image for prompt: '{prompt}'."
            )

        return images_data, total_cost
```

## File: agents/internal_linking_suggester.py
```python
import logging
from typing import Dict, Any, List, Tuple

from backend.external_apis.openai_client import OpenAIClientWrapper
from backend.data_access.database_manager import DatabaseManager


class InternalLinkingSuggester:
    def __init__(
        self,
        openai_client: OpenAIClientWrapper,
        config: Dict[str, Any],
        db_manager: DatabaseManager,
    ):
        self.openai_client = openai_client
        self.config = config
        self.db_manager = db_manager
        self.logger = logging.getLogger(self.__class__.__name__)

    def suggest_links(
        self,
        article_text: str,
        key_entities: List[str],
        target_domain: str,
        client_id: str,
    ) -> Tuple[List[Dict[str, str]], float]:
        existing_articles = self._fetch_existing_articles(client_id)
        if not existing_articles:
            return [], 0.0

        prompt_messages = self._build_suggestion_prompt(
            article_text, key_entities, existing_articles
        )

        schema = {
            "name": "suggest_contextual_internal_links",
            "type": "object",
            "properties": {
                "internal_links": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "anchor_text": {
                                "type": "string",
                                "description": "The exact, natural phrase from the paragraph to be used as anchor text.",
                            },
                            "url": {
                                "type": "string",
                                "description": "The corresponding relative URL from the available articles list.",
                            },
                            "context_paragraph_text": {
                                "type": "string",
                                "description": "The full, exact text of the paragraph where the anchor text was found. This will be used to pinpoint the link location.",
                            },
                        },
                        "required": ["anchor_text", "url", "context_paragraph_text"],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["internal_links"],
            "additionalProperties": False,
        }

        response, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
        )

        if error or not response:
            self.logger.error(
                f"Failed to get contextual internal linking suggestions from AI: {error}"
            )
            return [], self.openai_client.latest_cost

        return response.get("internal_links", []), self.openai_client.latest_cost

    def _fetch_existing_articles(self, client_id: str) -> List[Dict[str, str]]:
        """
        Fetches a list of already 'generated' articles for the given client from the local DB.
        """
        self.logger.info(
            f"Fetching existing published articles for internal linking for client: {client_id}."
        )

        if not self.config.get("enable_automated_internal_linking", False):
            self.logger.info(
                "Automated internal linking is disabled by client configuration."
            )
            return []

        existing_articles = self.db_manager.get_published_articles_for_linking(
            client_id
        )

        if not existing_articles:
            self.logger.warning(
                f"No existing 'published' articles found for client '{client_id}' to use for internal linking suggestions."
            )
            return []

        self.logger.info(
            f"Found {len(existing_articles)} published articles for internal linking."
        )
        return existing_articles

    def _build_suggestion_prompt(
        self,
        linking_context: str,
        key_entities: List[str],
        existing_articles: List[Dict[str, str]],
    ) -> List[Dict[str, str]]:
        """Builds the prompt for context-aware AI internal linking suggestion."""

        existing_articles_text = "\n".join(
            [
                f'- Title: "{article["title"]}", Relative URL: {article["url"]}'
                for article in existing_articles
            ]
        )

        prompt = f"""
        You are an expert SEO strategist. Your task is to analyze a new blog post and identify the most semantically relevant opportunities to link to existing articles on the site.

        **Main Article Text (as HTML):**
        ```html
        {linking_context}
        ```

        **Available Published Articles to Link To:**
        {existing_articles_text}

        **Instructions:**
        1. Read the main article HTML thoroughly to understand its context and structure.
        2. For each available published article, identify the single BEST paragraph in the main article to place a link. The best location is one that is highly contextually and semantically related to the title of the published article.
        3. From that best location, extract a natural, compelling phrase of 3-7 words to use as the anchor text.
        4. Suggest a maximum of 3-5 of the most relevant internal links.
        5. Return your suggestions in the required JSON format. For 'context_paragraph_text', provide the full, clean text content of the paragraph where the link should be placed.
        """
        return [{"role": "user", "content": prompt}]
```

## File: agents/prompt_assembler.py
```python
import logging
from typing import Dict, Any, List

from backend.data_access.database_manager import DatabaseManager


class DynamicPromptAssembler:
    def __init__(self, db_manager: DatabaseManager):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.db_manager = db_manager

    def build_prompt(self, opportunity: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        Constructs the detailed prompt for the article generation AI using a safe,
        user-configurable template, now with rich SERP data.
        """
        blueprint = opportunity.get("blueprint", {})
        brief = blueprint.get("ai_content_brief", {})
        strategy = blueprint.get("recommended_strategy", {})
        client_cfg = opportunity.get("client_cfg", {})
        num_images = client_cfg.get("num_in_article_images", 2)

        template = client_cfg.get("custom_prompt_template")
        if not template or not template.strip():
            template = """Write a comprehensive, helpful, and expert-level blog post on [TOPIC]. The article must demonstrate first-hand experience and deep expertise. Structure the content for maximum readability and SEO impact. The post must:

        - Be approximately [WORD_COUNT] words, providing authoritative depth on the topic.
        - Target the primary keyword "[PRIMARY KEYWORD]" and naturally incorporate related LSI keywords: [LSI/secondary keywords], along with relevant entities, synonyms, and contextually related concepts to ensure topical completeness.
        - **Demonstrate E-E-A-T (Experience, Expertise, Authoritativeness, Trustworthiness):**
            - Start with a clear, direct 1-2 sentence summary that immediately answers the user's core question.
            - Write from a first-person or expert perspective. Include at least one hypothetical scenario, relatable anecdote, or personal insight to signal direct experience.
            - Cite specific data or statistics and attribute them to a source (e.g., 'According to a 2023 study by...').
            - Include multiple answer formats: short direct responses, step-by-step instructions, and quick takeaway lists (bullet points) so AI models and users can easily extract information.
        - Structure the article with a logical flow using clear subheadings (H2s and H3s).
        - Include a "Frequently Asked Questions" (FAQ) section at the end using real-world questions users search for, written in a conversational Q&A style.
        - Naturally promote [CTA_URL] with a relevant call-to-action at the end of the post.
        - **AVOID:** Do not use generic filler, over-optimization, or unnatural keyword stuffing. Focus on topical relevance, not keyword density. Avoid making unsubstantiated claims.
        """

        default_cta_url = client_cfg.get(
            "default_cta_url", "https://profitparrot.com/contact/"
        )
        recommended_word_count = brief.get("target_word_count", 1000)
        expert_persona_from_brief = brief.get(
            "target_audience_persona",
            client_cfg.get("expert_persona", "an expert writer"),
        )
        replacements = {
            "[TOPIC]": brief.get("target_keyword", "the topic"),
            "[PRIMARY KEYWORD]": brief.get("target_keyword", "the topic"),
            "[LSI/secondary keywords]": ", ".join(brief.get("lsi_keywords", [])),
            "[WORD_COUNT]": str(recommended_word_count),
            "[CTA_URL]": default_cta_url,
            "%%NUM_IMAGES%%": str(num_images),
            "[PERSONA]": expert_persona_from_brief,
        }

        base_instructions = template
        for placeholder, value in replacements.items():
            base_instructions = base_instructions.replace(placeholder, str(value))

        persona = brief.get("target_audience_persona", "General audience")
        if "expert" in persona.lower() or "planner" in persona.lower():
            readability_instruction = "The tone must be highly sophisticated and authoritative. Maintain a Flesch-Kincaid Grade level of 10 or higher."
        elif "general" in persona.lower() or "beginner" in persona.lower():
            readability_instruction = "The tone must be clear and accessible. Maintain a Flesch-Kincaid Grade level between 7 and 9."
        else:
            readability_instruction = ""

        if readability_instruction:
            base_instructions += (
                f"\n- **Readability Target:** {readability_instruction}"
            )

        base_instructions += f"\n- Adopt the persona of {expert_persona_from_brief}."
        base_instructions += "\n- Include 1-2 'Expert Tips' in blockquotes."

        client_knowledge_base = client_cfg.get("client_knowledge_base")
        if client_knowledge_base and client_knowledge_base.strip():
            base_instructions += "\n\n**CLIENT KNOWLEDGE BASE (CRITICAL CONTEXT):**\n"
            base_instructions += "The following information is crucial for content accuracy and brand alignment. Incorporate relevant details naturally and factually:\n"
            base_instructions += f"{client_knowledge_base}\n"
            base_instructions += "Prioritize accuracy based on this knowledge base.\n"

        base_instructions += "\n- If the content contains data suitable for a table (e.g., comparisons, specifications, statistics), format it as a Markdown table."
        base_instructions += "\n- Include one link to a non-competing, high-authority external resource to back up a key statistic or claim."
        base_instructions += "\n- For in-article images, use a placeholder with the exact format `[[IMAGE: <A descriptive prompt for the image>]]`. For example: `[[IMAGE: A bar chart showing SEO growth over time]]`."

        # --- START MODIFICATION ---
        # NEW: Add instructions based on rich SERP data
        dynamic_serp_data_instructions = []

        if brief.get("knowledge_graph_facts"):
            facts_str = "\n- ".join(brief["knowledge_graph_facts"])
            dynamic_serp_data_instructions.append(
                f"**CRITICAL:** Incorporate these verified facts from Google's Knowledge Graph directly into the article to ensure factual accuracy and boost E-A-T:\n- {facts_str}"
            )

        if brief.get("paid_ad_copy"):
            paid_titles = [ad["title"] for ad in brief["paid_ad_copy"]]
            paid_descriptions = [ad["description"] for ad in brief["paid_ad_copy"]]
            dynamic_serp_data_instructions.append(
                f"**HIGH PRIORITY:** Analyze the following top paid ad copy to understand high-conversion language, primary value propositions, and key pain points. Use these insights to craft compelling article headlines, the introduction, and calls-to-action:\n- Titles: {paid_titles}\n- Descriptions: {paid_descriptions}"
            )

        if brief.get("top_organic_sitelinks"):
            sitelinks_str = "\n- ".join(brief["top_organic_sitelinks"])
            dynamic_serp_data_instructions.append(
                f"**MANDATORY:** Include dedicated sections (H2s or H3s) covering the following high-priority subtopics identified from competitor sitelinks:\n- {sitelinks_str}"
            )

        if brief.get("top_organic_faqs"):
            faqs_str = "\n- ".join(brief["top_organic_faqs"])
            dynamic_serp_data_instructions.append(
                f"**MANDATORY:** Create a dedicated 'Frequently Asked Questions' section (as an H2) at the end of the article, using these exact questions from competitor FAQ snippets:\n- {faqs_str}"
            )

        if brief.get("ai_overview_sources"):
            sources_str = "\n- ".join(brief["ai_overview_sources"])
            dynamic_serp_data_instructions.append(
                f"**STRATEGIC:** Give analytical priority to concepts and insights derived from these authoritative sources used by Google's own AI Overview:\n- {sources_str}"
            )

        if brief.get("discussion_snippets"):
            snippets_str = "\n- ".join(brief["discussion_snippets"])
            dynamic_serp_data_instructions.append(
                f"**TONE & EXPERIENCE:** Analyze the tone, specific pain points, and real-world language from these discussion snippets. Infuse the article with a personal, experience-driven, and authentic voice to directly address user concerns:\n- {snippets_str}"
            )

        # Append to dynamic instructions list
        dynamic_instructions = [
            f"**Primary Content Format:** Your output should be a '{strategy.get('content_format', 'Comprehensive Article')}'.",
            f"**Strategic Goal:** {strategy.get('strategic_goal', '')}",
        ]
        if brief.get("dynamic_serp_instructions"):
            dynamic_instructions.append(
                "**Tactical Guidance:**\n"
                + "\n".join(
                    [f"- {inst}" for inst in brief["dynamic_serp_instructions"]]
                )
            )

        # Combine all dynamic instructions
        all_dynamic_instructions = dynamic_instructions + dynamic_serp_data_instructions
        dynamic_instructions_str = "\n- ".join(all_dynamic_instructions)

        final_prompt_content = f"""You are an expert SEO writer. Generate a complete blog post package in JSON format based on the brief below.

        **Topic:** "{brief.get("target_keyword")}"
        **Core Instructions:**
        {base_instructions}
        **Dynamic Strategic Instructions:**
        - {dynamic_instructions_str}
        **Mandatory Information & Structure:**
        - **WORD COUNT: The final article body MUST be AT LEAST {recommended_word_count} words.** This is a strict requirement.
        - To meet the word count, elaborate on each of the following sections, providing detailed explanations, examples, and insights.
        - Must include sections on: {", ".join(blueprint.get("content_intelligence", {}).get("common_headings_to_cover", ["N/A"]))}
        - Must explore these unique angles: {", ".join(brief.get("unique_angles_to_cover", ["N/A"]))}
        - Mention these key entities: {", ".join(brief.get("key_entities_to_mention", ["N/A"]))}

        Generate a single, valid JSON object with three keys: "article_body_html", "meta_title", and "meta_description". The "article_body_html" must be well-structured HTML."""
        # --- END MODIFICATION ---

        feedback_examples_text = ""
        feedback_data = self.db_manager.get_content_feedback_examples(
            opportunity.get("client_id")
        )
        if feedback_data.get("good_examples") or feedback_data.get("bad_examples"):
            feedback_examples_text = "\n\n**Style Guide based on Past Feedback:**\n"
            if feedback_data.get("good_examples"):
                feedback_examples_text += (
                    "- **DO:** Emulate the style of these highly-rated articles:\n"
                )
                for ex in feedback_data["good_examples"]:
                    feedback_examples_text += f"  - Title: '{ex['keyword']}'. User Feedback: '{ex['comments']}' (Rated {ex['rating']}/5)\n"
            if feedback_data.get("bad_examples"):
                feedback_examples_text += "- **AVOID:** Avoid the issues found in these poorly-rated articles:\n"
                for ex in feedback_data["bad_examples"]:
                    feedback_examples_text += f"  - Title: '{ex['keyword']}'. User Feedback: '{ex['comments']}' (Rated {ex['rating']}/5)\n"

        system_message = "You are an expert SEO content strategist. Your output must be a single, valid JSON object."
        final_prompt_content = final_prompt_content + feedback_examples_text

        return [
            {"role": "system", "content": system_message},
            {"role": "user", "content": final_prompt_content},
        ]

    def flatten_prompt_for_display(self, messages: List[Dict[str, str]]) -> str:
        """Flattens the structured prompt messages into a single string for UI preview."""
        # ... (existing method, no change needed) ...
```

## File: agents/social_media_crafter.py
```python
# agents/social_media_crafter.py
import logging
from typing import Dict, Any, Tuple, Optional, List

from bs4 import BeautifulSoup

from backend.external_apis.openai_client import OpenAIClientWrapper


class SocialMediaCrafter:
    """
    AI agent for crafting social media posts based on the generated article.
    """

    def __init__(self, openai_client: OpenAIClientWrapper, config: Dict[str, Any]):
        self.openai_client = openai_client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def craft_posts(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[Optional[List[Dict[str, Any]]], float]:
        """
        Generates social media blurbs for different platforms.
        """
        prompt_messages = self._build_crafting_prompt(opportunity)

        schema = {
            "type": "object",
            "properties": {
                "social_media_posts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "platform": {
                                "type": "string",
                                "description": "The social media platform (e.g., 'Twitter', 'LinkedIn').",
                            },
                            "content": {
                                "type": "string",
                                "description": "The content of the social media post.",
                            },
                        },
                        "required": ["platform", "content"],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["social_media_posts"],
            "additionalProperties": False,
        }

        response, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
        )

        # Get the actual cost from the client after the API call
        cost = self.openai_client.latest_cost

        if error or not response:
            self.logger.error(f"Failed to craft social media posts: {error}")
            return None, cost  # Return the actual cost even on failure

        return response.get("social_media_posts"), cost

    def _build_crafting_prompt(
        self, opportunity: Dict[str, Any]
    ) -> list[Dict[str, str]]:
        """Constructs the prompt for the social media crafting AI."""
        ai_content = opportunity.get("ai_content", {})
        article_title = ai_content.get("meta_title", "Untitled")
        article_summary = ai_content.get("meta_description", "No summary available.")
        article_body_html = ai_content.get(
            "article_body_html", ""
        )  # NEW: Get full HTML for parsing

        client_cfg = opportunity.get("client_cfg", {})
        platforms = ", ".join(client_cfg.get("platforms", ["Twitter", "LinkedIn"]))

        # --- Extract key information from the generated article's HTML ---
        soup = BeautifulSoup(article_body_html, "html.parser")

        h1_tag = soup.find("h1")
        # W17 FIX: Ensure h1 extraction is guarded, falling back to a modified title if necessary
        h1_text = (
            h1_tag.get_text(strip=True)
            if h1_tag
            else f"The Article on {article_title.replace(':', ' - ')}"
        )

        # W17 FIX: Extract H2s robustly, filtering out any empty tags
        h2_texts = [
            h.get_text(strip=True)
            for h in soup.find_all("h2")
            if h.get_text(strip=True)
        ]

        key_entities = (
            opportunity.get("blueprint", {})
            .get("ai_content_brief", {})
            .get("key_entities_to_mention", [])
        )

        # --- Social Media Tag Analysis (existing logic) ---
        competitor_social_tags = []
        for competitor in opportunity.get("blueprint", {}).get(
            "competitor_analysis", []
        )[:3]:
            if competitor.get("social_media_tags"):
                tags = competitor["social_media_tags"]
                key_tags = {
                    "og:title": tags.get("og:title"),
                    "og:description": tags.get("og:description"),
                    "twitter:title": tags.get("twitter:title"),
                    "twitter:description": tags.get("twitter:description"),
                }
                competitor_social_tags.append(
                    {
                        "url": competitor["url"],
                        "tags": {k: v for k, v in key_tags.items() if v},
                    }
                )

        competitor_examples = ""
        if competitor_social_tags:
            competitor_examples = (
                "\n**Competitor Social Media Examples (for inspiration):**\n"
            )
            for item in competitor_social_tags:
                competitor_examples += f"- For {item['url']}:\n"
                for tag, value in item["tags"].items():
                    competitor_examples += f"  - {tag}: {value}\n"

        prompt = f"""
        You are a social media marketing expert. Your task is to create engaging social media posts to promote a new blog article.

        **Article Details:**
        - **Primary Headline (H1):** {h1_text} # Now guaranteed to be populated
        - **Key Sections (H2s):** {", ".join(h2_texts[:5]) if h2_texts else "N/A (No subheadings found)"} # Now guaranteed text if available
        - **Summary:** {article_summary}
        - **Key Entities/Topics:** {", ".join(key_entities) if key_entities else "N/A"}
        - **Link (use placeholder):** [LINK]
        {competitor_examples}
        **Instructions:**
        1.  Create a unique post for each of the following platforms: {platforms}.
        2.  Analyze the competitor examples to understand how they frame their content on social media.
        3.  Tailor the tone and length of each post to the specific platform.
        4.  Include relevant hashtags.
        5.  End each post with a call to action and the [LINK] placeholder.
        6.  Ensure posts directly reference information present in the article's headlines and key entities.

        Provide the output in the required JSON format.
        """
        return [{"role": "user", "content": prompt}]
```

## File: agents/summary_generator.py
```python
import logging
from typing import Dict, Any


class SummaryGenerator:
    """
    Generates a human-readable strategic summary of a keyword opportunity.
    """

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def generate_summary(self, opportunity: Dict[str, Any]) -> str:
        """Builds a narrative summary based on the opportunity's data."""
        full_data = opportunity.get("full_data", {})
        score_breakdown = full_data.get("score_breakdown", {})

        # Check qualification status first
        quality_status = full_data.get("quality_status", "passed")
        cannibal_status = full_data.get("cannibalization_status", "passed")
        if quality_status == "failed" or cannibal_status == "failed":
            reasons = ", ".join(full_data.get("reasons", ["Unknown reason"]))
            return f"**Disqualified:** This keyword was filtered out. Reason(s): {reasons}."

        # Build summary for qualified keywords using the new breakdown
        intent = full_data.get("search_intent_info", {}).get("main_intent", "unknown")
        summary_parts = [f"This is a promising **{intent.upper()}** opportunity."]

        ease_of_ranking_score = score_breakdown.get("ease_of_ranking", {}).get(
            "score", 0
        )
        if ease_of_ranking_score < 60:
            summary_parts.append(
                "However, the SERP shows some competitive challenges that will require a strong article."
            )
        elif ease_of_ranking_score < 85:
            summary_parts.append(
                "The competitive landscape appears favorable, with weaknesses to exploit."
            )
        else:  # 85+
            summary_parts.append(
                "The competitive landscape appears **extremely favorable**, with clear technical and content weaknesses among top competitors."
            )

        traffic_score = score_breakdown.get("traffic_potential", {}).get("score", 0)
        if traffic_score > 70:
            summary_parts.append("It has **excellent traffic potential**.")
        elif traffic_score > 40:
            summary_parts.append("It has solid traffic potential.")

        commercial_score = score_breakdown.get("commercial_intent", {}).get("score", 0)
        if commercial_score > 85:
            summary_parts.append(
                "The keyword shows **strong commercial value**, making it a high-priority target."
            )
        elif commercial_score > 60:
            summary_parts.append("It has good commercial value.")

        return " ".join(summary_parts)

    def generate_score_narrative(self, score_breakdown: Dict[str, Any]) -> str:
        narrative_parts = []
        if not score_breakdown:
            return "No score breakdown available to generate a narrative."

        # Ease of Ranking
        ease_score = score_breakdown.get("ease_of_ranking", {}).get("score", 0)
        if ease_score >= 80:
            narrative_parts.append(
                "Ranks highly due to a **very weak competitive landscape**."
            )
        elif ease_score >= 60:
            narrative_parts.append(
                "Has a good chance to rank because of a **favorable competitive landscape**."
            )
        else:
            narrative_parts.append(
                "Faces **strong competition**, making it a challenging keyword to rank for."
            )

        # Traffic Potential
        traffic_score = score_breakdown.get("traffic_potential", {}).get("score", 0)
        if traffic_score >= 75:
            narrative_parts.append("It has **excellent traffic potential**.")
        elif traffic_score >= 50:
            narrative_parts.append("It has **solid traffic potential**.")
        else:
            narrative_parts.append("Its traffic potential is moderate.")

        # Commercial Intent
        commercial_score = score_breakdown.get("commercial_intent", {}).get("score", 0)
        if commercial_score >= 80:
            narrative_parts.append("The keyword shows **strong commercial value**.")
        elif commercial_score >= 50:
            narrative_parts.append("It has **good commercial value**.")

        return "Overall: " + " ".join(narrative_parts)
```

## File: api/routers/auth.py
```python
from fastapi import APIRouter, HTTPException
from ..models import LoginRequest

router = APIRouter()


@router.post("/auth/login")
async def login(request: LoginRequest):
    """
    Dummy login endpoint for development.
    In a real app, verify a hashed password against a user database.
    """
    DUMMY_PASSWORD = "password123"
    if request.password == DUMMY_PASSWORD:
        dummy_user = {"username": "admin", "email": "admin@example.com"}
        dummy_token = "dummy-secret-token"
        return {"user": dummy_user, "token": dummy_token}
    else:
        raise HTTPException(status_code=401, detail="Incorrect password")


@router.post("/auth/logout")
async def logout():
    """Dummy logout endpoint."""
    return {"message": "Logged out successfully"}
```

## File: api/routers/client_settings.py
```python
# api/routers/client_settings.py
# NEW FILE
import bleach  # ADD THIS LINE
import json
from typing import Dict
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from ..models import ClientSettings  # Assuming a Pydantic model exists
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()


@router.get("/settings/{client_id}", response_model=ClientSettings)
async def get_client_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    settings = db.get_client_settings(client_id)
    if not settings:
        raise HTTPException(
            status_code=404, detail="Settings not found for this client."
        )
    return settings


@router.put("/settings/{client_id}", response_model=Dict[str, str])
async def update_client_settings_endpoint(
    client_id: str,
    settings: ClientSettings,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        settings_dict = settings.dict()
        for key in ['brand_tone', 'target_audience', 'terms_to_avoid', 'client_knowledge_base', 'expert_persona']:
            if key in settings_dict and settings_dict[key]:
                settings_dict[key] = bleach.clean(settings_dict[key], tags=[], strip=True)
        db.update_client_settings(client_id, settings_dict)
        return {"message": "Settings updated successfully."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## File: api/routers/clients.py
```python
import logging
from pydantic import BaseModel
from typing import List
from fastapi import APIRouter, Depends, HTTPException
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from .. import globals as api_globals
from backend.pipeline import WorkflowOrchestrator


class NewClientRequest(BaseModel):
    client_id: str
    client_name: str


router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/clients")
async def get_all_clients(db: DatabaseManager = Depends(get_db)):
    logger.info("Received request for /clients")
    clients = db.get_clients()
    logger.info(f"Found clients: {clients}")
    if not clients:
        return []
    return clients


@router.get("/clients/{client_id}/settings")
async def get_client_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    settings = db.get_client_settings(client_id)
    if not settings:
        raise HTTPException(
            status_code=404, detail=f"Settings not found for client '{client_id}'"
        )
    return settings


@router.get("/clients/{client_id}/dashboard-stats")
async def get_dashboard_stats_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    stats = db.get_dashboard_stats(client_id)
    if not stats:
        raise HTTPException(
            status_code=404, detail=f"Stats not found for client '{client_id}'"
        )
    return stats


@router.get("/clients/{client_id}/dashboard")
async def get_dashboard_data_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to fetch aggregated data for the main dashboard."""
    logger.info(f"Dashboard endpoint called for client: {client_id}")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        logger.info("Fetching dashboard data from database...")
        dashboard_data = db.get_dashboard_data(client_id)
        logger.info("Successfully fetched dashboard data.")
        if not dashboard_data:
            logger.warning(f"No dashboard data found for client {client_id}")
            raise HTTPException(
                status_code=404, detail=f"Dashboard data not found for client '{client_id}'"
            )
        logger.info("Returning dashboard data.")
        return dashboard_data
    except Exception as e:
        logger.error(f"Error fetching dashboard data: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal Server Error")


@router.get("/clients/{client_id}/processed-keywords")
async def get_processed_keywords_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Retrieves all processed keywords for a client to prevent duplicates."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    keywords = db.get_all_processed_keywords_for_client(client_id)
    return keywords


@router.post("/clients/{client_id}/check-keywords")
async def check_existing_keywords_endpoint(
    client_id: str,
    keywords: List[str],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Checks a batch of keywords and returns which ones already exist."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    existing = db.check_existing_keywords(client_id, keywords)
    return {"existing_keywords": existing}


# ADD the new endpoint to the router:
@router.get("/clients/{client_id}/search-all-assets")
async def search_all_assets_endpoint(
    client_id: str,
    query: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    logger.info(
        f"Received search-all-assets request for client {client_id} with query: '{query}'"
    )
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    if len(query) < 3:
        return []

    results = []

    # Search Opportunities
    opportunities = db.search_opportunities(client_id, query)
    for opp in opportunities:
        results.append({"id": opp["id"], "name": opp["keyword"], "type": "opportunity"})

    # Search Discovery Runs (by seed keywords or run status/error)
    discovery_runs = db.search_discovery_runs(client_id, query)
    for run in discovery_runs:
        seed_keywords_str = ", ".join(
            run.get("parameters", {}).get("seed_keywords", [])
        )
        results.append(
            {
                "id": run["id"],
                "name": f"Discovery Run #{run['id']}: {seed_keywords_str[:50]}...",
                "type": "discovery_run",
            }
        )

    # Deduplicate results by type-id if necessary (optional, but good practice)
    unique_results = {}
    for item in results:
        key = f"{item['type']}-{item['id']}"
        if key not in unique_results:
            unique_results[key] = item

    return list(unique_results.values())


@router.get("/clients/{client_id}/opportunities/high-priority")
async def get_high_priority_opportunities_endpoint(
    client_id: str,
    limit: int = 5,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Retrieves a short list of the highest-scored, validated opportunities."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    opportunities = db.get_high_priority_opportunities(client_id, limit)
    return opportunities


@router.post("/clients")
async def add_new_client(
    request: NewClientRequest, db: DatabaseManager = Depends(get_db)
):
    """Adds a new client to the database and initializes their settings."""
    try:
        # Ensure the config manager is available
        if not api_globals.config_manager:
            raise HTTPException(
                status_code=500, detail="Configuration manager not initialized."
            )

        default_settings = (
            api_globals.config_manager.get_default_client_settings_template()
        )

        success = db.add_client(
            client_id=request.client_id,
            client_name=request.client_name,
            default_settings=default_settings,
        )

        if success:
            return {"message": "Client added successfully."}
        else:
            # This could happen if the client_id already exists (IntegrityError)
            raise HTTPException(
                status_code=409,
                detail=f"Client with ID '{request.client_id}' already exists.",
            )
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(
            f"Error adding new client '{request.client_name}': {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail="Failed to add new client.")
```

## File: api/routers/discovery.py
```python
import logging
from fastapi import APIRouter, Depends, HTTPException
from data_access.database_manager import DatabaseManager
from backend.pipeline import WorkflowOrchestrator
from services.discovery_service import DiscoveryService
from ..dependencies import get_db, get_orchestrator, get_discovery_service
from ..models import (
    JobResponse,
    DiscoveryRunRequest,
)  # Ensure DiscoveryRunRequest is imported

# --- NEW IMPORTS AND MODELS FOR FRONTEND FEATURES ---
from typing import Dict, Optional


# --- END NEW IMPORTS AND MODELS ---

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/discovery/available-filters")
async def get_available_filters():
    """
    Returns a curated list of available discovery modes, filters, and sorting options,
    structured to be easily consumable by the frontend.
    """
    base_filters = [
        {
            "name": "search_volume",
            "label": "Search Volume",
            "type": "number",
            "operators": [">", "<", "="],
        },
        {
            "name": "keyword_difficulty",
            "label": "Keyword Difficulty",
            "type": "number",
            "operators": [">", "<", "="],
        },
        {
            "name": "main_intent",
            "label": "Search Intent",
            "type": "select",
            "options": ["informational", "navigational", "commercial", "transactional"],
            "operators": ["="],
        },
        {
            "name": "competition_level",
            "label": "Competition Level",
            "type": "select",
            "options": ["LOW", "MEDIUM", "HIGH"],
            "operators": ["="],
        },
        {"name": "cpc", "label": "CPC", "type": "number", "operators": [">", "<", "="]},
    ]

    base_sorting = [
        {"name": "search_volume", "label": "Search Volume"},
        {"name": "keyword_difficulty", "label": "Keyword Difficulty"},
        {"name": "cpc", "label": "CPC"},
        {"name": "competition", "label": "Competition"},
    ]

    def construct_paths(prefix, items):
        new_items = []
        for item in items:
            new_item = item.copy()
            if "search_volume" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.search_volume"
            elif "keyword_difficulty" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_properties.keyword_difficulty"
            elif "main_intent" in new_item["name"]:
                new_item["name"] = f"{prefix}search_intent_info.main_intent"
            elif "competition_level" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.competition_level"
            elif "cpc" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.cpc"
            elif "competition" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.competition"
            new_items.append(new_item)
        return new_items

    return [
        {
            "id": "keyword_ideas",
            "name": "Broad Market Exploration",
            "description": "Discover a wide range of foundational keywords related to your core topics. Ideal for initial research and uncovering new content pillars.",
            "filters": construct_paths("", base_filters),
            "sorting": [{"name": "relevance", "label": "Relevance"}]
            + construct_paths("", base_sorting),
        },
        {
            "id": "keyword_suggestions",
            "name": "Targeted Query Expansion",
            "description": "Generate specific, long-tail variations of your seed keywords. Perfect for finding niche opportunities and targeted article ideas.",
            "filters": construct_paths("", base_filters),
            "sorting": construct_paths("", base_sorting),
        },
        {
            "id": "related_keywords",
            "name": "Semantic & Competitor Analysis",
            "description": "Find semantically related terms and phrases that your competitors may be ranking for. Excellent for expanding content depth and authority.",
            "filters": construct_paths("keyword_data.", base_filters),
            "sorting": construct_paths("keyword_data.", base_sorting),
        },
        {
            "id": "find_questions",
            "name": "Find Questions",
            "description": "Discover question-based keywords (e.g., 'how to...', 'what is...') related to your core topics.",
            "filters": construct_paths("", base_filters),
            "sorting": construct_paths("", base_sorting),
        },
    ]


@router.post("/clients/{client_id}/discovery-runs-async", response_model=JobResponse)
async def start_discovery_run_async(
    client_id: str,
    request: DiscoveryRunRequest,
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
    discovery_service: DiscoveryService = Depends(get_discovery_service),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        filters = request.filters
        limit = request.limit or 1000
        
        # --- INTELLIGENT DISCOVERY MODE SELECTION ---
        # The individual API calls will now enforce specific limits and depth=1.
        # The overall 'depth' for the run is now fixed to 1 as per new requirements.
        discovery_modes = ["keyword_ideas", "keyword_suggestions", "related_keywords"]
        depth = 1
        # --- END OF CHANGE ---

        parameters = {
            "seed_keywords": request.seed_keywords,
            "discovery_modes": discovery_modes,
            "filters": filters,
            "order_by": request.order_by,
            "filters_override": request.filters_override,
            "limit": limit,
            "depth": depth,
            "include_clickstream_data": False,  # Hardcoded
            "closely_variants": False,  # Hardcoded
            "ignore_synonyms": True,  # Hardcoded
            "exact_match": True, # Hardcoded
        }
        run_id = discovery_service.create_discovery_run(
            client_id=client_id, parameters=parameters
        )

        job_id = orchestrator.run_discovery_and_save(
            run_id,
            request.seed_keywords,
            discovery_modes,
            filters,
            request.order_by,
            request.filters_override,
            limit,
            depth,
            ignore_synonyms=True,
            include_clickstream_data=False,
            closely_variants=False,
            exact_match=True,
        )
        return {"job_id": job_id, "message": f"Discovery run job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start discovery run for client {client_id}: {e}", exc_info=True
        )
        raise HTTPException(
            status_code=500, detail=f"Failed to start discovery run: {e}"
        )


@router.get("/clients/{client_id}/discovery-runs")
async def get_discovery_runs(
    client_id: str,
    page: int = 1,
    limit: int = 10,
    search_query: Optional[str] = None,
    date_range_start: Optional[str] = None,
    date_range_end: Optional[str] = None,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    
    filters = {
        "search_query": search_query,
        "start_date": date_range_start,
        "end_date": date_range_end,
    }

    runs, total_count = db.get_all_discovery_runs_paginated(client_id, page, limit, filters)
    if not runs:
        return {"items": [], "total_items": 0, "page": page, "limit": limit}
    return {"items": runs, "total_items": total_count, "page": page, "limit": limit}


# def calculate_discovery_cost(request: KeywordListRequest) -> Dict[str, Any]:

# # ... (existing setup) ...

#     # ... lines 141-143 unchanged

#     # item_cost = num_items * cost_per_item

#     # estimated_cost = task_cost + item_cost


#     # explanation = [

#     #     f"{num_tasks} tasks @ ${cost_per_task:.4f} each: ${task_cost:.4f}",

#     #     f"{num_items} items @ ${cost_per_item:.4f} each: ${item_cost:.4f}"

#     # ]

#     pass

# if request.include_clickstream_data:

#     estimated_cost *= 2

#     explanation.append("Cost multiplied by 2x due to 'include_clickstream_data' flag.")

# return {"estimated_cost": round(estimated_cost, 2), "explanation": explanation}


@router.post("/discovery-runs/rerun/{run_id}")
async def rerun_discovery_run(
    run_id: int,
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
    db: DatabaseManager = Depends(get_db),  # current_client_id dependency removed here
):
    """
    Initiates a new discovery run using the parameters from a previous run.
    """
    previous_run = db.get_discovery_run_by_id(run_id)
    if not previous_run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")

    # Authorization check within the function (using orchestrator's client_id)
    if (
        previous_run["client_id"] != orchestrator.client_id
    ):  # Use orchestrator's client_id
        raise HTTPException(
            status_code=403, detail="You do not have permission to re-run this job."
        )

    try:
        parameters = previous_run.get("parameters", {})
        seed_keywords = parameters.get("seed_keywords", [])
        filters = parameters.get("filters")
        order_by = parameters.get("order_by")
        filters_override = parameters.get("filters_override", {})
        limit = parameters.get("limit")
        depth = parameters.get("depth")

        if not seed_keywords:
            raise HTTPException(
                status_code=400, detail="No seed keywords found in the original run."
            )

        # Dynamic discovery logic based on limit
        limit = limit or 1000
        discovery_modes = ["keyword_ideas", "keyword_suggestions", "related_keywords"]

        if depth is None:
            if limit <= 500:
                depth = 2
            elif limit <= 2000:
                depth = 3
            else:
                depth = 4

        # Reconstruct parameters for the new run to be created
        new_run_parameters = {
            "seed_keywords": seed_keywords,
            "discovery_modes": discovery_modes,
            "filters": filters,
            "order_by": order_by,
            "filters_override": filters_override,
            "limit": limit,
            "depth": depth,
        }

        new_run_id = orchestrator.db_manager.create_discovery_run(
            client_id=previous_run["client_id"], parameters=new_run_parameters
        )
        job_id = orchestrator.run_discovery_and_save(
            new_run_id,
            seed_keywords,
            discovery_modes,
            filters,
            order_by,
            filters_override,
            limit,
            depth,
        )

        return {
            "job_id": job_id,
            "message": f"Re-run of job {run_id} started as new job {job_id}.",
        }
    except Exception as e:
        logger.error(f"Failed to re-run discovery run {run_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to start re-run: {e}")


@router.get("/discovery-runs/{run_id}")
async def get_discovery_run_by_id(
    run_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves a single discovery run by its ID.
    """
    run = db.get_discovery_run_by_id(run_id)
    if not run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")
    if run["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this discovery run.",
        )
    return run


@router.get("/discovery-runs/{run_id}/keywords")
async def get_run_keywords(
    run_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all keywords that were added to the database as part of a specific discovery run.
    """
    run = db.get_discovery_run_by_id(run_id)
    if not run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")
    if (
        run["client_id"] != orchestrator.client_id
    ):  # Use orchestrator's client_id for auth
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access these keywords.",
        )

    try:
        keywords = db.get_keywords_for_run(run_id)
        return keywords
    except Exception as e:
        logger.error(
            f"Failed to retrieve keywords for run {run_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=f"Failed to retrieve keywords: {e}")


@router.get("/discovery-runs/{run_id}/keywords/{reason}")
async def get_run_keywords_by_reason(
    run_id: int,
    reason: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all keywords that were added to the database as part of a specific discovery run
    that were disqualified for a specific reason.
    """
    run = db.get_discovery_run_by_id(run_id)
    if not run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")
    if (
        run["client_id"] != orchestrator.client_id
    ):  # Use orchestrator's client_id for auth
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access these keywords.",
        )

    try:
        keywords = db.get_keywords_for_run_by_reason(run_id, reason)
        return keywords
    except Exception as e:
        logger.error(
            f"Failed to retrieve keywords for run {run_id} and reason {reason}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=f"Failed to retrieve keywords: {e}")


@router.get(
    "/discovery-runs/{run_id}/disqualification-reasons", response_model=Dict[str, int]
)
async def get_disqualification_reasons_endpoint(
    run_id: int, discovery_service: DiscoveryService = Depends(get_discovery_service)
):
    """
    Retrieves a summary of disqualification reasons for a specific discovery run.
    """
    logger.info(f"Received request for disqualification reasons for run {run_id}")
    reasons = discovery_service.get_disqualification_reasons(run_id)
    return reasons
```

## File: api/routers/jobs.py
```python
# api/routers/jobs.py

import logging
from fastapi import APIRouter, Depends, HTTPException
from jobs import JobManager
from ..dependencies import get_job_manager, get_db, get_orchestrator
from ..models import JobResponse
from data_access.database_manager import DatabaseManager
from backend.pipeline import WorkflowOrchestrator
from typing import List, Dict, Any

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/jobs/active", response_model=List[Dict[str, Any]])
async def get_active_jobs_for_client(
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Retrieves all 'running' or 'pending' jobs for the current client."""
    try:
        client_id = orchestrator.client_id
        active_jobs = db.get_active_jobs_by_client(client_id)
        return active_jobs
    except Exception as e:
        logger.error(f"Failed to retrieve active jobs for client {orchestrator.client_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to retrieve active jobs.")

@router.get("/jobs/{job_id}", response_model=JobResponse)
async def get_job_status(
    job_id: str, job_manager: JobManager = Depends(get_job_manager)
):
    """
    Retrieves the status of a background job.
    """
    logger.info(f"Received request for job status for job_id: {job_id}")
    job = job_manager.get_job_status(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    return {
        "job_id": job["id"],
        "message": f"Status: {job['status']}",
        "status": job["status"],
        "progress": job["progress"],
        "result": job.get("result"),
        "error": job.get("error"),
        "progress_log": job.get("progress_log"),
    }
```

## File: api/routers/opportunities.py
```python
import logging
import bleach
import json
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException
from data_access.database_manager import DatabaseManager
from fastapi.concurrency import run_in_threadpool
from services.opportunities_service import OpportunitiesService
from ..dependencies import get_db, get_opportunities_service, get_orchestrator
from ..models import (
    OpportunityListResponse,
    ContentHistoryItem,
    RestoreRequest,
    SocialMediaPostsUpdate,
    ContentUpdatePayload,
)
from pydantic import BaseModel
from .. import globals as api_globals
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/settings/discovery-strategies", response_model=List[str])
async def get_discovery_strategies():
    """Returns the available discovery strategies from the global config."""
    strategies = api_globals.config_manager.get_global_config().get(
        "discovery_strategies", []
    )
    return strategies


class ContentFeedbackRequest(BaseModel):
    rating: int
    comments: Optional[str] = None


@router.get(
    "/clients/{client_id}/opportunities", response_model=OpportunityListResponse
)
async def get_all_opportunities_summary_endpoint(
    client_id: str,
    status: Optional[str] = None,
    keyword: Optional[str] = None,
    page: int = 1,
    limit: int = 20,
    sort_by: str = "date_added",
    sort_direction: str = "desc",
    opportunities_service: OpportunitiesService = Depends(get_opportunities_service),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint for fetching a paginated summary of opportunities for the main table view."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    params = {
        "status": status,
        "keyword": keyword,
        "page": page,
        "limit": limit,
        "sort_by": sort_by,
        "sort_direction": sort_direction,
    }
    opportunities, total_count = await run_in_threadpool(
        opportunities_service.get_all_opportunities_summary,
        client_id,
        params,
        select_columns="id, keyword, status, date_added, strategic_score, cpc, competition, main_intent, blog_qualification_status, blog_qualification_reason, latest_job_id, cluster_name, full_data, search_volume, keyword_difficulty",
    )
    return {
        "items": opportunities,
        "total_items": total_count,
        "page": page,
        "limit": limit,
    }


@router.get(
    "/clients/{client_id}/opportunities/by-cluster",
    response_model=Dict[str, List[Dict[str, Any]]],
)
async def get_opportunities_by_cluster_endpoint(
    client_id: str,
    opportunities_service: OpportunitiesService = Depends(get_opportunities_service),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all opportunities for a client, grouped by cluster.
    """
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    opportunities_by_cluster = await run_in_threadpool(
        opportunities_service.get_opportunities_by_cluster, client_id
    )
    return opportunities_by_cluster


@router.put("/opportunities/{opportunity_id}/status", response_model=Dict[str, str])
async def update_opportunity_status_endpoint(
    opportunity_id: int, status: str, db: DatabaseManager = Depends(get_db)
):
    """
    Manually updates the status of an opportunity.
    """
    logger.info(
        f"Received request to update status for opportunity {opportunity_id} to {status}"
    )
    db.update_opportunity_status(opportunity_id, status)
    return {"message": "Opportunity status updated successfully."}


@router.post("/opportunities/bulk-action", response_model=Dict[str, str])
async def bulk_action_endpoint(
    action: str, opportunity_ids: List[int], db: DatabaseManager = Depends(get_db)
):
    """
    Performs a bulk action on a list of opportunities.
    """
    logger.info(
        f"Received request to perform bulk action '{action}' on {len(opportunity_ids)} opportunities"
    )
    for opportunity_id in opportunity_ids:
        if action == "reject":
            db.update_opportunity_status(opportunity_id, "rejected")
        elif action == "approve":
            db.update_opportunity_status(opportunity_id, "qualified")

    return {"message": "Bulk action completed successfully."}


@router.post("/opportunities/compare", response_model=List[Dict[str, Any]])
async def compare_opportunities_endpoint(
    opportunity_ids: List[int], db: DatabaseManager = Depends(get_db)
):
    """
    Retrieves a list of opportunities for comparison.
    """
    logger.info(f"Received request to compare {len(opportunity_ids)} opportunities")
    opportunities = []
    for opportunity_id in opportunity_ids:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if opportunity:
            opportunities.append(opportunity)

    return opportunities


@router.get(
    "/clients/{client_id}/opportunities/search", response_model=List[Dict[str, Any]]
)
async def search_opportunities_endpoint(
    client_id: str,
    query: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    logger.info(f"Received search request for client {client_id} with query: '{query}'")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    if len(query) < 3:
        return []

    opportunities = db.search_opportunities(client_id, query)
    return opportunities


@router.get("/opportunities/{opportunity_id}", response_model=Dict[str, Any])
async def get_opportunity_by_id_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),  # Add this
):
    logger.info(f"Received request for opportunity {opportunity_id}")
    opportunity = db.get_opportunity_by_id(opportunity_id)
    if not opportunity:
        raise HTTPException(status_code=404, detail="Opportunity not found")
    # Add authorization check
    if opportunity["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this opportunity.",
        )

    logger.info(f"Retrieved opportunity from DB: {opportunity}")
    return opportunity


@router.get(
    "/opportunities/{opportunity_id}/content-history",
    response_model=List[ContentHistoryItem],
)
async def get_content_history_endpoint(
    opportunity_id: int, db: DatabaseManager = Depends(get_db)
):
    logger.info(f"Fetching content history for opportunity {opportunity_id}")
    history = db.get_content_history(opportunity_id)
    if not history:
        return []
    return history


@router.post(
    "/opportunities/{opportunity_id}/restore-content", response_model=Dict[str, Any]
)
async def restore_content_version_endpoint(
    opportunity_id: int, request: RestoreRequest, db: DatabaseManager = Depends(get_db)
):
    logger.info(
        f"Restoring content version from {request.version_timestamp} for opportunity {opportunity_id}"
    )
    try:
        restored_content = db.restore_content_version(
            opportunity_id, request.version_timestamp
        )
        if restored_content:
            return {
                "message": "Content version restored successfully.",
                "restored_content": restored_content,
            }
        else:
            raise HTTPException(
                status_code=404, detail="Failed to restore content version."
            )
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(
            f"Error restoring content for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500,
            detail="An internal error occurred during content restoration.",
        )


@router.put(
    "/opportunities/{opportunity_id}/social-media-posts", response_model=Dict[str, str]
)
async def update_social_media_posts_endpoint(
    opportunity_id: int,
    payload: SocialMediaPostsUpdate,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),  # Add this
):
    logger.info(f"Updating social media posts for opportunity {opportunity_id}")
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        db.update_opportunity_social_posts(opportunity_id, payload.social_media_posts)
        return {"message": "Social media posts updated successfully."}
    except Exception as e:
        logger.error(
            f"Error updating social media posts for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Failed to update social media posts."
        )


@router.put("/opportunities/{opportunity_id}/content", response_model=Dict[str, str])
async def update_opportunity_content_endpoint(
    opportunity_id: int,
    payload: ContentUpdatePayload,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),  # Add this
):
    """Updates the main HTML content of an opportunity's ai_content blob with server-side sanitization."""
    logger.info(f"Received manual content update for opportunity {opportunity_id}")
    from datetime import datetime

    try:
        current_opp = db.get_opportunity_by_id(opportunity_id)
        if not current_opp:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if current_opp["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        # 1. SERVER-SIDE SANITIZATION (CRITICAL SECURITY FIX)
        ALLOWED_TAGS = bleach.sanitizer.ALLOWED_TAGS + [
            "h1",
            "h2",
            "h3",
            "h4",
            "h5",
            "h6",
            "p",
            "br",
            "a",
            "i",
            "u",
            "em",
            "strong",
            "blockquote",
            "li",
            "ul",
            "ol",
            "img",
            "div",
            "span",
            "table",
            "thead",
            "tbody",
            "tr",
            "td",
            "th",
            "code",
            "pre",
        ]
        ALLOWED_ATTRIBUTES_SAFE = {
            "*": ["id", "class"],
            "a": ["href", "title"],
            "img": ["src", "alt", "width", "height"],
        }

        clean_html = bleach.clean(
            payload.article_body_html,
            tags=ALLOWED_TAGS,
            attributes=ALLOWED_ATTRIBUTES_SAFE,
        )

        # 2. Save the current version to history before overwriting
        current_ai_content = current_opp.get("ai_content", {})
        if current_ai_content:
            db.save_content_version_to_history(
                opportunity_id,
                current_ai_content,
                timestamp=f"{datetime.now().isoformat()} (Before Manual Edit)",
            )

        # 3. Update the content with the new payload
        updated_ai_content = current_ai_content.copy()
        updated_ai_content["article_body_html"] = clean_html

        # Use the query that also updates status and timestamp
        db.update_opportunity_ai_content_and_status(
            opportunity_id,
            updated_ai_content,
            current_opp.get("ai_content_model"),
            "generated",  # Reset status to 'generated' to reflect it's ready
        )
        return {"message": "Content updated and previous version saved to history."}
    except Exception as e:
        logger.error(
            f"Error updating content for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Failed to update content due to a server error."
        )


@router.post(
    "/opportunities/{opportunity_id}/override-disqualification",
    response_model=Dict[str, str],
)
async def override_disqualification_endpoint(
    opportunity_id: int, db: DatabaseManager = Depends(get_db)
):
    """Manually overrides a 'failed' or 'rejected' qualification status."""
    success = db.override_disqualification(opportunity_id)
    if not success:
        raise HTTPException(
            status_code=404,
            detail="Opportunity not found or its status did not permit an override.",
        )
    return {
        "message": "Opportunity has been re-qualified and moved to the pending queue."
    }


@router.post("/opportunities/{opportunity_id}/feedback", response_model=Dict[str, str])
async def submit_content_feedback_endpoint(
    opportunity_id: int,
    request: ContentFeedbackRequest,
    db: DatabaseManager = Depends(get_db),
):
    """Submits user feedback for the generated content."""
    if not (1 <= request.rating <= 5):
        raise HTTPException(status_code=400, detail="Rating must be between 1 and 5.")
    try:
        db.save_content_feedback(opportunity_id, request.rating, request.comments)
        return {"message": "Feedback submitted successfully."}
    except Exception as e:
        logger.error(
            f"Error submitting feedback for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail="Failed to submit feedback.")
```

## File: api/routers/orchestrator.py
```python
import logging
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from typing import Optional, Dict, List
from data_access.database_manager import DatabaseManager
from jobs import JobManager
from ..dependencies import get_db, get_job_manager, get_orchestrator
from ..models import (
    JobResponse,
    AnalysisRequest,
    AutoWorkflowRequest,
    RefineContentRequest,
    ApproveAnalysisRequest,
)  # Add ApproveAnalysisRequest
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


class GenerationRequest(BaseModel):
    model_override: Optional[str] = None
    temperature: Optional[float] = None


@router.post(
    "/orchestrator/{opportunity_id}/run-generation-async", response_model=JobResponse
)
async def run_generation_async_endpoint(
    opportunity_id: int,
    request: GenerationRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        # Add authorization check
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        # orchestrator is already initialized with the correct client_id from the header
        job_id = orchestrator.run_full_content_generation(
            opportunity_id, request.model_override, request.temperature
        )
        return {
            "job_id": job_id,
            "message": f"Content generation job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start generation job for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/run-validation-async", response_model=JobResponse
)
async def run_validation_async_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        job_id = orchestrator.run_validation(opportunity_id)
        return {"job_id": job_id, "message": f"Validation job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start validation job for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/run-analysis-async", response_model=JobResponse
)
async def run_analysis_async_endpoint(
    opportunity_id: int,
    request: AnalysisRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        # --- START MODIFICATION ---
        # Pass selected_competitor_urls from the request
        job_id = orchestrator.run_full_analysis(
            opportunity_id, request.selected_competitor_urls
        )
        # --- END MODIFICATION ---
        return {"job_id": job_id, "message": f"Full analysis job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start analysis job for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/orchestrator/{opportunity_id}/full-prompt", response_model=str)
async def get_full_prompt_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to get the full, flattened prompt for an opportunity."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        full_prompt = orchestrator.get_full_prompt_for_display(opportunity_id)
        if not full_prompt:
            raise HTTPException(
                status_code=404,
                detail="Prompt has not been generated yet or opportunity data is missing.",
            )
        return full_prompt
    except Exception as e:
        logger.error(
            f"Failed to get full prompt for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/regenerate-social-async", response_model=JobResponse
)
async def regenerate_social_async_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to start a job for regenerating only the social media posts."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.regenerate_social_posts(opportunity_id)
        return {
            "job_id": job_id,
            "message": f"Social media post regeneration job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start social post regeneration for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


class DiscoveryCostParams(BaseModel):
    seed_keywords: List[str]
    discovery_modes: List[str]
    # We only need fields that affect cost calculation
    discovery_max_pages: Optional[int] = 1


class CostEstimationRequest(BaseModel):
    action_type: str
    discovery_params: Optional[DiscoveryCostParams] = None


@router.post("/orchestrator/estimate-cost")
async def estimate_cost_endpoint(
    request: CostEstimationRequest,
    opportunity_id: Optional[int] = None,  # Make opportunity_id optional
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Endpoint to estimate the cost of a workflow action.
    For 'discovery', it uses discovery_params.
    For other actions, it uses opportunity_id.
    """
    action = request.action_type.lower()
    ALLOWED_ACTIONS = {"analyze", "generate", "validate", "discovery"}

    if action not in ALLOWED_ACTIONS:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid action '{action}'. Must be one of: {', '.join(ALLOWED_ACTIONS)}.",
        )

    try:
        if action == "discovery":
            if not request.discovery_params:
                raise HTTPException(
                    status_code=400,
                    detail="discovery_params are required for 'discovery' action.",
                )
            # For discovery, we don't need to check for an opportunity
            cost_estimation = orchestrator.estimate_action_cost(
                action=action, discovery_params=request.discovery_params.dict()
            )
        else:
            if not opportunity_id:
                raise HTTPException(
                    status_code=400,
                    detail="opportunity_id is required for this action.",
                )

            opportunity = db.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                raise HTTPException(status_code=404, detail="Opportunity not found")

            if opportunity["client_id"] != orchestrator.client_id:
                raise HTTPException(
                    status_code=403,
                    detail="You do not have permission to access this opportunity.",
                )

            cost_estimation = orchestrator.estimate_action_cost(
                action=action, opportunity_id=opportunity_id
            )

        return cost_estimation
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to estimate cost for action {action}: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, detail="Failed to estimate cost due to a server error."
        )


@router.get("/jobs/{job_id}/status")
async def get_job_status_endpoint(
    job_id: str, jm: JobManager = Depends(get_job_manager)
):
    """Endpoint to get the status of a background job."""
    logger.info(f"API: Received request for job status: {job_id}")
    job_status = jm.get_job_status(job_id)
    if not job_status:
        logger.warning(f"API: Job with ID {job_id} not found in JobManager.")
        raise HTTPException(status_code=404, detail="Job not found")
    logger.info(f"API: Found job {job_id}, status: {job_status.get('status')}")
    return {"job_id": job_status["id"], "message": f"Status: {job_status['status']}", **job_status}


@router.get("/jobs")
async def get_all_jobs_endpoint(db: DatabaseManager = Depends(get_db)):
    """Endpoint to get all jobs for the activity log."""
    try:
        jobs = db.get_all_jobs()
        return jobs
    except Exception as e:
        logger.error(f"Failed to retrieve all jobs: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to retrieve jobs.")


@router.post("/jobs/{job_id}/cancel")
async def cancel_job_endpoint(job_id: str, jm: JobManager = Depends(get_job_manager)):
    """Endpoint to cancel a running job."""
    try:
        success = jm.cancel_job(job_id)
        if success:
            return {"message": "Job cancellation request sent."}
        else:
            raise HTTPException(
                status_code=404, detail="Job not found or already completed."
            )
    except Exception as e:
        logger.error(f"Failed to cancel job {job_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to cancel job.")


@router.post(
    "/orchestrator/{opportunity_id}/run-full-auto-async", response_model=JobResponse
)
async def run_full_auto_async_endpoint(
    opportunity_id: int,
    request: AutoWorkflowRequest,  # ADD request body
    db: DatabaseManager = Depends(
        get_db
    ),  # Ensure DatabaseManager is correctly imported and injected
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to start the full 'auto' workflow from validation to generation."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.run_full_auto_workflow(
            opportunity_id, request.override_validation
        )  # Pass override flag
        return {
            "job_id": job_id,
            "message": f"Full auto workflow job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start full auto workflow for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/rerun-analysis-async", response_model=JobResponse
)
async def clear_cache_and_analyze_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Clears API cache for the opportunity's keyword and starts a new analysis job."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.run_full_analysis(opportunity_id)
        return {
            "job_id": job_id,
            "message": f"Cache cleared and analysis job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to clear cache and analyze for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.get(
    "/orchestrator/{opportunity_id}/score-narrative", response_model=Dict[str, str]
)
async def get_score_narrative_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to generate a human-readable narrative for the score breakdown."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        narrative = orchestrator.summary_generator.generate_score_narrative(
            opportunity.get("full_data", {}).get("score_breakdown", {})
        )
        return {"narrative": narrative}
    except Exception as e:
        logger.error(
            f"Failed to generate score narrative for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/refresh-content-async", response_model=JobResponse
)
async def refresh_content_async_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to trigger a refresh of an existing opportunity's content."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.run_content_refresh_workflow(opportunity_id)
        return {"job_id": job_id, "message": f"Content refresh job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start content refresh for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


class FeaturedImageRequest(BaseModel):
    prompt: str


@router.post(
    "/orchestrator/{opportunity_id}/generate-featured-image-async",
    response_model=JobResponse,
)
async def generate_featured_image_async_endpoint(
    opportunity_id: int,
    request: FeaturedImageRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Starts a job to generate a new featured image for an opportunity."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.regenerate_featured_image(opportunity_id, request.prompt)
        return {
            "job_id": job_id,
            "message": f"Featured image generation job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start featured image generation for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/refine-content", response_model=Dict[str, str]
)
async def refine_content_endpoint(
    opportunity_id: int,
    request: RefineContentRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to refine a snippet of HTML content using an AI command."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        prompt_messages = [
            {
                "role": "system",
                "content": "You are an expert web content editor. You will receive a block of HTML and a specific command. Your task is to apply the command to the provided HTML segment and return ONLY the modified HTML block. You MUST preserve all original HTML tags, attributes (like IDs, classes, styles), and structure, only changing the text content or adding/removing tags as absolutely necessary to fulfill the command. Do not add any introductory or concluding remarks, just the refined HTML.",
            },
            {
                "role": "user",
                "content": f"Command: '{request.command}'\n\nHTML to modify:\n```html\n{request.html_content}\n```\n\nReturn ONLY the refined HTML:",
            },
        ]

        refined_html, error = orchestrator.openai_client.call_chat_completion(
            messages=prompt_messages,
            model=orchestrator.client_cfg.get("default_model", "gpt-5-nano"),
            temperature=0.4,
        )

        if error or not refined_html:
            raise HTTPException(
                status_code=500, detail=f"AI content refinement failed: {error}"
            )

        # Clean up potential markdown code block fences from the AI response
        refined_html = refined_html.strip()
        if refined_html.startswith("```html"):
            refined_html = refined_html[len("```html") :]
        if refined_html.endswith("```"):
            refined_html = refined_html[: -len("```")]

        return {"refined_html": refined_html.strip()}
    except Exception as e:
        logger.error(
            f"Failed to refine content for opp {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(
            status_code=500, detail="Failed to refine content due to a server error."
        )


@router.post(
    "/orchestrator/{opportunity_id}/generate-content-override",
    response_model=JobResponse,
)
async def generate_content_override(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to manually trigger content generation override"""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        job_id = orchestrator.run_full_auto_workflow(
            opportunity_id, True
        )  # Run with override = True
        return {
            "job_id": job_id,
            "message": f"Content generation override job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start content generation override for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/approve-analysis/{opportunity_id}", response_model=JobResponse
)
async def approve_analysis_endpoint(
    opportunity_id: int,
    request: ApproveAnalysisRequest,  # Use the new request body model
    db: DatabaseManager = Depends(get_db),
    jm: JobManager = Depends(get_job_manager),
    orchestrator: WorkflowOrchestrator = Depends(
        get_orchestrator
    ),  # Add orchestrator to get client_id
):
    """Endpoint to approve the analysis and continue the workflow by starting content generation with optional overrides."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        # Add authorization check
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        # Convert Pydantic model to dict if it exists, else pass None
        overrides_dict = request.overrides.dict() if request.overrides else None

        job_id = orchestrator.run_full_content_generation(
            opportunity_id, overrides=overrides_dict
        )

        return {
            "job_id": job_id,
            "message": f"Analysis approved. Started content generation job {job_id}.",
        }
    except ValueError as ve:
        logger.error(
            f"State mismatch trying to approve analysis for {opportunity_id}: {ve}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=409, detail=str(ve)
        )  # 409 Conflict for state issues
    except Exception as e:
        logger.error(
            f"Failed to approve analysis for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


    @router.post(
        "/orchestrator/reject-opportunity/{opportunity_id}", response_model=Dict[str, str]
    )
    async def reject_opportunity_endpoint(
        opportunity_id: int,
        db: DatabaseManager = Depends(get_db),
        jm: JobManager = Depends(get_job_manager),
        orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
    ):
        """Endpoint to reject the opportunity and cancel any active job."""
        try:
            opportunity = db.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                raise HTTPException(status_code=404, detail="Opportunity not found.")
            
            if opportunity["client_id"] != orchestrator.client_id:
                raise HTTPException(status_code=403, detail="You do not have permission to access this opportunity.")
    
            job_id = opportunity.get("latest_job_id")
            if job_id:
                job_status = jm.get_job_status(job_id)
                if job_status and job_status.get("status") in ["running", "pending", "paused"]:
                    jm.cancel_job(job_id)
                    logger.info(f"Cancelled running job {job_id} for rejected opportunity {opportunity_id}.")
    
            db.update_opportunity_workflow_state(
                opportunity_id,
                "rejected_by_user",
                "rejected",
                error_message="Opportunity rejected by user.",
            )
            return {"message": "Opportunity rejected and any active job was cancelled."}
        except HTTPException as h:
            raise h
        except Exception as e:
            logger.error(
                f"Failed to reject opportunity {opportunity_id}: {e}", exc_info=True
            )
            raise HTTPException(status_code=500, detail=str(e))

class SocialMediaStatusUpdateRequest(BaseModel):
    new_status: str


# Add this new endpoint at the end of the file:
@router.post(
    "/orchestrator/{opportunity_id}/social-media-status", response_model=Dict[str, str]
)
async def update_social_media_status_endpoint(
    opportunity_id: int,
    request: SocialMediaStatusUpdateRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(
        get_orchestrator
    ),  # For client_id auth
):
    """Endpoint to update the status of social media posts (e.g., 'approved', 'rejected')."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        valid_statuses = ["draft", "approved", "rejected", "scheduled", "published"]
        if request.new_status not in valid_statuses:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid status: {request.new_status}. Must be one of {valid_statuses}.",
            )

        db.update_social_media_posts_status(opportunity_id, request.new_status)
        return {
            "message": "Social media posts status updated successfully.",
            "new_status": request.new_status,
        }
    except Exception as e:
        logger.error(
            f"Failed to update social media status for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))
```

## File: api/routers/qualification_settings.py
```python
# api/routers/qualification_settings.py

import logging
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get(
    "/clients/{client_id}/qualification-settings", response_model=Dict[str, Any]
)
async def get_qualification_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves the qualification settings for a specific client.
    """
    logger.info(f"Received request for qualification settings for client {client_id}")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    settings = db.get_qualification_settings(client_id)
    if not settings:
        raise HTTPException(status_code=404, detail="Qualification settings not found")
    return settings


@router.put(
    "/clients/{client_id}/qualification-settings", response_model=Dict[str, Any]
)
async def update_qualification_settings_endpoint(
    client_id: str,
    settings: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Updates the qualification settings for a specific client.
    """
    logger.info(
        f"Received request to update qualification settings for client {client_id}"
    )
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    db.update_qualification_settings(client_id, settings)
    return {"message": "Qualification settings updated successfully."}
```

## File: api/routers/qualification_strategies.py
```python
# api/routers/qualification_strategies.py

import logging
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any, List
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get(
    "/clients/{client_id}/qualification-strategies", response_model=List[Dict[str, Any]]
)
async def get_qualification_strategies_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all qualification strategies for a specific client.
    """
    logger.info(f"Received request for qualification strategies for client {client_id}")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    strategies = db.get_qualification_strategies(client_id)
    return strategies


@router.post(
    "/clients/{client_id}/qualification-strategies", response_model=Dict[str, Any]
)
async def create_qualification_strategy_endpoint(
    client_id: str,
    strategy: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Creates a new qualification strategy for a specific client.
    """
    logger.info(
        f"Received request to create qualification strategy for client {client_id}"
    )
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    strategy_id = db.create_qualification_strategy(client_id, strategy)
    return {"id": strategy_id}


@router.put("/qualification-strategies/{strategy_id}", response_model=Dict[str, str])
async def update_qualification_strategy_endpoint(
    strategy_id: int,
    strategy: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Updates a qualification strategy.
    """
    logger.info(f"Received request to update qualification strategy {strategy_id}")
    # Auth check
    strat_to_update = db.get_qualification_strategy_by_id(strategy_id)
    if not strat_to_update:
        raise HTTPException(status_code=404, detail="Strategy not found.")
    if strat_to_update["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to modify this resource.",
        )

    db.update_qualification_strategy(strategy_id, strategy)
    return {"message": "Qualification strategy updated successfully."}


@router.delete("/qualification-strategies/{strategy_id}", response_model=Dict[str, str])
async def delete_qualification_strategy_endpoint(
    strategy_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Deletes a qualification strategy.
    """
    logger.info(f"Received request to delete qualification strategy {strategy_id}")
    # Auth check
    strat_to_delete = db.get_qualification_strategy_by_id(strategy_id)
    if not strat_to_delete:
        raise HTTPException(status_code=404, detail="Strategy not found.")
    if strat_to_delete["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to delete this resource.",
        )

    db.delete_qualification_strategy(strategy_id)
    return {"message": "Qualification strategy deleted successfully."}
```

## File: api/routers/settings.py
```python
# api/routers/settings.py
import logging
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)

@router.get("/settings/{client_id}", response_model=Dict[str, Any])
async def get_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint for fetching all client-specific settings."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        settings = db.get_client_settings(client_id)
        if not settings:
            raise HTTPException(status_code=404, detail="Settings not found for this client.")
        return settings
    except Exception as e:
        logger.error(f"Failed to retrieve settings for client {client_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to retrieve settings.")

@router.put("/settings/{client_id}", response_model=Dict[str, str])
async def update_settings_endpoint(
    client_id: str,
    settings: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint for updating client-specific settings."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        db.update_client_settings(client_id, settings)
        return {"message": "Settings updated successfully."}
    except Exception as e:
        logger.error(f"Failed to update settings for client {client_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to update settings.")
```

## File: api/dependencies.py
```python
# api/dependencies.py
import os
from fastapi import Depends, HTTPException, Security, Request
from fastapi.security import APIKeyHeader
from data_access.database_manager import DatabaseManager
from backend.pipeline import WorkflowOrchestrator
from jobs import JobManager
from services.opportunities_service import OpportunitiesService
from services.discovery_service import DiscoveryService
from . import globals as api_globals


def get_db() -> DatabaseManager:
    """Dependency injector for DatabaseManager."""
    return api_globals.db_manager


def get_opportunities_service(
    db: DatabaseManager = Depends(get_db),
) -> OpportunitiesService:
    """Dependency injector for OpportunitiesService."""
    return OpportunitiesService(db)


def get_discovery_service(db: DatabaseManager = Depends(get_db)) -> DiscoveryService:
    """Dependency injector for DiscoveryService."""
    return DiscoveryService(db)


def get_job_manager() -> JobManager:
    """Dependency injector for JobManager."""
    return api_globals.job_manager


# Replace the entire `get_current_client_id` function with this:
def get_current_client_id(request: Request) -> str:
    """
    Dependency to get the current client_id from the X-Client-ID header.
    In a real multi-tenant application, this would also be validated against user's permissions.
    """
    client_id = request.headers.get("X-Client-ID")
    if not client_id:
        # Fallback to default if header is missing, or raise HTTPException
        # For development, we might fallback. For production, raising is safer.
        # raise HTTPException(status_code=400, detail="X-Client-ID header is required")
        return "Lark_Main_Site"  # Fallback for local dev/testing
    return client_id


# Update the `get_orchestrator` dependency to *not* directly use `get_current_client_id` within its signature
# because it will be passed explicitly to the endpoint if needed.
# Modify `get_orchestrator` signature from `get_orchestrator(client_id: str, ...)` to:
def get_orchestrator(
    request: Request,  # Add Request to get client_id
    db: DatabaseManager = Depends(get_db),
    jm: JobManager = Depends(get_job_manager),
) -> WorkflowOrchestrator:
    """
    Dependency injector for WorkflowOrchestrator.
    Creates a new instance for each request, configured for the specific client_id.
    """
    client_id = request.headers.get("X-Client-ID")  # Get client_id from request headers
    if not client_id:
        # Fallback to default for orchestrator initialization if header is missing
        client_id = "Lark_Main_Site"

    if not db.get_client_settings(client_id):
        raise HTTPException(
            status_code=404, detail=f"Client with ID '{client_id}' not found."
        )

    return WorkflowOrchestrator(api_globals.config_manager, db, client_id, jm)


API_KEY = os.getenv("INTERNAL_API_KEY")
API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)


async def get_api_key(api_key: str = Security(api_key_header)):
    if api_key == API_KEY:
        return api_key
    else:
        raise HTTPException(status_code=403, detail="Could not validate credentials")
```

## File: api/globals.py
```python
# api/globals.py
from typing import Optional
from app_config.manager import ConfigManager
from data_access.database_manager import DatabaseManager
from jobs import JobManager

config_manager: Optional[ConfigManager] = None
db_manager: Optional[DatabaseManager] = None
job_manager: Optional[JobManager] = None
```

## File: api/main.py
```python
# api/main.py
# api/main.py (New File, or existing FastAPI entry point)
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles  # ADD THIS for Task 3
import logging
import os
import sys

# Add project root to sys.path to resolve imports from agents, pipeline, etc.
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# Import from your existing project structure
from app_config.manager import ConfigManager
from data_access.database_manager import DatabaseManager
from jobs import JobManager  # Import the class

from . import globals as api_globals


logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI()

# Mount the static directory for generated images
# Images will be accessible at /api/images/{filename}
static_images_path = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "generated_images")
)
app.mount(
    "/api/images", StaticFiles(directory=static_images_path), name="static_images"
)


# --- Global Dependency Initialization (simplified for example) ---
# In a real app, use @lru_cache or proper dependency injection
@app.on_event("startup")
async def startup_event():
    api_globals.config_manager = ConfigManager()
    api_globals.db_manager = DatabaseManager(cfg_manager=api_globals.config_manager)
    api_globals.db_manager.initialize()  # Ensure DB tables are created/migrated
    api_globals.job_manager = JobManager(
        db_manager=api_globals.db_manager
    )  # Initialize JobManager with db_manager

    logger.info("FastAPI application startup complete. Dependencies initialized.")

    from .routers import (
        auth,
        clients,
        opportunities,
        discovery,
        orchestrator,
        jobs,
        qualification_settings,
        qualification_strategies,
        settings,
    )

    app.include_router(auth.router)
    app.include_router(clients.router)
    app.include_router(opportunities.router)
    app.include_router(discovery.router)
    app.include_router(orchestrator.router)
    app.include_router(jobs.router)
    app.include_router(qualification_settings.router)
    app.include_router(qualification_strategies.router)
    app.include_router(settings.router)
```

## File: api/models.py
```python
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any


class DiscoveryCostParams(BaseModel):
    seed_keywords: List[str]
    discovery_modes: Optional[List[str]] = ["ideas"]
    limit: Optional[int] = 1000
    include_clickstream_data: Optional[bool] = False
    people_also_ask_click_depth: Optional[int] = 0


# Define ContentUpdateRequest (W18 FIX)
class ContentUpdatePayload(BaseModel):
    article_body_html: str = Field(
        ..., description="The new HTML content for the article body."
    )


# Define ImageUpdatePayload (W18 FIX)
class ImageRegenRequest(BaseModel):
    original_prompt: str
    new_prompt: str


class DiscoveryRunRequest(BaseModel):
    seed_keywords: List[str]
    discovery_modes: Optional[List[str]] = ["ideas"]
    filters: Optional[List[Any]] = None
    order_by: Optional[List[str]] = None
    filters_override: Optional[Dict[str, Any]] = {}
    depth: Optional[int] = None
    limit: Optional[int] = None
    ignore_synonyms: Optional[bool] = False
    # NEW: Parameters for user flexibility
    include_clickstream_data: Optional[bool] = None
    closely_variants: Optional[bool] = None


class KeywordListRequest(BaseModel):
    seed_keywords: List[str]
    # NEW: Parameter for dynamic cost estimation
    include_clickstream_data: Optional[bool] = False


class JobResponse(BaseModel):
    job_id: str
    message: str
    status: Optional[str] = None
    progress: Optional[int] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    progress_log: Optional[List[Dict[str, Any]]] = None


class LoginRequest(BaseModel):
    password: str


class TemplateContent(BaseModel):
    name: str
    content: str
    description: Optional[str] = None


class TemplateResponse(BaseModel):
    name: str
    content: str
    description: Optional[str] = None
    last_updated: str


class PromptPreviewRequest(BaseModel):
    custom_template_content: Optional[str] = None


class PromptPreviewResponse(BaseModel):
    prompt: str


class ContentHistoryItem(BaseModel):
    id: int
    opportunity_id: int
    timestamp: str
    ai_content_json: Dict[str, Any]


class RestoreRequest(BaseModel):
    version_timestamp: str


class SingleImageRegenRequest(BaseModel):
    opportunity_id: int
    original_prompt: str
    new_prompt: str


class AutoWorkflowRequest(BaseModel):
    override_validation: bool = False


class SocialMediaPostsUpdate(BaseModel):
    social_media_posts: List[Dict[str, Any]]


class GlobalSettingsUpdate(BaseModel):
    settings: Dict[str, Any]


class OpportunityListResponse(BaseModel):
    items: List[Dict[str, Any]]
    total_items: int
    page: int
    limit: int


class AnalysisRequest(BaseModel):
    selected_competitor_urls: Optional[List[str]] = None


class RefineContentRequest(BaseModel):
    html_content: str
    command: str


class ClientSettings(BaseModel):
    brand_tone: Optional[str] = None
    target_audience: Optional[str] = None
    terms_to_avoid: Optional[str] = None


class GenerationOverrides(BaseModel):
    target_word_count: Optional[int] = None
    expert_persona: Optional[str] = None
    additional_instructions: Optional[str] = None


class ApproveAnalysisRequest(BaseModel):
    overrides: Optional[GenerationOverrides] = None
```

## File: app_config/__init__.py
```python
# app_config/__init__.py
# This file marks the directory as a Python package.
```

## File: app_config/manager.py
```python
import os
import configparser
from dotenv import load_dotenv
from typing import Dict, Any, List, Optional
import logging


class ConfigManager:
    """
    Manages loading, validating, and providing configuration settings.
    Handles global defaults from settings.ini and client-specific overrides.
    """

    _setting_types = {
        # Integers
        "location_code": int,
        "max_sv_for_scoring": int,
        "max_domain_rank_for_scoring": int,
        "max_referring_domains_for_scoring": int,
        "max_avg_referring_domains_filter": int,
        "serp_freshness_old_threshold_days": int,
        "serp_volatility_stable_threshold_days": int,
        "min_competitor_word_count": int,
        "max_competitor_technical_warnings": int,
        "num_competitors_to_analyze": int,
        "num_common_headings": int,
        "num_unique_angles": int,
        "max_initial_serp_urls_to_analyze": int,
        "people_also_ask_click_depth": int,
        "min_search_volume": int,
        "max_keyword_difficulty": int,
        "num_in_article_images": int,
        "onpage_max_domains_per_request": int,
        "onpage_max_tasks_per_request": int,
        "deep_dive_top_n_keywords": int,
        "max_completion_tokens_for_generation": int,
        "discovery_max_pages": int,
        "min_serp_results": int,
        "max_serp_results": int,
        "min_avg_backlinks": int,
        "max_avg_backlinks": int,
        "discovery_related_depth": int,
        "yearly_trend_decline_threshold": int,
        "quarterly_trend_decline_threshold": int,
        "max_kd_hard_limit": int,
        "max_referring_main_domains_limit": int,
        "max_avg_domain_rank_threshold": int,
        "min_keyword_word_count": int,
        "max_keyword_word_count": int,
        "crowded_serp_features_threshold": int,
        "min_serp_stability_days": int,
        "max_non_blog_results": int,
        "max_ai_overview_words": int,
        "max_first_organic_y_pixel": int,
        "max_words_for_ai_analysis": int,
        "num_competitors_for_ai_analysis": int,
        "max_avg_lcp_time": int,  # NEW
        "high_value_sv_override_threshold": int,
        "overlay_font_size": int,
        # Floats
        "informational_score": float,
        "commercial_score": float,
        "transactional_score": float,
        "navigational_score": float,
        "question_keyword_bonus": float,
        "max_cpc_for_scoring": float,
        "featured_snippet_bonus": float,
        "ai_overview_bonus": float,
        "serp_freshness_bonus_max": float,
        "min_cpc_filter": float,
        "min_yearly_trend_filter": float,
        "min_cpc": float,
        "max_cpc": float,
        "min_competition": float,
        "max_competition": float,
        "min_cpc_filter_api": float,
        "category_intent_bonus": float,
        "search_volume_volatility_threshold": float,
        "max_paid_competition_score": float,
        "max_high_top_of_page_bid": float,
        "max_pages_to_domain_ratio": float,
        "ai_generation_temperature": float,
        "recommended_word_count_multiplier": float,
        "default_multiplier": float,  # ADDED
        "comprehensive_article": float,  # ADDED
        "how_to_guide": float,  # ADDED
        "comparison_post": float,  # ADDED
        "review_article": float,  # ADDED
        "video_led_article": float,  # ADDED
        "forum_summary_post": float,  # ADDED
        "recipe_article": float,
        "scholarly_summary": float,
        "product_comparison": float,
        "high_value_cpc_override_threshold": float,
        # Booleans
        "require_question_keywords": bool,
        "enforce_intent_filter": bool,
        "calculate_rectangles": bool,
        "enable_cache": bool,
        "deep_dive_discovery": bool,
        "use_pexels_first": bool,
        "cleanup_local_images": bool,
        "onpage_enable_javascript": bool,
        "onpage_load_resources": bool,
        "onpage_disable_cookie_popup": bool,
        "onpage_return_despite_timeout": bool,
        "onpage_enable_browser_rendering": bool,
        "onpage_store_raw_html": bool,
        "onpage_validate_micromarkup": bool,
        "discovery_replace_with_core_keyword": bool,
        "discovery_ignore_synonyms": bool,
        "enable_automated_internal_linking": bool,
        "generate_toc": bool,
        "overlay_text_enabled": bool,
        "include_clickstream_data": bool,
        "load_async_ai_overview": bool,  # ADD THIS FOR W3
        "onpage_check_spell": bool,  # ADD THIS LINE (W5 FIX)
        "disable_ai_overview_check": bool,
        "onpage_accept_language": str,  # ADD THIS LINE (W7 FIX)
        "onpage_enable_switch_pool": bool,  # ADD THIS LINE (W13 FIX)
        "onpage_enable_custom_js": bool,  # ADD THIS LINE (W12 FIX)
        "onpage_custom_js": str,  # ADD THIS LINE (W12 FIX)
        "discovery_exact_match": bool,  # ADD THIS LINE (W7 FIX)
        "onpage_browser_screen_resolution_ratio": float,  # ADD THIS LINE (W7 FIX)
        # Lists (comma-separated strings)
        "allowed_intents": list,
        "negative_keywords": list,
        "competitor_blacklist_domains": list,
        "serp_feature_filters": list,
        "serp_features_exclude_filter": list,
        "platforms": list,
        "default_wordpress_categories": list,
        "default_wordpress_tags": list,
        "ugc_and_parasite_domains": list,
        "high_value_categories": list,
        "hostile_serp_features": list,
        "final_validation_non_blog_domains": list,
        # Weights
        "ease_of_ranking_weight": float,
        "traffic_potential_weight": float,
        "commercial_intent_weight": float,
        "serp_features_weight": float,
        "growth_trend_weight": float,
        "serp_freshness_weight": float,
        "serp_volatility_weight": float,
        "competitor_weakness_weight": float,
        "competitor_performance_weight": float,  # ADDED THIS LINE
        # Strings
        "max_competition_level": str,
        "non_evergreen_year_pattern": str,
        "db_file_name": str,  # NEW
        "db_type": str,  # NEW
        "overlay_text_color": str,
        "overlay_background_color": str,
        "overlay_position": str,
        "closely_variants": bool,
        "max_cpc_filter": float,
        "discovery_order_by_field": str,
        "discovery_order_by_direction": str,
        "search_phrase_regex": str,
        "onpage_custom_checks_thresholds": str,  # ADD THIS LINE (W9 FIX)
        "serp_remove_from_url_params": str,
        "schema_author_type": str,
        "client_knowledge_base": str,
        "wordpress_url": str,
        "wordpress_user": str,
        "wordpress_app_password": str,
        "wordpress_seo_plugin": str,
    }

    def __init__(self, settings_path: str = "backend/app_config/settings.ini"):
        load_dotenv()
        self.config_parser = configparser.ConfigParser(inline_comment_prefixes=(";",))
        if not os.path.exists(settings_path):
            raise FileNotFoundError(f"Configuration file not found at: {settings_path}")
        self.config_parser.read(settings_path)
        self.logger = logging.getLogger(self.__class__.__name__)
        self._configure_logging()
        self._global_settings = self._load_and_validate_global()

    def _configure_logging(self):
        """Sets up basic logging for the application."""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        logging.getLogger("urllib3").setLevel(logging.WARNING)
        self.logger.info("Logging configured.")

    def _get_int_from_config(
        self, section: str, key: str, fallback: Optional[int] = None
    ) -> int:
        try:
            value_str = self.config_parser.get(section, key)
            return int(value_str)
        except (configparser.NoOptionError, configparser.NoSectionError, ValueError):
            if fallback is not None:
                return fallback
            raise ValueError(
                f"Missing or invalid integer configuration for [{section}]{key}"
            )

    def _get_float_from_config(
        self, section: str, key: str, fallback: Optional[float] = None
    ) -> float:
        try:
            value_str = self.config_parser.get(section, key)
            return float(value_str)
        except (configparser.NoOptionError, configparser.NoSectionError, ValueError):
            if fallback is not None:
                return fallback
            raise ValueError(
                f"Missing or invalid float configuration for [{section}]{key}"
            )

    def _get_list_from_config(
        self, section: str, key: str, fallback: str = ""
    ) -> List[str]:
        try:
            value_str = self.config_parser.get(section, key)
            return [item.strip() for item in value_str.split(",") if item.strip()]
        except (configparser.NoOptionError, configparser.NoSectionError):
            return [item.strip() for item in fallback.split(",") if item.strip()]

    def _load_and_validate_global(self) -> Dict[str, Any]:
        """Loads all global settings from settings.ini and .env."""
        settings = {}

        # API Credentials (from .env)
        settings["dataforseo_login"] = os.getenv("DATAFORSEO_LOGIN")
        settings["dataforseo_password"] = os.getenv("DATAFORSEO_PASSWORD")
        if not settings["dataforseo_login"] or not settings["dataforseo_password"]:
            self.logger.critical(
                "DATAFORSEO_LOGIN and DATAFORSEO_PASSWORD must be set in the .env file."
            )
            raise ValueError(
                "DATAFORSEO_LOGIN and DATAFORSEO_PASSWORD must be set in the .env file."
            )

        settings["openai_api_key"] = os.getenv("OPENAI_API_KEY")
        if not settings["openai_api_key"]:
            self.logger.warning(
                "OPENAI_API_KEY is not set in the .env file. AI content generation will likely fail."
            )

        settings["pexels_api_key"] = os.getenv("PEXELS_API_KEY")
        if not settings["pexels_api_key"]:
            self.logger.warning(
                "PEXELS_API_KEY is not set in the .env file. Pexels integration will be unavailable."
            )

        # UI Password (from .env)
        settings["ui_password"] = os.getenv("UI_PASSWORD")
        if not settings["ui_password"]:
            self.logger.critical(
                "UI_PASSWORD must be set in the .env file for Streamlit authentication."
            )
            raise ValueError("UI_PASSWORD must be set in the .env file.")

        # Load all settings from settings.ini
        for section in self.config_parser.sections():
            for key, value in self.config_parser.items(section):
                try:
                    target_type = self._setting_types.get(key)
                    if target_type is bool:
                        settings[key] = self.config_parser.getboolean(section, key)
                    elif target_type is int:
                        settings[key] = self.config_parser.getint(section, key)
                    elif target_type is float:
                        settings[key] = self.config_parser.getfloat(section, key)
                    elif target_type is list:
                        raw_values = self._get_list_from_config(section, key)
                        if key == "serp_feature_filters":
                            parsed_filters = []
                            for f_str in raw_values:
                                if f_str.startswith("no_"):
                                    parsed_filters.append(
                                        {"type": "has_not", "feature": f_str[3:]}
                                    )
                                elif f_str.startswith("has_"):
                                    parsed_filters.append(
                                        {"type": "has", "feature": f_str[4:]}
                                    )
                            settings[key] = parsed_filters
                        else:
                            settings[key] = raw_values
                    else:  # Default to string if no type is mapped
                        settings[key] = value
                except Exception as e:
                    self.logger.critical(
                        f"FATAL CONFIG ERROR: Could not parse key [{section}]{key} with value '{value}' to expected type: {e}"
                    )
                    raise ValueError(
                        f"Configuration key parsing failed for [{section}]{key}. Value: '{value}'."
                    )

        self.logger.info("Global settings loaded.")
        return settings

    def get_global_config(self) -> Dict[str, Any]:
        """Returns the loaded global configuration."""
        return self._global_settings

    def get_default_client_settings_template(self) -> Dict[str, Any]:
        """Returns a template of client settings based on global config, for new client creation."""
        template = self._global_settings.copy()
        template.pop("dataforseo_login", None)
        template.pop("dataforseo_password", None)
        template.pop("openai_api_key", None)
        template.pop("pexels_api_key", None)
        template.pop("ui_password", None)

        for key, value in template.items():
            if isinstance(value, list):
                template[key] = value[:]
        return template

    def load_client_config(self, client_id: str, db_manager: Any) -> Dict[str, Any]:
        """
        Loads client-specific settings from the database and merges them with global settings.
        Scoring weights are always loaded from the global settings to ensure consistency.
        """
        client_settings_from_db = db_manager.get_client_settings(client_id)
        overridden_settings = self._global_settings.copy()

        # Define keys that should NOT be overridden by client-specific settings to ensure they are globally managed.
        globally_managed_keys = set(
            [
                "dataforseo_login",
                "dataforseo_password",
                "openai_api_key",
                "pexels_api_key",
                "ui_password",
                "db_file_name",
                "cache_file_name",
                "default_client_id",
                "db_type",
            ]
        )  # UPDATED

        for key, value in client_settings_from_db.items():
            if key in globally_managed_keys:
                continue  # Skip override for globally managed keys

            if value is not None and value != "":
                overridden_settings[key] = value

        self.logger.info(f"Loaded client-specific configuration for '{client_id}'.")
        return overridden_settings

    def save_client_settings(
        self, client_id: str, new_settings: Dict[str, Any], db_manager: Any
    ):
        """Saves specified client settings to the database."""
        db_manager.update_client_settings(client_id, new_settings)
        self.logger.info(f"Client settings for '{client_id}' saved to database.")

    def save_global_settings_to_file(self, updated_global_settings: Dict[str, Any]):
        """Saves specified settings back to the settings.ini file (for global defaults)."""
        for section in self.config_parser.sections():
            for key in self.config_parser.options(section):
                if (
                    key in updated_global_settings
                    and updated_global_settings[key] is not None
                ):
                    if isinstance(updated_global_settings[key], list):
                        self.config_parser.set(
                            section, key, ",".join(updated_global_settings[key])
                        )
                    elif isinstance(updated_global_settings[key], bool):
                        self.config_parser.set(
                            section, key, str(updated_global_settings[key]).lower()
                        )
                    else:
                        self.config_parser.set(
                            section, key, str(updated_global_settings[key])
                        )

        with open("backend/app_config/settings.ini", "w") as configfile:
            self.config_parser.write(configfile)

        self._global_settings = self._load_and_validate_global()
        self.logger.info("Global settings updated and reloaded from settings.ini.")
```

## File: app_config/settings.ini
```
[SEO_CRITERIA]
location_code = 2840
language_code = en
target_domain = profitparrot.com
device = desktop ; NEW: Global default device for SERP and OnPage calls
os = windows ; NEW: Global default OS for SERP calls

[INTENT_SCORING]
informational_score = 100
commercial_score = 70
transactional_score = 50
navigational_score = 10
question_keyword_bonus = 5

[SCORING_WEIGHTS]
ease_of_ranking_weight = 40
traffic_potential_weight = 15
commercial_intent_weight = 5
serp_features_weight = 5
growth_trend_weight = 5
serp_freshness_weight = 5
serp_volatility_weight = 5
competitor_weakness_weight = 20
competitor_performance_weight = 5 ; NEW: Weight for competitor technical performance

[SCORING_NORMALIZATION]
max_cpc_for_scoring = 20.0
max_sv_for_scoring = 50000
max_domain_rank_for_scoring = 700
max_referring_domains_for_scoring = 200 ; NEW: For ease of ranking calculation
max_avg_referring_domains_filter = 20 ; NEW: Filter for discovery - max avg referring domains for top competitors

[SERP_FEATURE_SCORING]
featured_snippet_bonus = 15
ai_overview_bonus = 10
serp_freshness_bonus_max = 20
serp_freshness_old_threshold_days = 180
serp_volatility_stable_threshold_days = 90 ; NEW: Threshold for a "stable" SERP

[QUALITY_FILTERS]
require_question_keywords = true
enforce_intent_filter = true
allowed_intents = informational
negative_keywords = login, sign in, account, free, cheap
min_search_volume = 100
max_keyword_difficulty = 80
min_cpc = 0.0
max_cpc = 5.0
min_competition = 0.0
max_competition = 1.0
max_competition_level = LOW
min_serp_results = 100000
max_serp_results = 10000000
min_avg_backlinks = 0
max_avg_backlinks = 20
min_keyword_word_count = 2 ; NEW
max_keyword_word_count = 8 ; NEW
high_value_sv_override_threshold = 10000 ; NEW
high_value_cpc_override_threshold = 5.0 ; NEW


[DEFAULT]
enable_cache = true
cache_file_name = data/cache.json
max_completion_tokens_for_generation = 32768
db_file_name = data/opportunities.db
ai_generation_temperature = 0.7
include_clickstream_data = false

[Lark_Main_Site]
target_domain = profitparrot.com
# Other client-specific settings can go here
expert_persona = a certified financial planner with 15 years of experience

[DISCOVERY_SETTINGS]
discovery_strategies = Keyword Ideas, Related Keywords, Keyword Suggestions ; W20 FIX: Centralize strategies
deep_dive_discovery = false
deep_dive_top_n_keywords = 5
serp_feature_filters = no_ai_overview, has_featured_snippet
load_async_ai_overview = false ; ADD THIS (W3 FIX)
discovery_max_pages = 100
people_also_ask_click_depth = 2
serp_features_exclude_filter = popular_products,local_pack,shopping,app,jobs,refine_products
closely_variants = false
discovery_exact_match = false ; ADD THIS (W7 Default: Disable for broad match by default)
min_cpc_filter = 0.0
max_cpc_filter = 999.0
min_competition = 0.0
max_competition = 1.0
max_competition_level = HIGH
discovery_ignore_synonyms = false


search_phrase_regex =

[IMAGE_GENERATION]
num_in_article_images = 2
use_pexels_first = true
cleanup_local_images = true
overlay_text_enabled = true
overlay_text_color = #FFFFFF
overlay_background_color = #00000080 ; RGBA hex for semi-transparent black
overlay_font_size = 40 ; Pixels
overlay_position = bottom_center ; top_left, top_right, bottom_left, bottom_center, bottom_right

[CONTENT]
generate_toc = true
default_author_name = Profit Parrot Marketing Expert
schema_author_type = Organization ; ADD THIS (W12 Default)

[ONPAGE_API_CLIENT_CONFIG] ; NEW: Separate section for OnPage API client parameters
onpage_enable_javascript = true
onpage_load_resources = true ; Set to true if browser rendering is enabled, as it forces loading resources.
onpage_disable_cookie_popup = true
onpage_return_despite_timeout = false
onpage_enable_browser_rendering = true
onpage_store_raw_html = false
onpage_validate_micromarkup = true
onpage_check_spell = true ; ADD THIS (W5 FIX)
onpage_accept_language = en ; ADD THIS (W7 FIX)
onpage_custom_user_agent = Mozilla/5.0 (compatible; RSiteAuditor)
onpage_max_domains_per_request = 5 ; Max 5 identical domains in one batch
onpage_max_tasks_per_request = 20
onpage_custom_checks_thresholds = {"high_loading_time": 2500} ; ADD THIS (W9 Default: Target < 2.5s instead of > 3s)
onpage_enable_switch_pool = true
ip_pool_for_scan = us
onpage_enable_custom_js = false ; ADD THIS (W12 Default)
onpage_custom_js = meta = {}; meta.url = document.URL; meta; ; ADD THIS (W12 Example)
onpage_browser_screen_resolution_ratio = 1.0 ; ADD THIS (W7 Default)

[APP_SETTINGS]
default_client_id = Lark_Main_Site
recommended_word_count_multiplier = 1.2
enable_automated_internal_linking = false

[SOCIAL_MEDIA]
platforms = facebook,linkedin,twitter,google_business_profile

[WORDPRESS_SETTINGS]
wordpress_url = 
wordpress_user = 
wordpress_app_password = 
wordpress_seo_plugin = aioseo
default_wordpress_categories = Blog
default_wordpress_tags = SEO,Content Marketing

[PAGE_CLASSIFICATION]
forum_domains = reddit.com,quora.com,stackoverflow.com,forums.somethingawful.com
ecommerce_domains = amazon.com,ebay.com,walmart.com,target.com,bestbuy.com,etsy.com
news_domains = nytimes.com,bbc.com,cnn.com,theguardian.com,wsj.com
blog_url_patterns = /blog/,/post/,/article/,/\d{4}/\d{2}/
forum_url_patterns = /thread/,/forum/,/discussion/,/q/,/questions/
ugc_and_parasite_domains = linkedin.com, pinterest.com, amazon.com, ebay.com, wikipedia.org, reddit.com, facebook.com, youtube.com, medium.com, quora.com

[DISQUALIFICATION_RULES]
; Tier 1
allowed_intents = informational
negative_keywords = login, sign in, account, free, cheap, porn
min_search_volume = 100

; Tier 2
yearly_trend_decline_threshold = -25
quarterly_trend_decline_threshold = 0
search_volume_volatility_threshold = 1.5

; Tier 3
max_paid_competition_score = 0.8
max_high_top_of_page_bid = 15.00
max_kd_hard_limit = 70
max_referring_main_domains_limit = 100
max_avg_domain_rank_threshold = 500
max_pages_to_domain_ratio = 15

; Tier 4
non_evergreen_year_pattern = 20[12]\d ; e.g., 2010-2029
min_keyword_word_count = 2
max_keyword_word_count = 8
hostile_serp_features = shopping,local_pack,google_flights,google_hotels,popular_products,local_services
crowded_serp_features_threshold = 4
min_serp_stability_days = 14
max_y_pixel_threshold = 800
max_forum_results_in_top_10 = 3
max_ecommerce_results_in_top_10 = 2
disallowed_page_types_in_top_3 = E-commerce,Forum

[FINAL_VALIDATION]
; Thresholds for the final, live SERP validation gate before running a full analysis.
disable_ai_overview_check = true
max_non_blog_results = 3
max_ai_overview_words = 250
max_first_organic_y_pixel = 1500
final_validation_non_blog_domains = amazon.com,walmart.com,ebay.com,reddit.com,quora.com
max_avg_lcp_time = 4000 ; NEW: For Rule 21 in run_final_validation

[ANALYSIS]
enable_deep_competitor_analysis = false
num_competitors_for_ai_analysis = 3
serp_analysis_depth = 100
max_words_for_ai_analysis = 2000
serp_remove_from_url_params = srsltid,utm_source,ref_id ; ADD THIS (W11 Default)

[WORD_COUNT_MULTIPLIERS]
default_multiplier = 1.2
comprehensive_article = 1.3
how_to_guide = 1.5
comparison_post = 1.1
review_article = 1.2
video_led_article = 0.8
forum_summary_post = 1.0
recipe_article = 1.0
scholarly_summary = 1.1
product_comparison = 1.2

[OPENAI_PRICING]
# Prices are per 1 million tokens
gpt-4o_input = 5.00
gpt-4o_output = 15.00
gpt-3.5-turbo_input = 0.50
gpt-3.5-turbo_output = 1.50

[OpenAI]
default_model = gpt-5-nano
default_image_model = dall-e-3
api_key = ${OPENAI_API_KEY}
```

## File: core/serp_analyzers/disqualification_analyzer.py
```python
# core/serp_analyzers/disqualification_analyzer.py
from typing import Dict, Any


class DisqualificationAnalyzer:
    def analyze(
        self, analysis: Dict[str, Any], config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Applies a set of granular rules to determine if a keyword should be disqualified.
        """
        disqualification_rules = config.get("disqualification_rules", {})

        # Rule: SERP is too crowded (pixel ranking)
        max_y_pixel = disqualification_rules.get("max_y_pixel_threshold")
        if max_y_pixel and analysis.get("first_organic_y_pixel") is not None:
            if analysis["first_organic_y_pixel"] > max_y_pixel:
                return {
                    "is_disqualified": True,
                    "disqualification_reason": f"First organic result is pushed down by {analysis['first_organic_y_pixel']} pixels, exceeding the {max_y_pixel}px threshold.",
                }

        # Rules based on page types in top 10
        top_10_results = analysis.get("top_organic_results", [])[:10]
        page_types = [result.get("page_type") for result in top_10_results]

        max_forum_results = disqualification_rules.get("max_forum_results_in_top_10")
        if (
            max_forum_results is not None
            and page_types.count("Forum") > max_forum_results
        ):
            return {
                "is_disqualified": True,
                "disqualification_reason": f"SERP contains {page_types.count('Forum')} forum results in the top 10, exceeding the threshold of {max_forum_results}.",
            }

        max_ecommerce_results = disqualification_rules.get(
            "max_ecommerce_results_in_top_10"
        )
        if (
            max_ecommerce_results is not None
            and page_types.count("E-commerce") > max_ecommerce_results
        ):
            return {
                "is_disqualified": True,
                "disqualification_reason": f"SERP contains {page_types.count('E-commerce')} e-commerce results in the top 10, exceeding the threshold of {max_ecommerce_results}.",
            }

        # Rule: Disallowed page types in top 3
        disallowed_in_top_3 = disqualification_rules.get(
            "disallowed_page_types_in_top_3", []
        )
        if disallowed_in_top_3:
            top_3_page_types = [
                result.get("page_type") for result in top_10_results[:3]
            ]
            for page_type in disallowed_in_top_3:
                if page_type in top_3_page_types:
                    return {
                        "is_disqualified": True,
                        "disqualification_reason": f"A '{page_type}' result was found in the top 3, which is a disallowed page type for high-ranking positions.",
                    }

        return {"is_disqualified": False, "disqualification_reason": None}
```

## File: core/serp_analyzers/featured_snippet_analyzer.py
```python
# core/serp_analyzers/featured_snippet_analyzer.py

from typing import Dict, Any


class FeaturedSnippetAnalyzer:
    def analyze(self, serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyzes the featured snippet in the SERP."""
        analysis = {
            "serp_has_featured_snippet": "featured_snippet"
            in serp_data.get("item_types", []),
            "featured_snippet_content": None,
        }

        for item in serp_data.get("items", []):
            if item.get("type") == "featured_snippet":
                analysis["featured_snippet_content"] = item.get("description")
                break

        return analysis
```

## File: core/serp_analyzers/pixel_ranking_analyzer.py
```python
# core/serp_analyzers/pixel_ranking_analyzer.py

from typing import Dict, Any


class PixelRankingAnalyzer:
    def analyze(self, serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyzes the pixel ranking data in the SERP."""
        analysis = {
            "pixel_ranking_summary": None,
            "raw_pixel_ranking_data": [],
            "first_organic_y_pixel": None,
        }

        for item in serp_data.get("items", []):
            if item.get("rectangle"):
                analysis["raw_pixel_ranking_data"].append(
                    {
                        "type": item.get("type"),
                        "rank_group": item.get("rank_group"),
                        "rank_absolute": item.get("rank_absolute"),
                        "title": item.get("title"),
                        "rectangle": item.get("rectangle"),
                    }
                )

        if analysis["raw_pixel_ranking_data"]:
            top_organic_rects_y_coords = [
                r["rectangle"]["y"]
                for r in analysis["raw_pixel_ranking_data"]
                if r["type"] == "organic"
                and r.get("rank_absolute", 99) <= 3
                and "y" in r.get("rectangle", {})
            ]
            if top_organic_rects_y_coords:
                avg_y = sum(top_organic_rects_y_coords) / len(
                    top_organic_rects_y_coords
                )
                analysis["pixel_ranking_summary"] = (
                    f"Top 3 organic results start an average of {avg_y:.0f} pixels from the top of the page."
                )

        first_organic_result = next(
            (
                r
                for r in analysis["raw_pixel_ranking_data"]
                if r["type"] == "organic" and r.get("rank_absolute") == 1
            ),
            None,
        )
        if first_organic_result and "y" in first_organic_result.get("rectangle", {}):
            analysis["first_organic_y_pixel"] = first_organic_result["rectangle"]["y"]

        return analysis
```

## File: core/serp_analyzers/video_analyzer.py
```python
# core/serp_analyzers/video_analyzer.py

from typing import Dict, Any


class VideoAnalyzer:
    def analyze(self, serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyzes the video results in the SERP."""
        analysis = {
            "serp_has_video_results": "video" in serp_data.get("item_types", [])
        }

        return analysis
```

## File: core/__init__.py
```python
# core/__init__.py
# This file marks the directory as a Python package.
```

## File: core/blueprint_factory.py
```python
import json
from datetime import datetime
from typing import Dict, Any
import logging

from backend.agents.brief_assembler import BriefAssembler
from backend.agents.internal_linking_suggester import InternalLinkingSuggester
from backend.core import utils
from backend.data_access.database_manager import DatabaseManager


class BlueprintFactory:
    def __init__(
        self, openai_client, client_cfg, dataforseo_client, db_manager: DatabaseManager
    ):
        self.brief_assembler = BriefAssembler(openai_client)
        self.internal_linking_suggester = InternalLinkingSuggester(
            openai_client, client_cfg, db_manager
        )
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = client_cfg

    def _create_executive_summary(self, blueprint_data: Dict[str, Any]) -> str:
        # Placeholder until SummaryGenerator is integrated here
        return "The executive summary will be generated by the AI based on the full analysis when implementation is complete."

    def create_blueprint(
        self,
        seed_topic: str,
        winning_keyword_data: Dict[str, Any],
        analysis_data: Dict[str, Any],
        total_api_cost: float,
        client_id: str,
    ) -> Dict[str, Any]:
        """Assembles all data into the final, structured JSON blueprint."""

        analysis_notes = None
        competitor_analysis_data = analysis_data.get("competitor_analysis", [])
        if not competitor_analysis_data or (
            len(competitor_analysis_data) == 1
            and "message" in competitor_analysis_data[0]
        ):
            analysis_notes = "No qualified article-based competitors were found in the top results after rigorous qualification. This SERP may be dominated by social media, video, or other non-article formats, making it a challenging topic to rank for with a standard blog post."
            competitor_analysis_data = []  # Ensure it's always an empty list of competitors

        recommended_strategy_data = analysis_data.get("recommended_strategy", {})
        self.logger.info(
            f"STRATEGY DATA FOR BLUEPRINT: {json.dumps(recommended_strategy_data)}"
        )

        blueprint_data = {
            "metadata": {
                "seed_topic": seed_topic,
                "blueprint_version": "6.0",
                "generated_at": datetime.now().isoformat(),
                "total_api_cost": round(total_api_cost, 4),
                "client_id": client_id,
            },
            "winning_keyword": winning_keyword_data,
            "serp_overview": analysis_data.get("serp_overview", {}),
            "content_intelligence": analysis_data.get("content_intelligence", {}),
            "competitor_analysis": competitor_analysis_data,
            "recommended_strategy": recommended_strategy_data,
            "final_qualification_assessment": recommended_strategy_data.get(
                "final_qualification_assessment", {}
            ),
            "analysis_notes": analysis_notes,
        }

        blueprint_data["executive_summary"] = self._create_executive_summary(
            blueprint_data
        )

        # --- START MODIFICATION ---
        # Pass rich serp_overview data to brief_assembler
        blueprint_data["ai_content_brief"] = self.brief_assembler.assemble_brief(
            blueprint_data, client_id, self.client_cfg
        )

        brief_text_for_linking = json.dumps(blueprint_data["ai_content_brief"])
        target_domain = self.client_cfg.get("target_domain")
        key_entities = blueprint_data.get("ai_content_brief", {}).get(
            "key_entities_to_mention", []
        )

        if brief_text_for_linking and target_domain:
            suggestions, linking_cost = self.internal_linking_suggester.suggest_links(
                brief_text_for_linking, key_entities, target_domain, client_id
            )
            blueprint_data["internal_linking_suggestions"] = suggestions
            blueprint_data["metadata"]["total_api_cost"] = round(
                blueprint_data["metadata"]["total_api_cost"] + linking_cost, 4
            )

        keyword_for_slug = winning_keyword_data.get("keyword", seed_topic)
        # Ensure the slug is part of the blueprint
        opportunity_slug = (
            f"{utils.slugify(keyword_for_slug)}-{int(datetime.now().timestamp())}"
        )
        blueprint_data["slug"] = opportunity_slug
        # --- END MODIFICATION ---

        return blueprint_data
```

## File: core/page_classifier.py
```python
# core/page_classifier.py
import re
from typing import Dict, Any
from urllib.parse import urlparse


class PageClassifier:
    """
    Categorizes a webpage based on its URL, domain, title, and other attributes.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config.get("page_classification", {})
        self.forum_domains = self.config.get("forum_domains", [])
        self.ecommerce_domains = self.config.get("ecommerce_domains", [])
        self.news_domains = self.config.get("news_domains", [])

        # Pre-compile regex for efficiency
        self.blog_patterns = [
            re.compile(p) for p in self.config.get("blog_url_patterns", [])
        ]
        self.forum_patterns = [
            re.compile(p) for p in self.config.get("forum_url_patterns", [])
        ]

    def classify(self, url: str, domain: str, title: str) -> str:
        """
        Classifies the given URL into a specific page type.

        Args:
            url: The full URL of the page.
            domain: The domain of the page.
            title: The title of the page.

        Returns:
            A string representing the classified page type.
        """
        if domain in self.ecommerce_domains:
            return "E-commerce"
        if domain in self.forum_domains:
            return "Forum"
        if domain in self.news_domains:
            return "News"

        # Check URL patterns for more specific types
        parsed_url = urlparse(url)
        path = parsed_url.path

        for pattern in self.forum_patterns:
            if pattern.search(path) or pattern.search(title.lower()):
                return "Forum"

        for pattern in self.blog_patterns:
            if pattern.search(path):
                return "Blog/Article"

        # Check for homepage/landing page (short path)
        if len(path.strip("/").split("/")) <= 1:
            return "Homepage/Landing Page"

        return "Blog/Article"  # Default category
```

## File: core/serp_analyzer.py
```python
import logging
from typing import Dict, Any, Tuple, Optional
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from core import utils
from core.serp_analyzers.featured_snippet_analyzer import FeaturedSnippetAnalyzer
from core.serp_analyzers.video_analyzer import VideoAnalyzer
from core.serp_analyzers.pixel_ranking_analyzer import PixelRankingAnalyzer
from core.page_classifier import PageClassifier
from core.serp_analyzers.disqualification_analyzer import DisqualificationAnalyzer


class FullSerpAnalyzer:
    """
    Performs a comprehensive analysis of the SERP for a given keyword.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.featured_snippet_analyzer = FeaturedSnippetAnalyzer()
        self.video_analyzer = VideoAnalyzer()
        self.pixel_ranking_analyzer = PixelRankingAnalyzer()
        self.page_classifier = PageClassifier(config)
        self.disqualification_analyzer = DisqualificationAnalyzer()

    def analyze_serp(self, keyword: str) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Fetches SERP data and extracts a wide range of insights, including rich SERP elements.
        """
        high_cost_operators = [
            "allinanchor:",
            "allintext:",
            "allintitle:",
            "allinurl:",
            "define:",
            "filetype:",
            "id:",
            "inanchor:",
            "info:",
            "intext:",
            "intitle:",
            "inurl:",
            "link:",
            "site:",
        ]
        keyword_lower = keyword.lower()

        if any(op in keyword_lower for op in high_cost_operators):
            raise ValueError(
                f"Keyword '{keyword}' contains a high-cost search operator. Please remove it and try again."
            )

        location_code = self.config.get("location_code")
        language_code = self.config.get("language_code")
        serp_call_params = {}

        serp_call_params["depth"] = 10  # Default to depth 10 for quick fetch

        paa_click_depth = self.config.get("people_also_ask_click_depth", 0)
        if isinstance(paa_click_depth, int) and 1 <= paa_click_depth <= 4:
            serp_call_params["people_also_ask_click_depth"] = paa_click_depth

        device = self.config.get("device", "desktop")
        os_name = self.config.get("os", "windows")
        if device == "mobile" and os_name not in ["android", "ios"]:
            os_name = "android"
        serp_call_params["device"] = device
        serp_call_params["os"] = os_name

        serp_results, cost = self.client.get_serp_results(
            keyword,
            location_code,
            language_code,
            client_cfg=self.config,
            serp_call_params=serp_call_params,
        )

        if not serp_results:
            return None, cost

        serp_times = utils.calculate_serp_times(
            serp_results.get("datetime"), serp_results.get("previous_updated_time")
        )

        analysis = {
            "serp_has_ai_overview": "ai_overview" in serp_results.get("item_types", []),
            "has_popular_products": "popular_products"
            in serp_results.get("item_types", []),
            "people_also_ask": [],  # Kept for backward compatibility, new field below is preferred
            "paa_questions": [],  # Primary field for cleaned PAA questions
            "top_organic_results": [],
            "related_searches": [],
            "knowledge_graph_data": {},  # Kept for backward compatibility
            "knowledge_graph_facts": [],  # NEW: Structured facts from KG
            "paid_ad_copy": [],  # NEW: Top paid ad titles/descriptions
            "ai_overview_content": None,
            "ai_overview_sources": [],  # NEW: URLs from AI Overview references
            "top_organic_faqs": [],  # NEW: FAQ questions from organic results
            "top_organic_sitelinks": [],  # NEW: Sitelinks from organic results
            "discussion_snippets": [],  # NEW: Snippets from discussions/forums/perspectives
            "product_considerations_summary": None,
            "refinement_chips": [],
            "extracted_serp_features": [],
            "serp_last_updated_days_ago": serp_times.get("days_ago"),
            "serp_update_interval_days": serp_times.get("update_interval_days"),
            "dominant_content_format": "Article",
        }

        analysis.update(self.featured_snippet_analyzer.analyze(serp_results))
        analysis.update(self.video_analyzer.analyze(serp_results))
        analysis.update(self.pixel_ranking_analyzer.analyze(serp_results))

        for item in serp_results.get("items") or []:
            item_type = item.get("type")

            # Organic Results
            if item_type == "organic":
                organic_result = {
                    "rank": item.get("rank_absolute"),
                    "url": item.get("url"),
                    "title": item.get("title"),
                    "domain": item.get("domain"),
                    "description": item.get("description"),
                    "page_type": self.page_classifier.classify(
                        item.get("url"), item.get("domain"), item.get("title")
                    ),
                }
                if item.get("rating"):
                    organic_result["rating"] = {
                        "value": item["rating"].get("value"),
                        "votes_count": item["rating"].get("votes_count"),
                        "rating_max": item["rating"].get("rating_max"),
                    }
                if item.get("about_this_result"):
                    organic_result["about_this_result_source_info"] = item[
                        "about_this_result"
                    ].get("source_info")
                    organic_result["about_this_result_search_terms"] = item[
                        "about_this_result"
                    ].get("search_terms")
                    organic_result["about_this_result_related_terms"] = item[
                        "about_this_result"
                    ].get("related_terms")

                # NEW: Extract FAQ and Sitelinks directly from organic results
                if item.get("faq") and item["faq"].get("items"):
                    analysis["top_organic_faqs"].extend(
                        [
                            faq_item.get("title")
                            for faq_item in item["faq"]["items"]
                            if faq_item.get("title")
                        ]
                    )
                if item.get("links"):
                    analysis["top_organic_sitelinks"].extend(
                        [
                            link.get("title")
                            for link in item["links"]
                            if link.get("title")
                        ]
                    )

                analysis["top_organic_results"].append(organic_result)

            # Paid Ads
            elif item_type == "paid":
                if item.get("title") and item.get("description"):
                    analysis["paid_ad_copy"].append(
                        {
                            "title": item.get("title"),
                            "description": item.get("description"),
                            "url": item.get("url"),
                        }
                    )

            # People Also Ask (PAA)
            elif item_type == "people_also_ask":
                all_paa_questions = []
                for paa_item in item.get("items") or []:
                    if paa_item and paa_item.get("title"):
                        (all_paa_questions.append(paa_item.get("title")),)
                    if paa_item and paa_item.get("expanded_element"):
                        for expanded_item in paa_item.get("expanded_element") or []:
                            if expanded_item and expanded_item.get("title"):
                                (all_paa_questions.append(expanded_item.get("title")),)
                analysis["paa_questions"] = list(set(all_paa_questions))
                analysis["people_also_ask"] = analysis[
                    "paa_questions"
                ]  # For backward compatibility

            # Knowledge Graph
            elif item_type == "knowledge_graph":
                analysis["knowledge_graph_data"] = {  # For backward compatibility
                    "title": item.get("title"),
                    "description": item.get("description"),
                    "url": item.get("url"),
                    "image_url": item.get("image_url"),
                }
                # NEW: Deep parse Knowledge Graph structured items
                if item.get("items"):
                    for kg_sub_item in item["items"]:
                        if (
                            kg_sub_item
                            and kg_sub_item.get("type") == "knowledge_graph_row_item"
                        ):
                            analysis["knowledge_graph_facts"].append(
                                f"{kg_sub_item.get('title')}: {kg_sub_item.get('text')}"
                            )
                        elif (
                            kg_sub_item
                            and kg_sub_item.get("type")
                            == "knowledge_graph_carousel_item"
                            and kg_sub_item.get("items")
                        ):
                            analysis["knowledge_graph_facts"].extend(
                                [
                                    carousel_el.get("title")
                                    for carousel_el in kg_sub_item["items"]
                                    if carousel_el.get("title")
                                ]
                            )
                        elif (
                            kg_sub_item
                            and kg_sub_item.get("type") == "knowledge_graph_list_item"
                            and kg_sub_item.get("items")
                        ):
                            analysis["knowledge_graph_facts"].extend(
                                [
                                    list_el.get("title")
                                    for list_el in kg_sub_item["items"]
                                    if list_el.get("title")
                                ]
                            )

            # AI Overview
            elif item_type == "ai_overview":
                ai_items = item.get("items") or []
                ai_parts = [
                    sub_item.get("markdown")
                    for sub_item in ai_items
                    if sub_item and sub_item.get("markdown")
                ]
                analysis["ai_overview_content"] = "\n".join(ai_parts)
                # NEW: Extract AI Overview References
                for sub_item in ai_items:
                    if sub_item and sub_item.get("references"):
                        analysis["ai_overview_sources"].extend(
                            [
                                ref.get("url")
                                for ref in sub_item["references"]
                                if ref.get("url")
                            ]
                        )

            # Discussions and Forums / Perspectives
            elif item_type in [
                "discussions_and_forums",
                "perspectives",
            ]:  # Combined handling
                if item.get("items"):
                    analysis["discussion_snippets"].extend(
                        [
                            d_item.get("title")
                            for d_item in item["items"]
                            if d_item.get("title")
                        ]
                    )

            # Related Searches
            elif item_type == "related_searches":
                related_items = item.get("items") or []
                for s in related_items:
                    if isinstance(s, str):
                        analysis["related_searches"].append(s)
                    elif isinstance(s, dict) and s.get("title"):
                        (analysis["related_searches"].append(s.get("title")),)

            # Product Considerations (existing)
            elif item_type == "product_considerations":
                title = item.get("title")
                items = item.get("items") or []
                if title and items:
                    considerations = [
                        sub.get("title") for sub in items if sub and sub.get("title")
                    ]
                    analysis["product_considerations_summary"] = (
                        f"{title}: {', '.join(considerations)}"
                    )

        # Deduplicate all lists
        analysis["ai_overview_sources"] = list(set(analysis["ai_overview_sources"]))
        analysis["top_organic_faqs"] = list(set(analysis["top_organic_faqs"]))
        analysis["top_organic_sitelinks"] = list(set(analysis["top_organic_sitelinks"]))
        analysis["discussion_snippets"] = list(set(analysis["discussion_snippets"]))

        # Determine dominant content format (existing logic)
        # ...

        disqualification_results = self.disqualification_analyzer.analyze(
            analysis, self.config
        )
        analysis.update(disqualification_results)

        return analysis, cost
```

## File: core/utils.py
```python
# core/utils.py
import logging
import re
from typing import Optional, Union, Dict
from datetime import datetime


def slugify(text: str) -> str:
    """
    Convert a string to a URL-friendly slug.
    """
    if not text:
        return ""
    text = text.lower()
    # Remove special characters
    text = re.sub(r"[^\w\s-]", "", text)
    # Replace spaces with hyphens
    text = re.sub(r"\s+", "-", text)
    return text


def is_question_keyword(keyword: str) -> bool:
    """
    Checks if a keyword is likely a question.
    Covers common question formats and leading words.
    """
    if not keyword:
        return False

    keyword_lower = keyword.lower().strip()

    # Common question prefixes
    question_starters = [
        "what",
        "when",
        "where",
        "who",
        "why",
        "how",
        "which",
        "whose",
        "is",
        "are",
        "am",
        "was",
        "were",
        "do",
        "does",
        "did",
        "can",
        "could",
        "will",
        "would",
        "should",
        "may",
        "might",
        "have",
        "has",
        "had",
        "are there",
        "is there",
    ]

    # Check if the keyword starts with a question word or ends with a question mark
    if keyword_lower.endswith("?"):
        return True

    for starter in question_starters:
        if keyword_lower.startswith(starter + " "):
            return True

    return False


def safe_compare(
    value: Optional[Union[int, float]],
    threshold: Optional[Union[int, float]],
    operation: str,
) -> bool:
    """
    Safely compares a potentially None value against a potentially None threshold.
    Returns False if either value is None to prevent TypeErrors.

    :param value: The value to check (e.g., from API data).
    :param threshold: The threshold to compare against (e.g., from config).
    :param operation: The comparison to perform ('gt' for >, 'lt' for <).
    :return: Boolean result of the comparison, or False if unsafe.
    """
    if value is None or threshold is None:
        return False

    if operation == "gt":
        return value > threshold
    elif operation == "lt":
        return value < threshold

    return False


def parse_datetime_string(dt_str: Optional[str]) -> Optional[str]:
    """
    Parses a DataForSEO datetime string (e.g., "yyyy-mm-dd hh-mm-ss +00:00")
    into a consistent ISO format string or returns None.
    """
    if not dt_str:
        return None

    # Remove timezone offset for consistent parsing if it's always +00:00
    cleaned_dt_str = dt_str.replace(" +00:00", "").strip()

    formats = [
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%dT%H:%M:%S",  # Added ISO 8601 format
        "%Y-%m-%d %H:%M:%S.%f",  # With microseconds
        "%Y-%m-%d",  # Date only
    ]

    for fmt in formats:
        try:
            return datetime.strptime(cleaned_dt_str, fmt).isoformat()
        except ValueError:
            pass

    logging.getLogger(__name__).warning(
        f"Could not parse datetime string: {dt_str}. Returning None."
    )
    return None


def calculate_serp_times(
    datetime_str: Optional[str], previous_datetime_str: Optional[str]
) -> Dict[str, Optional[int]]:
    """
    Calculates the age of the SERP and the interval between the last two updates.
    """
    days_ago = None
    update_interval_days = None

    if datetime_str:
        parsed_date_iso = parse_datetime_string(datetime_str)
        if parsed_date_iso:
            serp_date = datetime.fromisoformat(parsed_date_iso)
            days_ago = (datetime.utcnow() - serp_date).days
        else:
            logging.getLogger(__name__).warning(
                f"Could not parse SERP datetime for days_ago: {datetime_str}"
            )

    if datetime_str and previous_datetime_str:
        parsed_last_update_iso = parse_datetime_string(datetime_str)
        parsed_prev_update_iso = parse_datetime_string(previous_datetime_str)

        if parsed_last_update_iso and parsed_prev_update_iso:
            last_update_dt = datetime.fromisoformat(parsed_last_update_iso)
            prev_update_dt = datetime.fromisoformat(parsed_prev_update_iso)
            update_interval_days = abs((last_update_dt - prev_update_dt).days)
        else:
            logging.getLogger(__name__).warning(
                f"Could not parse SERP previous update times for interval: {datetime_str}, {previous_datetime_str}"
            )

    return {"days_ago": days_ago, "update_interval_days": update_interval_days}
```

## File: data_access/migrations/001_add_new_tables.sql
```sql
-- data_access/migrations/001_add_new_tables.sql

CREATE TABLE keyword_info (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    search_volume INTEGER,
    keyword_difficulty INTEGER,
    cpc REAL,
    competition REAL,
    search_volume_trend TEXT,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);

CREATE TABLE serp_overview (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    serp_has_featured_snippet BOOLEAN,
    serp_has_video_results BOOLEAN,
    serp_has_ai_overview BOOLEAN,
    people_also_ask TEXT,
    ai_overview_content TEXT,
    featured_snippet_content TEXT,
    avg_referring_domains_top5_organic REAL,
    avg_main_domain_rank_top5_organic REAL,
    serp_last_updated_days_ago INTEGER,
    dominant_content_format TEXT,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);
```

## File: data_access/migrations/001_initial_schema.sql
```sql
-- data_access/migrations/001_initial_schema.sql
-- This script sets up the initial schema for existing tables.
-- It should only be run if the tables do not exist.
-- The database_manager already creates them, so this is mainly for tracking.

-- CREATE TABLE IF NOT EXISTS opportunities (
--     id INTEGER PRIMARY KEY AUTOINCREMENT,
--     keyword TEXT NOT NULL,
--     status TEXT NOT NULL DEFAULT 'pending',
--     client_id TEXT NOT NULL DEFAULT 'default',
--     date_added TEXT NOT NULL,
--     date_processed TEXT,
--     full_data TEXT NOT NULL,
--     blueprint_data TEXT,
--     ai_content_json TEXT,
--     ai_content_model TEXT,
--     featured_image_url TEXT,
--     featured_image_local_path TEXT,
--     in_article_images_data TEXT,
--     social_media_posts_json TEXT,
--     last_workflow_step TEXT,
--     error_message TEXT,
--     wordpress_payload_json TEXT,
--     final_package_json TEXT,
--     UNIQUE(keyword, client_id)
-- );

-- CREATE TABLE IF NOT EXISTS clients (
--     client_id TEXT PRIMARY KEY,
--     client_name TEXT NOT NULL,
--     date_created TEXT NOT NULL
-- );

-- CREATE TABLE IF NOT EXISTS client_settings (
--     client_id TEXT PRIMARY KEY,
--     openai_api_key TEXT,
--     pexels_api_key TEXT,
--     location_code INTEGER,
--     language_code TEXT,
--     target_domain TEXT,
--     device TEXT,
--     os TEXT,
--     informational_score REAL,
--     commercial_score REAL,
--     transactional_score REAL,
--     navigational_score REAL,
--     question_keyword_bonus REAL,
--     ease_of_ranking_weight INTEGER,
--     traffic_potential_weight INTEGER,
--     commercial_intent_weight INTEGER,
--     growth_trend_weight INTEGER,
--     serp_features_weight INTEGER,
--     serp_freshness_weight INTEGER,
--     serp_volatility_weight INTEGER,
--     competitor_weakness_weight INTEGER,
--     max_cpc_for_scoring REAL,
--     max_sv_for_scoring INTEGER,
--     max_domain_rank_for_scoring INTEGER,
--     max_referring_domains_for_scoring INTEGER,
--     max_avg_referring_domains_filter INTEGER,
--     featured_snippet_bonus REAL,
--     ai_overview_bonus REAL,
--     serp_freshness_bonus_max REAL,
--     serp_freshness_old_threshold_days INTEGER,
--     serp_volatility_stable_threshold_days INTEGER,
--     enforce_intent_filter INTEGER,
--     allowed_intents TEXT,
--     require_question_keywords INTEGER,
--     negative_keywords TEXT,
--     min_monthly_trend_percentage REAL,
--     min_competitor_word_count INTEGER,
--     max_competitor_technical_warnings INTEGER,
--     competitor_blacklist_domains TEXT,
--     ugc_and_parasite_domains TEXT,
--     num_competitors_to_analyze INTEGER,
--     num_common_headings INTEGER,
--     num_unique_angles INTEGER,
--     max_initial_serp_urls_to_analyze INTEGER,
--     calculate_rectangles INTEGER,
--     people_also_ask_click_depth INTEGER,
--     min_search_volume INTEGER,
--     max_keyword_difficulty INTEGER,
--     ai_content_model TEXT,
--     num_in_article_images INTEGER,
--     use_pexels_first INTEGER,
--     cleanup_local_images INTEGER,
--     onpage_enable_javascript INTEGER,
--     onpage_load_resources INTEGER,
--     onpage_disable_cookie_popup INTEGER,
--     onpage_return_despite_timeout INTEGER,
--     onpage_enable_browser_rendering INTEGER,
--     onpage_store_raw_html INTEGER,
--     onpage_validate_micromarkup INTEGER,
--     onpage_custom_user_agent TEXT,
--     onpage_max_domains_per_request INTEGER,
--     onpage_max_tasks_per_request INTEGER,
--     platforms TEXT,
--     custom_prompt_template TEXT,
--     wordpress_url TEXT,
--     wordpress_user TEXT,
--     wordpress_app_password TEXT,
--     wordpress_seo_plugin TEXT,
--     default_wordpress_categories TEXT,
--     default_wordpress_tags TEXT,
--     enable_automated_internal_linking INTEGER,
--     db_type TEXT,
--     max_words_for_ai_analysis INTEGER,
--     ai_generation_temperature REAL,
--     recommended_word_count_multiplier REAL,
--     max_avg_lcp_time INTEGER,
--     prohibited_intents TEXT,
--     last_updated TEXT NOT NULL,
--     FOREIGN KEY (client_id) REFERENCES clients (client_id)
-- );

-- CREATE TABLE IF NOT EXISTS discovery_runs (
--     id INTEGER PRIMARY KEY AUTOINCREMENT,
--     client_id TEXT NOT NULL,
--     start_time TEXT NOT NULL,
--     end_time TEXT,
--     status TEXT NOT NULL,
--     parameters TEXT,
--     results_summary TEXT,
--     log_file_path TEXT,
--     error_message TEXT,
--     FOREIGN KEY (client_id) REFERENCES clients (client_id)
-- );

-- These are initially created by `database_manager.py` before migrations start.
-- This file exists for migration tracking purposes.
```

## File: data_access/migrations/002_add_keywords_table.sql
```sql
-- data_access/migrations/011_add_keywords_table.sql

CREATE TABLE keywords (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    keyword TEXT NOT NULL UNIQUE,
    search_volume INTEGER,
    keyword_difficulty INTEGER,
    cpc REAL,
    competition REAL,
    search_volume_trend TEXT,
    main_intent TEXT,
    core_keyword TEXT
);

ALTER TABLE opportunities ADD COLUMN keyword_id INTEGER REFERENCES keywords(id);
CREATE INDEX IF NOT EXISTS idx_opportunities_keyword_id ON opportunities (keyword_id);
```

## File: data_access/migrations/002_remove_json_columns.sql
```sql
-- data_access/migrations/002_remove_json_columns.sql

-- This migration is intentionally left blank.
-- SQLite does not support dropping columns.
-- The data from the JSON columns will be migrated to the new tables in the application logic.
-- The old columns will be ignored by the application.
```

## File: data_access/migrations/003_add_indexes.sql
```sql
-- data_access/migrations/003_add_indexes.sql

CREATE INDEX IF NOT EXISTS idx_opportunities_status ON opportunities (status);
CREATE INDEX IF NOT EXISTS idx_opportunities_client_id ON opportunities (client_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_strategic_score ON opportunities (strategic_score);
CREATE INDEX IF NOT EXISTS idx_opportunities_run_id ON opportunities (run_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_keyword_id ON opportunities (keyword_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_slug ON opportunities (slug);
```

## File: data_access/migrations/003_add_total_api_cost.sql
```sql
-- Add a column to store the total API cost for the entire workflow
ALTER TABLE opportunities ADD COLUMN total_api_cost REAL DEFAULT 0.0;
```

## File: data_access/migrations/004_add_qualification_settings_table.sql
```sql
-- data_access/migrations/004_add_qualification_settings_table.sql

CREATE TABLE qualification_settings (
    client_id TEXT PRIMARY KEY,
    ease_of_ranking_weight REAL,
    traffic_potential_weight REAL,
    commercial_intent_weight REAL,
    serp_features_weight REAL,
    growth_trend_weight REAL,
    serp_freshness_weight REAL,
    serp_volatility_weight REAL,
    competitor_weakness_weight REAL,
    competitor_performance_weight REAL,
    informational_intent_weight REAL,
    navigational_intent_weight REAL,
    transactional_intent_weight REAL,
    competitor_strength_weight REAL,
    trend_weight REAL,
    seasonality_weight REAL,
    review_threshold REAL,
    disqualification_rules TEXT,
    brand_keywords TEXT,
    competitor_brand_keywords TEXT,
    min_search_volume INTEGER,
    max_keyword_difficulty INTEGER,
    negative_keywords TEXT,
    prohibited_intents TEXT,
    max_y_pixel_threshold INTEGER,
    max_forum_results_in_top_10 INTEGER,
    max_ecommerce_results_in_top_10 INTEGER,
    disallowed_page_types_in_top_3 TEXT,
    FOREIGN KEY (client_id) REFERENCES clients (client_id)
);
```

## File: data_access/migrations/005_add_qualification_columns.sql
```sql
-- data_access/migrations/005_add_qualification_columns.sql

ALTER TABLE opportunities ADD COLUMN strategic_score REAL;
ALTER TABLE opportunities ADD COLUMN score_breakdown TEXT;
ALTER TABLE opportunities ADD COLUMN qualification_status TEXT;
ALTER TABLE opportunities ADD COLUMN qualification_reason TEXT;
```

## File: data_access/migrations/006_add_intent_weights.sql
```sql
-- data_access/migrations/006_add_intent_weights.sql

ALTER TABLE qualification_settings ADD COLUMN informational_intent_weight REAL;
ALTER TABLE qualification_settings ADD COLUMN navigational_intent_weight REAL;
ALTER TABLE qualification_settings ADD COLUMN commercial_intent_weight REAL;
ALTER TABLE qualification_settings ADD COLUMN transactional_intent_weight REAL;
```

## File: data_access/migrations/007_add_competitor_strength_weight.sql
```sql
-- data_access/migrations/007_add_competitor_strength_weight.sql

ALTER TABLE qualification_settings ADD COLUMN competitor_strength_weight REAL;
```

## File: data_access/migrations/008_add_serp_features_weight.sql
```sql
-- data_access/migrations/008_add_serp_features_weight.sql

ALTER TABLE qualification_settings ADD COLUMN serp_features_weight REAL;
```

## File: data_access/migrations/009_add_trend_weight.sql
```sql
-- data_access/migrations/009_add_trend_weight.sql

ALTER TABLE qualification_settings ADD COLUMN trend_weight REAL;
```

## File: data_access/migrations/010_add_history_columns.sql
```sql
-- data_access/migrations/010_add_history_columns.sql

ALTER TABLE opportunities ADD COLUMN last_seen_at TEXT;
ALTER TABLE opportunities ADD COLUMN metrics_history TEXT;
```

## File: data_access/migrations/012_add_seasonality_weight.sql
```sql
-- data_access/migrations/012_add_seasonality_weight.sql

ALTER TABLE qualification_settings ADD COLUMN seasonality_weight REAL;
```

## File: data_access/migrations/013_add_serp_volatility_weight.sql
```sql
-- data_access/migrations/013_add_serp_volatility_weight.sql

ALTER TABLE qualification_settings ADD COLUMN serp_volatility_weight REAL;
```

## File: data_access/migrations/014_add_disqualification_rules.sql
```sql
-- data_access/migrations/014_add_disqualification_rules.sql

ALTER TABLE qualification_settings ADD COLUMN disqualification_rules TEXT;
```

## File: data_access/migrations/015_add_brand_keywords.sql
```sql
-- data_access/migrations/015_add_brand_keywords.sql

ALTER TABLE qualification_settings ADD COLUMN brand_keywords TEXT;
ALTER TABLE qualification_settings ADD COLUMN competitor_brand_keywords TEXT;
```

## File: data_access/migrations/016_add_review_threshold.sql
```sql
-- data_access/migrations/016_add_review_threshold.sql

ALTER TABLE qualification_settings ADD COLUMN review_threshold REAL;
```

## File: data_access/migrations/017_add_strategies_table.sql
```sql
-- data_access/migrations/017_add_strategies_table.sql

CREATE TABLE qualification_strategies (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    client_id TEXT NOT NULL,
    name TEXT NOT NULL,
    traffic_potential_weight REAL,
    keyword_difficulty_weight REAL,
    cpc_weight REAL,
    search_intent_weight REAL,
    min_search_volume INTEGER,
    max_keyword_difficulty INTEGER,
    negative_keywords TEXT,
    prohibited_intents TEXT,
    informational_intent_weight REAL,
    navigational_intent_weight REAL,
    commercial_intent_weight REAL,
    transactional_intent_weight REAL,
    competitor_strength_weight REAL,
    serp_features_weight REAL,
    trend_weight REAL,
    seasonality_weight REAL,
    serp_volatility_weight REAL,
    review_threshold REAL,
    disqualification_rules TEXT,
    brand_keywords TEXT,
    competitor_brand_keywords TEXT,
    FOREIGN KEY (client_id) REFERENCES clients (client_id)
);
```

## File: data_access/migrations/018_add_job_id_and_cluster_name.sql
```sql
-- data_access/migrations/018_add_job_id_and_cluster_name.sql

ALTER TABLE opportunities ADD COLUMN latest_job_id TEXT;
ALTER TABLE opportunities ADD COLUMN cluster_name TEXT;
```

## File: data_access/migrations/019_add_core_keyword_and_competitor_metrics_to_opportunities.sql
```sql
-- data_access/migrations/019_add_core_keyword_and_competitor_metrics_to_opportunities.sql

-- Add new columns for direct access to frequently used keyword metrics
ALTER TABLE opportunities ADD COLUMN cpc REAL DEFAULT 0.0;
ALTER TABLE opportunities ADD COLUMN competition REAL DEFAULT 0.0;
ALTER TABLE opportunities ADD COLUMN main_intent TEXT DEFAULT 'informational';
ALTER TABLE opportunities ADD COLUMN search_volume_trend_json TEXT;

-- Add new columns for storing aggregated competitor data for easier access
ALTER TABLE opportunities ADD COLUMN competitor_social_media_tags_json TEXT;
ALTER TABLE opportunities ADD COLUMN competitor_page_timing_json TEXT;

-- Create indexes on these new columns for improved query performance
CREATE INDEX idx_opportunities_cpc ON opportunities (cpc);
CREATE INDEX idx_opportunities_competition ON opportunities (competition);
CREATE INDEX idx_opportunities_main_intent ON opportunities (main_intent);
```

## File: data_access/migrations/020_backfill_core_keyword_metrics.sql
```sql
-- data_access/migrations/020_backfill_core_keyword_metrics.sql

-- Backfill cpc, competition, main_intent, and search_volume_trend_json from existing keyword_info and search_intent_info JSON blobs
UPDATE opportunities
SET
    cpc = CAST(JSON_EXTRACT(keyword_info, '$.cpc') AS REAL),
    competition = CAST(JSON_EXTRACT(keyword_info, '$.competition') AS REAL),
    main_intent = JSON_EXTRACT(search_intent_info, '$.main_intent'),
    search_volume_trend_json = JSON_EXTRACT(keyword_info, '$.search_volume_trend')
WHERE
    keyword_info IS NOT NULL AND search_intent_info IS NOT NULL;

-- Backfill aggregated competitor social media tags and page timing from blueprint_data
-- This requires iterating through the competitor_analysis array within blueprint_data
-- Note: SQLite's JSON functions can be limited for complex array aggregation directly in SQL.
-- This might require application-level backfill for more complex aggregations if `blueprint_data` is large.
-- For a simple direct copy of the *first* competitor's data (as an example), or an empty JSON if none:
UPDATE opportunities
SET
    competitor_social_media_tags_json = (
        SELECT CASE
            WHEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].social_media_tags') IS NOT NULL
            THEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].social_media_tags')
            ELSE '{}'
        END
    ),
    competitor_page_timing_json = (
        SELECT CASE
            WHEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].page_timing') IS NOT NULL
            THEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].page_timing')
            ELSE '{}'
        END
    )
WHERE
    blueprint_data IS NOT NULL;
```

## File: data_access/migrations/021_add_unique_keyword_constraint.sql
```sql
CREATE UNIQUE INDEX IF NOT EXISTS idx_unique_client_keyword ON opportunities (client_id, keyword);
```

## File: data_access/migrations/023_add_run_id_to_opportunities.sql
```sql
ALTER TABLE opportunities ADD COLUMN run_id INTEGER;
CREATE INDEX IF NOT EXISTS idx_opportunities_run_id ON opportunities (run_id);
```

## File: data_access/migrations/024_add_total_api_cost_to_opportunities.sql
```sql
ALTER TABLE opportunities ADD COLUMN total_api_cost REAL DEFAULT 0.0;
```

## File: data_access/migrations/025_add_cost_to_discovery_runs.sql
```sql
-- Add a column to store the total API cost for a discovery run
ALTER TABLE discovery_runs ADD COLUMN total_api_cost REAL DEFAULT 0.0;
```

## File: data_access/migrations/026_add_client_id_to_jobs.sql
```sql
-- Add client_id to the jobs table to associate jobs with clients
ALTER TABLE jobs ADD COLUMN client_id TEXT;

-- Add function_name to the jobs table for better UI descriptions
ALTER TABLE jobs ADD COLUMN function_name TEXT;

-- Create an index for efficient querying of active jobs by client
CREATE INDEX IF NOT EXISTS idx_jobs_client_id_status ON jobs (client_id, status);
```

## File: data_access/migrations/027_promote_json_fields.sql
```sql
-- Add top-level columns to the opportunities table for frequently accessed data
ALTER TABLE opportunities ADD COLUMN search_volume INTEGER;
ALTER TABLE opportunities ADD COLUMN keyword_difficulty INTEGER;

-- Backfill the new columns with data from the existing JSON blobs
UPDATE opportunities
SET
    search_volume = CAST(JSON_EXTRACT(full_data, '$.keyword_info.search_volume') AS INTEGER),
    keyword_difficulty = CAST(JSON_EXTRACT(full_data, '$.keyword_properties.keyword_difficulty') AS INTEGER)
WHERE full_data IS NOT NULL AND (search_volume IS NULL OR keyword_difficulty IS NULL);
```

## File: data_access/__init__.py
```python
# data_access/__init__.py
# This file marks the directory as a Python package.
```

## File: data_access/database_manager.py
```python
import sqlite3
import json
import threading
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
import logging
import bleach  # ADD THIS LINE
import os
from . import queries
from backend.app_config.manager import ConfigManager

ALLOWED_ATTRIBUTES_DB = {
    "*": ["id", "class"],
    "a": ["href", "title"],
    "img": ["src", "alt", "width", "height"],
}

# W20 FIX: Define sanitization constants globally or as module constants (based on router update logic)
ALLOWED_TAGS = list(bleach.sanitizer.ALLOWED_TAGS) + [
    "h1",
    "h2",
    "h3",
    "h4",
    "h5",
    "h6",
    "p",
    "br",
    "a",
    "i",
    "u",
    "em",
    "strong",
    "blockquote",
    "li",
    "ul",
    "ol",
    "img",
    "div",
    "span",
    "table",
    "thead",
    "tbody",
    "tr",
    "td",
    "th",
    "code",
    "pre",
]
ALLOWED_ATTRIBUTES = bleach.sanitizer.ALLOWED_ATTRIBUTES.copy()
ALLOWED_ATTRIBUTES.update(
    {
        "a": ["href", "title"],
        "img": ["src", "alt", "width", "height", "style"],
        "*": ["id", "class", "style"],
    }
)

DB_FILE = "data/opportunities.db"


class DatabaseManager:
    """Handles all interactions with the SQLite opportunity queue database."""

    def __init__(
        self, cfg_manager: Optional[ConfigManager] = None, db_path: Optional[str] = None
    ):  # db_path is now passed via cfg_manager
        self.cfg_manager = cfg_manager

        # Define project root relative to this file's location
        project_root = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "..")
        )

        if db_path:
            self.db_path = db_path
            self.db_type = "sqlite"
        elif cfg_manager:
            global_cfg = cfg_manager.get_global_config()
            db_file_name = global_cfg.get("db_file_name", DB_FILE)
            self.db_path = os.path.join(
                project_root, db_file_name
            )  # Ensure path is relative to project root
            self.db_type = global_cfg.get("db_type", "sqlite")
        else:
            db_file_name = DB_FILE
            self.db_path = os.path.join(
                project_root, db_file_name
            )  # Ensure path is relative to project root
            self.db_type = "sqlite"

        self.logger = logging.getLogger(self.__class__.__name__)
        self._thread_local = threading.local()

    def initialize(self):
        """Connects to the DB, creates tables, applies migrations, and ensures default client exists."""
        conn = self._get_conn()
        try:
            # Combine all CREATE TABLE statements into a single script for atomic execution
            full_schema_script = f"""
                {queries.CREATE_OPPORTUNITIES_TABLE}
                {queries.CREATE_CLIENTS_TABLE}
                {queries.CREATE_CLIENT_SETTINGS_TABLE}
                {queries.CREATE_DISCOVERY_RUNS_TABLE}
                {queries.CREATE_SCHEMA_VERSION_TABLE}
                {queries.CREATE_JOBS_TABLE}
                {queries.CREATE_CONTENT_HISTORY_TABLE}
                {queries.CREATE_CONTENT_FEEDBACK_TABLE}
                {queries.CREATE_API_CACHE_TABLE}
            """
            with conn:  # Ensure transaction for initial setup
                conn.executescript(full_schema_script)

            self._apply_migrations_from_files()
            self._ensure_default_client_exists(conn)  # Add this line
            self.logger.info("Database initialized.")
        finally:
            self._close_conn()

    def _get_conn(self):
        """Gets a connection from the thread-local storage or creates a new one."""
        if not hasattr(self._thread_local, "conn") or self._thread_local.conn is None:
            if self.db_type == "sqlite":
                self._thread_local.conn = sqlite3.connect(
                    self.db_path, check_same_thread=False
                )
                self._thread_local.conn.row_factory = sqlite3.Row
            else:
                raise NotImplementedError(
                    f"External database type '{self.db_type}' is not yet implemented."
                )
        return self._thread_local.conn

    def _close_conn(self):
        """Closes the connection for the current thread."""
        if hasattr(self._thread_local, "conn") and self._thread_local.conn is not None:
            self._thread_local.conn.close()
            self._thread_local.conn = None

    def _ensure_default_client_exists(self, conn):
        """Checks for and creates the default client if it doesn't exist in the database."""
        if not self.cfg_manager:
            return

        global_cfg = self.cfg_manager.get_global_config()
        default_id = global_cfg.get("default_client_id")

        if not default_id:
            self.logger.warning("No default_client_id found in configuration.")
            return

        conn = self._get_conn()
        cursor = conn.execute(
            "SELECT 1 FROM clients WHERE client_id = ?", (default_id,)
        )
        if cursor.fetchone() is None:
            self.logger.warning(
                f"Default client '{default_id}' not found in database. Creating it now."
            )
            default_settings_template = (
                self.cfg_manager.get_default_client_settings_template()
            )
            self.add_client(default_id, default_id, default_settings_template)

    def _get_current_schema_version(self, conn) -> int:
        """Retrieves the current schema version from the database."""
        cursor = conn.execute(queries.GET_SCHEMA_VERSION)
        result = cursor.fetchone()
        return result["version"] if result else 0

    def _apply_migrations_from_files(self):
        """Applies SQL migration scripts from the migrations directory."""
        conn = self._get_conn()
        try:
            current_version = self._get_current_schema_version(conn)
            migrations_dir = os.path.join(os.path.dirname(__file__), "migrations")

            if not os.path.exists(migrations_dir):
                self.logger.warning(
                    f"Migrations directory not found: {migrations_dir}. Skipping migrations."
                )
                return

            migration_files = sorted(
                [f for f in os.listdir(migrations_dir) if f.endswith(".sql")]
            )

            for filename in migration_files:
                version_str = filename.split("_")[0]
                if not version_str.isdigit():
                    self.logger.warning(
                        f"Skipping malformed migration file: {filename}"
                    )
                    continue

                version = int(version_str)

                if version > current_version:
                    self.logger.info(f"Applying migration {filename}...")
                    filepath = os.path.join(migrations_dir, filename)
                    with open(filepath, "r") as f:
                        sql_script = f.read()

                    print(f"Executing migration script: {filename}")
                    print(sql_script)

                    try:
                        with conn:  # Execute in a transaction
                            conn.executescript(sql_script)
                            conn.execute(
                                queries.INSERT_SCHEMA_VERSION,
                                (version, datetime.now().isoformat()),
                            )
                        self.logger.info(f"Migration {filename} applied successfully.")
                        current_version = version
                    except sqlite3.OperationalError as e:
                        if "duplicate column name" in str(e):
                            self.logger.warning(
                                f"Migration {filename} failed because a column already exists. Assuming it was already applied and continuing."
                            )
                            conn.execute(
                                queries.INSERT_SCHEMA_VERSION,
                                (version, datetime.now().isoformat()),
                            )
                            current_version = version
                        else:
                            raise e
                else:
                    self.logger.debug(f"Migration {filename} already applied.")
            self.logger.info("Database migration check complete.")
        except Exception as e:
            self.logger.error(f"Error during database migration: {e}", exc_info=True)
            raise
        finally:
            self._close_conn()  # Ensure connection is closed after migrations

    def _deserialize_rows(self, rows: List[sqlite3.Row]) -> List[Dict[str, Any]]:
        """Deserializes JSON strings from database rows into a clean dictionary, prioritizing top-level columns."""
        results = []

        json_keys = [
            "blueprint_data", "ai_content_json", "in_article_images_data",
            "social_media_posts_json", "final_package_json", "wordpress_payload_json",
            "score_breakdown", "full_data", "search_volume_trend_json",
            "competitor_social_media_tags_json", "competitor_page_timing_json",
            "keyword_info", "keyword_properties", "search_intent_info", "serp_overview",
            "metrics_history", "related_keywords", "keyword_categories"
        ]

        for row in rows:
            final_item = dict(row)

            for key in json_keys:
                if key in final_item and isinstance(final_item[key], str):
                    try:
                        final_item[key] = json.loads(final_item[key])
                    except json.JSONDecodeError:
                        final_item[key] = None
            
            self.logger.info(f"Before backfill: {final_item}")
            self.logger.info(f"Before backfill: {final_item}")
            # Backward compatibility: If top-level fields are null, pull from full_data
            full_data = final_item.get('full_data') or {}
            if isinstance(full_data, str):
                try:
                    full_data = json.loads(full_data)
                except json.JSONDecodeError:
                    full_data = {}

            if final_item.get('search_volume') is None:
                final_item['search_volume'] = (full_data.get('keyword_info') or {}).get('search_volume')
            
            if final_item.get('keyword_difficulty') is None:
                final_item['keyword_difficulty'] = (full_data.get('keyword_properties') or {}).get('keyword_difficulty')
            self.logger.info(f"After backfill: {final_item}")
            self.logger.info(f"After backfill: {final_item}")

            # Reconstruct nested objects for any part of the app that might still use them
            final_item['keyword_info'] = final_item.get('keyword_info') or {}
            final_item['keyword_properties'] = final_item.get('keyword_properties') or {}
            final_item['search_intent_info'] = final_item.get('search_intent_info') or {}
            
            final_item['keyword_info']['search_volume'] = final_item.get('search_volume')
            final_item['keyword_info']['cpc'] = final_item.get('cpc')
            final_item['keyword_info']['competition'] = final_item.get('competition')
            
            final_item['keyword_properties']['keyword_difficulty'] = final_item.get('keyword_difficulty')
            final_item['search_intent_info']['main_intent'] = final_item.get('main_intent')

            if "blueprint_data" in final_item:
                final_item["blueprint"] = final_item.pop("blueprint_data")
            if "ai_content_json" in final_item:
                final_item["ai_content"] = final_item.pop("ai_content_json")

            results.append(final_item)
        return results

    def add_opportunity(self, client_id: str, opportunity_data: Dict[str, Any]):
        """Adds a new opportunity to the database from a dictionary."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO opportunities (
                    keyword, client_id, status, date_added, date_processed, 
                    strategic_score, keyword_info, keyword_properties, 
                    search_intent_info, serp_overview, score_breakdown, ai_content_json
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    opportunity_data.get("keyword"),
                    client_id,
                    opportunity_data.get("status", "pending"),
                    opportunity_data.get("date_added", datetime.now().isoformat()),
                    opportunity_data.get("date_processed"),
                    opportunity_data.get("strategic_score"),
                    json.dumps(opportunity_data.get("keyword_info")),
                    json.dumps(opportunity_data.get("keyword_properties")),
                    json.dumps(opportunity_data.get("search_intent_info")),
                    json.dumps(opportunity_data.get("serp_overview")),
                    json.dumps(opportunity_data.get("score_breakdown")),
                    json.dumps(opportunity_data.get("ai_content")),
                ),
            )
            return cursor.lastrowid

    def add_opportunities(
        self, opportunities: List[Dict[str, Any]], client_id: str, run_id: int
    ) -> int:
        """Adds multiple opportunities to the database in a single transaction, updating existing ones."""
        conn = self._get_conn()
        num_added = 0
        with conn:
            cursor = conn.cursor()
            for opp in opportunities:
                keyword = opp.get("keyword")
                cursor.execute("SELECT id FROM keywords WHERE keyword = ?", (keyword,))
                keyword_row = cursor.fetchone()

                keyword_info = opp.get("keyword_info", {})
                keyword_properties = opp.get("keyword_properties", {})
                search_intent_info = opp.get("search_intent_info", {})

                if keyword_row:
                    keyword_id = keyword_row["id"]
                    # Update existing keyword
                    cursor.execute(
                        """
                        UPDATE keywords
                        SET search_volume = ?, keyword_difficulty = ?, cpc = ?, competition = ?, search_volume_trend = ?, main_intent = ?, core_keyword = ?
                        WHERE id = ?
                    """,
                        (
                            keyword_info.get("search_volume"),
                            keyword_properties.get("keyword_difficulty"),
                            keyword_info.get("cpc"),
                            keyword_info.get("competition"),
                            json.dumps(keyword_info.get("search_volume_trend")),
                            search_intent_info.get("main_intent"),
                            keyword_properties.get("core_keyword"),
                            keyword_id,
                        ),
                    )
                else:
                    # Insert new keyword
                    cursor.execute(
                        """
                        INSERT INTO keywords (keyword, search_volume, keyword_difficulty, cpc, competition, search_volume_trend, main_intent, core_keyword)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            keyword,
                            keyword_info.get("search_volume"),
                            keyword_properties.get("keyword_difficulty"),
                            keyword_info.get("cpc"),
                            keyword_info.get("competition"),
                            json.dumps(keyword_info.get("search_volume_trend")),
                            search_intent_info.get("main_intent"),
                            keyword_properties.get("core_keyword"),
                        ),
                    )
                    keyword_id = cursor.lastrowid

                cursor.execute(
                    "SELECT id, metrics_history FROM opportunities WHERE client_id = ? AND keyword_id = ?",
                    (client_id, keyword_id),
                )
                opportunity_row = cursor.fetchone()

                if opportunity_row:
                    # Update existing opportunity
                    history = (
                        json.loads(opportunity_row["metrics_history"])
                        if opportunity_row["metrics_history"]
                        else []
                    )
                    history.append(
                        {
                            "date": datetime.now().isoformat(),
                            "search_volume": keyword_info.get("search_volume"),
                            "keyword_difficulty": keyword_properties.get(
                                "keyword_difficulty"
                            ),
                            "cpc": keyword_info.get("cpc"),
                        }
                    )
                    cursor.execute(
                        """
                        UPDATE opportunities
                        SET last_seen_at = ?, metrics_history = ?, search_volume = ?, keyword_difficulty = ?
                        WHERE id = ?
                    """,
                        (
                            datetime.now().isoformat(),
                            json.dumps(history),
                            keyword_info.get("search_volume"),
                            keyword_properties.get("keyword_difficulty"),
                            opportunity_row["id"],
                        ),
                    )
                else:
                    # Insert new opportunity
                    num_added += 1
                    # Extract values for direct columns, potentially nulling them out from JSON if no longer needed there
                    cpc_val = keyword_info.get("cpc")
                    competition_val = keyword_info.get("competition")
                    main_intent_val = search_intent_info.get("main_intent")
                    search_volume_trend_json_val = json.dumps(
                        keyword_info.get("search_volume_trend")
                    )

                    # Aggregate top competitor data for direct columns
                    top_competitor = next(
                        (
                            comp
                            for comp in opp.get("blueprint", {}).get(
                                "competitor_analysis", []
                            )
                            if comp.get("url")
                        ),
                        None,
                    )
                    competitor_social_media_tags_json_val = (
                        json.dumps(top_competitor.get("social_media_tags", {}))
                        if top_competitor
                        else None
                    )
                    competitor_page_timing_json_val = (
                        json.dumps(top_competitor.get("page_timing", {}))
                        if top_competitor
                        else None
                    )

                    # Create a copy of the opportunity data to avoid modifying the original object in memory
                    full_data_copy = opp.copy()
                    # Remove keys that are now stored in dedicated top-level columns to prevent data duplication.
                    # This only applies to new records being inserted.
                    full_data_copy.pop("keyword_info", None)
                    full_data_copy.pop("keyword_properties", None)
                    full_data_copy.pop("search_intent_info", None)
                    
                    cursor.execute(
                        """
                        INSERT INTO opportunities (
                            keyword, client_id, run_id, status, date_added, date_processed,
                            strategic_score, blog_qualification_status, blog_qualification_reason,
                            keyword_info, keyword_properties,
                            search_intent_info, serp_overview, score_breakdown, ai_content_json,
                            keyword_info_normalized_with_bing, keyword_info_normalized_with_clickstream, monthly_searches, traffic_value,
                            check_url, related_keywords, keyword_categories, core_keyword, last_seen_at, metrics_history, keyword_id,
                            full_data,
                            cpc, competition, main_intent, search_volume_trend_json,
                            competitor_social_media_tags_json, competitor_page_timing_json,
                        social_media_posts_status, search_volume, keyword_difficulty
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            keyword,
                            client_id,
                            run_id,
                            opp.get("status", "pending"),
                            opp.get("date_added", datetime.now().isoformat()),
                            opp.get("date_processed"),
                            opp.get("strategic_score"),
                            opp.get("blog_qualification_status"),
                            opp.get("blog_qualification_reason"),
                            json.dumps(keyword_info),
                            json.dumps(keyword_properties),
                            json.dumps(search_intent_info),
                            json.dumps(opp.get("serp_overview")),
                            json.dumps(opp.get("score_breakdown")),
                            json.dumps(opp.get("ai_content")),
                            json.dumps(opp.get("keyword_info_normalized_with_bing")),
                            json.dumps(
                                opp.get("keyword_info_normalized_with_clickstream")
                            ),
                            json.dumps(keyword_info.get("monthly_searches")),
                            opp.get("traffic_value", 0),
                            opp.get("serp_info", {}).get("check_url"),
                            json.dumps(opp.get("related_keywords")),
                            json.dumps(keyword_info.get("categories")),
                            keyword_properties.get("core_keyword"),
                            datetime.now().isoformat(),
                            json.dumps([]),
                            keyword_id,
                            json.dumps(full_data_copy), # Use the modified copy for full_data
                            cpc_val,
                            competition_val,
                            main_intent_val,
                            search_volume_trend_json_val,
                            competitor_social_media_tags_json_val,
                            competitor_page_timing_json_val,
                            opp.get("social_media_posts_status", "draft"),
                            keyword_info.get("search_volume"),
                            keyword_properties.get("keyword_difficulty"),
                        ),
                    )

        return num_added

    def get_opportunity_queue(self, client_id: str = "default") -> List[Dict[str, Any]]:
        """Retrieves all pending opportunities for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_PENDING_OPPORTUNITIES, (client_id,))
            return self._deserialize_rows(cursor.fetchall())

    def get_all_opportunities(
        self,
        client_id: str,
        params: Dict[str, Any],
        summary: bool = False,
        select_columns: str = None,
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        Retrieves keyword opportunities for a client, supporting filtering, sorting, and pagination.
        If summary is True, only essential fields for the table view are returned.
        """
        conn = self._get_conn()
        limit = int(params.get("limit", 20))
        page = int(params.get("page", 1))
        offset = (page - 1) * limit

        sort_by_map = {
            "strategic_score": "strategic_score",
            "date_added": "date_added",
            "keyword": "keyword",
            "status": "status",
            "search_volume": "JSON_EXTRACT(full_data, '$.keyword_info.search_volume')",
            "keyword_difficulty": "JSON_EXTRACT(full_data, '$.keyword_properties.keyword_difficulty')",
            "cpc": "JSON_EXTRACT(full_data, '$.keyword_info.cpc')",
        }
        sort_by = sort_by_map.get(params.get("sort_by"), "date_added")
        sort_direction = "ASC" if params.get("sort_direction") == "asc" else "DESC"

        where_parts = ["client_id = ?"]
        query_values = [client_id]

        status_filter = params.get("status")
        if status_filter:
            statuses = [s.strip() for s in status_filter.split(",")]
            placeholders = ",".join(["?"] * len(statuses))
            where_parts.append(f"status IN ({placeholders})")
            query_values.extend(statuses)

        where_clause = " AND ".join(where_parts)

        count_query = f"SELECT COUNT(*) FROM opportunities WHERE {where_clause}"
        with conn:
            cursor = conn.cursor()
            cursor.execute(count_query, query_values)
            total_count = cursor.fetchone()[0]

        select_columns = (
            select_columns
            if select_columns
            else "id, keyword, status, date_added, strategic_score, search_volume, keyword_difficulty, cpc, competition, main_intent, search_volume_trend_json, competitor_social_media_tags_json, competitor_page_timing_json, blog_qualification_status, latest_job_id, cluster_name, score_breakdown, full_data"
        )
        if summary and "full_data" not in select_columns:
            select_columns += ", full_data"

        final_query = f"SELECT {select_columns} FROM opportunities WHERE {where_clause} ORDER BY {sort_by} {sort_direction} LIMIT ? OFFSET ?"

        paged_values = query_values + [limit, offset]
        with conn:
            cursor = conn.cursor()
            cursor.execute(final_query, paged_values)
            opportunities = self._deserialize_rows(cursor.fetchall())

        return opportunities, total_count

    def get_opportunity_by_id(self, opportunity_id: int) -> Optional[Dict[str, Any]]:
        """Retrieves a single opportunity by its primary key ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_OPPORTUNITY_BY_ID, (opportunity_id,))
            row = cursor.fetchone()
            if row:
                return self._deserialize_rows([row])[0]
        return None

    def get_opportunity_summary_by_id(
        self, opportunity_id: int
    ) -> Optional[Dict[str, Any]]:
        """
        Retrieves only essential summary fields for an opportunity. (W13 FIX)
        """
        conn = self._get_conn()
        with conn:
            # Use a selective query to avoid fetching large JSON blobs
            cursor = conn.execute(
                "SELECT id, keyword, status, date_added, strategic_score, blog_qualification_status, featured_image_local_path FROM opportunities WHERE id = ?",
                (opportunity_id,),
            )
            row = cursor.fetchone()
            if row:
                # Need to manually or selectively deserialize, but for simplicity, we treat the row as the summary
                return dict(row)
            return None

    def get_opportunity_by_slug(self, slug: str) -> Optional[Dict[str, Any]]:
        """Retrieves a single opportunity by its URL slug."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_OPPORTUNITY_BY_SLUG, (slug,))
            row = cursor.fetchone()
            if row:
                return self._deserialize_rows([row])[0]
        return None

    def search_opportunities(self, client_id: str, query: str) -> List[Dict[str, Any]]:
        """Searches for opportunities by keyword for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            search_term = f"%{query}%"
            cursor.execute(
                queries.SEARCH_OPPORTUNITIES_BY_KEYWORD, (client_id, search_term)
            )
            # No need for full deserialization as we are fetching simple columns
            return [dict(row) for row in cursor.fetchall()]

    def get_published_articles_for_linking(
        self, client_id: str
    ) -> List[Dict[str, str]]:
        """
        Retrieves a list of published articles (title and slug) for internal linking suggestions.
        """
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            # Select only 'keyword' (as title) and 'slug' for published articles
            # 'full_data' is not needed here to save memory/processing
            cursor.execute(
                """
                SELECT keyword, slug FROM opportunities
                WHERE client_id = ? AND status IN ('generated', 'published') AND slug IS NOT NULL;
            """,
                (client_id,),
            )

            articles = []
            for row in cursor.fetchall():
                articles.append(
                    {
                        "title": row["keyword"],  # Use keyword as a proxy for title
                        "url": f"/article/{row['slug']}",  # Construct the relative URL
                    }
                )
            return articles

    def get_all_processed_keywords_for_client(self, client_id: str) -> List[str]:
        """Retrieves a flat list of all primary keywords for a client that are not in a 'rejected' or 'failed' state."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_ALL_PROCESSED_KEYWORDS, (client_id,))
            return [row["keyword"] for row in cursor.fetchall()]

    def check_existing_keywords(self, client_id: str, keywords: List[str]) -> List[str]:
        """Checks a list of keywords against the DB and returns those that exist."""
        if not keywords:
            return []
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            placeholders = ",".join("?" for _ in keywords)
            query = f"SELECT keyword FROM opportunities WHERE client_id = ? AND keyword IN ({placeholders})"
            cursor.execute(query, [client_id] + keywords)
            return [row["keyword"] for row in cursor.fetchall()]

    def update_opportunity_status(self, opportunity_id: int, new_status: str):
        """Updates a keyword's status in the database."""
        conn = self._get_conn()
        with conn:
            if new_status in ["generated", "analyzed", "failed"]:
                conn.execute(
                    queries.UPDATE_OPPORTUNITY_STATUS_WITH_DATE,
                    (new_status, datetime.now().isoformat(), opportunity_id),
                )
            else:
                conn.execute(
                    queries.UPDATE_OPPORTUNITY_STATUS, (new_status, opportunity_id)
                )

    def update_opportunity_workflow_state(
        self,
        opportunity_id: int,
        step: str,
        status: str = "in_progress",
        error_message: Optional[str] = None,
    ):
        """Updates the workflow step and status for a given opportunity.
        Possible statuses include: 'in_progress', 'completed', 'failed', 'pending', 'validated', 'analyzed', 'generated', 'published', 'rejected', 'paused_for_approval'."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Updating workflow state to step='{step}', status='{status}'"
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_WORKFLOW_STATE,
                (step, status, error_message, opportunity_id),
            )

    def update_opportunity_blueprint(
        self, opportunity_id: int, blueprint_data: Dict[str, Any], slug: str
    ):
        """Stores the generated blueprint data and the URL slug for a specific opportunity."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_BLUEPRINT_AND_SLUG,
                (json.dumps(blueprint_data), slug, opportunity_id),
            )

    def update_opportunity_ai_content(
        self, opportunity_id: int, ai_content_data: Dict[str, Any], ai_model: str
    ):
        """
        Stores the generated AI content package and model used for a specific opportunity.
        Applies server-side sanitization to the HTML body (W20 FIX).
        """
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving AI content generated by model '{ai_model}'."
        )

        # W20 FIX: Sanitize content before saving
        html_body = ai_content_data.get("article_body_html")
        if html_body:
            clean_html = bleach.clean(
                html_body, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES_DB
            )
            ai_content_data["article_body_html"] = clean_html

        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_AI_CONTENT,
                (
                    json.dumps(ai_content_data),
                    ai_model,
                    datetime.now().isoformat(),
                    opportunity_id,
                ),
            )

    def update_opportunity_images(
        self,
        opportunity_id: int,
        featured_image_url: Optional[str],
        featured_image_local_path: Optional[str],
        in_article_images_data: List[Dict[str, Any]],
    ):
        """Stores image generation details for a specific opportunity."""
        # W5 FIX: Ensure path is relative before storing, e.g., by checking if it starts with the expected API prefix
        if featured_image_local_path and os.path.isabs(featured_image_local_path):
            # Assuming the standard path is relative to the API's image mounting point,
            # extract the filename or the relative part required by the API
            # This is a simplified example; robust path management is needed.
            base_path = os.path.abspath(
                os.path.join(os.path.dirname(__file__), "..", "generated_images")
            )
            featured_image_local_path = os.path.relpath(
                featured_image_local_path, base_path
            )

        self.logger.info(
            f"Opportunity {opportunity_id}: Saving Pexels image data (featured URL: {featured_image_url})."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_IMAGES,
                (
                    featured_image_url,
                    featured_image_local_path,
                    json.dumps(in_article_images_data),
                    opportunity_id,
                ),
            )

    def update_opportunity_full_data(
        self, opportunity_id: int, full_data: Dict[str, Any]
    ):
        """Updates the full_data JSON blob for a specific opportunity."""
        self.logger.info(f"Opportunity {opportunity_id}: Updating full_data field.")
        conn = self._get_conn()
        with conn:
            conn.execute(
                "UPDATE opportunities SET full_data = ? WHERE id = ?",
                (json.dumps(full_data), opportunity_id),
            )

    def update_opportunity_scores(
        self,
        opportunity_id: int,
        strategic_score: float,
        score_breakdown: Dict[str, Any],
        blueprint_data: Optional[Dict[str, Any]] = None,
    ):
        """Updates the primary strategic score, breakdown, and blueprint for an opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Updating strategic_score to {strategic_score:.2f} and saving blueprint."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_SCORES,
                (
                    strategic_score,
                    json.dumps(score_breakdown),
                    json.dumps(blueprint_data) if blueprint_data else None,
                    opportunity_id,
                ),
            )

    def update_opportunity_final_package(
        self, opportunity_id: int, final_package: Dict[str, Any]
    ):
        """Stores the generated final content package JSON for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving final standalone content package."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_FINAL_PACKAGE,
                (json.dumps(final_package), opportunity_id),
            )

    def add_client(
        self, client_id: str, client_name: str, default_settings: Dict[str, Any]
    ) -> bool:
        """Adds a new client to the clients table and initializes their settings."""
        conn = self._get_conn()
        try:
            with conn:
                cursor = conn.execute("PRAGMA table_info(client_settings)")
                schema_columns = {row["name"] for row in cursor.fetchall()}

                conn.execute(
                    queries.INSERT_CLIENT,
                    (client_id, client_name, datetime.now().isoformat()),
                )

                settings_to_insert = {
                    k: v for k, v in default_settings.items() if k in schema_columns
                }
                if (
                    "client_knowledge_base" not in settings_to_insert
                    and "client_knowledge_base" in schema_columns
                ):
                    settings_to_insert["client_knowledge_base"] = (
                        ""  # Initialize with empty string
                    )
                settings_to_insert["client_id"] = client_id
                settings_to_insert["last_updated"] = datetime.now().isoformat()

                keys = ", ".join(settings_to_insert.keys())
                placeholders = ", ".join("?" * len(settings_to_insert))

                values = []
                for key in settings_to_insert.keys():
                    value = settings_to_insert[key]
                    if isinstance(value, list):
                        values.append(",".join(map(str, value)))
                    elif isinstance(value, bool):
                        values.append(1 if value else 0)
                    else:
                        values.append(value)

                insert_query = (
                    f"INSERT INTO client_settings ({keys}) VALUES ({placeholders})"
                )

                conn.execute(insert_query, tuple(values))
                self.logger.info(
                    f"Added new client '{client_name}' ({client_id}) and initialized settings."
                )

                # Also initialize qualification settings with a comprehensive set of default values
                conn.execute(
                    """
                    INSERT INTO qualification_settings (
                        client_id, ease_of_ranking_weight, traffic_potential_weight, commercial_intent_weight,
                        serp_features_weight, growth_trend_weight, serp_freshness_weight, serp_volatility_weight,
                        competitor_weakness_weight, competitor_performance_weight, min_search_volume, max_keyword_difficulty,
                        negative_keywords, prohibited_intents, max_y_pixel_threshold,
                        max_forum_results_in_top_10, max_ecommerce_results_in_top_10,
                        disallowed_page_types_in_top_3
                    ) VALUES (?, 40, 15, 5, 5, 5, 5, 5, 20, 5, 100, 80, 'login,free,cheap', 'navigational', 800, 3, 2, 'E-commerce,Forum')
                """,
                    (client_id,),
                )
                self.logger.info(
                    f"Initialized default qualification settings for client '{client_name}' ({client_id})."
                )
            return True
        except sqlite3.IntegrityError:
            self.logger.warning(f"Client with ID '{client_id}' already exists.")
            return False
        except Exception as e:
            self.logger.error(
                f"Error adding client '{client_name}': {e}", exc_info=True
            )
            return False

    def get_clients(self) -> List[Dict[str, str]]:
        """Retrieves all clients from the database."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_ALL_CLIENTS)
            return [dict(row) for row in cursor.fetchall()]

    def get_processed_opportunities(self, client_id: str) -> List[Dict[str, Any]]:
        """Retrieves all opportunities with a generated blueprint for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_PROCESSED_OPPORTUNITIES, (client_id,))
            return self._deserialize_rows(cursor.fetchall())

    def get_client_settings(self, client_id: str) -> Dict[str, Any]:
        """Retrieves settings for a specific client, converting from DB types."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_CLIENT_SETTINGS, (client_id,))
            row = cursor.fetchone()
            if row:
                settings = dict(row)
                settings.pop("client_id", None)
                settings.pop("last_updated", None)

                list_keys = [
                    "allowed_intents",
                    "negative_keywords",
                    "competitor_blacklist_domains",
                    "serp_feature_filters",
                    "serp_features_exclude_filter",
                    "platforms",
                    "default_wordpress_categories",
                    "default_wordpress_tags",
                    "ugc_and_parasite_domains",
                    "high_value_categories",
                    "hostile_serp_features",
                    "final_validation_non_blog_domains",
                    "prohibited_intents",  # NEW
                ]
                for key in list_keys:
                    if settings.get(key) is not None and isinstance(settings[key], str):
                        settings[key] = [
                            item.strip()
                            for item in settings[key].split(",")
                            if item.strip()
                        ]
                    elif settings.get(key) is None:
                        settings[key] = []

                bool_keys = [
                    "enforce_intent_filter",
                    "require_question_keywords",
                    "use_pexels_first",
                    "cleanup_local_images",
                    "onpage_enable_javascript",
                    "onpage_load_resources",
                    "calculate_rectangles",
                    "onpage_disable_cookie_popup",
                    "onpage_return_despite_timeout",
                    "onpage_enable_browser_rendering",
                    "onpage_store_raw_html",
                    "onpage_validate_micromarkup",
                    "discovery_replace_with_core_keyword",
                    "discovery_ignore_synonyms",
                    "enable_automated_internal_linking",  # NEW
                ]
                for key in bool_keys:
                    if settings.get(key) is not None:
                        settings[key] = bool(settings[key])

                int_keys = [
                    "num_in_article_images",
                    "location_code",
                    "serp_freshness_old_threshold_days",
                    "min_competitor_word_count",
                    "max_competitor_technical_warnings",
                    "num_competitors_to_analyze",
                    "num_common_headings",
                    "num_unique_angles",
                    "max_initial_serp_urls_to_analyze",
                    "min_search_volume",
                    "max_keyword_difficulty",
                    "people_also_ask_click_depth",
                    "onpage_max_domains_per_request",
                    "onpage_max_tasks_per_request",
                    "ease_of_ranking_weight",
                    "traffic_potential_weight",
                    "commercial_intent_weight",
                    "growth_trend_weight",
                    "serp_features_weight",
                    "serp_freshness_weight",
                    "serp_volatility_weight",
                    "competitor_weakness_weight",
                    "max_sv_for_scoring",
                    "max_domain_rank_for_scoring",
                    "max_referring_domains_for_scoring",
                    "serp_volatility_stable_threshold_days",
                    "discovery_related_depth",
                    "max_avg_referring_domains_filter",
                    "yearly_trend_decline_threshold",
                    "quarterly_trend_decline_threshold",
                    "max_kd_hard_limit",
                    "max_referring_main_domains_limit",
                    "max_avg_domain_rank_threshold",
                    "min_keyword_word_count",
                    "max_keyword_word_count",
                    "crowded_serp_features_threshold",
                    "min_serp_stability_days",
                    "max_non_blog_results",
                    "max_ai_overview_words",
                    "max_first_organic_y_pixel",
                    "max_words_for_ai_analysis",  # NEW
                    "max_avg_lcp_time",  # NEW
                ]
                for key in int_keys:
                    if settings.get(key) is not None:
                        try:
                            settings[key] = int(settings[key])
                        except (ValueError, TypeError):
                            pass

                float_keys = [
                    "informational_score",
                    "commercial_score",
                    "transactional_score",
                    "navigational_score",
                    "question_keyword_bonus",
                    "max_cpc_for_scoring",
                    "min_monthly_trend_percentage",
                    "featured_snippet_bonus",
                    "ai_overview_bonus",
                    "serp_freshness_bonus_max",
                    "min_cpc_filter_api",
                    "category_intent_bonus",
                    "search_volume_volatility_threshold",
                    "max_paid_competition_score",
                    "max_high_top_of_page_bid",
                    "max_pages_to_domain_ratio",
                    "ai_generation_temperature",  # NEW
                    "recommended_word_count_multiplier",  # NEW
                ]
                for key in float_keys:
                    if settings.get(key) is not None:
                        try:
                            settings[key] = float(settings[key])
                        except (ValueError, TypeError):
                            pass

                return settings
            return {}

    def update_client_settings(self, client_id: str, settings: Dict[str, Any]):
        """Updates client-specific settings, handling type conversions for DB storage."""
        self.logger.info(f"Updating client settings for {client_id}.")
        conn = self._get_conn()
        with conn:
            cursor = conn.execute("PRAGMA table_info(client_settings)")
            schema_columns = {row["name"] for row in cursor.fetchall()}

            current_time = datetime.now().isoformat()

            set_clauses = ["last_updated = ?"]
            values = [current_time]

            for key, value in settings.items():
                if key in schema_columns and key not in ["client_id", "last_updated"]:
                    db_value = value
                    if isinstance(value, list):
                        db_value = ",".join(map(str, value))
                    elif isinstance(value, bool):
                        db_value = 1 if value else 0

                    set_clauses.append(f"{key} = ?")
                    values.append(db_value)

            if len(set_clauses) > 1:
                update_query = f"UPDATE client_settings SET {', '.join(set_clauses)} WHERE client_id = ?"
                values.append(client_id)
                conn.execute(update_query, values)
                self.logger.info(f"Client settings for {client_id} updated in DB.")
            else:
                self.logger.info(
                    f"No valid client settings found to update for {client_id}."
                )

    def get_all_opportunities_for_export(self) -> List[Dict[str, Any]]:
        """Retrieves all opportunities for all clients from the database."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_ALL_OPPORTUNITIES_FOR_EXPORT)
            return self._deserialize_rows(cursor.fetchall())

    def get_dashboard_stats(self, client_id: str) -> Dict[str, Any]:
        """Retrieves statistics for the dashboard UI."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # Get counts by status
            cursor.execute(queries.COUNT_OPPORTUNITIES_BY_STATUS, (client_id,))
            status_counts = {row["status"]: row["count"] for row in cursor.fetchall()}

            # Get recent items
            cursor.execute(queries.SELECT_RECENTLY_GENERATED, (client_id,))
            recent_items = [dict(row) for row in cursor.fetchall()]

            return {"status_counts": status_counts, "recent_items": recent_items}

    def get_total_api_cost(self, client_id: str) -> float:
        """Calculates the total API cost for a client by summing costs from both opportunities and discovery runs."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            
            # Sum cost from opportunities
            cursor.execute(
                "SELECT SUM(total_api_cost) FROM opportunities WHERE client_id = ?",
                (client_id,),
            )
            opportunities_cost = cursor.fetchone()[0] or 0.0
            
            # Sum cost from discovery runs
            cursor.execute(
                "SELECT SUM(total_api_cost) FROM discovery_runs WHERE client_id = ?",
                (client_id,),
            )
            runs_cost = cursor.fetchone()[0] or 0.0
            
            return opportunities_cost + runs_cost

    def get_dashboard_data(self, client_id: str) -> Dict[str, Any]:
        """Retrieves aggregated data for the main dashboard UI."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # 1. KPIs
            cursor.execute(
                "SELECT COUNT(*) FROM opportunities WHERE client_id = ?", (client_id,)
            )
            total_opportunities = cursor.fetchone()[0]
            cursor.execute(
                "SELECT COUNT(*) FROM opportunities WHERE client_id = ? AND status = 'generated'",
                (client_id,),
            )
            content_generated = cursor.fetchone()[0]
            cursor.execute(
                "SELECT SUM(traffic_value) FROM opportunities WHERE client_id = ?",
                (client_id,),
            )
            total_traffic_value = cursor.fetchone()[0] or 0
            total_api_cost = self.get_total_api_cost(client_id)

            kpis = {
                "totalOpportunities": total_opportunities,
                "contentGenerated": content_generated,
                "totalTrafficValue": total_traffic_value,
                "totalApiCost": total_api_cost,
            }

            # 2. Funnel and Stats Data
            dashboard_stats = self.get_dashboard_stats(client_id)
            status_counts = dashboard_stats.get("status_counts", {})
            recent_items = dashboard_stats.get("recent_items", [])

            funnel_data = [
                {"stage": "Total", "count": total_opportunities},
                {"stage": "Validated", "count": status_counts.get("validated", 0)},
                {"stage": "Analyzed", "count": status_counts.get("analyzed", 0)},
                {"stage": "Generated", "count": content_generated},
                {"stage": "Disqualified", "count": (status_counts.get("rejected", 0) or 0) + (status_counts.get("failed", 0) or 0)},
            ]

            # 3. Action Items
            cursor.execute(queries.SELECT_ACTION_ITEMS, (client_id,))
            action_items_raw = [dict(row) for row in cursor.fetchall()]
            action_items = {
                "awaitingApproval": [
                    item
                    for item in action_items_raw
                    if item["status"] == "paused_for_approval"
                ],
                "failed": [
                    item for item in action_items_raw if item["status"] == "failed"
                ],
            }

            return {
                "kpis": kpis,
                "funnelData": funnel_data,
                "actionItems": action_items,
                "status_counts": status_counts,
                "recent_items": recent_items,
            }

    def update_opportunity_wordpress_payload(
        self, opportunity_id: int, wordpress_payload: Dict[str, Any]
    ):
        """Stores the generated WordPress JSON payload for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving final WordPress JSON payload."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_WORDPRESS_PAYLOAD,
                (json.dumps(wordpress_payload), opportunity_id),
            )

    def get_api_cache(self, key: str) -> Optional[Dict[str, Any]]:
        """Retrieves a cached item from the api_cache table."""
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(queries.SELECT_API_CACHE, (key,))
            row = cursor.fetchone()
            if row:
                # Check TTL during retrieval
                if row["timestamp"] + (row["ttl_days"] * 86400) > time.time():
                    return json.loads(row["data"])
                else:
                    self.logger.debug(f"Cache STALE for key: {key}")
                    self.delete_api_cache_by_key(key)  # Clean up stale entry
            return None

    def set_api_cache(self, key: str, value: Any, ttl_days: int = 7):
        """Stores an item in the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.INSERT_API_CACHE,
                (key, json.dumps(value), time.time(), ttl_days),
            )
        self.logger.debug(f"Cache SET for key: {key}")

    def delete_api_cache_by_key(self, key: str):
        """Deletes a specific item from the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.DELETE_API_CACHE_BY_KEY, (key,))

    def clear_api_cache(self):
        """Clears all items from the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.TRUNCATE_API_CACHE)
        self.logger.info("API cache cleared.")

    def clear_expired_api_cache(self):
        """Deletes all expired items from the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.DELETE_EXPIRED_API_CACHE, (time.time(),))
        self.logger.debug("Expired API cache entries cleaned up.")

    # --- Discovery Run Methods ---

    def create_discovery_run(self, client_id: str, parameters: Dict[str, Any]) -> int:
        """Creates a new discovery run record and returns its ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                queries.INSERT_DISCOVERY_RUN,
                (
                    client_id,
                    datetime.now().isoformat(),
                    "running",
                    json.dumps(parameters),
                ),
            )
            return cursor.lastrowid

    def update_discovery_run_status(self, run_id: int, status: str):
        """Updates the status of a discovery run."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.UPDATE_DISCOVERY_RUN_STATUS, (status, run_id))

    def update_discovery_run_completed(
        self, run_id: int, results_summary: Dict[str, Any]
    ):
        """Marks a discovery run as completed and stores the results summary."""
        conn = self._get_conn()
        total_cost = results_summary.get("total_cost", 0.0)
        with conn:
            conn.execute(
                queries.UPDATE_DISCOVERY_RUN_COMPLETED,
                (
                    datetime.now().isoformat(),
                    json.dumps(results_summary),
                    total_cost,
                    run_id,
                ),
            )

    def update_discovery_run_failed(self, run_id: int, error_message: str):
        """Marks a discovery run as failed and stores the error message."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_DISCOVERY_RUN_FAILED,
                (datetime.now().isoformat(), error_message, run_id),
            )

    def get_all_discovery_runs_paginated(
        self, client_id: str, page: int, limit: int, filters: Optional[Dict[str, Any]] = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """Retrieves all discovery runs for a specific client with pagination and filtering."""
        conn = self._get_conn()
        
        base_query = "FROM discovery_runs WHERE client_id = ?"
        query_params = [client_id]
        
        where_clauses = []
        if filters:
            if filters.get("search_query"):
                where_clauses.append("(parameters LIKE ? OR status LIKE ?)")
                search_term = f"%{filters['search_query']}%"
                query_params.extend([search_term, search_term])
            if filters.get("start_date") and filters.get("end_date"):
                where_clauses.append("start_time BETWEEN ? AND ?")
                query_params.extend([filters["start_date"], filters["end_date"]])

        if where_clauses:
            base_query += " AND " + " AND ".join(where_clauses)

        with conn:
            cursor = conn.cursor()
            
            # Get total count with filters
            count_query = f"SELECT COUNT(*) {base_query}"
            cursor.execute(count_query, query_params)
            total_count = cursor.fetchone()[0]
            
            # Get paginated data with filters
            select_query = f"SELECT * {base_query} ORDER BY start_time DESC LIMIT ? OFFSET ?"
            cursor.execute(select_query, query_params + [limit, (page - 1) * limit])
            
            runs = []
            for row in cursor.fetchall():
                run = dict(row)
                try:
                    if run.get("parameters"):
                        run["parameters"] = json.loads(run["parameters"])
                    if run.get("results_summary"):
                        run["results_summary"] = json.loads(run["results_summary"])
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse JSON for discovery run ID {run.get('id')}."
                    )
                runs.append(run)
            return runs, total_count

    def get_discovery_run_by_id(self, run_id: int) -> Optional[Dict[str, Any]]:
        """Retrieves a single discovery run by its ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_DISCOVERY_RUN_BY_ID, (run_id,))
            row = cursor.fetchone()
            if row:
                run = dict(row)
                try:
                    if run.get("parameters"):
                        run["parameters"] = json.loads(run["parameters"])
                    if run.get("results_summary"):
                        run["results_summary"] = json.loads(run["results_summary"])
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse JSON for discovery run ID {run.get('id')}."
                    )
                return run
        return None

    def update_discovery_run_log_path(self, run_id: int, log_path: str):
        """Updates the log file path for a discovery run."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.UPDATE_DISCOVERY_RUN_LOG_PATH, (log_path, run_id))

    def get_keywords_for_run(self, run_id: int) -> List[Dict[str, Any]]:
        """Retrieves all opportunities associated with a specific discovery run ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM opportunities WHERE run_id = ?", (run_id,))
            return self._deserialize_rows(cursor.fetchall())

    def get_keywords_for_run_by_reason(
        self, run_id: int, reason: str
    ) -> List[Dict[str, Any]]:
        """Retrieves all opportunities associated with a specific discovery run ID that were disqualified for a specific reason."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_KEYWORDS_FOR_RUN_BY_REASON, (run_id, reason))
            return self._deserialize_rows(cursor.fetchall())

    def search_discovery_runs(self, client_id: str, query: str) -> List[Dict[str, Any]]:
        """Searches for discovery runs by seed keywords or status for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            search_term = f"%{query}%"
            cursor.execute(
                """
                SELECT id, client_id, start_time, status, parameters, results_summary
                FROM discovery_runs
                WHERE client_id = ? 
                  AND (parameters LIKE ? OR status LIKE ? OR error_message LIKE ?)
                ORDER BY start_time DESC
                LIMIT 10;
            """,
                (client_id, search_term, search_term, search_term),
            )

            runs = []
            for row in cursor.fetchall():
                run = dict(row)
                if run.get("parameters"):
                    run["parameters"] = json.loads(run["parameters"])
                if run.get("results_summary"):
                    run["results_summary"] = json.loads(run["results_summary"])
                runs.append(run)
            return runs

    def get_job(self, job_id: str) -> Optional[Dict[str, Any]]:
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(queries.GET_JOB, (job_id,))
            row = cursor.fetchone()
            if row:
                job_data = dict(row)
                if job_data.get("result"):
                    job_data["result"] = json.loads(job_data["result"])
                return job_data
        return None

    def get_all_jobs(self) -> List[Dict[str, Any]]:
        """Retrieves all jobs from the database, ordered by start time."""
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(queries.GET_ALL_JOBS)
            jobs = []
            for row in cursor.fetchall():
                job_data = dict(row)
                if job_data.get("result"):
                    try:
                        job_data["result"] = json.loads(job_data["result"])
                    except json.JSONDecodeError:
                        job_data["result"] = {"raw_result": job_data["result"]}
                jobs.append(job_data)
            return jobs

    def update_job(self, job_info: Dict[str, Any]):
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_JOB,
                (
                    job_info["id"],
                    job_info.get("client_id"),
                    job_info["status"],
                    job_info["progress"],
                    json.dumps(job_info.get("result")) if job_info.get("result") else None,
                    job_info.get("error"),
                    job_info.get("function_name"),
                    job_info["started_at"],
                    job_info.get("finished_at"),
                ),
            )

    def get_client_prompt_templates(self, client_id: str) -> List[Dict[str, Any]]:
        """MOCK: Retrieves all prompt templates for a client."""
        return []

    def save_client_prompt_template(
        self, client_id: str, template_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """MOCK: Saves or updates a client's prompt template."""
        template_data["last_updated"] = datetime.now().isoformat()
        return template_data

    def delete_client_prompt_template(self, client_id: str, template_name: str):
        """MOCK: Deletes a client's prompt template."""
        pass

    def update_opportunity_ai_content_and_status(
        self,
        opportunity_id: int,
        ai_content_data: Dict[str, Any],
        ai_model: str,
        status: str,
    ):
        """Stores the generated AI content package, model, and status used for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving AI content generated by model '{ai_model}' and setting status to '{status}'."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_AI_CONTENT_AND_STATUS,
                (
                    json.dumps(ai_content_data),
                    ai_model,
                    datetime.now().isoformat(),
                    status,
                    opportunity_id,
                ),
            )

    def save_content_version_to_history(
        self,
        opportunity_id: int,
        ai_content_json: Dict[str, Any],
        timestamp: Optional[str] = None,
    ):
        """Saves the current content package of an opportunity to the history table."""
        conn = self._get_conn()
        timestamp = timestamp or datetime.now().isoformat()
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving content version to history at {timestamp}."
        )
        with conn:
            conn.execute(
                queries.INSERT_CONTENT_HISTORY,
                (opportunity_id, timestamp, json.dumps(ai_content_json)),
            )

    def save_content_feedback(
        self, opportunity_id: int, rating: int, comments: Optional[str] = None
    ):
        """Saves user feedback for generated content."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.INSERT_CONTENT_FEEDBACK,
                (opportunity_id, rating, comments, datetime.now().isoformat()),
            )
        self.logger.info(
            f"Saved content feedback for opportunity {opportunity_id} (Rating: {rating})."
        )

    def get_content_history(self, opportunity_id: int) -> List[Dict[str, Any]]:
        """Retrieves all historical content versions for an opportunity."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_CONTENT_HISTORY_BY_OPP_ID, (opportunity_id,))
            rows = cursor.fetchall()

            results = []
            for row in rows:
                row_dict = dict(row)
                if row_dict.get("ai_content_json"):
                    row_dict["ai_content_json"] = json.loads(
                        row_dict["ai_content_json"]
                    )
                results.append(row_dict)
            return results

    def restore_content_version(
        self, opportunity_id: int, version_timestamp: str
    ) -> Optional[Dict[str, Any]]:
        """
        Restores a content version from history. Before restoring, it saves the current
        'generated' content to the history table to prevent data loss.
        """
        conn = self._get_conn()
        with conn:
            # First, fetch the current opportunity to save its content
            cursor = conn.execute(queries.SELECT_OPPORTUNITY_BY_ID, (opportunity_id,))
            current_opp_row = cursor.fetchone()
            if not current_opp_row:
                self.logger.error(
                    f"Cannot restore: Opportunity {opportunity_id} not found."
                )
                return None

            current_opp = self._deserialize_rows([current_opp_row])[0]
            current_content = current_opp.get("ai_content")

            # Save the current 'generated' content to history before overwriting
            if current_content and current_opp.get("status") == "generated":
                self.save_content_version_to_history(opportunity_id, current_content)

            # Now, find the historical version to restore
            cursor = conn.execute(
                "SELECT ai_content_json FROM content_history WHERE opportunity_id = ? AND timestamp = ?",
                (opportunity_id, version_timestamp),
            )
            version_to_restore_row = cursor.fetchone()

            if (
                not version_to_restore_row
                or not version_to_restore_row["ai_content_json"]
            ):
                self.logger.error(
                    f"Version not found or content missing for timestamp: {version_timestamp}"
                )
                raise ValueError(f"Content version at {version_timestamp} not found.")

            restored_content_str = version_to_restore_row["ai_content_json"]
            restored_content = json.loads(restored_content_str)

            # Update the main opportunities table with the restored content
            conn.execute(
                queries.UPDATE_OPPORTUNITY_AI_CONTENT_AND_STATUS,
                (
                    restored_content_str,
                    current_opp.get(
                        "ai_content_model", "gpt-4o"
                    ),  # Keep the last used model
                    datetime.now().isoformat(),
                    "generated",  # Reset status to 'generated'
                    opportunity_id,
                ),
            )
            self.logger.info(
                f"Successfully restored content from {version_timestamp} for opportunity {opportunity_id}."
            )

        return restored_content

    def update_opportunity_social_posts(
        self, opportunity_id: int, social_media_posts: List[Dict[str, Any]]
    ):
        """Stores the updated social media posts JSON for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving updated social media posts."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_SOCIAL_POSTS,
                (json.dumps(social_media_posts), opportunity_id),
            )

    def update_social_media_posts_status(self, opportunity_id: int, new_status: str):
        """Updates the status of social media posts for an opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Updating social media posts status to '{new_status}'."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                "UPDATE opportunities SET social_media_posts_status = ? WHERE id = ?",
                (new_status, opportunity_id),
            )

    def save_full_content_package(
        self,
        opportunity_id: int,
        ai_content_data: Dict[str, Any],
        ai_model: str,
        featured_image_data: Optional[Dict[str, Any]],
        in_article_images_data: List[Dict[str, Any]],
        social_posts: Optional[List[Dict[str, Any]]],
        final_package: Dict[str, Any],
        total_api_cost: float,
    ):
        """
        Saves the entire generated content package and updates the status to 'generated' in a single transaction.
        Applies server-side sanitization to the main HTML body. (W20 FIX)
        """
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving full content package and finalizing status."
        )

        # W20 FIX: Sanitize the final package HTML content before saving
        html_body = final_package.get("article_html_final")
        if html_body:
            clean_html = bleach.clean(
                html_body, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES_DB
            )
            final_package["article_html_final"] = clean_html

        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_GENERATED_CONTENT_AND_STATUS,
                (
                    json.dumps(ai_content_data),
                    ai_model,
                    featured_image_data.get("remote_url")
                    if featured_image_data
                    else None,
                    featured_image_data.get("local_path")
                    if featured_image_data
                    else None,
                    json.dumps(in_article_images_data),
                    json.dumps(social_posts) if social_posts else None,
                    json.dumps(final_package),
                    datetime.now().isoformat(),
                    total_api_cost,
                    opportunity_id,
                ),
            )
        self.logger.info(
            f"Opportunity {opportunity_id}: Successfully saved full content package and set status to 'generated'."
        )

        def update_opportunity_published_url(self, opportunity_id: int, url: str):
            """Updates the published_url for a specific opportunity."""
            self.logger.info(
                f"Opportunity {opportunity_id}: Storing published URL: {url}"
            )
            conn = self._get_conn()
            with conn:
                conn.execute(
                    "UPDATE opportunities SET published_url = ? WHERE id = ?",
                    (url, opportunity_id),
                )

    def get_high_priority_opportunities(
        self, client_id: str, limit: int = 5
    ) -> List[Dict[str, Any]]:
        """Retrieves the top N validated opportunities with the highest strategic score."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                queries.SELECT_HIGH_PRIORITY_OPPORTUNITIES, (client_id, limit)
            )
            return self._deserialize_rows(cursor.fetchall())

    def get_content_feedback_examples(
        self, client_id: str, limit: int = 2
    ) -> Dict[str, List]:
        """Retrieves examples of good and bad content based on user feedback."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # Get highest rated
            cursor.execute(
                """
                SELECT o.keyword, cf.comments, cf.rating
                FROM content_feedback cf
                JOIN opportunities o ON cf.opportunity_id = o.id
                WHERE o.client_id = ? AND cf.rating >= 4
                ORDER BY cf.rating DESC, cf.timestamp DESC
                LIMIT ?;
            """,
                (client_id, limit),
            )
            good_examples = [dict(row) for row in cursor.fetchall()]

            # Get lowest rated
            cursor.execute(
                """
                SELECT o.keyword, cf.comments, cf.rating
                FROM content_feedback cf
                JOIN opportunities o ON cf.opportunity_id = o.id
                WHERE o.client_id = ? AND cf.rating <= 2
                ORDER BY cf.rating ASC, cf.timestamp DESC
                LIMIT ?;
            """,
                (client_id, limit),
            )
            bad_examples = [dict(row) for row in cursor.fetchall()]

        return {"good_examples": good_examples, "bad_examples": bad_examples}

    def get_qualification_settings(self, client_id: str) -> Dict[str, Any]:
        """Retrieves qualification settings for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT * FROM qualification_settings WHERE client_id = ?", (client_id,)
            )
            row = cursor.fetchone()
            if row:
                return dict(row)
            return {}

    def get_qualification_strategies(self, client_id: str) -> List[Dict[str, Any]]:
        """Retrieves all qualification strategies for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT * FROM qualification_strategies WHERE client_id = ?",
                (client_id,),
            )
            return [dict(row) for row in cursor.fetchall()]

    def get_qualification_strategy_by_id(
        self, strategy_id: int
    ) -> Optional[Dict[str, Any]]:
        """Retrieves a single qualification strategy by its ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT * FROM qualification_strategies WHERE id = ?", (strategy_id,)
            )
            row = cursor.fetchone()
            if row:
                return dict(row)
            return None

    def create_qualification_strategy(
        self, client_id: str, strategy: Dict[str, Any]
    ) -> int:
        """Creates a new qualification strategy for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            keys = ", ".join(strategy.keys())
            placeholders = ", ".join("?" * len(strategy))
            values = list(strategy.values())
            cursor.execute(
                f"INSERT INTO qualification_strategies (client_id, {keys}) VALUES (?, {placeholders})",
                [client_id] + values,
            )
            return cursor.lastrowid

    def update_qualification_strategy(self, strategy_id: int, strategy: Dict[str, Any]):
        """Updates a qualification strategy."""
        conn = self._get_conn()
        with conn:
            set_clauses = []
            values = []
            for key, value in strategy.items():
                set_clauses.append(f"{key} = ?")
                values.append(value)

            if set_clauses:
                update_query = f"UPDATE qualification_strategies SET {', '.join(set_clauses)} WHERE id = ?"
                values.append(strategy_id)
                conn.execute(update_query, values)

    def delete_qualification_strategy(self, strategy_id: int):
        """Deletes a qualification strategy."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                "DELETE FROM qualification_strategies WHERE id = ?", (strategy_id,)
            )

    def update_qualification_settings(self, client_id: str, settings: Dict[str, Any]):
        """Updates qualification settings for a specific client."""
        conn = self._get_conn()
        with conn:
            set_clauses = []
            values = []
            for key, value in settings.items():
                set_clauses.append(f"{key} = ?")
                values.append(value)

            if set_clauses:
                update_query = f"UPDATE qualification_settings SET {', '.join(set_clauses)} WHERE client_id = ?"
                values.append(client_id)
                conn.execute(update_query, values)

    def get_content_snippet_by_slug(self, slug: str) -> Optional[Dict[str, Any]]:
        """Retrieves the title and a content snippet for an opportunity by its slug."""
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(
                """
                SELECT keyword as title,
                       SUBSTR(JSON_EXTRACT(ai_content_json, '$.meta_description'), 1, 200) as snippet_desc
                FROM opportunities WHERE slug = ?;
            """,
                (slug,),
            )

    def get_active_jobs_by_client(self, client_id: str) -> List[Dict[str, Any]]:
        """Retrieves all jobs with 'running' or 'pending' status for a client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(queries.GET_ACTIVE_JOBS_BY_CLIENT, (client_id,))
            jobs = []
            for row in cursor.fetchall():
                job_data = dict(row)
                if job_data.get("result"):
                    try:
                        job_data["result"] = json.loads(job_data["result"])
                    except json.JSONDecodeError:
                        job_data["result"] = {"raw_result": job_data["result"]}
                jobs.append(job_data)
            return jobs
            row = cursor.fetchone()
            if row:
                return dict(row)
        return None

    def fail_stale_jobs(self):
        """Finds all jobs with a 'running' status and marks them as 'failed' on startup."""
        self.logger.info("Scanning for stale jobs from previous sessions...")
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            error_message = "Job failed due to application restart."
            finished_time = time.time()
            cursor.execute(
                """
                UPDATE jobs
                SET status = 'failed', error = ?, finished_at = ?
                WHERE status = 'running';
            """,
                (error_message, finished_time),
            )

            if cursor.rowcount > 0:
                self.logger.warning(
                    f"Marked {cursor.rowcount} stale 'running' jobs as 'failed'."
                )
            else:
                self.logger.info("No stale jobs found.")

        def override_disqualification(self, opportunity_id: int) -> bool:
            """Manually overrides a failed qualification, resetting status to pending."""
            self.logger.info(
                f"Overriding disqualification for opportunity ID: {opportunity_id}"
            )
            conn = self._get_conn()
            with conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    UPDATE opportunities
                    SET status = 'pending', blog_qualification_status = 'passed_manual_override', blog_qualification_reason = 'Manually overridden by user.'
                    WHERE id = ? AND status IN ('failed', 'rejected');
                """,
                    (opportunity_id,),
                )

                if cursor.rowcount > 0:
                    self.logger.info(
                        f"Successfully overrode disqualification for opportunity ID: {opportunity_id}"
                    )
                    return True
                else:
                    self.logger.warning(
                        f"Could not override disqualification for ID: {opportunity_id}. Status was not 'failed' or 'rejected'."
                    )
                    return False
```

## File: data_access/initialize.py
```python
import logging
import os
import sys

# Ensure the project root is in sys.path for module imports
# Correctly identify the project root, which is two levels above the 'backend' directory
project_root = os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
backend_root = os.path.join(project_root, "backend")

# Add the backend directory to sys.path to allow for absolute imports from the backend module
if backend_root not in sys.path:
    sys.path.insert(0, backend_root)

# Import necessary classes from your project
try:
    from app_config.manager import ConfigManager
    from data_access.database_manager import DatabaseManager
except ImportError as e:
    print(f"Error importing project modules: {e}")
    print(
        "Please ensure you are running this script from the project's root directory or that paths are set up correctly."
    )
    print(f"Current working directory: {os.getcwd()}")
    print(f"Project root: {project_root}")
    print(f"Backend root added to sys.path: {backend_root}")
    sys.exit(1)

# Configure logging for this script run
logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")
logger = logging.getLogger(__name__)


def main():
    """Initializes the database, runs migrations, and seeds the default client."""

    logger.info(
        "Starting database initialization and default client seeding process..."
    )

    # Construct the absolute path to the settings.ini file
    settings_path = os.path.join(backend_root, "app_config", "settings.ini")
    if not os.path.exists(settings_path):
        logger.error(f"settings.ini file not found at expected path: {settings_path}")
        sys.exit(1)

    # Instantiate ConfigManager with the correct path
    config_manager = ConfigManager(settings_path=settings_path)
    global_cfg = config_manager.get_global_config()
    db_file_name = global_cfg.get(
        "db_file_name", "data/opportunities.db"
    )  # Get DB file name from config

    # The database path is relative to the project root
    db_path = os.path.join(project_root, db_file_name)
    db_data_dir = os.path.dirname(db_path)

    # --- Force re-creation of the database ---
    if os.path.exists(db_path):
        logger.info(
            f"Deleting existing database file at {db_path} to ensure a clean slate."
        )
        os.remove(db_path)
    # Ensure the data directory exists
    if not os.path.exists(db_data_dir):
        os.makedirs(db_data_dir)
        logger.info(f"Created data directory: {db_data_dir}")
    # --- End ---

    try:
        # 1. Initialize DatabaseManager
        db_manager = DatabaseManager(cfg_manager=config_manager, db_path=db_path)

        # 2. Ensure database and all tables are created, and migrations are applied
        db_manager.initialize()
        logger.info("Database initialized and all migrations applied.")

        # 3. Define default client details (using values from settings.ini)
        client_id = global_cfg.get("default_client_id", "Lark_Main_Site")
        client_name = client_id.replace(
            "_", " "
        ).title()  # Simple conversion for display name

        # 4. Get default settings template (which includes brand voice settings from settings.ini)
        default_settings = config_manager.get_default_client_settings_template()

        if not default_settings:
            logger.error("Could not load default client settings template. Aborting.")
            return

        # 5. Add the default client
        success = db_manager.add_client(client_id, client_name, default_settings)

        if success:
            logger.info(
                f"Successfully seeded the database with default client: '{client_name}' ({client_id})"
            )
        else:
            # This would only happen if the client_id already existed, which shouldn't after deleting the DB.
            logger.warning(
                f"Default client '{client_name}' ({client_id}) already existed in the database."
            )

        logger.info("Database setup process complete. No keyword data has been added.")

    except Exception as e:
        logger.error(f"An error occurred during database setup: {e}", exc_info=True)


if __name__ == "__main__":
    main()
```

## File: data_access/models.py
```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional


@dataclass
class KeywordData:
    keyword: str
    search_volume: int
    keyword_difficulty: int
    main_intent: str
    cpc: float
    search_volume_trend: Optional[Dict[str, Any]] = None
    core_keyword: Optional[str] = None
    # Add other fields as they come from DataForSEO API


@dataclass
class CompetitorPage:
    url: str
    rank: int
    word_count: int
    readability_score: float
    technical_warnings: List[str] = field(default_factory=list)
    headings: Dict[str, List[str]] = field(default_factory=dict)
    full_content_plain_text: Optional[str] = None
    error: Optional[str] = None


@dataclass
class SerpOverview:
    serp_has_featured_snippet: bool
    serp_has_video_results: bool
    serp_has_ai_overview: bool
    people_also_ask: List[str] = field(default_factory=list)
    ai_overview_content: Optional[str] = None
    featured_snippet_content: Optional[str] = None
    avg_referring_domains_top5_organic: Optional[float] = None
    avg_main_domain_rank_top5_organic: Optional[float] = None
    serp_last_updated_days_ago: Optional[int] = None
    dominant_content_format: Optional[str] = None


@dataclass
class ContentIntelligence:
    recommended_word_count: int
    average_readability_score: float
    common_headings_to_cover: List[str] = field(default_factory=list)
    unique_angles_to_include: List[str] = field(default_factory=list)
    ai_generated_outline_h2: List[str] = field(default_factory=list)
    ai_generated_outline_h3: List[str] = field(default_factory=list)


@dataclass
class AIBrief:
    target_keyword: str
    content_type: str
    target_audience_persona: str
    primary_goal: str
    target_word_count: int
    mandatory_sections: List[str] = field(default_factory=list)
    unique_angles_to_cover: List[str] = field(default_factory=list)
    questions_to_answer_directly: List[str] = field(default_factory=list)
    internal_linking_suggestions: List[Dict[str, str]] = field(default_factory=list)
    dynamic_serp_instructions: List[str] = field(default_factory=list)
    source_and_inspiration_content: Dict[str, Any] = field(default_factory=dict)
    client_id: str = "default"


@dataclass
class Blueprint:
    metadata: Dict[str, Any]
    winning_keyword: Dict[
        str, Any
    ]  # Full keyword data for the selected winning keyword
    serp_overview: SerpOverview
    content_intelligence: ContentIntelligence
    competitor_analysis: List[CompetitorPage]
    executive_summary: str
    ai_content_brief: AIBrief


@dataclass
class AIContentPackage:
    article_body_html: str
    meta_title: str
    meta_description: str
    social_blurbs: List[Dict[str, Any]]
    editor_notes: str
    ai_focus_keyword: str


@dataclass
class GeneratedImage:
    type: str  # 'featured' or 'in_article_N'
    original_prompt: str
    enhanced_prompt: str
    revised_prompt: str
    local_path: str
    model: str
    wordpress_id: Optional[int] = None
    remote_url: Optional[str] = None
    alt_text: Optional[str] = None
    insertion_marker: Optional[str] = None
    error: Optional[str] = None


@dataclass
class Opportunity:
    id: Optional[int]
    keyword: str  # This is the cluster_topic
    status: str
    client_id: str
    date_added: datetime
    date_processed: Optional[datetime]
    scheduled_for: Optional[datetime]
    full_data: Dict[str, Any]  # Original keyword data
    blueprint_data: Optional[Blueprint]
    ai_content_json: Optional[AIContentPackage]
    ai_content_model: Optional[str]
    featured_image_prompt: Optional[str]
    featured_image_model: Optional[str]
    featured_image_url: Optional[str]
    featured_image_local_path: Optional[str]
    in_article_images_data: List[GeneratedImage] = field(default_factory=list)
    wordpress_post_url: Optional[str]
    wordpress_post_id: Optional[int]
    social_media_posts_json: List[Dict[str, Any]] = field(default_factory=list)
    last_workflow_step: Optional[str]
    error_message: Optional[str]
    search_volume: Optional[int] = None
    keyword_difficulty: Optional[int] = None


@dataclass
class Client:
    client_id: str
    client_name: str
    date_created: datetime


@dataclass
class ClientSettings:
    client_id: str
    openai_api_key: Optional[str] = None
    pexels_api_key: Optional[str] = None  # NEW, was missing from dataclass
    wordpress_url: Optional[str] = None
    wordpress_user: Optional[str] = None
    wordpress_app_password: Optional[str] = None
    hootsuite_api_key: Optional[str] = None
    custom_ai_prompt_template: Optional[str] = None
    ai_image_style_formula: Optional[str] = None
    featured_image_base_prompt: Optional[str] = None
    wordpress_seo_plugin: Optional[str] = None
    ai_content_model: Optional[str] = None
    image_ai_model: Optional[str] = None
    num_in_article_images: Optional[int] = None
    image_quality: Optional[str] = None
    enable_automated_internal_linking: Optional[bool] = False
    default_wordpress_categories: List[str] = field(default_factory=list)
    default_wordpress_tags: List[str] = field(default_factory=list)
    negative_keywords: List[str] = field(default_factory=list)
    competitor_blacklist_domains: List[str] = field(default_factory=list)
    require_question_keywords: Optional[bool] = None
    enforce_intent_filter: Optional[bool] = None
    allowed_intents: List[str] = field(default_factory=list)
    max_competition_level: Optional[str] = None
    discovery_order_by: Optional[str] = None
    db_type: Optional[str] = "sqlite"  # NEW
    max_words_for_ai_analysis: Optional[int] = 1500  # NEW
    ai_generation_temperature: Optional[float] = 0.7  # NEW
    recommended_word_count_multiplier: Optional[float] = 1.2  # NEW
    max_avg_lcp_time: Optional[int] = 4000  # NEW
    prohibited_intents: List[str] = field(default_factory=list)  # NEW
    last_updated: datetime = field(default_factory=datetime.now)
```

## File: data_access/queries.py
```python
# data_access/queries.py

# --- Table Creation ---
CREATE_OPPORTUNITIES_TABLE = """
CREATE TABLE IF NOT EXISTS opportunities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    keyword TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending',
    client_id TEXT NOT NULL DEFAULT 'default',
    date_added TEXT NOT NULL,
    date_processed TEXT,
    strategic_score REAL,
    blog_qualification_status TEXT,
    blog_qualification_reason TEXT,
    keyword_info TEXT,
    keyword_properties TEXT,
    search_intent_info TEXT,
    serp_overview TEXT,
    score_breakdown TEXT,
    full_data TEXT NOT NULL,
    blueprint_data TEXT,
    ai_content_json TEXT,
    ai_content_model TEXT,
    featured_image_url TEXT,
    featured_image_local_path TEXT,
    in_article_images_data TEXT,
    social_media_posts_json TEXT,
    social_media_posts_status TEXT DEFAULT 'draft', -- NEW COLUMN
    last_workflow_step TEXT,
    error_message TEXT,
    wordpress_payload_json TEXT,
    final_package_json TEXT,
    slug TEXT UNIQUE,
    run_id INTEGER,
    keyword_info_normalized_with_bing TEXT,
    keyword_info_normalized_with_clickstream TEXT,
    monthly_searches TEXT,
    traffic_value REAL DEFAULT 0,
    check_url TEXT,
    related_keywords TEXT,
    keyword_categories TEXT,
     core_keyword TEXT,
    published_url TEXT,
     UNIQUE(keyword, client_id)
 );"""

CREATE_CLIENTS_TABLE = """
CREATE TABLE IF NOT EXISTS clients (
    client_id TEXT PRIMARY KEY,
    client_name TEXT NOT NULL,
    date_created TEXT NOT NULL
);
"""

CREATE_CLIENT_SETTINGS_TABLE = """
CREATE TABLE IF NOT EXISTS client_settings (
    client_id TEXT PRIMARY KEY,
    openai_api_key TEXT,
    pexels_api_key TEXT,
    location_code INTEGER,
    language_code TEXT,
    target_domain TEXT,
    device TEXT,
    os TEXT,
    informational_score REAL,
    commercial_score REAL,
    transactional_score REAL,
    navigational_score REAL,
    question_keyword_bonus REAL,
    ease_of_ranking_weight INTEGER,
    traffic_potential_weight INTEGER,
    commercial_intent_weight INTEGER,
    growth_trend_weight INTEGER,
    serp_features_weight INTEGER,
    serp_freshness_weight INTEGER,
    serp_volatility_weight INTEGER,
    competitor_weakness_weight INTEGER,
    max_cpc_for_scoring REAL,
    max_sv_for_scoring INTEGER,
    max_domain_rank_for_scoring INTEGER,
    max_referring_domains_for_scoring INTEGER,
    max_avg_referring_domains_filter INTEGER,
    featured_snippet_bonus REAL,
    ai_overview_bonus REAL,
    serp_freshness_bonus_max REAL,
    serp_freshness_old_threshold_days INTEGER,
    serp_volatility_stable_threshold_days INTEGER,
    enforce_intent_filter INTEGER,
    allowed_intents TEXT,
    require_question_keywords INTEGER,
    negative_keywords TEXT,
    min_monthly_trend_percentage REAL,
    min_competitor_word_count INTEGER,
    max_competitor_technical_warnings INTEGER,
    competitor_blacklist_domains TEXT,
    ugc_and_parasite_domains TEXT,
    num_competitors_to_analyze INTEGER,
    num_common_headings INTEGER,
    num_unique_angles INTEGER,
    max_initial_serp_urls_to_analyze INTEGER,
    calculate_rectangles INTEGER,
    people_also_ask_click_depth INTEGER,
    min_search_volume INTEGER,
    max_keyword_difficulty INTEGER,
    ai_content_model TEXT,
    num_in_article_images INTEGER,
    use_pexels_first INTEGER,
    cleanup_local_images INTEGER,
    onpage_enable_javascript INTEGER,
    onpage_load_resources INTEGER,
    onpage_disable_cookie_popup INTEGER,
    onpage_return_despite_timeout INTEGER,
    onpage_enable_browser_rendering INTEGER,
    onpage_store_raw_html INTEGER,
    onpage_validate_micromarkup INTEGER,
    onpage_check_spell INTEGER,
    onpage_accept_language TEXT,
    onpage_custom_user_agent TEXT,
    onpage_max_domains_per_request INTEGER,
    onpage_max_tasks_per_request INTEGER,
    onpage_enable_switch_pool INTEGER,
    onpage_browser_screen_resolution_ratio REAL,
    discovery_exact_match INTEGER,
    onpage_enable_custom_js INTEGER,
    onpage_custom_js TEXT,
    platforms TEXT,


    enable_automated_internal_linking INTEGER,
    db_type TEXT,
    max_words_for_ai_analysis INTEGER,
    ai_generation_temperature REAL,
    recommended_word_count_multiplier REAL,
    max_avg_lcp_time INTEGER,
    prohibited_intents TEXT,
    load_async_ai_overview INTEGER,
    onpage_custom_checks_thresholds TEXT,
    serp_remove_from_url_params TEXT,
     schema_author_type TEXT,
     client_knowledge_base TEXT,
    wordpress_url TEXT,
    wordpress_user TEXT,
    wordpress_app_password TEXT,
    wordpress_seo_plugin TEXT,
    default_wordpress_categories TEXT,
    default_wordpress_tags TEXT,
     last_updated TEXT NOT NULL,
     FOREIGN KEY (client_id) REFERENCES clients (client_id)
 );"""

INSERT_CLIENT_SETTINGS = """
INSERT INTO client_settings (client_id, brand_tone, target_audience, terms_to_avoid) VALUES (%s, %s, %s, %s)
ON CONFLICT (client_id) DO UPDATE SET brand_tone = %s, target_audience = %s, terms_to_avoid = %s
"""

SELECT_CLIENT_SETTINGS = """
SELECT brand_tone, target_audience, terms_to_avoid FROM client_settings WHERE client_id = %s;
"""

CREATE_DISCOVERY_RUNS_TABLE = """
CREATE TABLE IF NOT EXISTS discovery_runs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    client_id TEXT NOT NULL,
    start_time TEXT NOT NULL,
    end_time TEXT,
    status TEXT NOT NULL,
    parameters TEXT,
    results_summary TEXT,
    log_file_path TEXT,
    error_message TEXT,
    FOREIGN KEY (client_id) REFERENCES clients (client_id)
);
"""

CREATE_SCHEMA_VERSION_TABLE = """
CREATE TABLE IF NOT EXISTS schema_version (
    version INTEGER PRIMARY KEY,
    applied_at TEXT NOT NULL
);
"""

# NEW: Add a query to get current schema version
GET_SCHEMA_VERSION = """
SELECT version FROM schema_version ORDER BY version DESC LIMIT 1;
"""

# NEW: Add a query to insert schema version
INSERT_SCHEMA_VERSION = """
INSERT INTO schema_version (version, applied_at) VALUES (?, ?);
"""

CREATE_API_CACHE_TABLE = """
CREATE TABLE IF NOT EXISTS api_cache (
    key TEXT PRIMARY KEY,
    data TEXT NOT NULL,
    timestamp REAL NOT NULL,
    ttl_days INTEGER NOT NULL
);
"""

# --- Cache table queries (related to migration 002_add_slug_and_cache_table.sql)
INSERT_API_CACHE = """
INSERT OR REPLACE INTO api_cache (key, data, timestamp, ttl_days)
VALUES (?, ?, ?, ?);
"""

SELECT_API_CACHE = """
SELECT data, timestamp, ttl_days FROM api_cache WHERE key = ?;
"""

DELETE_EXPIRED_API_CACHE = """
DELETE FROM api_cache WHERE timestamp + (ttl_days * 86400) < ?;
"""

DELETE_API_CACHE_BY_KEY = """
DELETE FROM api_cache WHERE key = ?;
"""

TRUNCATE_API_CACHE = """
DELETE FROM api_cache;
"""

# --- Opportunity Queries ---
INSERT_OPPORTUNITY_OR_IGNORE = """
INSERT OR IGNORE INTO opportunities 
(keyword, client_id, date_added, full_data)
VALUES (?, ?, ?, ?);
"""

INSERT_OPPORTUNITY_WITH_BLUEPRINT = """
INSERT OR IGNORE INTO opportunities
(keyword, client_id, date_added, full_data, blueprint_data, slug)
VALUES (?, ?, ?, ?, ?, ?);
"""

SELECT_PENDING_OPPORTUNITIES = """
SELECT * FROM opportunities 
WHERE status = 'pending' AND client_id = ?;
"""

SELECT_ALL_OPPORTUNITIES_BY_CLIENT = """
SELECT * FROM opportunities 
WHERE client_id = ?
ORDER BY date_added DESC;
"""

SELECT_OPPORTUNITY_BY_ID = """
SELECT * FROM opportunities WHERE id = ?;
"""

SELECT_ALL_PROCESSED_KEYWORDS = """
SELECT keyword FROM opportunities 
WHERE client_id = ? AND status NOT IN ('rejected', 'failed');
"""

UPDATE_OPPORTUNITY_STATUS_WITH_DATE = """
UPDATE opportunities SET status = ?, date_processed = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_STATUS = """
UPDATE opportunities SET status = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_WORKFLOW_STATE = """
UPDATE opportunities SET last_workflow_step = ?, status = ?, error_message = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_BLUEPRINT = """
UPDATE opportunities
SET blueprint_data = ?
WHERE id = ?;
"""

UPDATE_OPPORTUNITY_BLUEPRINT_AND_SLUG = """
UPDATE opportunities
SET blueprint_data = ?, slug = ?
WHERE id = ?;
"""

UPDATE_OPPORTUNITY_AI_CONTENT = """
UPDATE opportunities
SET ai_content_json = ?, ai_content_model = ?, date_processed = ?
WHERE id = ?;
"""

UPDATE_OPPORTUNITY_IMAGES = """
UPDATE opportunities SET featured_image_url = ?, featured_image_local_path = ?, in_article_images_data = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_SOCIAL_POSTS = """
UPDATE opportunities SET social_media_posts_json = ? WHERE id = ?;
"""

SELECT_ALL_OPPORTUNITIES_FOR_EXPORT = """
SELECT * FROM opportunities ORDER BY client_id, date_added DESC;
"""

SELECT_PROCESSED_OPPORTUNITIES = """
SELECT * FROM opportunities
WHERE client_id = ? AND blueprint_data IS NOT NULL
ORDER BY date_processed DESC;
"""

UPDATE_OPPORTUNITY_WORDPRESS_PAYLOAD = (
    "UPDATE opportunities SET wordpress_payload_json = ? WHERE id = ?;"
)

UPDATE_OPPORTUNITY_FINAL_PACKAGE = (
    "UPDATE opportunities SET final_package_json = ? WHERE id = ?;"
)

# --- Dashboard Queries ---
COUNT_OPPORTUNITIES_BY_STATUS = """
SELECT status, COUNT(*) as count FROM opportunities WHERE client_id = ? GROUP BY status;
"""

SUM_DISCOVERY_COST_BY_CLIENT = """
SELECT SUM(CAST(JSON_EXTRACT(results_summary, '$.total_cost') AS REAL)) FROM discovery_runs WHERE client_id = ?;
"""

SUM_ANALYSIS_COST_BY_CLIENT = """
SELECT SUM(CAST(JSON_EXTRACT(blueprint_data, '$.metadata.total_api_cost') AS REAL)) FROM opportunities WHERE client_id = ?;
"""

SELECT_RECENTLY_GENERATED = """
SELECT id, keyword, status, date_processed FROM opportunities 
WHERE client_id = ? AND status = 'generated' 
ORDER BY date_processed DESC 
LIMIT 5;
"""


# --- Client Queries ---
INSERT_CLIENT = """
INSERT INTO clients (client_id, client_name, date_created) VALUES (?, ?, ?);
"""

SELECT_ALL_CLIENTS = """
SELECT client_id, client_name FROM clients ORDER BY date_created DESC;
"""

SELECT_CLIENT_SETTINGS = """
SELECT * FROM client_settings WHERE client_id = ?;
"""

# --- Discovery Run Queries ---
INSERT_DISCOVERY_RUN = """
INSERT INTO discovery_runs (client_id, start_time, status, parameters)
VALUES (?, ?, ?, ?);
"""

UPDATE_DISCOVERY_RUN_STATUS = """
UPDATE discovery_runs SET status = ? WHERE id = ?;
"""

UPDATE_DISCOVERY_RUN_COMPLETED = """
UPDATE discovery_runs SET end_time = ?, status = 'completed', results_summary = ?, total_api_cost = ? WHERE id = ?;
"""

UPDATE_DISCOVERY_RUN_FAILED = """
UPDATE discovery_runs SET end_time = ?, status = 'failed', error_message = ? WHERE id = ?;
"""

SELECT_ALL_DISCOVERY_RUNS_BY_CLIENT = """
SELECT * FROM discovery_runs WHERE client_id = ? ORDER BY start_time DESC;
"""

SELECT_ALL_DISCOVERY_RUNS_BY_CLIENT_PAGINATED = """
SELECT * FROM discovery_runs WHERE client_id = ? ORDER BY start_time DESC LIMIT ? OFFSET ?;
"""

SELECT_DISCOVERY_RUN_BY_ID = """
SELECT * FROM discovery_runs WHERE id = ?;
"""

SELECT_KEYWORDS_FOR_RUN_BY_REASON = """
SELECT * FROM opportunities WHERE run_id = ? AND blog_qualification_reason = ?;
"""

UPDATE_DISCOVERY_RUN_LOG_PATH = """
UPDATE discovery_runs SET log_file_path = ? WHERE id = ?;
"""

SELECT_OPPORTUNITY_BY_SLUG = """

SELECT * FROM opportunities WHERE slug = ?;

"""


CREATE_JOBS_TABLE = """

CREATE TABLE IF NOT EXISTS jobs (

    id TEXT PRIMARY KEY,

    status TEXT NOT NULL,

    progress INTEGER NOT NULL,

    result TEXT,

    error TEXT,

    started_at REAL NOT NULL,

    finished_at REAL

);

"""

GET_JOB = "SELECT * FROM jobs WHERE id = ?;"

UPDATE_JOB_STATUS_DIRECT = """
UPDATE jobs SET status = ?, progress = ?, finished_at = ? WHERE id = ?;
"""

UPDATE_JOB = """
INSERT OR REPLACE INTO jobs (id, client_id, status, progress, result, error, function_name, started_at, finished_at)
VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);
"""

GET_ALL_JOBS = "SELECT * FROM jobs ORDER BY started_at DESC LIMIT 100;"

CREATE_CONTENT_HISTORY_TABLE = """
CREATE TABLE IF NOT EXISTS content_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    timestamp TEXT NOT NULL,
    ai_content_json TEXT NOT NULL,
    restored_from_id INTEGER,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);
"""

# In data_access/queries.py, add history queries:
INSERT_CONTENT_HISTORY = """
INSERT INTO content_history
(opportunity_id, timestamp, ai_content_json)
VALUES (?, ?, ?);
"""

SELECT_CONTENT_HISTORY_BY_OPP_ID = """
SELECT id, opportunity_id, timestamp, ai_content_json FROM content_history
WHERE opportunity_id = ?
ORDER BY timestamp DESC;
"""

# Update/Add status update query:
UPDATE_OPPORTUNITY_AI_CONTENT_AND_STATUS = """
UPDATE opportunities
SET ai_content_json = ?, ai_content_model = ?, date_processed = ?, status = ?
WHERE id = ?;
"""

# In data_access/queries.py, define template queries:
COUNT_ALL_OPPORTUNITIES_BY_CLIENT = """
SELECT COUNT(*) FROM opportunities
WHERE client_id = ? 
"""

SELECT_ALL_OPPORTUNITIES_PAGINATED = """
SELECT {select_columns} FROM opportunities
WHERE client_id = ? {where_clause}
ORDER BY {order_by} {order_direction}
LIMIT ? OFFSET ?;
"""

SEARCH_OPPORTUNITIES_BY_KEYWORD = """
SELECT id, keyword, status FROM opportunities
WHERE client_id = ? AND keyword LIKE ?
ORDER BY date_added DESC
LIMIT 20;
"""

SELECT_HIGH_PRIORITY_OPPORTUNITIES = """
SELECT id, keyword, strategic_score, score_breakdown, keyword_info, keyword_properties, traffic_value FROM opportunities
WHERE client_id = ? AND status = 'validated'
ORDER BY strategic_score DESC
LIMIT ?;
"""

SELECT_ACTION_ITEMS = """
SELECT id, keyword, status, error_message, COALESCE(date_processed, date_added) as updated_at, strategic_score FROM opportunities
WHERE client_id = ? AND status IN ('paused_for_approval', 'failed')
ORDER BY updated_at DESC;
"""

CREATE_CONTENT_FEEDBACK_TABLE = """
CREATE TABLE IF NOT EXISTS content_feedback (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    rating INTEGER NOT NULL, -- 1-5 scale
    comments TEXT,
    timestamp TEXT NOT NULL,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);
"""

INSERT_CONTENT_FEEDBACK = """
INSERT INTO content_feedback (opportunity_id, rating, comments, timestamp)
VALUES (?, ?, ?, ?);
"""

UPDATE_OPPORTUNITY_SCORES = """
UPDATE opportunities
SET strategic_score = ?, score_breakdown = ?, blueprint_data = ?
WHERE id = ?;
"""

UPDATE_GENERATED_CONTENT_AND_STATUS = """
UPDATE opportunities
SET
    ai_content_json = ?,
    ai_content_model = ?,
    featured_image_url = ?,
    featured_image_local_path = ?,
    in_article_images_data = ?,
    social_media_posts_json = ?,
    final_package_json = ?,
    status = 'generated',
    last_workflow_step = 'generation_complete',
    date_processed = ?,
    total_api_cost = ?
WHERE id = ?;
"""

INSERT_DEFAULT_QUALIFICATION_SETTINGS = """
INSERT INTO qualification_settings (
    client_id, ease_of_ranking_weight, traffic_potential_weight, commercial_intent_weight,
    serp_features_weight, growth_trend_weight, serp_freshness_weight, serp_volatility_weight,
    competitor_weakness_weight, competitor_performance_weight, min_search_volume, max_keyword_difficulty,
    negative_keywords, prohibited_intents, max_y_pixel_threshold,
    max_forum_results_in_top_10, max_ecommerce_results_in_top_10,
    disallowed_page_types_in_top_3
) VALUES (?, 40, 15, 5, 5, 5, 5, 5, 20, 5, 100, 80, 'login,free,cheap', 'navigational', 800, 3, 2, 'E-commerce,Forum')
"""

FAIL_STALE_JOBS = """
UPDATE jobs
SET status = 'failed', error = ?, finished_at = ?
WHERE status = 'running';
"""

GET_ACTIVE_JOBS_BY_CLIENT = "SELECT * FROM jobs WHERE client_id = ? AND status IN ('running', 'pending');"
```

## File: data_mappers/dataforseo_mapper.py
```python
# FILE: data_mappers/dataforseo_mapper.py

from typing import Dict, Any
import logging
from backend.core.utils import parse_datetime_string  # ADDED IMPORT
import json

logger = logging.getLogger(__name__)


class DataForSEOMapper:
    """
    Provides static methods to sanitize and normalize raw DataForSEO API responses
    immediately after they are received, before they enter the main pipeline.
    Ensures consistent data types and handles common API quirks.
    """

    @staticmethod
    def _sanitize_serp_info(serp_info: Dict[str, Any]) -> Dict[str, Any]:
        """Sanitizes the 'serp_info' object, specifically handling 'se_results_count' and datetimes."""
        if isinstance(serp_info.get("se_results_count"), str):
            try:
                serp_info["se_results_count"] = int(serp_info["se_results_count"])
            except (ValueError, TypeError):
                logger.warning(
                    f"Failed to convert 'se_results_count' (was string) to int. Setting to 0. Raw: {serp_info.get('se_results_count')}"
                )
                serp_info["se_results_count"] = 0

        # Sanitize datetime fields
        serp_info["last_updated_time"] = parse_datetime_string(
            serp_info.get("last_updated_time")
        )
        serp_info["previous_updated_time"] = parse_datetime_string(
            serp_info.get("previous_updated_time")
        )

        return serp_info

    @staticmethod
    def sanitize_keyword_data_item(item: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sanitizes a single keyword data item (e.g., from keyword_ideas, keyword_suggestions, related_keywords).
        Applies cleaning to nested structures like 'keyword_info', 'keyword_properties', 'serp_info'.
        """
        if not isinstance(item, dict):
            logger.warning(
                f"Invalid item type received for sanitization: {type(item)}. Skipping."
            )
            return {}

        sanitized_item = item.copy()

        # Sanitize keyword_info
        if isinstance(sanitized_item.get("keyword_info"), dict):
            # Ensure CPC and Competition are floats, defaulting to 0.0 if missing or None
            sanitized_item["keyword_info"]["cpc"] = float(
                sanitized_item["keyword_info"].get("cpc") or 0.0
            )
            sanitized_item["keyword_info"]["competition"] = float(
                sanitized_item["keyword_info"].get("competition") or 0.0
            )

            # Ensure other numeric fields are handled
            sanitized_item["keyword_info"]["search_volume"] = int(
                sanitized_item["keyword_info"].get("search_volume") or 0
            )
            sanitized_item["keyword_info"]["low_top_of_page_bid"] = float(
                sanitized_item["keyword_info"].get("low_top_of_page_bid") or 0.0
            )
            sanitized_item["keyword_info"]["high_top_of_page_bid"] = float(
                sanitized_item["keyword_info"].get("high_top_of_page_bid") or 0.0
            )

            # Sanitize last_updated_time
            sanitized_item["keyword_info"]["last_updated_time"] = parse_datetime_string(
                sanitized_item["keyword_info"].get("last_updated_time")
            )

            # Ensure monthly_searches are properly parsed if they are raw strings
            if isinstance(sanitized_item["keyword_info"].get("monthly_searches"), str):
                try:
                    sanitized_item["keyword_info"]["monthly_searches"] = json.loads(
                        sanitized_item["keyword_info"]["monthly_searches"]
                    )
                except json.JSONDecodeError:
                    logger.warning(
                        f"Failed to parse monthly_searches JSON string for keyword '{sanitized_item.get('keyword')}'. Resetting."
                    )
                    sanitized_item["keyword_info"]["monthly_searches"] = []

            # Ensure individual monthly_searches items are sanitized for type consistency
            if isinstance(sanitized_item["keyword_info"].get("monthly_searches"), list):
                for month_data in sanitized_item["keyword_info"]["monthly_searches"]:
                    if isinstance(month_data, dict):
                        month_data["year"] = int(month_data.get("year") or 0)
                        month_data["month"] = int(month_data.get("month") or 0)
                        month_data["search_volume"] = int(
                            month_data.get("search_volume") or 0
                        )

        # Sanitize keyword_properties
        if isinstance(sanitized_item.get("keyword_properties"), dict):
            sanitized_item["keyword_properties"]["keyword_difficulty"] = int(
                sanitized_item["keyword_properties"].get("keyword_difficulty") or 0
            )

        # Sanitize search_intent_info
        if isinstance(sanitized_item.get("search_intent_info"), dict):
            sanitized_item["search_intent_info"]["foreign_intent"] = (
                sanitized_item["search_intent_info"].get("foreign_intent") or []
            )
            sanitized_item["search_intent_info"]["last_updated_time"] = (
                parse_datetime_string(
                    sanitized_item["search_intent_info"].get("last_updated_time")
                )
            )

        # Sanitize serp_info (crucial for se_results_count string/int issue)
        if isinstance(sanitized_item.get("serp_info"), dict):
            sanitized_item["serp_info"] = DataForSEOMapper._sanitize_serp_info(
                sanitized_item["serp_info"]
            )
            sanitized_item["serp_info"]["serp_item_types"] = (
                sanitized_item["serp_info"].get("serp_item_types") or []
            )

        # Sanitize avg_backlinks_info
        if isinstance(sanitized_item.get("avg_backlinks_info"), dict):
            for key in [
                "backlinks",
                "dofollow",
                "referring_pages",
                "referring_domains",
                "referring_main_domains",
                "rank",
                "main_domain_rank",
            ]:
                sanitized_item["avg_backlinks_info"][key] = float(
                    sanitized_item["avg_backlinks_info"].get(key) or 0.0
                )
            sanitized_item["avg_backlinks_info"]["last_updated_time"] = (
                parse_datetime_string(
                    sanitized_item["avg_backlinks_info"].get("last_updated_time")
                )
            )

        # Keyword Info Normalized with Bing
        for normalized_key in [
            "keyword_info_normalized_with_bing",
            "keyword_info_normalized_with_clickstream",
        ]:
            if isinstance(sanitized_item.get(normalized_key), dict):
                normalized_data = sanitized_item[normalized_key]
                normalized_data["search_volume"] = int(
                    normalized_data.get("search_volume") or 0
                )
                normalized_data["last_updated_time"] = parse_datetime_string(
                    normalized_data.get("last_updated_time")
                )
                if isinstance(normalized_data.get("monthly_searches"), list):
                    for month_data in normalized_data["monthly_searches"]:
                        if isinstance(month_data, dict):
                            month_data["year"] = int(month_data.get("year") or 0)
                            month_data["month"] = int(month_data.get("month") or 0)
                            month_data["search_volume"] = int(
                                month_data.get("search_volume") or 0
                            )

        return sanitized_item

    @staticmethod
    def sanitize_serp_overview_response(serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sanitizes a full SERP overview response from FullSerpAnalyzer.analyze_serp.
        Ensures consistent types for nested items, especially 'pixel_rank_data' and 'top_organic_results'.
        """
        sanitized_serp_data = serp_data.copy()

        # Sanitize top-level date/time fields
        sanitized_serp_data["datetime"] = parse_datetime_string(
            sanitized_serp_data.get("datetime")
        )
        sanitized_serp_data["last_updated_time"] = parse_datetime_string(
            sanitized_serp_data.get("last_updated_time")
        )
        sanitized_serp_data["previous_updated_time"] = parse_datetime_string(
            sanitized_serp_data.get("previous_updated_time")
        )

        # Sanitize pixel_rank_data
        if isinstance(sanitized_serp_data.get("raw_pixel_ranking_data"), list):
            for item in sanitized_serp_data["raw_pixel_ranking_data"]:
                if isinstance(item.get("rectangle"), dict):
                    item["rectangle"]["x"] = float(item["rectangle"].get("x") or 0.0)
                    item["rectangle"]["y"] = float(item["rectangle"].get("y") or 0.0)
                    item["rectangle"]["width"] = float(
                        item["rectangle"].get("width") or 0.0
                    )
                    item["rectangle"]["height"] = float(
                        item["rectangle"].get("height") or 0.0
                    )
                item["rank_absolute"] = int(item.get("rank_absolute") or 0)
                item["rank_group"] = int(item.get("rank_group") or 0)

        if sanitized_serp_data.get("first_organic_y_pixel") is not None:
            sanitized_serp_data["first_organic_y_pixel"] = float(
                sanitized_serp_data["first_organic_y_pixel"]
            )

        # Sanitize top_organic_results
        if isinstance(sanitized_serp_data.get("top_organic_results"), list):
            for result in sanitized_serp_data["top_organic_results"]:
                result["rank"] = int(result.get("rank") or 0)
                if isinstance(result.get("rating"), dict):
                    result["rating"]["value"] = float(
                        result["rating"].get("value") or 0.0
                    )
                    result["rating"]["votes_count"] = int(
                        result["rating"].get("votes_count") or 0
                    )
                    result["rating"]["rating_max"] = int(
                        result["rating"].get("rating_max") or 0
                    )

                # Ensure about_this_result_search_terms and related_terms are lists
                result["about_this_result_search_terms"] = (
                    result.get("about_this_result_search_terms") or []
                )
                result["about_this_result_related_terms"] = (
                    result.get("about_this_result_related_terms") or []
                )

        sanitized_serp_data["people_also_ask"] = (
            sanitized_serp_data.get("people_also_ask") or []
        )
        sanitized_serp_data["related_searches"] = (
            sanitized_serp_data.get("related_searches") or []
        )
        sanitized_serp_data["extracted_serp_features"] = (
            sanitized_serp_data.get("extracted_serp_features") or []
        )

        if isinstance(sanitized_serp_data.get("ai_overview_items"), list):
            for ai_item in sanitized_serp_data["ai_overview_items"]:
                ai_item["rank_group"] = int(ai_item.get("rank_group") or 0)
                ai_item["rank_absolute"] = int(ai_item.get("rank_absolute") or 0)
                if isinstance(ai_item.get("references"), list):
                    for ref in ai_item["references"]:
                        ref["date"] = parse_datetime_string(ref.get("date"))
                        ref["timestamp"] = parse_datetime_string(ref.get("timestamp"))
                if isinstance(ai_item.get("table"), dict):
                    if not isinstance(ai_item["table"].get("table_header"), list):
                        ai_item["table"]["table_header"] = []
                    if not isinstance(ai_item["table"].get("table_content"), list):
                        ai_item["table"]["table_content"] = []

        return sanitized_serp_data

    @staticmethod
    def sanitize_onpage_data_item(item: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sanitizes a single OnPage API response item from `get_onpage_data_for_urls`.
        Ensures consistent data types for numeric fields, especially in 'meta' and 'page_timing'.
        """
        sanitized_item = item.copy()

        # Sanitize meta fields
        meta = sanitized_item.get("meta", {})
        if isinstance(meta, dict):
            meta["charset"] = int(meta.get("charset") or 0)
            meta["internal_links_count"] = int(meta.get("internal_links_count") or 0)
            meta["external_links_count"] = int(meta.get("external_links_count") or 0)
            meta["inbound_links_count"] = int(meta.get("inbound_links_count") or 0)
            meta["images_count"] = int(meta.get("images_count") or 0)
            meta["images_size"] = int(meta.get("images_size") or 0)
            meta["scripts_count"] = int(meta.get("scripts_count") or 0)
            meta["scripts_size"] = int(meta.get("scripts_size") or 0)
            meta["stylesheets_count"] = int(meta.get("stylesheets_count") or 0)
            meta["stylesheets_size"] = int(meta.get("stylesheets_size") or 0)
            meta["title_length"] = int(meta.get("title_length") or 0)
            meta["description_length"] = int(meta.get("description_length") or 0)
            meta["render_blocking_scripts_count"] = int(
                meta.get("render_blocking_scripts_count") or 0
            )
            meta["render_blocking_stylesheets_count"] = int(
                meta.get("render_blocking_stylesheets_count") or 0
            )
            meta["cumulative_layout_shift"] = float(
                meta.get("cumulative_layout_shift") or 0.0
            )

            # Sanitize date/time fields
            meta["last_updated_time"] = parse_datetime_string(
                meta.get("last_updated_time")
            )

            content_meta = meta.get("content", {})
            if isinstance(content_meta, dict):
                content_meta["plain_text_size"] = int(
                    content_meta.get("plain_text_size") or 0
                )
                content_meta["plain_text_rate"] = float(
                    content_meta.get("plain_text_rate") or 0.0
                )
                content_meta["plain_text_word_count"] = int(
                    content_meta.get("plain_text_word_count") or 0
                )
                content_meta["automated_readability_index"] = float(
                    content_meta.get("automated_readability_index") or 0.0
                )
                content_meta["coleman_liau_readability_index"] = float(
                    content_meta.get("coleman_liau_readability_index") or 0.0
                )
                content_meta["dale_chall_readability_index"] = float(
                    content_meta.get("dale_chall_readability_index") or 0.0
                )
                content_meta["flesch_kincaid_readability_index"] = float(
                    content_meta.get("flesch_kincaid_readability_index") or 0.0
                )
                content_meta["smog_readability_index"] = float(
                    content_meta.get("smog_readability_index") or 0.0
                )
                content_meta["description_to_content_consistency"] = float(
                    content_meta.get("description_to_content_consistency") or 0.0
                )
                content_meta["title_to_content_consistency"] = float(
                    content_meta.get("title_to_content_consistency") or 0.0
                )
                content_meta["meta_keywords_to_content_consistency"] = float(
                    content_meta.get("meta_keywords_to_content_consistency") or 0.0
                )
            sanitized_item["meta"] = meta

        # Sanitize page_timing fields
        page_timing = sanitized_item.get("page_timing", {})
        if isinstance(page_timing, dict):
            page_timing["time_to_interactive"] = int(
                page_timing.get("time_to_interactive") or 0
            )
            page_timing["dom_complete"] = int(page_timing.get("dom_complete") or 0)
            page_timing["largest_contentful_paint"] = float(
                page_timing.get("largest_contentful_paint") or 0.0
            )
            page_timing["first_input_delay"] = float(
                page_timing.get("first_input_delay") or 0.0
            )
            page_timing["connection_time"] = int(
                page_timing.get("connection_time") or 0
            )
            page_timing["time_to_secure_connection"] = int(
                page_timing.get("time_to_secure_connection") or 0
            )
            page_timing["request_sent_time"] = int(
                page_timing.get("request_sent_time") or 0
            )
            page_timing["waiting_time"] = int(page_timing.get("waiting_time") or 0)
            page_timing["download_time"] = int(page_timing.get("download_time") or 0)
            page_timing["duration_time"] = int(page_timing.get("duration_time") or 0)
            page_timing["fetch_start"] = int(page_timing.get("fetch_start") or 0)
            page_timing["fetch_end"] = int(page_timing.get("fetch_end") or 0)
            sanitized_item["page_timing"] = page_timing

        # Sanitize top-level numeric fields
        sanitized_item["onpage_score"] = float(
            sanitized_item.get("onpage_score") or 0.0
        )
        sanitized_item["total_dom_size"] = int(
            sanitized_item.get("total_dom_size") or 0
        )
        sanitized_item["size"] = int(sanitized_item.get("size") or 0)
        sanitized_item["encoded_size"] = int(sanitized_item.get("encoded_size") or 0)
        sanitized_item["total_transfer_size"] = int(
            sanitized_item.get("total_transfer_size") or 0
        )
        sanitized_item["url_length"] = int(sanitized_item.get("url_length") or 0)
        sanitized_item["relative_url_length"] = int(
            sanitized_item.get("relative_url_length") or 0
        )

        # Sanitize fetch_time
        sanitized_item["fetch_time"] = parse_datetime_string(
            sanitized_item.get("fetch_time")
        )

        # Sanitize cache_control ttl
        if isinstance(sanitized_item.get("cache_control"), dict):
            sanitized_item["cache_control"]["ttl"] = int(
                sanitized_item["cache_control"].get("ttl") or 0
            )

        # Sanitize last_modified dates
        if isinstance(sanitized_item.get("last_modified"), dict):
            sanitized_item["last_modified"]["header"] = parse_datetime_string(
                sanitized_item["last_modified"].get("header")
            )
            sanitized_item["last_modified"]["sitemap"] = parse_datetime_string(
                sanitized_item["last_modified"].get("sitemap")
            )
            sanitized_item["last_modified"]["meta_tag"] = parse_datetime_string(
                sanitized_item["last_modified"].get("meta_tag")
            )

        return sanitized_item
```

## File: data_mappers/keyword_data_mapper.py
```python
# data_mappers/keyword_data_mapper.py

from typing import Dict, Any
from data_access.models import KeywordData


def map_keyword_data(raw_data: Dict[str, Any]) -> KeywordData:
    """Maps raw keyword data from the DataForSEO API to the KeywordData model."""
    full_data = raw_data.get("full_data", {})
    keyword_info = full_data.get("keyword_info", {})
    keyword_properties = full_data.get("keyword_properties", {})
    search_intent_info = full_data.get("search_intent_info", {})

    return KeywordData(
        keyword=raw_data.get("keyword"),
        search_volume=keyword_info.get("search_volume"),
        keyword_difficulty=keyword_properties.get("keyword_difficulty"),
        cpc=keyword_info.get("cpc"),
        main_intent=search_intent_info.get("main_intent"),
        search_volume_trend=keyword_info.get("search_volume_trend"),
        core_keyword=keyword_properties.get("core_keyword"),
    )
```

## File: data_mappers/serp_overview_mapper.py
```python
# data_mappers/serp_overview_mapper.py

from typing import Dict, Any
from data_access.models import SerpOverview


def map_serp_overview(raw_data: Dict[str, Any]) -> SerpOverview:
    """Maps raw SERP data from the DataForSEO API to the SerpOverview model."""
    serp_info = raw_data.get("serp_info", {})
    avg_backlinks_info = raw_data.get("avg_backlinks_info", {})

    return SerpOverview(
        serp_has_featured_snippet="featured_snippet"
        in serp_info.get("serp_item_types", []),
        serp_has_video_results="video" in serp_info.get("serp_item_types", []),
        serp_has_ai_overview="ai_overview" in serp_info.get("serp_item_types", []),
        people_also_ask=serp_info.get("people_also_ask", []),
        ai_overview_content=serp_info.get("ai_overview_content"),
        featured_snippet_content=serp_info.get("featured_snippet_content"),
        avg_referring_domains_top5_organic=avg_backlinks_info.get("referring_domains"),
        avg_main_domain_rank_top5_organic=avg_backlinks_info.get("main_domain_rank"),
        serp_last_updated_days_ago=serp_info.get("last_updated_time"),
        dominant_content_format=serp_info.get("dominant_content_format"),
    )
```

## File: external_apis/dataforseo_client_v2.py
```python
"""
This module provides a simplified and corrected client for the DataForSEO OnPage API,
focusing exclusively on the `instant_pages` endpoint.
"""

import base64
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import requests
from urllib.parse import urlparse
import hashlib
from backend.data_access.database_manager import DatabaseManager
from backend.data_mappers.dataforseo_mapper import DataForSEOMapper


class DataForSEOClientV2:
    """
    A simplified client for the DataForSEO API, using only the `instant_pages` endpoint.
    """

    # W17 FIX: Move hardcoded endpoints to constants
    LABS_KEYWORD_IDEAS = "dataforseo_labs/google/keyword_ideas/live"
    LABS_KEYWORD_SUGGESTIONS = "dataforseo_labs/google/keyword_suggestions/live"
    LABS_RELATED_KEYWORDS = "dataforseo_labs/google/related_keywords/live"
    LABS_RANKED_KEYWORDS = "dataforseo_labs/google/ranked_keywords/live"
    LABS_COMPETITORS_DOMAIN = "dataforseo_labs/google/competitors_domain/live"
    SERP_ADVANCED = "serp/google/organic/live/advanced"
    ONPAGE_INSTANT_PAGES = "on_page/instant_pages"
    ONPAGE_CONTENT_PARSING = "on_page/content_parsing/live"  # Add this line

    KEYWORD_IDEAS_MODE_LIMIT = 10
    KEYWORD_SUGGESTIONS_MODE_LIMIT = 100
    RELATED_KEYWORDS_MODE_LIMIT = 100

    def __init__(
        self,
        login: str,
        password: str,
        db_manager: DatabaseManager,
        config: Dict[str, Any],
        enable_cache: bool = True,
    ):
        self.base_url = "https://api.dataforseo.com/v3"
        if not login or not password:
            raise ValueError("DataForSEO API login and password cannot be empty.")
        credentials = f"{login}:{password}"
        self.headers = {
            "Authorization": f"Basic {base64.b64encode(credentials.encode()).decode()}",
            "Content-Type": "application/json",
        }
        self.logger = logging.getLogger(self.__class__.__name__)
        self.db_manager = db_manager
        self.config = config  # Store the config object
        self.enable_cache = enable_cache

    def _enforce_api_filter_limit(
        self, filters: Optional[List[Any]], max_limit: int = 8
    ) -> Optional[List[Any]]:
        """
        Enforces the API filter limit (max 8 conditions) by truncating the filter list.
        This is a backend safeguard if frontend validation is bypassed.
        """
        if not filters:
            return None

        # Count actual filter conditions (which are lists, not "and"/"or" strings)
        condition_elements = [f for f in filters if isinstance(f, list)]
        if len(condition_elements) <= max_limit:
            return filters  # No need to truncate

        self.logger.warning(
            f"Backend safeguard: API filter list exceeds {max_limit} conditions ({len(condition_elements)} found). Truncating to the first {max_limit} conditions."
        )

        truncated_filters = []
        condition_count = 0
        for item in filters:
            if isinstance(item, list):  # This is a condition
                if condition_count < max_limit:
                    truncated_filters.append(item)
                    condition_count += 1
                else:
                    break  # Stop adding conditions
            elif truncated_filters and isinstance(truncated_filters[-1], list):
                # Add logical operator only if it follows a condition and we're still building
                truncated_filters.append(item)

        # Ensure the list doesn't end with a dangling logical operator
        if (
            truncated_filters
            and isinstance(truncated_filters[-1], str)
            and truncated_filters[-1].lower() in ["and", "or"]
        ):
            truncated_filters.pop()

        return truncated_filters

    def _post_request(
        self, endpoint: str, data: List[Dict[str, Any]], tag: Optional[str] = None
    ) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Handles the actual POST request to the API, with retries and exponential backoff for rate limits.
        """
        cache_key_string = json.dumps(
            {
                "endpoint": endpoint,
                "data": data,
                "filters": data[0].get("filters") if data else None,
            },
            sort_keys=True,
        )
        cache_key = hashlib.md5(cache_key_string.encode("utf-8")).hexdigest()

        if self.enable_cache:
            cached_response = self.db_manager.get_api_cache(cache_key)
            if cached_response:
                self.logger.info(f"Cache HIT for endpoint {endpoint} with tag '{tag}'.")
                return cached_response, 0.0

        self.logger.info(
            f"Cache MISS for endpoint {endpoint} with tag '{tag}'. Making live API call."
        )

        if tag:
            for task_item in data:
                if isinstance(task_item, dict):
                    task_item["tag"] = tag

        full_url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
        self.logger.info(
            f"Making POST request to {full_url} with data: {json.dumps(data)}"
        )
        retries = 3
        backoff_factor = 5

        for attempt in range(retries):
            try:
                response = requests.post(
                    full_url, headers=self.headers, data=json.dumps(data), timeout=120
                )

                # W20 FIX: Early exit for critical top-level HTTP errors
                if response.status_code >= 500:
                    self.logger.error(
                        f"DataForSEO API returned a server error ({response.status_code}). Aborting after {attempt + 1} attempts."
                    )
                    return None, 0.0  # Do not retry on server errors

                response.raise_for_status()  # Raise HTTPError for 4xx client errors

                response_json = response.json()

                # W20 FIX: Check top-level status_code from DataForSEO
                if response_json.get("status_code") != 20000:
                    self.logger.error(
                        f"DataForSEO API returned non-20000 status_code: {response_json.get('status_code')} - {response_json.get('status_message')}"
                    )
                    # No retry for auth errors, etc.
                    if response_json.get("status_code") in [40101, 40102, 40103]:
                        return None, 0.0

                # W20 FIX: Log critical task-level errors
                if response_json.get("tasks_error", 0) > 0:
                    for task in response_json.get("tasks", []):
                        if task.get("status_code") != 20000:
                            # Log specific, known critical errors
                            if task.get("status_code") == 40501:  # Duplicate crawl host
                                self.logger.critical(
                                    f"CRITICAL API ERROR (40501): Duplicate crawl host detected for URL {task.get('data', {}).get('url')}. This batch is invalid."
                                )
                            else:
                                self.logger.warning(
                                    f"Task-level error for {task.get('data', {}).get('url', 'N/A')}: {task.get('status_code')} - {task.get('status_message')}"
                                )

                cost = response_json.get("cost", 0.0)

                if self.enable_cache:
                    self.db_manager.set_api_cache(cache_key, response_json)

                return response_json, cost

            except requests.exceptions.HTTPError as e:
                # This will now primarily catch 4xx errors
                if (
                    response.status_code == 429 and attempt < retries - 1
                ):  # Specifically handle rate limits
                    wait_time = backoff_factor * (2**attempt)
                    self.logger.warning(
                        f"Rate limit exceeded (429). Retrying in {wait_time} seconds... (Attempt {attempt + 1}/{retries})"
                    )
                    time.sleep(wait_time)
                    continue
                else:
                    self.logger.error(
                        f"HTTP error during DataForSEO API request to {full_url}: {e}",
                        exc_info=True,
                    )
                    return None, 0.0
            except requests.exceptions.RequestException as e:
                self.logger.error(
                    f"Network error during DataForSEO API request to {full_url}: {e}",
                    exc_info=True,
                )
                if attempt < retries - 1:
                    time.sleep(backoff_factor * (2**attempt))
                    continue
                return None, 0.0

        return None, 0.0

    def _prioritize_and_limit_filters(self, filters: Optional[List[Any]]) -> List[Any]:
        """Enforces the 8-filter maximum rule by prioritizing essential filters."""
        if not filters:
            return []

        # Count actual filter conditions (excluding logical operators like "and", "or")
        condition_count = sum(1 for f in filters if isinstance(f, list))

        # If already within the limit, return as is.
        if condition_count <= 8:
            return filters

        self.logger.warning(
            f"Filter list exceeds 8 conditions ({condition_count} found). Prioritizing essential filters."
        )

        # Simple prioritization logic: Keep filters based on field name presence
        # Prioritized fields (essential for targeting): keyword_difficulty, search_volume, main_intent, competition_level, cpc, competition
        PRIORITIZED_FIELDS = [
            "keyword_difficulty",
            "search_volume",
            "main_intent",
            "competition_level",
            "cpc",
            "competition",
        ]

        prioritized_filters = []
        other_filters = []

        # Iterate through the filters to separate prioritized from others
        for element in filters:
            if isinstance(element, list) and len(element) >= 3:
                # Assuming element[0] is the field name, like "keyword_info.search_volume"
                field_name = element[0].lower()
                is_priority = any(
                    p_field in field_name for p_field in PRIORITIZED_FIELDS
                )

                if is_priority:
                    prioritized_filters.append(element)
                else:
                    other_filters.append(element)
            else:
                # Logical operators ('and', 'or') will be re-added later if needed
                pass

        # Combine prioritized filters (up to 8 slots)
        limited_filters_list = []  # Only actual conditions

        # 1. Add prioritized filters first
        for f in prioritized_filters:
            if len(limited_filters_list) < 8:
                limited_filters_list.append(f)
            else:
                break

        # 2. Fill remaining slots with non-prioritized filters if space permits
        for f in other_filters:
            if len(limited_filters_list) < 8:
                limited_filters_list.append(f)
            else:
                break

        # Reconstruct the filter list with "and" operators
        final_filters_structure = []
        for i, filt in enumerate(limited_filters_list):
            final_filters_structure.append(filt)
            if i < len(limited_filters_list) - 1:
                final_filters_structure.append("and")

        return final_filters_structure

    def get_technical_onpage_data(
        self, urls: List[str], client_cfg: Dict[str, Any]
    ) -> Tuple[Optional[List[Dict[str, Any]]], float]:
        """
        Performs a batch OnPage scan using the Instant Pages endpoint to get technical SEO data.
        """
        if not urls:
            return [], 0.0

        self.logger.info(
            f"Fetching OnPage data for {len(urls)} URLs with device preset '{client_cfg.get('device', 'desktop')}' using Instant Pages..."
        )

        endpoint = self.ONPAGE_INSTANT_PAGES
        # Group URLs into batches that comply with max_domains and max_tasks
        url_batches = self._group_urls_by_domain(
            urls,
            max_domains=client_cfg.get("onpage_max_domains_per_request", 5),
            batch_size=client_cfg.get("onpage_max_tasks_per_request", 20),
        )

        all_results = []
        total_cost = 0.0

        for i, batch in enumerate(url_batches):
            post_data = []
            for url in batch:
                task_data = {
                    "url": url,
                    # W8 FIX: Use configured value, not hardcoded False
                    "enable_browser_rendering": client_cfg.get(
                        "onpage_enable_browser_rendering", False
                    ),
                    # W5 FIX: Inject critical analysis parameters
                    "validate_micromarkup": client_cfg.get(
                        "onpage_validate_micromarkup", False
                    ),
                    "return_despite_timeout": client_cfg.get(
                        "onpage_return_despite_timeout", False
                    ),
                    "check_spell": client_cfg.get("onpage_check_spell", False),
                    # W7 FIX: Inject language header
                    "accept_language": client_cfg.get("onpage_accept_language"),
                    # Include configured user agent if available
                    "custom_user_agent": client_cfg.get("onpage_custom_user_agent"),
                    "switch_pool": client_cfg.get("onpage_enable_switch_pool", False),
                }

                # Add ip_pool_for_scan if it's in the client config
                if "ip_pool_for_scan" in client_cfg:
                    task_data["ip_pool_for_scan"] = client_cfg["ip_pool_for_scan"]

                # W7 FIX & W17 FIX: Include custom screen resolution if rendering is enabled and validate its range
                screen_ratio = client_cfg.get("onpage_browser_screen_resolution_ratio")
                if screen_ratio is not None and (
                    task_data["enable_browser_rendering"]
                    or client_cfg.get("onpage_enable_javascript", False)
                ):
                    # W17 FIX: Validate the range [0.5, 3.0]
                    if 0.5 <= screen_ratio <= 3.0:
                        task_data["browser_screen_resolution_ratio"] = screen_ratio
                    else:
                        self.logger.error(
                            f"Invalid screen ratio configured: {screen_ratio}. Must be between 0.5 and 3.0. Omitting parameter."
                        )
                        # Parameter will be omitted if outside valid range, relying on DataForSEO default.

                # W9 FIX: Include custom checks threshold if provided in config (continues from Task 4.4)
                thresholds_str = client_cfg.get("onpage_custom_checks_thresholds")
                if thresholds_str:
                    try:
                        task_data["checks_threshold"] = json.loads(thresholds_str)
                    except json.JSONDecodeError:
                        self.logger.error(
                            "Failed to parse onpage_custom_checks_thresholds JSON from config. Using default thresholds."
                        )

                # W12 FIX: Include custom JS if enabled (continues from Task 12.4)
                if client_cfg.get("onpage_enable_custom_js", False):
                    custom_js_script = client_cfg.get("onpage_custom_js")
                    if custom_js_script:
                        task_data["custom_js"] = custom_js_script

                # Remove keys if their value is None to maintain a clean API request
                task_data = {k: v for k, v in task_data.items() if v is not None}
                post_data.append(task_data)

            # --- Attempt 1 for the batch ---
            response, cost = self._post_request(
                endpoint, post_data, tag=f"onpage_instant_pages_content:batch{i + 1}"
            )
            total_cost += cost

            current_batch_results = []
            failed_urls_in_batch = []

            if response and response.get("tasks"):
                for task in response["tasks"]:
                    task_url = task.get("data", {}).get("url")

                    # Explicit Failure Criteria Check: Task failed OR result is malformed/empty
                    if (
                        task.get("status_code") != 20000
                        or not task.get("result")
                        or not task["result"][0].get("items")
                    ):
                        self.logger.warning(
                            f"Task for URL {task_url} failed/malformed response (Status: {task.get('status_code')}). Queuing for retry."
                        )
                        failed_urls_in_batch.append(task_url)
                    else:
                        for result_item in task["result"]:
                            if result_item and result_item.get("items"):
                                current_batch_results.extend(
                                    [
                                        DataForSEOMapper.sanitize_onpage_data_item(it)
                                        for it in result_item["items"]
                                    ]
                                )  # ADDED SANITIZATION
            else:
                self.logger.error(
                    f"Failed to get any response for instant_pages batch starting with URL: {batch[0]}"
                )
                failed_urls_in_batch.extend(batch)

            # --- Retry mechanism for failed URLs in this batch ---
            if failed_urls_in_batch:
                self.logger.info(
                    f"Retrying {len(failed_urls_in_batch)} failed URLs from batch {i + 1}..."
                )

                should_switch_pool = client_cfg.get("onpage_enable_switch_pool", False)

                # W15 FIX: Re-group failed URLs to enforce the max_domains limit (5 domains max per request)
                max_domains_per_retry = client_cfg.get(
                    "onpage_max_domains_per_request", 5
                )
                retry_batches = self._group_urls_by_domain(
                    failed_urls_in_batch,
                    max_domains=max_domains_per_retry,
                    batch_size=client_cfg.get("onpage_max_tasks_per_request", 20),
                )

                current_retry_cost = 0.0

                for retry_batch in retry_batches:
                    retry_post_data = []
                    for url in retry_batch:
                        # Reconstruct task_data using original structure but force `return_despite_timeout`
                        original_task_data = next(
                            (item for item in post_data if item.get("url") == url), {}
                        )
                        retry_task_data = {
                            **original_task_data,
                            "return_despite_timeout": True,
                        }

                        if should_switch_pool:
                            retry_task_data["switch_pool"] = True

                        retry_post_data.append(retry_task_data)

                    retry_response, retry_cost = self._post_request(
                        endpoint,
                        retry_post_data,
                        tag=f"onpage_instant_pages_content:retry_batch{i + 1}",
                    )
                    current_retry_cost += retry_cost

                    # Process retry_response (existing logic, moved and adapted)
                    if retry_response and retry_response.get("tasks"):
                        for task in retry_response["tasks"]:
                            task_url = task.get("data", {}).get("url")
                            if task.get("status_code") == 20000 and task.get("result"):
                                for result_item in task["result"]:
                                    if result_item and result_item.get("items"):
                                        current_batch_results.extend(
                                            result_item["items"]
                                        )
                            else:
                                self.logger.warning(
                                    f"Retry task for URL {task_url} failed again (Status: {task.get('status_code')})."
                                )

                total_cost += current_retry_cost

            all_results.extend(current_batch_results)
        return all_results, total_cost

    def get_content_onpage_data(
        self,
        urls: List[str],
        client_cfg: Dict[str, Any],
        enable_javascript: bool = False,
    ) -> Tuple[Optional[List[Dict[str, Any]]], float]:
        """
        Performs OnPage scans using the Content Parsing endpoint, with control over JS rendering.
        This function now sends requests for multiple URLs in parallel using a thread pool
        for improved performance, as the endpoint does not support batch processing.
        """
        if not urls:
            return [], 0.0

        self.logger.info(
            f"Fetching OnPage Content Parsing data for {len(urls)} URLs with enable_javascript={enable_javascript} in parallel..."
        )

        all_tasks = []
        total_cost = 0.0

        # Helper function to be executed in each thread
        def _fetch_single_url(url: str) -> Tuple[Optional[Dict[str, Any]], float]:
            post_data = [
                {
                    "url": url,
                    "enable_javascript": enable_javascript,
                    "store_raw_html": True,
                    "markdown_view": True,
                    "disable_cookie_popup": client_cfg.get(
                        "onpage_disable_cookie_popup", True
                    ),
                }
            ]
            tag = f"onpage_content_parsing_js_{str(enable_javascript).lower()}:{urlparse(url).netloc}"
            return self._post_request(self.ONPAGE_CONTENT_PARSING, post_data, tag=tag)

        # Use a ThreadPoolExecutor to send requests concurrently
        # The number of workers can be tuned, but 5 is a safe default to avoid overwhelming the API
        with ThreadPoolExecutor(max_workers=5) as executor:
            # map executes the function for each item in the urls list
            future_to_url = {
                executor.submit(_fetch_single_url, url): url for url in urls
            }
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    response, cost = future.result()
                    total_cost += cost
                    if response and response.get("tasks"):
                        all_tasks.extend(response["tasks"])
                    else:
                        self.logger.error(
                            f"Failed to get a valid response for content_parsing for URL: {url}"
                        )
                        all_tasks.append(
                            {
                                "status_code": 50000,
                                "status_message": "No response from API",
                                "data": {"url": url},
                            }
                        )
                except Exception as exc:
                    self.logger.error(f"{url} generated an exception: {exc}")
                    all_tasks.append(
                        {
                            "status_code": 50001,
                            "status_message": f"Request generated an exception: {exc}",
                            "data": {"url": url},
                        }
                    )

        if all_tasks:
            return all_tasks, total_cost

        self.logger.error(
            f"Failed to get any response for any of the {len(urls)} URLs."
        )
        return [
            {
                "status_code": 50000,
                "status_message": "No response from API",
                "data": {"url": url},
            }
            for url in urls
        ], 0.0

    def get_serp_results(
        self,
        keyword: str,
        location_code: int,
        language_code: str,
        client_cfg: Dict[str, Any],
        serp_call_params: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Fetches the advanced SERP data for a single keyword, with caching.
        """
        device = client_cfg.get("device", "desktop")
        self.logger.info(
            f"Fetching live SERP results for '{keyword}' on device '{device}'..."
        )
        endpoint = self.SERP_ADVANCED
        base_serp_params = {
            "keyword": keyword,
            "location_code": location_code,
            "language_code": language_code,
            "group_organic_results": False,  # NEW: Ensure no grouping for full analysis
        }
        if serp_call_params:
            base_serp_params.update(serp_call_params)

        if client_cfg.get("calculate_rectangles", False):
            base_serp_params["calculate_rectangles"] = True

        base_serp_params["depth"] = int(base_serp_params.get("depth", 10))

        # Add advanced features from client_cfg if they are enabled.
        if client_cfg.get("calculate_rectangles", False):
            base_serp_params["calculate_rectangles"] = True

        paa_depth = client_cfg.get("people_also_ask_click_depth", 0)
        if isinstance(paa_depth, int) and 1 <= paa_depth <= 4:
            base_serp_params["people_also_ask_click_depth"] = paa_depth

        # W3 FIX: Add support for loading AI overview asynchronously
        if client_cfg.get("load_async_ai_overview", False):
            base_serp_params["load_async_ai_overview"] = True

        # W11 FIX: Include URL removal parameters
        remove_params_str = client_cfg.get("serp_remove_from_url_params")
        if remove_params_str:
            # Assuming config value is a comma-separated string of parameters
            params_list = [p.strip() for p in remove_params_str.split(",") if p.strip()]

            # W14 FIX: Validate and clip URL removal parameters (max 10)
            if len(params_list) > 10:
                self.logger.warning(
                    f"Configuration defined {len(params_list)} parameters for removal, but DataForSEO limit is 10. Truncating."
                )

            base_serp_params["remove_from_url"] = params_list[:10]

        # Ensure device and OS are passed based on client config
        device = client_cfg.get("device", "desktop")
        os_name = client_cfg.get("os", "windows")

        # Adjust OS if device is mobile for compatibility
        if device == "mobile" and os_name not in ["android", "ios"]:
            os_name = "android"

        base_serp_params["device"] = device
        base_serp_params["os"] = os_name

        request_tag = f"serp_advanced:{keyword[:50]}"
        response, cost = self._post_request(
            endpoint, [base_serp_params], tag=request_tag
        )

        if response and response.get("tasks") and response["tasks"][0].get("result"):
            result_data = response["tasks"][0]["result"][0]
            sanitized_result_data = DataForSEOMapper.sanitize_serp_overview_response(
                result_data
            )  # ADDED SANITIZATION
            return sanitized_result_data, cost

        return None, cost

    def post_with_paging(
        self,
        endpoint: str,
        initial_task: Dict[str, Any],
        max_pages: int,
        paginated: bool = True,
        tag: Optional[str] = None,
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Executes a POST request and, if paginated=True, recursively retrieves all results using the correct pagination method.
        """
        all_items = []
        total_cost = 0.0
        current_task = initial_task.copy()

        if "filters" in current_task and (
            current_task["filters"] is None or len(current_task["filters"]) == 0
        ):
            current_task.pop("filters")

        page_count = 0
        previous_offset_token = None  # ADDED: For infinite loop prevention

        while True:
            if not paginated and page_count > 0:
                break

            if page_count >= max_pages:
                self.logger.info(
                    f"Reached max page limit ({max_pages}) for endpoint {endpoint}."
                )
                break

            page_count += 1
            self.logger.info(
                f"Submitting task to {endpoint} (Page {page_count}/{max_pages})..."
            )

            request_tag = (
                tag + f":p{page_count}"
                if tag
                else endpoint.split("/")[-1] + f":p{page_count}"
            )
            response, cost = self._post_request(
                endpoint, [current_task], tag=request_tag
            )
            total_cost += cost

            if (
                not response
                or response.get("status_code") != 20000
                or response.get("tasks_error", 0) > 0
            ):
                self.logger.error(
                    f"Paging for endpoint {endpoint} failed on page {page_count}. Response: {response}"
                )
                break

            tasks = response.get("tasks", [])
            if not tasks or "result" not in tasks[0]:
                self.logger.info(
                    f"No 'result' field in the first task for endpoint {endpoint} on page {page_count}. Stopping pagination."
                )
                break

            task_result = tasks[0].get("result")
            if not task_result:
                self.logger.info(
                    f"Task result is empty for endpoint {endpoint} on page {page_count}. Stopping pagination."
                )
                break

            items_count = 0
            offset_token = None
            if task_result and isinstance(task_result, list) and len(task_result) > 0:
                offset_token = task_result[0].get("offset_token")
                for result_item in task_result:
                    # Capture items from the main list
                    items = result_item.get("items")
                    if items:
                        items_count += len(items)
                        all_items.extend(items)

                    # Capture the valuable seed_keyword_data if it exists (from Keyword Suggestions)
                    # and if this is specifically from the Keyword Suggestions API.
                    # This avoids adding the same seed_keyword twice if it was also in the 'items' list
                    # or if the main search (e.g., Keyword Ideas) already returned it.
                    if endpoint == self.LABS_KEYWORD_SUGGESTIONS:
                        seed_data = result_item.get("seed_keyword_data")
                        if isinstance(seed_data, dict) and seed_data.get("keyword"):
                            seed_data["discovery_source"] = (
                                "keyword_suggestions_seed"  # Mark its source
                            )
                            all_items.append(
                                DataForSEOMapper.sanitize_keyword_data_item(seed_data)
                            )  # ADDED SANITIZATION

            if not paginated or page_count >= max_pages or items_count == 0:
                break

            if offset_token:
                # ADDED: Infinite loop prevention check
                if offset_token == previous_offset_token:
                    self.logger.warning(
                        f"API returned a duplicate offset_token. Halting pagination to prevent infinite loop for endpoint {endpoint}."
                    )
                    break
                previous_offset_token = offset_token  # Update the previous token

                current_task = {
                    "offset_token": offset_token,
                    "limit": initial_task.get("limit", 1000),
                }
                if "filters" in initial_task and initial_task["filters"] is not None:
                    current_task["filters"] = initial_task["filters"]
                if "order_by" in initial_task and initial_task["order_by"] is not None:
                    current_task["order_by"] = initial_task["order_by"]

                time.sleep(1)
            else:
                break

        return all_items, total_cost

    def _group_urls_by_domain(
        self, urls: List[str], max_domains: int = 5, batch_size: int = 20
    ) -> List[List[str]]:
        """
        Groups URLs into batches that comply with the identical-domain limit and batch size.
        """
        from collections import defaultdict, deque

        domain_cache = {}

        def get_domain(url):
            if url not in domain_cache:
                try:
                    domain_cache[url] = urlparse(url).netloc
                except Exception:
                    self.logger.warning(f"Could not parse domain for URL: {url}")
                    domain_cache[url] = url
            return domain_cache[url]

        # Use deques for efficient popping from the left
        domain_groups = defaultdict(deque)
        for url in urls:
            domain_groups[get_domain(url)].append(url)

        batches = []

        # Continue as long as there are URLs to process
        while sum(len(q) for q in domain_groups.values()) > 0:
            current_batch = []
            domain_counts = defaultdict(int)

            # A set of domains that have reached their limit for the current batch
            exhausted_domains = set()

            # Loop until the batch is full or no more URLs can be added
            while len(current_batch) < batch_size:
                url_added_in_this_pass = False

                # Iterate through domains that have URLs and are not exhausted for this batch
                for domain, url_queue in domain_groups.items():
                    if len(current_batch) >= batch_size:
                        break

                    if url_queue and domain not in exhausted_domains:
                        if domain_counts[domain] < max_domains:
                            current_batch.append(url_queue.popleft())
                            domain_counts[domain] += 1
                            url_added_in_this_pass = True
                        else:
                            exhausted_domains.add(domain)

                # If we went through all domains and couldn't add a single URL, stop filling this batch
                if not url_added_in_this_pass:
                    break

            if current_batch:
                batches.append(current_batch)
            # If we created an empty batch and there are still urls, something is wrong.
            # This should not happen with this logic, but as a safeguard:
            elif sum(len(q) for q in domain_groups.values()) > 0:
                self.logger.error(
                    "Could not form a valid batch. Breaking to prevent infinite loop."
                )
                break

        self.logger.info(f"Grouped {len(urls)} URLs into {len(batches)} batches.")
        return batches

    def _convert_filters_to_api_format(
        self, filters: Optional[List[Dict[str, Any]]]
    ) -> Optional[List[Any]]:
        if not filters:
            return None

        api_filters = []
        for i, f in enumerate(filters):
            api_filters.append([f["field"], f["operator"], f["value"]])
            if i < len(filters) - 1:
                api_filters.append("and")
        return api_filters

    def get_keyword_ideas(
        self,
        seed_keywords: List[str],
        location_code: int,
        language_code: str,
        client_cfg: Dict[str, Any],
        discovery_modes: List[str],
        filters: Dict[str, Any],
        order_by: Optional[Dict[str, List[str]]],
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms_override: Optional[bool] = None,
        include_clickstream_override: Optional[bool] = None,
        closely_variants_override: Optional[bool] = None,
        exact_match_override: Optional[bool] = None,
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Performs a comprehensive discovery burst using Keyword Ideas, Suggestions, and Related Keywords endpoints.
        """
        all_items = []
        total_cost = 0.0
        max_pages = client_cfg.get("discovery_max_pages", 1)

        # Dynamic parameters (fall back to client_cfg if override is None)
        ignore_synonyms = (
            ignore_synonyms_override
            if ignore_synonyms_override is not None
            else client_cfg.get("discovery_ignore_synonyms", False)
        )
        include_clickstream = (
            include_clickstream_override
            if include_clickstream_override is not None
            else client_cfg.get("include_clickstream_data", False)
        )
        closely_variants = (
            closely_variants_override
            if closely_variants_override is not None
            else client_cfg.get("closely_variants", False)
        )
        exact_match = (
            exact_match_override
            if exact_match_override is not None
            else client_cfg.get("exact_match", False)
        )

        if "keyword_ideas" in discovery_modes:
            self.logger.info(
                f"Fetching keyword ideas for {len(seed_keywords)} seeds..."
            )
            ideas_endpoint = self.LABS_KEYWORD_IDEAS

            sanitized_ideas_filters = self._prioritize_and_limit_filters(
                self._convert_filters_to_api_format(filters.get("ideas"))
            )

            ideas_task = {
                "keywords": seed_keywords,
                "location_code": location_code,
                "language_code": language_code,
                "limit": self.KEYWORD_IDEAS_MODE_LIMIT,
                "include_serp_info": True,
                "ignore_synonyms": ignore_synonyms,
                "closely_variants": closely_variants,
                "filters": sanitized_ideas_filters,
                "order_by": order_by.get("ideas") if order_by else None,
                "include_clickstream_data": include_clickstream,
            }
            ideas_items, cost = self.post_with_paging(
                ideas_endpoint, ideas_task, max_pages=1, tag="discovery_ideas"
            )
            total_cost += cost

            for item in ideas_items:
                item["discovery_source"] = "keyword_ideas"
                item["depth"] = 0
                all_items.append(DataForSEOMapper.sanitize_keyword_data_item(item))
            self.logger.info(f"Found {len(ideas_items)} ideas from Keyword Ideas API.")

        if "keyword_suggestions" in discovery_modes:
            self.logger.info("Fetching keyword suggestions...")
            suggestions_endpoint = self.LABS_KEYWORD_SUGGESTIONS
            for seed_keyword in seed_keywords:
                suggestions_task = {
                    "keyword": seed_keyword,
                    "location_code": location_code,
                    "language_code": language_code,
                    "limit": self.KEYWORD_SUGGESTIONS_MODE_LIMIT,
                    "include_serp_info": True,
                    "exact_match": exact_match,
                    "ignore_synonyms": ignore_synonyms,
                    "include_seed_keyword": True,
                    "filters": self._prioritize_and_limit_filters(
                        self._convert_filters_to_api_format(filters.get("suggestions"))
                    ),
                    "order_by": order_by.get("suggestions") if order_by else None,
                    "include_clickstream_data": include_clickstream,
                }
                suggestions_items, cost = self.post_with_paging(
                    suggestions_endpoint,
                    suggestions_task,
                    max_pages=1,
                    tag=f"discovery_suggestions:{seed_keyword[:20]}",
                )
                total_cost += cost
                for item in suggestions_items:
                    item["discovery_source"] = "keyword_suggestions"
                    item["depth"] = 0
                    all_items.append(DataForSEOMapper.sanitize_keyword_data_item(item))
                self.logger.info(
                    f"Found {len(suggestions_items)} suggestions for '{seed_keyword}'."
                )

        if "related_keywords" in discovery_modes:
            self.logger.info("Fetching related keywords...")
            related_endpoint = self.LABS_RELATED_KEYWORDS
            
            # Limit to max 10 seed keywords for related keyword generation
            seed_keywords_for_related = seed_keywords[:10]

            for seed in seed_keywords_for_related:
                related_task = {
                    "keyword": seed,
                    "location_code": location_code,
                    "language_code": language_code,
                    "depth": 1, # Max to one page for discovery
                    "limit": self.RELATED_KEYWORDS_MODE_LIMIT,
                    "include_serp_info": True,
                    "filters": self._prioritize_and_limit_filters(
                        self._convert_filters_to_api_format(filters.get("related"))
                    ),
                    "order_by": order_by.get("related") if order_by else None,
                    "include_clickstream_data": include_clickstream,
                    "replace_with_core_keyword": client_cfg.get(
                        "discovery_replace_with_core_keyword", False
                    ),
                }

                related_items, cost = self.post_with_paging(
                    related_endpoint,
                    related_task,
                    max_pages=1,
                    tag=f"discovery_related:{seed[:20]}",
                )
                total_cost += cost
                for item in related_items:
                    keyword_data = item.get("keyword_data")
                    if keyword_data:
                        keyword_data["discovery_source"] = "related"
                        keyword_data["depth"] = item.get("depth")
                        all_items.append(
                            DataForSEOMapper.sanitize_keyword_data_item(keyword_data)
                        )
            self.logger.info(f"Total raw items from all sources: {len(all_items)}")

        return all_items, total_cost
```

## File: external_apis/on-page-api.py
```python
import os
import requests
import json
import sys
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# --- Configuration ---
DATAFORSEO_LOGIN = os.getenv("DATAFORSEO_LOGIN")
DATAFORSEO_PASSWORD = os.getenv("DATAFORSEO_PASSWORD")
API_URL = "https://api.dataforseo.com/v3/on_page/content_parsing/live"


def fetch_page_content_live(url: str):
    """
    Fetches the parsed content and raw HTML for a given URL using the synchronous
    'live' endpoint of the Dataforseo On-Page Content Parsing API.

    Args:
        url: The URL of the page to analyze.

    Returns:
        A dictionary containing the API response, or None if an error occurs.
    """
    if not DATAFORSEO_LOGIN or not DATAFORSEO_PASSWORD:
        print("Error: Dataforseo credentials not found in .env file.", file=sys.stderr)
        return None

    # The request body is a list containing a dictionary for the URL.
    post_data = [
        {
            "url": url,
            "store_raw_html": True,
            "enable_javascript": False,
            "convert_to_markdown": True,
        }
    ]

    headers = {"Content-Type": "application/json"}

    try:
        print(f"Sending request for URL: {url}", file=sys.stderr)
        response = requests.post(
            API_URL,
            auth=(DATAFORSEO_LOGIN, DATAFORSEO_PASSWORD),
            headers=headers,
            json=post_data,
        )
        # Raise an exception for bad status codes (4xx or 5xx)
        response.raise_for_status()

        response_json = response.json()

        # Check the response status from the API itself
        if response_json.get("status_code") == 20000:
            print(f"Successfully received data for: {url}", file=sys.stderr)
            return response_json
        else:
            status_msg = response_json.get("status_message", "No status message.")
            print(f"API returned an error for '{url}': {status_msg}", file=sys.stderr)
            return None

    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred for URL '{url}': {http_err}", file=sys.stderr)
        print(f"Response content: {response.text}", file=sys.stderr)
        return None
    except requests.exceptions.RequestException as req_err:
        print(f"A request error occurred for URL '{url}': {req_err}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"An unexpected error occurred for URL '{url}': {e}", file=sys.stderr)
        return None


if __name__ == "__main__":
    test_urls = [
        "https://www.theverge.com/2024/02/15/24074327/openai-sora-text-to-video-generator-ai",
        "https://www.wired.com/story/what-is-generative-ai/",
        "https://blog.google/technology/ai/google-gemini-ai/",
    ]

    all_results = []

    print("--- Starting On-Page Content Parsing (Live API) Tests ---", file=sys.stderr)
    for url in test_urls:
        result = fetch_page_content_live(url)
        if result:
            all_results.append(result)
        else:
            # Add a placeholder for failed requests to keep track
            all_results.append({"url": url, "error": "Failed to fetch content"})
        print("-" * 20, file=sys.stderr)

    print("\n--- All Test Results (JSON Output) ---", file=sys.stderr)
    # Use json.dumps to pretty-print the final list of results
    print(json.dumps(all_results, indent=2))
    print("--- Test Run Complete ---", file=sys.stderr)
```

## File: external_apis/openai_client.py
```python
from openai import OpenAI
import json
import logging
from typing import Dict, Any, List, Optional, Tuple
import time


class OpenAIClientWrapper:
    """
    Provides a robust wrapper for OpenAI API calls,
    handling authentication, retries, and structured outputs (JSON object format).
    """

    def __init__(self, api_key: str, client_cfg: Dict[str, Any]):
        if not api_key:
            raise ValueError("OpenAI API key is required.")
        self.client = OpenAI(api_key=api_key)
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = client_cfg
        self.latest_cost = 0.0

    def _calculate_cost(self, usage: Dict[str, Any], model: str) -> float:
        """Calculates the cost of a chat completion based on token usage."""
        # Pricing per 1M tokens
        pricing = {
            "gpt-4o": {"input": 5.00, "output": 15.00},
            "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
            "gpt-5-mini": {"input": 0.250, "output": 2.000},
            "gpt-5-nano": {"input": 0.050, "output": 0.400},
        }
        model_pricing = pricing.get(
            model, pricing["gpt-4o"]
        )  # Default to gpt-4o pricing

        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)

        input_cost = (prompt_tokens / 1_000_000) * model_pricing["input"]
        output_cost = (completion_tokens / 1_000_000) * model_pricing["output"]

        return input_cost + output_cost
        pass

    def call_chat_completion(
        self,
        messages: List[Dict[str, str]],
        schema: Optional[Dict[str, Any]] = None,
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_completion_tokens: int = 64000,
        retries: int = 3,
    ) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
        """
        Makes a robust OpenAI chat completion call, optionally enforcing JSON output
        and calculating the cost.
        """
        model = 'gpt-5-mini'  # Override to always use gpt-5-mini
        
        self.latest_cost = 0.0
        for attempt in range(retries):
            try:
                response_kwargs = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_completion_tokens": max_completion_tokens,
                }

                # gpt-5-nano and gpt-5-mini do not support temperature
                if model in ['gpt-5-nano', 'gpt-5-mini']:
                    del response_kwargs['temperature']

                if schema:
                    response_kwargs["response_format"] = {
                        "type": "json_schema",
                        "json_schema": {
                            "name": schema.get("name", "structured_output"),
                            "schema": schema,
                            "strict": True,
                        },
                    }

                response = self.client.chat.completions.create(**response_kwargs)

                if response.choices and response.choices[0].finish_reason == "length":
                    self.logger.warning(
                        f"OpenAI API response was truncated because the token limit was reached. "
                        f"Consider increasing 'max_completion_tokens_for_generation' in settings.ini. "
                        f"Current limit for this call: {max_completion_tokens}."
                    )

                if response.usage:
                    self.latest_cost = self._calculate_cost(
                        response.usage.dict(), model
                    )

                if response.choices and response.choices[0].message.content:
                    if schema:
                        try:
                            parsed_output = json.loads(
                                response.choices[0].message.content
                            )
                            self.logger.info(
                                f"Successfully parsed structured output from OpenAI (Attempt {attempt + 1}/{retries}). Cost: ${self.latest_cost:.4f}"
                            )
                            return parsed_output, None
                        except json.JSONDecodeError as e:
                            self.logger.warning(
                                f"Failed to decode JSON from OpenAI (Attempt {attempt + 1}/{retries}): {e}."
                            )
                            continue
                    else:
                        return response.choices[0].message.content, None

                self.logger.warning(
                    f"OpenAI returned no content (Attempt {attempt + 1}/{retries})."
                )
                continue

            except Exception as e:
                self.logger.error(
                    f"OpenAI API call failed (Attempt {attempt + 1}/{retries}): {e}"
                )
                if attempt < retries - 1:
                    time.sleep(5 * (attempt + 1))
                    continue
                return None, str(e)

        return None, "All OpenAI API call attempts failed."

    def call_image_generation(
        self,
        prompt: str,
        style_formula: str,
        quality: str,
        size: str,
        model: Optional[str] = None,
        retries: int = 3,
    ) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """
        Mocks OpenAI image generation. This function is present but *not used* in the final plan
        as Pexels is the exclusive image source. It's kept for potential future re-integration.
        """
        if model is None:
            model = self.client_cfg.get('default_image_model', 'dall-e-3')
        
        self.logger.info(
            "OpenAI image generation is configured but Pexels is prioritized. This function will not be called."
        )
        return (
            None,
            None,
            "OpenAI image generation bypassed; Pexels is the primary source.",
        )
```

## File: external_apis/pexels_client.py
```python
import requests
import logging
import os
from typing import List, Dict, Any, Optional, Tuple


class PexelsClient:
    """
    Manages communication with the Pexels API for free stock photos and videos.
    """

    def __init__(self, api_key: str):
        if not api_key:
            raise ValueError("Pexels API key is required.")
        self.base_url_photos = "https://api.pexels.com/v1/"
        self.base_url_videos = "https://api.pexels.com/videos/"  # Not used in this plan, but included for completeness
        self.headers = {"Authorization": api_key}
        self.logger = logging.getLogger(self.__class__.__name__)

    def search_photos(
        self,
        query: str,
        orientation: Optional[str] = None,
        size: Optional[str] = None,
        per_page: int = 1,
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Searches for photos on Pexels based on a query.
        Returns a list of photo dicts (simplified for direct use) and a dummy cost (Pexels is free).
        """
        endpoint = f"{self.base_url_photos}search"
        params = {
            "query": query,
            "per_page": per_page,
        }
        if orientation:
            params["orientation"] = orientation
        if size:
            params["size"] = size

        try:
            response = requests.get(
                endpoint, headers=self.headers, params=params, timeout=10
            )
            response.raise_for_status()
            data = response.json()

            photos = []
            for photo in data.get("photos", []):
                # Simplify the photo data to what's immediately useful
                photos.append(
                    {
                        "id": photo["id"],
                        "url": photo["url"],
                        "photographer": photo["photographer"],
                        "photographer_url": photo["photographer_url"],
                        "src": photo["src"],  # Contains different sizes
                        "alt": photo.get(
                            "alt", f"Photo by {photo['photographer']} on Pexels"
                        ),
                    }
                )

            self.logger.info(
                f"Found {len(photos)} photos on Pexels for query '{query}'."
            )
            return photos, 0.0  # Pexels is free, so cost is 0

        except requests.exceptions.RequestException as e:
            self.logger.error(
                f"Error searching Pexels photos for '{query}': {e}", exc_info=True
            )
            return [], 0.0
        except Exception as e:
            self.logger.error(
                f"Unexpected error in Pexels photo search for '{query}': {e}",
                exc_info=True,
            )
            return [], 0.0


def download_image_from_url(image_url: str, save_path: str) -> Optional[str]:
    """
    Downloads an image from a given URL and saves it locally.
    Returns the local file path on success, None on failure.
    """
    try:
        response = requests.get(image_url, stream=True, timeout=30)
        response.raise_for_status()

        # Ensure directory exists
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        with open(save_path, "wb") as out_file:
            for chunk in response.iter_content(chunk_size=8192):
                out_file.write(chunk)

        logging.getLogger(__name__).info(
            f"Downloaded image from {image_url} to {save_path}"
        )
        return save_path
    except requests.exceptions.RequestException as e:
        logging.getLogger(__name__).error(
            f"Failed to download image from {image_url}: {e}", exc_info=True
        )
        return None
    except Exception as e:
        logging.getLogger(__name__).error(
            f"An unexpected error occurred during image download: {e}", exc_info=True
        )
        return None
```

## File: pipeline/orchestrator/__init__.py
```python
# backend/pipeline/orchestrator/__init__.py
```

## File: pipeline/orchestrator/analysis_orchestrator.py
```python
# backend/pipeline/orchestrator/analysis_orchestrator.py
import logging
import traceback
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)


class AnalysisOrchestrator:
    def run_analysis_phase(
        self,
        opportunity_id: int,
        selected_competitor_urls: Optional[List[str]] = None,
        use_cached_serp: bool = False,
    ) -> Dict[str, Any]:
        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            return {
                "status": "failed",
                "message": f"Opportunity ID {opportunity_id} not found.",
            }

        keyword = opportunity.get("keyword")
        self.logger.info(
            f"--- Orchestrator: Starting Full Analysis for '{keyword}' ---"
        )
        self.db_manager.update_opportunity_workflow_state(
            opportunity_id, "analysis_started", "in_progress"
        )
        total_api_cost = 0.0

        try:
            # 1. Fetch Live SERP Data
            if use_cached_serp and opportunity.get("full_data", {}).get(
                "serp_overview"
            ):
                self.logger.info(f"Using cached SERP data for '{keyword}'...")
                live_serp_data = opportunity["full_data"]["serp_overview"]
                serp_api_cost = 0.0
            else:
                self.logger.info(f"Running live SERP data fetch for '{keyword}'...")
                from core.serp_analyzer import FullSerpAnalyzer

                serp_analyzer = FullSerpAnalyzer(
                    self.dataforseo_client, self.client_cfg
                )
                live_serp_data, serp_api_cost = serp_analyzer.analyze_serp(keyword)
                total_api_cost += serp_api_cost

            if not live_serp_data:
                self.logger.error(f"Failed to retrieve live SERP data for analysis for keyword: {keyword}")
                raise ValueError("Failed to retrieve live SERP data for analysis.")

            # --- START MODIFICATION ---
            # 2. NEW: Pre-Analysis Validation Gate (Safeguard for AI Calls)
            # Count valid "blog/article" results in top 15
            top_results_for_validation = live_serp_data.get("top_organic_results", [])[
                :15
            ]
            min_relevant_results = self.client_cfg.get(
                "min_relevant_analysis_results", 3
            )
            article_type_results_count = sum(
                1
                for r in top_results_for_validation
                if r.get("page_type") in ["Blog/Article", "News"]
            )

            if article_type_results_count < min_relevant_results:
                reason = f"Analysis failed: SERP is dominated by non-article formats ({article_type_results_count} relevant results found in top 15), making it unsuitable for this workflow."
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id, "pre_analysis_validation_failed", "failed", reason
                )
                self.logger.warning(f"Analysis halted for '{keyword}': {reason}")
                return {
                    "status": "failed",
                    "message": reason,
                    "api_cost": total_api_cost,
                }

            self.logger.info(
                f"Pre-analysis validation passed for '{keyword}' ({article_type_results_count} relevant results in top 15). Proceeding with blueprint generation."
            )
            # --- END MODIFICATION ---

            # 3. Conditional Competitor OnPage Analysis
            competitor_analysis = []
            competitor_api_cost = 0.0

            if self.client_cfg.get("enable_deep_competitor_analysis", False):
                self.logger.info(
                    "Deep competitor analysis is ENABLED. Running OnPage competitor analysis."
                )
                from pipeline.step_04_analysis.competitor_analyzer import (
                    FullCompetitorAnalyzer,
                )

                competitor_analyzer = FullCompetitorAnalyzer(
                    self.dataforseo_client, self.client_cfg
                )
                top_organic_urls = [
                    result["url"]
                    for result in live_serp_data.get("top_organic_results", [])[
                        : self.client_cfg.get("num_competitors_to_analyze", 5)
                    ]
                ]
                competitor_analysis, competitor_api_cost = (
                    competitor_analyzer.analyze_competitors(
                        top_organic_urls, selected_competitor_urls
                    )
                )
                total_api_cost += competitor_api_cost
            else:
                self.logger.info(
                    "Deep competitor analysis is DISABLED. Skipping OnPage competitor analysis."
                )

            # 4. Content Intelligence Synthesis
            from pipeline.step_04_analysis.content_analyzer import ContentAnalyzer

            content_analyzer = ContentAnalyzer(self.openai_client, self.client_cfg)
            content_intelligence, content_api_cost = (
                content_analyzer.synthesize_content_intelligence(
                    keyword,
                    live_serp_data,
                    competitor_analysis,  # Pass this list; it will be empty for the fast workflow
                )
            )
            total_api_cost += content_api_cost

            # 5. Determine Strategy & Generate Outline
            from pipeline.step_05_strategy.decision_engine import (
                StrategicDecisionEngine,
            )

            strategy_engine = StrategicDecisionEngine(self.client_cfg)
            recommended_strategy = strategy_engine.determine_strategy(
                live_serp_data, competitor_analysis, content_intelligence
            )

            ai_outline, outline_api_cost = content_analyzer.generate_ai_outline(
                keyword, live_serp_data, content_intelligence
            )
            total_api_cost += outline_api_cost
            content_intelligence.update(ai_outline)

            if not content_intelligence.get("article_structure"):
                self.logger.critical(
                    f"AI outline generation failed to produce an 'article_structure' for keyword: {keyword}."
                )
                raise ValueError("AI outline generation failed.")

            # 6. Assemble and Save Blueprint & Re-Score
            analysis_data = {
                "serp_overview": live_serp_data,
                "competitor_analysis": competitor_analysis,
                "content_intelligence": content_intelligence,
                "recommended_strategy": recommended_strategy,
            }

            blueprint = self.blueprint_factory.create_blueprint(
                seed_topic=keyword,
                winning_keyword_data=opportunity.get("full_data", {}).copy(),
                analysis_data=analysis_data,
                total_api_cost=total_api_cost,
                client_id=opportunity.get("client_id"),
            )

            opportunity["blueprint"] = blueprint

            final_score, final_score_breakdown = self.scoring_engine.calculate_score(
                opportunity
            )

            self.db_manager.update_opportunity_scores(
                opportunity_id, final_score, final_score_breakdown, blueprint
            )
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "analysis_completed", "paused_for_approval"
            )

            return {
                "status": "success",
                "message": "Analysis phase completed and opportunity re-scored.",
                "api_cost": total_api_cost,
            }

        except Exception as e:
            error_message = f"Analysis phase failed unexpectedly: {e}"
            self.logger.error(f"{error_message}\n{traceback.format_exc()}")
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "analysis_failed", "failed", error_message=str(e)
            )
            return {"status": "failed", "message": str(e), "api_cost": total_api_cost}

    def _run_analysis_background(
        self,
        job_id: str,
        opportunity_id: int,
        selected_competitor_urls: Optional[List[str]],
    ):
        try:
            self.run_analysis_phase(opportunity_id, selected_competitor_urls)
            self.job_manager.update_job_status(job_id, "completed", progress=100)
        except Exception as e:
            self.job_manager.update_job_status(job_id, "failed", error=str(e))
            raise

    def run_full_analysis(
        self, opportunity_id: int, selected_competitor_urls: Optional[List[str]] = None
    ) -> str:
        job_id = self.job_manager.create_job(
            self.client_id,
            target_function=self._run_analysis_background,
            args=(opportunity_id, selected_competitor_urls),
        )
        return job_id
```

## File: pipeline/orchestrator/content_orchestrator.py
```python
# backend/pipeline/orchestrator/content_orchestrator.py
import logging
import traceback
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)


class ContentOrchestrator:
    def _build_abstract_content_tree(
        self, opportunity: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Builds the Abstract Content Tree (ACT) from the blueprint's outline."""
        self.logger.info(
            f"Building Abstract Content Tree for opportunity ID: {opportunity['id']}"
        )

        blueprint = opportunity.get("blueprint", {})
        content_intelligence = blueprint.get("content_intelligence", {})
        outline_structure = content_intelligence.get("article_structure", [])

        if not outline_structure:
            raise ValueError(
                "Cannot build ACT: `article_structure` not found in blueprint."
            )

        act = []

        for i, section in enumerate(outline_structure):
            h2_title = section.get("h2")
            h3s = section.get("h3s", [])

            if not h2_title:
                continue

            node_type = "section_h2"
            if h2_title.lower().strip().startswith("introduction"):
                node_type = "introduction"
            elif h2_title.lower().strip().startswith("conclusion"):
                node_type = "conclusion"

            act.append(
                {
                    "id": f"section-{i}",
                    "type": node_type,
                    "title": h2_title,
                    "sub_points": h3s,
                    "status": "pending",
                    "content_html": "",
                }
            )

        self.logger.info(f"Successfully built ACT with {len(act)} nodes.")
        return act

    def _run_full_content_generation_background(
        self,
        job_id: str,
        opportunity_id: int,
        overrides: Optional[Dict[str, Any]] = None,
    ):
        """
        Internal method to execute the full agentic content generation and enrichment pipeline.
        This is the complete, final version of this function.
        """
        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            error_msg = f"Opportunity {opportunity_id} not found."
            self.logger.error(error_msg)
            self.job_manager.update_job_status(job_id, "failed", error=error_msg)
            return

        if opportunity.get("status") not in ["analyzed", "paused_for_approval"]:
            error_msg = f"Invalid state for content generation: '{opportunity.get("status")}'. Must be 'analyzed' or 'paused_for_approval'."
            self.logger.error(error_msg)
            self.job_manager.update_job_status(job_id, "failed", error=error_msg)
            return

        self.db_manager.update_opportunity_workflow_state(
            opportunity_id, "content_creation_started", "running"
        )
        self.job_manager.update_job_status(
            job_id, "running", progress=5, result={"step": "Initializing Generation"}
        )

        try:
            # --- START COST TRACKING MODIFICATION ---
            total_api_cost = opportunity.get("blueprint", {}).get("metadata", {}).get("total_api_cost", 0.0)
            self.logger.info(f"Initial cost from blueprint: ${total_api_cost:.4f}")
            # --- END COST TRACKING MODIFICATION ---

            self.job_manager.update_job_status(
                job_id, "running", progress=10, result={"step": "Building Content Tree"}
            )
            act = self._build_abstract_content_tree(opportunity)

            from agents.article_generator import SectionalArticleGenerator

            sectional_generator = SectionalArticleGenerator(
                self.openai_client, self.client_cfg, self.db_manager
            )

            full_article_context_for_conclusion = ""
            previous_content = ""
            for i, node in enumerate(act):
                progress = 15 + int((i / len(act)) * 40)
                self.job_manager.update_job_status(
                    job_id,
                    "running",
                    progress=progress,
                    result={"step": f"Generating: {node['title']}"},
                )

                content_html, cost = None, 0.0
                if node["type"] == "introduction":
                    content_html, cost = sectional_generator.generate_introduction(
                        opportunity
                    )
                elif node["type"] == "section_h2":
                    content_html, cost = sectional_generator.generate_section(
                        opportunity,
                        node["title"],
                        node.get("sub_points", []),
                        previous_content,
                    )
                elif node["type"] == "conclusion":
                    content_html, cost = sectional_generator.generate_conclusion(
                        opportunity, full_article_context_for_conclusion
                    )
                
                total_api_cost += cost # Aggregate cost

                if content_html:
                    node["content_html"] = content_html
                    full_article_context_for_conclusion += (
                        f"<h2>{node['title']}</h2>\n{content_html}\n"
                    )
                    previous_content = content_html
                else:
                    raise RuntimeError(
                        f"Failed to generate content for section '{node['title']}'."
                    )

            self.job_manager.update_job_status(
                job_id, "running", progress=60, result={"step": "Assembling Article"}
            )
            final_html_parts = [
                f"<h2>{node['title']}</h2>\n{node['content_html']}" for node in act
            ]
            final_article_html = "\n".join(final_html_parts)

            opportunity["ai_content"] = {"article_body_html": final_article_html}

            MAX_REFINEMENT_ATTEMPTS = 3
            current_html = opportunity["ai_content"]["article_body_html"]
            final_audit_results = {}

            for attempt in range(MAX_REFINEMENT_ATTEMPTS):
                self.job_manager.update_job_status(
                    job_id,
                    "running",
                    progress=65 + (attempt * 5),
                    result={"step": f"Auditing Content (Attempt {attempt + 1})"},
                )

                audit_results = self.content_auditor.audit_content(
                    article_html=current_html,
                    primary_keyword=opportunity.get("keyword", ""),
                    blueprint=opportunity.get("blueprint", {}),
                    client_cfg=self.client_cfg,
                )
                final_audit_results = audit_results

                structured_issues = audit_results.get("publish_readiness_issues", [])

                if not structured_issues:
                    self.logger.info(
                        f"Audit passed on attempt {attempt + 1}. No refinement needed."
                    )
                    break

                self.logger.warning(
                    f"Audit failed on attempt {attempt + 1}. Issues found: {len(structured_issues)}. Triggering self-healing."
                )
                self.job_manager.update_job_status(
                    job_id,
                    "running",
                    progress=70 + (attempt * 5),
                    result={"step": f"Self-Healing (Attempt {attempt + 1})"},
                )

                all_refinement_commands = []
                for issue in structured_issues:
                    if issue["issue"] == "unresolved_placeholder":
                        all_refinement_commands.append(
                            "- CRITICAL FIX: Remove all image placeholders like '[[IMAGE_ID:...]]' from the text."
                        )
                    elif issue["issue"] == "empty_heading":
                        all_refinement_commands.append(
                            f"- FIX: The following heading tag is empty: `{issue['context']}`. Based on the surrounding content, either remove this tag entirely or populate it with a relevant heading."
                        )
                    elif issue["issue"] == "short_paragraph":
                        all_refinement_commands.append(
                            f"- FIX: The paragraph `{issue['context']}` is too brief. Expand this paragraph to be at least 3 sentences long, providing more detail, or merge it with an adjacent paragraph if appropriate."
                        )
                    elif issue["issue"] == "word_count_deviation":
                        target_word_count = (
                            opportunity.get("blueprint", {})
                            .get("ai_content_brief", {})
                            .get("target_word_count", 1500)
                        )
                        all_refinement_commands.append(
                            f"- FIX: The article's word count is significantly off target. Review the entire article and expand or condense it to be approximately {target_word_count} words. {issue['context']}"
                        )

                if not all_refinement_commands:
                    break

                combined_command = (
                    "Please refine the entire HTML document by addressing the following issues:\n"
                    + "\n".join(all_refinement_commands)
                )

                refine_prompt_messages = [
                    {
                        "role": "system",
                        "content": "You are an expert content editor. You will receive a full HTML document and a list of specific issues to fix. Apply all fixes and return ONLY the complete, corrected HTML document. Preserve all original HTML tags and structure unless a fix requires changing them. Do not add any introductory text, just the refined HTML.",
                    },
                    {
                        "role": "user",
                        "content": f"COMMANDS:\n{combined_command}\n\nFULL HTML TO FIX:\n```html\n{current_html}\n```",
                    },
                ]

                refined_html, error = self.openai_client.call_chat_completion(
                    messages=refine_prompt_messages,
                    model=self.client_cfg.get("default_model", "gpt-5-nano"),
                    temperature=0.2,
                )
                total_api_cost += self.openai_client.latest_cost # Aggregate cost

                if error or not refined_html:
                    self.logger.error(
                        f"AI Refinement Agent failed on attempt {attempt + 1}: {error}"
                    )
                    break

                current_html = (
                    refined_html.strip()
                    .removeprefix("```html")
                    .removesuffix("```")
                    .strip()
                )

            opportunity["ai_content"]["article_body_html"] = current_html
            opportunity["ai_content"]["audit_results"] = final_audit_results

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=85,
                result={"step": "Generating Images & Social Posts"},
            )
            featured_image_data, image_cost = self.image_generator.generate_featured_image(
                opportunity
            )
            total_api_cost += image_cost
            social_posts, social_cost = self.social_crafter.craft_posts(opportunity)
            total_api_cost += social_cost

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=90,
                result={"step": "Formatting & Internal Linking"},
            )
            internal_link_suggestions, link_cost = (
                self.internal_linking_suggester.suggest_links(
                    opportunity["ai_content"]["article_body_html"],
                    opportunity.get("blueprint", {})
                    .get("ai_content_brief", {})
                    .get("key_entities_to_mention", []),
                    self.client_cfg.get("target_domain"),
                    self.client_id,
                )
            )
            total_api_cost += link_cost

            final_package = self.html_formatter.format_final_package(
                opportunity,
                internal_linking_suggestions=internal_link_suggestions,
                in_article_images_data=[],
            )

            self.job_manager.update_job_status(
                job_id, "running", progress=95, result={"step": "Saving to Database"}
            )
            self.db_manager.save_full_content_package(
                opportunity_id,
                opportunity["ai_content"],
                self.client_cfg.get("ai_content_model", "gpt-4o"),
                featured_image_data,
                [],
                social_posts,
                final_package,
                total_api_cost, # Pass total cost
            )

            self.job_manager.update_job_status(
                job_id,
                "completed",
                progress=100,
                result={
                    "status": "success",
                    "message": "Content generation completed.",
                },
            )

        except Exception as e:
            error_msg = (
                f"Agentic content generation failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "content_generation_failed", "failed", str(e)
            )
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def run_full_content_generation(
        self, opportunity_id: int, overrides: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Public method to initiate content generation asynchronously.
        Returns a job_id.
        """
        self.logger.info(
            f"--- Orchestrator: Initiating Full Content Generation for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            self.client_id,
            target_function=self._run_full_content_generation_background,
            args=(opportunity_id, overrides),
        )
        return job_id
```

## File: pipeline/orchestrator/cost_estimator.py
```python
# backend/pipeline/orchestrator/cost_estimator.py
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


class CostEstimator:
    def estimate_action_cost(
        self,
        action: str,
        opportunity_id: Optional[int] = None,
        discovery_params: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Estimates the API cost for a given workflow action without executing it.
        - For 'discovery', uses discovery_params.
        - For other actions, uses opportunity_id.
        """
        estimated_cost = 0.0
        explanation = []

        if action == "discovery":
            if not discovery_params:
                raise ValueError(
                    "discovery_params are required for 'discovery' action estimation."
                )

            KEYWORD_IDEAS_RATE = 0.005
            KEYWORD_SUGGESTIONS_RATE = 0.005
            RELATED_KEYWORDS_RATE = 0.005

            seed_keywords = discovery_params.get("seed_keywords", [])
            discovery_modes = discovery_params.get("discovery_modes", [])
            max_pages = self.client_cfg.get("discovery_max_pages", 1)
            num_seeds = len(seed_keywords)

            if "keyword_ideas" in discovery_modes:
                cost = KEYWORD_IDEAS_RATE * max_pages
                estimated_cost += cost
                explanation.append(
                    {
                        "service": "Keyword Ideas API",
                        "details": f"1 call x {max_pages} page(s) @ ${KEYWORD_IDEAS_RATE}/call",
                        "cost": cost,
                    }
                )

            if "keyword_suggestions" in discovery_modes:
                cost = KEYWORD_SUGGESTIONS_RATE * num_seeds * max_pages
                estimated_cost += cost
                explanation.append(
                    {
                        "service": "Keyword Suggestions API",
                        "details": f"{num_seeds} seed(s) x {max_pages} page(s) @ ${KEYWORD_SUGGESTIONS_RATE}/call",
                        "cost": cost,
                    }
                )

            if "related_keywords" in discovery_modes:
                cost = RELATED_KEYWORDS_RATE * num_seeds * max_pages
                estimated_cost += cost
                explanation.append(
                    {
                        "service": "Related Keywords API",
                        "details": f"{num_seeds} seed(s) x {max_pages} page(s) @ ${RELATED_KEYWORDS_RATE}/call",
                        "cost": cost,
                    }
                )

            return {"total_cost": estimated_cost, "breakdown": explanation}

        if not opportunity_id:
            raise ValueError(f"opportunity_id is required for action '{action}'.")

        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise ValueError("Opportunity not found.")

        if action == "analyze" or action == "validate":
            serp_base_cost = 0.005
            serp_cost_explanation = (
                f"1 x SERP Live Advanced call (~${serp_base_cost:.3f} base)"
            )

            if self.client_cfg.get("load_async_ai_overview", False):
                serp_base_cost += 0.002
                serp_cost_explanation += " + $0.002 (Async AI Overview)"

            if self.client_cfg.get("calculate_rectangles", False):
                serp_base_cost += 0.002
                serp_cost_explanation += " + $0.002 (Pixel Ranking)"

            paa_depth = self.client_cfg.get("people_also_ask_click_depth", 0)
            if isinstance(paa_depth, int) and paa_depth > 0:
                paa_cost = paa_depth * 0.00015
                serp_base_cost += paa_cost
                serp_cost_explanation += f" + ${paa_cost:.5f} (PAA Depth {paa_depth})"

            estimated_cost += serp_base_cost
            explanation.append(
                {
                    "service": "SERP Live Advanced Task",
                    "details": serp_cost_explanation,
                    "cost": serp_base_cost,
                }
            )

            if action == "analyze":
                num_competitors = self.client_cfg.get("num_competitors_to_analyze", 5)

                ONPAGE_BASIC_RATE = 0.000125
                ONPAGE_RENDER_RATE = 0.00425
                ONPAGE_CUSTOM_JS_RATE = 0.00025

                if self.client_cfg.get("onpage_enable_browser_rendering", False):
                    onpage_per_task_cost = ONPAGE_RENDER_RATE
                    onpage_cost_explanation = f"x OnPage Instant Pages (Browser Rendering ON @ ${onpage_per_task_cost:.5f} each)"
                else:
                    onpage_per_task_cost = ONPAGE_BASIC_RATE
                    onpage_cost_explanation = f"x OnPage Instant Pages (Basic Crawl @ ${onpage_per_task_cost:.5f} each)"

                if self.client_cfg.get("onpage_enable_custom_js", False):
                    onpage_per_task_cost += ONPAGE_CUSTOM_JS_RATE
                    onpage_cost_explanation += " + $0.00025 (Custom JavaScript)"

                onpage_cost = num_competitors * onpage_per_task_cost
                estimated_cost += onpage_cost

                explanation.append(
                    {
                        "service": f"{num_competitors} Competitor OnPage Tasks",
                        "details": onpage_cost_explanation,
                        "cost": onpage_cost,
                    }
                )

            ai_analysis_cost = 0.05
            estimated_cost += ai_analysis_cost
            explanation.append(
                {
                    "service": "OpenAI Analysis Buffer",
                    "details": "1 x OpenAI GPT-4o call for analysis",
                    "cost": ai_analysis_cost,
                }
            )

        elif action == "generate":
            model = self.client_cfg.get("ai_content_model", "gpt-4o")

            pricing = self.client_cfg.get("OPENAI_PRICING", {})
            input_rate = pricing.get(f"{model}_input", 5.00) / 1000000
            output_rate = pricing.get(f"{model}_output", 15.00) / 1000000

            article_input_tokens = 10000
            article_output_tokens = 5000
            article_cost = (article_input_tokens * input_rate) + (
                article_output_tokens * output_rate
            )

            social_cost = (2000 * input_rate) + (500 * output_rate)

            buffer_tokens = 5000
            buffer_cost = (
                (buffer_tokens * input_rate) + (buffer_tokens * output_rate)
            ) * 0.5

            estimated_cost += article_cost
            explanation.append(
                {
                    "service": "AI Article Generation",
                    "details": f"1 x OpenAI {model} call (10k in, 5k out)",
                    "cost": article_cost,
                }
            )

            estimated_cost += social_cost
            explanation.append(
                {
                    "service": "AI Social Posts",
                    "details": f"1 x OpenAI {model} call (2k in, 0.5k out)",
                    "cost": social_cost,
                }
            )

            estimated_cost += buffer_cost
            explanation.append(
                {
                    "service": "AI Refinement/Linking Buffer",
                    "details": "50% chance of refinement/linking tokens",
                    "cost": buffer_cost,
                }
            )

            if self.client_cfg.get("use_pexels_first", True):
                explanation.append(
                    {
                        "service": "Image Sourcing (Pexels)",
                        "details": "Cost: $0.00",
                        "cost": 0.00,
                    }
                )
            else:
                image_cost = self.client_cfg.get("num_in_article_images", 0) * 0.04
                estimated_cost += image_cost
                explanation.append(
                    {
                        "service": f"Image Generation ({self.client_cfg.get('default_image_model', 'dall-e-3')})",
                        "details": f"Estimated {self.client_cfg.get('num_in_article_images', 0)} images @ $0.04 each",
                        "cost": image_cost,
                    }
                )

        elif action == "validate":
            # Assuming SERP_LIVE_ADVANCED_RATE is 0.020 USD
            SERP_LIVE_ADVANCED_RATE = 0.020
            validation_cost = SERP_LIVE_ADVANCED_RATE
            details = f"1 x SERP Live Advanced call (~${SERP_LIVE_ADVANCED_RATE:.3f})"
            if self.client_cfg.get("load_async_ai_overview", False):
                validation_cost += 0.002
                details += " + $0.002 for asynchronous AI Overview retrieval."
            estimated_cost += validation_cost
            explanation.append(
                {
                    "service": "SERP Validation",
                    "details": details,
                    "cost": validation_cost,
                }
            )

        return {"estimated_cost": round(estimated_cost, 2), "explanation": explanation}
```

## File: pipeline/orchestrator/discovery_orchestrator.py
```python
# backend/pipeline/orchestrator/discovery_orchestrator.py
import logging
import traceback
import os
from typing import Dict, Any, List, Optional

from backend.services.serp_analysis_service import SerpAnalysisService

logger = logging.getLogger(__name__)


class DiscoveryOrchestrator:
    def _run_discovery_background(
        self,
        job_id: str,
        run_id: int,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        filters_override: Optional[Dict[str, Any]],
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = None,
        include_clickstream_data: Optional[bool] = None,
        closely_variants: Optional[bool] = None,
        exact_match: Optional[bool] = None,
    ):
        """Internal method to execute the consolidated discovery phase for a job."""
        log_dir = "discovery_logs"
        os.makedirs(log_dir, exist_ok=True)
        log_file_path = os.path.join(log_dir, f"run_{run_id}.log")

        run_logger = logging.getLogger(f"run_{run_id}")
        handler = logging.FileHandler(log_file_path)
        handler.setFormatter(
            logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        )
        run_logger.addHandler(handler)
        run_logger.setLevel(logging.INFO)

        self.db_manager.update_discovery_run_log_path(run_id, log_file_path)
        self.db_manager.update_discovery_run_status(run_id, "running")
        self.job_manager.update_job_status(job_id, "running", progress=0)

        run_config = self.global_cfg_manager.load_client_config(
            self.client_id, self.db_manager
        )
        if filters_override:
            run_logger.info(f"Applying filter overrides: {filters_override}")
            run_config.update(filters_override)

        run_logger.info(
            f"Starting discovery with modes: {discovery_modes}, filters: {filters}, order_by: {order_by}, limit: {limit}, depth: {depth}"
        )

        try:
            job_status = self.job_manager.get_job_status(job_id)
            if job_status and job_status.get("status") == "failed":
                run_logger.warning(
                    f"Job {job_id} found marked as 'failed' (cancelled). Exiting gracefully."
                )
                self.db_manager.update_discovery_run_status(run_id, "cancelled")
                return {"message": "Job cancelled by user request."}
            
            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=10,
                result={"step": "Fetching Keywords from API..."},
            )
            from pipeline.step_01_discovery.run_discovery import run_discovery_phase

            discovery_result = run_discovery_phase(
                seed_keywords=seed_keywords,
                dataforseo_client=self.dataforseo_client,
                db_manager=self.db_manager,
                client_id=self.client_id,
                client_cfg=run_config,
                discovery_modes=discovery_modes,
                filters=filters,
                order_by=order_by,
                limit=limit,
                depth=depth,
                ignore_synonyms=ignore_synonyms,
                include_clickstream_data=include_clickstream_data,
                closely_variants=closely_variants,
                run_logger=run_logger,
            )

            stats = discovery_result.get("stats", {})
            total_cost = discovery_result.get("total_cost", 0.0)
            processed_opportunities = discovery_result.get("opportunities", [])
            
            self.job_manager.update_job_status(
                job_id, "running", progress=75, result={"step": "Saving Results to Database..."}
            )
            
            num_added = 0
            if processed_opportunities:
                run_logger.info(
                    f"Attempting to save {len(processed_opportunities)} processed opportunities..."
                )
                num_added = self.db_manager.add_opportunities(
                    processed_opportunities, self.client_id, run_id
                )
                run_logger.info(
                    f"Successfully saved {num_added} new keyword records. The database ignored {len(processed_opportunities) - num_added} duplicates."
                )

            results_summary = {
                "total_cost": total_cost,
                "source_counts": stats.get("raw_counts", {}),
                "total_raw_count": stats.get("total_raw_count", 0),
                "total_unique_count": stats.get("total_unique_count", 0),
                "disqualification_reasons": stats.get("disqualification_reasons", {}),
                "disqualified_count": stats.get("disqualified_count", 0),
                "final_qualified_count": stats.get("final_qualified_count", 0),
                "duplicates_removed": len(processed_opportunities) - num_added,
                "final_added_to_db": num_added,
            }

            self.db_manager.update_discovery_run_completed(run_id, results_summary)
            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=results_summary
            )
            run_logger.info("Discovery run completed successfully.")
            return results_summary
        except Exception as e:
            error_message = f"Discovery workflow failed: {e}\n{traceback.format_exc()}"
            run_logger.error(error_message)
            self.db_manager.update_discovery_run_failed(run_id, str(e))
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            # --- ADD THIS NEW BLOCK ---
            # Mark any partially processed opportunities from this run as failed.
            run_logger.info(f"Marking partially fetched opportunities from run_id {run_id} as 'failed_scoring'.")
            conn = self.db_manager._get_conn()
            with conn:
                conn.execute(
                    "UPDATE opportunities SET status = 'failed_scoring', error_message = ? WHERE run_id = ? AND status = 'fetched'",
                    (f"Parent discovery job {job_id} failed.", run_id)
                )
            # --- END NEW BLOCK ---
            raise

    def run_discovery_and_save(
        self,
        run_id: int,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]] = None,
        order_by: Optional[List[str]] = None,
        filters_override: Optional[Dict[str, Any]] = None,
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = None,
        include_clickstream_data: Optional[bool] = None,
        closely_variants: Optional[bool] = None,
        exact_match: Optional[bool] = None,
    ) -> str:
        """
        Public method to initiate a discovery run asynchronously.
        Returns a job_id.
        """
        self.logger.info(
            f"--- Orchestrator: Initiating Full Discovery & Qualification for Run ID: {run_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            self.client_id,
            target_function=self._run_discovery_background,
            args=(
                run_id,
                seed_keywords,
                discovery_modes,
                filters,
                order_by,
                filters_override,
                limit,
                depth,
                ignore_synonyms,
                include_clickstream_data,
                closely_variants,
                exact_match,
            ),
        )
        return job_id
```

## File: pipeline/orchestrator/image_orchestrator.py
```python
# backend/pipeline/orchestrator/image_orchestrator.py
import logging
import traceback
import json

logger = logging.getLogger(__name__)


class ImageOrchestrator:
    def _run_single_image_generation_background(
        self, job_id: str, opportunity_id: int, original_prompt: str, new_prompt: str
    ):
        """Internal method to regenerate a single in-article image."""
        self.job_manager.update_job_status(job_id, "running", progress=0)

        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity or not opportunity.get("ai_content_json"):
                error_msg = f"Opportunity {opportunity_id} or its AI content missing for single image regeneration."
                self.logger.error(error_msg)
                raise ValueError(error_msg)

            opportunity["client_cfg"] = self.client_cfg

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=30,
                result={"step": "Generating single image"},
            )

            images_data, _ = self.image_generator.generate_images_from_prompts(
                [new_prompt]
            )

            if not images_data or not images_data[0]:
                error_msg = f"Image generation failed or returned no data for opportunity {opportunity_id}."
                self.logger.error(error_msg)
                raise RuntimeError(error_msg)

            new_image_data = images_data[0]

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=70,
                result={"step": "Updating content package"},
            )

            in_article_images = opportunity.get("in_article_images_data", [])
            if isinstance(in_article_images, str):
                in_article_images = (
                    json.loads(in_article_images) if in_article_images else []
                )

            updated_images = []
            found_and_updated = False
            for img in in_article_images:
                if img.get("original_prompt") == original_prompt:
                    img.update(new_image_data)
                    found_and_updated = True
                updated_images.append(img)

            if not found_and_updated:
                new_image_data["original_prompt"] = new_prompt
                updated_images.append(new_image_data)

            self.db_manager.update_opportunity_images(
                opportunity_id,
                opportunity.get("featured_image_url"),
                opportunity.get("featured_image_local_path"),
                updated_images,
            )

            final_package = self.html_formatter.format_final_package(opportunity)
            self.db_manager.update_opportunity_final_package(
                opportunity_id, final_package
            )

            result_message = {
                "status": "success",
                "message": f"Single image regenerated for prompt: {original_prompt}",
            }

            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=result_message
            )
            return result_message

        except Exception as e:
            error_msg = (
                f"Single image regeneration failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def regenerate_single_image(
        self, opportunity_id: int, original_prompt: str, new_prompt: str
    ) -> str:
        """Public method to initiate single image regeneration asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Single Image Regeneration for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            self.client_id,
            target_function=self._run_single_image_generation_background,
            args=(opportunity_id, original_prompt, new_prompt),
        )
        return job_id

    def _run_featured_image_regeneration_background(
        self, job_id: str, opportunity_id: int, prompt: str
    ):
        """Internal method to regenerate a featured image."""
        self.job_manager.update_job_status(
            job_id, "running", progress=10, result={"step": "Generating Featured Image"}
        )
        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                error_msg = f"Opportunity {opportunity_id} not found for featured image regeneration."
                self.logger.error(error_msg)
                raise ValueError(error_msg)

            opportunity["keyword"] = prompt
            opportunity["ai_content"] = {"meta_title": prompt}

            featured_image_data, cost = self.image_generator.generate_featured_image(
                opportunity
            )

            if featured_image_data:
                self.db_manager.update_opportunity_images(
                    opportunity_id,
                    featured_image_data.get("remote_url"),
                    featured_image_data.get("local_path"),
                    json.loads(opportunity.get("in_article_images_data", "[]")),
                )

            result_message = {
                "status": "success",
                "message": "Featured image regenerated successfully.",
                "api_cost": cost,
            }
            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=result_message
            )
            return result_message

        except Exception as e:
            error_msg = (
                f"Featured image regeneration failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def regenerate_featured_image(self, opportunity_id: int, prompt: str) -> str:
        """Public method to initiate featured image regeneration asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Featured Image Regeneration for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            self.client_id,
            target_function=self._run_featured_image_regeneration_background,
            args=(opportunity_id, prompt),
        )
        return job_id
```

## File: pipeline/orchestrator/main.py
```python
# backend/pipeline/orchestrator/main.py
import logging

from backend.app_config.manager import ConfigManager
from backend.data_access.database_manager import DatabaseManager
from backend.external_apis.dataforseo_client_v2 import DataForSEOClientV2
from backend.external_apis.openai_client import OpenAIClientWrapper
from backend.agents.image_generator import ImageGenerator
from backend.agents.social_media_crafter import SocialMediaCrafter
from backend.agents.internal_linking_suggester import InternalLinkingSuggester
from backend.agents.html_formatter import HtmlFormatter
from backend.core.blueprint_factory import BlueprintFactory
from backend.agents.content_auditor import ContentAuditor
from backend.agents.prompt_assembler import DynamicPromptAssembler
from backend.services.serp_analysis_service import SerpAnalysisService
from backend.pipeline.step_03_prioritization.scoring_engine import ScoringEngine
from backend.pipeline.step_01_discovery.cannibalization_checker import (
    CannibalizationChecker,
)
from backend.jobs import JobManager

from .discovery_orchestrator import DiscoveryOrchestrator
from .analysis_orchestrator import AnalysisOrchestrator
from .content_orchestrator import ContentOrchestrator
from .image_orchestrator import ImageOrchestrator
from .social_orchestrator import SocialOrchestrator
from .validation_orchestrator import ValidationOrchestrator
from .workflow_orchestrator import WorkflowOrchestrator
from .cost_estimator import CostEstimator

logger = logging.getLogger(__name__)


class WorkflowOrchestrator(
    DiscoveryOrchestrator,
    AnalysisOrchestrator,
    ContentOrchestrator,
    ImageOrchestrator,
    SocialOrchestrator,
    ValidationOrchestrator,
    WorkflowOrchestrator,
    CostEstimator,
):
    def __init__(
        self,
        global_cfg_manager: ConfigManager,
        db_manager: DatabaseManager,
        client_id: str,
        job_manager: "JobManager",
    ):
        self.global_cfg_manager = global_cfg_manager
        self.db_manager = db_manager
        self.client_id = client_id
        self.job_manager = job_manager
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = self.global_cfg_manager.load_client_config(
            self.client_id, self.db_manager
        )

        self.openai_client = OpenAIClientWrapper(
            self.client_cfg.get("openai_api_key"), self.client_cfg
        )
        self.dataforseo_client = DataForSEOClientV2(
            login=self.client_cfg["dataforseo_login"],
            password=self.client_cfg["dataforseo_password"],
            config=self.client_cfg,
            db_manager=self.db_manager,
            enable_cache=self.client_cfg.get("enable_cache", True),
        )

        self.image_generator = ImageGenerator(self.client_cfg)
        self.social_crafter = SocialMediaCrafter(self.openai_client, self.client_cfg)
        self.internal_linking_suggester = InternalLinkingSuggester(
            self.openai_client, self.client_cfg, self.db_manager
        )
        self.html_formatter = HtmlFormatter()
        self.blueprint_factory = BlueprintFactory(
            self.openai_client, self.client_cfg, self.dataforseo_client, self.db_manager
        )
        self.scoring_engine = ScoringEngine(self.client_cfg)
        self.cannibalization_checker = CannibalizationChecker(
            self.client_cfg.get("target_domain"),
            self.dataforseo_client,
            self.client_cfg,
            self.db_manager,
        )
        self.content_auditor = ContentAuditor()
        self.prompt_assembler = DynamicPromptAssembler(self.db_manager)
        self.serp_analysis_service = SerpAnalysisService(self.dataforseo_client, self.client_cfg)
```

## File: pipeline/orchestrator/social_orchestrator.py
```python
# backend/pipeline/orchestrator/social_orchestrator.py
import logging
import traceback

logger = logging.getLogger(__name__)


class SocialOrchestrator:
    def _run_social_posts_regeneration_background(
        self, job_id: str, opportunity_id: int
    ):
        """Internal method to regenerate social media posts."""
        self.job_manager.update_job_status(
            job_id, "running", progress=10, result={"step": "Crafting Social Posts"}
        )
        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                error_msg = f"Opportunity {opportunity_id} not found for social media regeneration."
                self.logger.error(error_msg)
                raise ValueError(error_msg)

            opportunity["client_cfg"] = self.client_cfg

            social_posts, cost = self.social_crafter.craft_posts(opportunity)
            if social_posts:
                self.db_manager.update_opportunity_social_posts(
                    opportunity_id, social_posts
                )

            result_message = {
                "status": "success",
                "message": "Social media posts regenerated successfully.",
                "api_cost": cost,
            }
            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=result_message
            )
            return result_message

        except Exception as e:
            error_msg = (
                f"Social media post regeneration failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def regenerate_social_posts(self, opportunity_id: int) -> str:
        """Public method to initiate social media post regeneration asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Social Media Post Regeneration for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            self.client_id,
            target_function=self._run_social_posts_regeneration_background,
            args=(opportunity_id,),
        )
        return job_id
```

## File: pipeline/orchestrator/state_validator.py
```python
from typing import Dict, Set

# Defines the valid states an opportunity can transition *to* from a given state.
VALID_TRANSITIONS: Dict[str, Set[str]] = {
    'review': {'validated', 'rejected', 'running', 'in_progress'},
    'validated': {'analyzed', 'paused_for_approval', 'running', 'in_progress', 'rejected'},
    'analyzed': {'running', 'in_progress', 'generated', 'rejected'},
    'paused_for_approval': {'running', 'in_progress', 'generated', 'rejected'},
    'failed': {'running', 'in_progress', 'validated', 'rejected'},
    'rejected': {'validated', 'pending'}, # From manual override
    'generated': {'published', 'rejected'},
}

def is_valid_transition(from_status: str, to_status: str) -> bool:
    """Checks if a state transition is allowed."""
    return to_status in VALID_TRANSITIONS.get(from_status, set())
```

## File: pipeline/orchestrator/validation_orchestrator.py
```python
# backend/pipeline/orchestrator/validation_orchestrator.py
import logging
import traceback

logger = logging.getLogger(__name__)


class ValidationOrchestrator:
    def run_validation_phase(self, opportunity_id: int):
        """
        Runs a cost-effective final validation gate before committing to a full analysis.
        Makes one live SERP call and a deep cannibalization check.
        """
        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            error_msg = f"Opportunity ID {opportunity_id} not found."
            self.logger.error(error_msg)
            return {
                "status": "failed",
                "message": error_msg,
            }

        self.logger.info(
            f"--- Orchestrator: Starting Live SERP Validation for '{opportunity.get('keyword')}' ---"
        )
        self.db_manager.update_opportunity_workflow_state(
            opportunity_id, "validation_started", "in_progress"
        )
        total_cost = 0.0

        try:
            from core.serp_analyzer import FullSerpAnalyzer

            serp_analyzer = FullSerpAnalyzer(self.dataforseo_client, self.client_cfg)
            serp_overview, serp_api_cost = serp_analyzer.analyze_serp(
                opportunity.get("keyword")
            )
            total_cost += serp_api_cost
            if not serp_overview:
                error_msg = f"Failed to retrieve live SERP data for validation for keyword: {opportunity.get('keyword')}"
                self.logger.error(error_msg)
                raise ValueError(error_msg)

            from pipeline.step_04_analysis.run_analysis import run_final_validation

            is_valid, reason = run_final_validation(
                serp_overview, opportunity, self.client_cfg, self.dataforseo_client
            )

            if is_valid:
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id, "validation_passed", "validated"
                )
                self.logger.info(
                    f"Validation PASSED for '{opportunity.get('keyword')}'."
                )
                return {
                    "status": "success",
                    "message": "Validation passed. Ready for full analysis.",
                    "api_cost": total_cost,
                }
            else:
                self.db_manager.update_opportunity_status(opportunity_id, "rejected")
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id,
                    "validation_failed",
                    "rejected",
                    error_message=reason,
                )
                self.logger.warning(
                    f"Validation FAILED for '{opportunity.get('keyword')}': {reason}"
                )
                return {"status": "failed", "message": reason, "api_cost": total_cost}
        except Exception as e:
            error_msg = f"Validation phase failed unexpectedly: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "validation_failed", "failed", error_message=str(e)
            )
            return {"status": "failed", "message": str(e), "api_cost": total_cost}

    def _run_validation_background(self, job_id: str, opportunity_id: int):
        """Internal method to execute the validation phase for a job."""
        self.job_manager.update_job_status(job_id, "running", progress=0)
        try:
            result = self.run_validation_phase(opportunity_id)

            if result["status"] == "success":
                self.job_manager.update_job_status(
                    job_id, "completed", progress=100, result=result
                )
            else:
                self.job_manager.update_job_status(
                    job_id, "failed", progress=100, error=result["message"]
                )
            return result
        except Exception as e:
            error_msg = f"Validation background failed: {e}\n{traceback.format_exc()}"
            self.logger.error(error_msg)
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "validation_failed", "failed", str(e)
            )
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def run_validation(self, opportunity_id: int) -> str:
        """Public method to initiate the validation phase asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Validation for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            self.client_id,
            target_function=self._run_validation_background, args=(opportunity_id,)
        )
        return job_id
```

## File: pipeline/orchestrator/workflow_orchestrator.py
```python
# backend/pipeline/orchestrator/workflow_orchestrator.py
import logging
import traceback
import threading

logger = logging.getLogger(__name__)


class WorkflowOrchestrator:
    def _run_full_auto_workflow_background(
        self, job_id: str, opportunity_id: int, override_validation: bool
    ):
        """Internal method to execute the full workflow from validation to generation."""
        try:
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "in_progress", "running"
            )
            self.job_manager.update_job_progress(job_id, "Workflow Started", "Starting full automation workflow.")

            if not override_validation:
                self.job_manager.update_job_progress(job_id, "Validation", "Running validation checks.")
                validation_result = self.run_validation_phase(opportunity_id)
                if validation_result.get("status") == "failed":
                    error_message = validation_result.get("message", "Unknown validation error.")
                    self.logger.warning(
                        f"Workflow for opportunity {opportunity_id} stopped due to validation failure: {error_message}"
                    )
                    raise RuntimeError(f"Validation failed: {error_message}")
            else:
                self.logger.info(
                    f"Validation step skipped for opportunity {opportunity_id} due to override."
                )
                self.job_manager.update_job_progress(job_id, "Validation", "Validation skipped by user override.")
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id,
                    "validation_overridden",
                    "validated",
                    error_message="Validation manually overridden.",
                )

            self.job_manager.update_job_progress(job_id, "Analysis", "Starting in-depth analysis.")
            analysis_result = self.run_analysis_phase(
                opportunity_id, selected_competitor_urls=None
            )
            if analysis_result.get("status") == "failed":
                reason = analysis_result.get("message", "Unknown analysis error.")
                self.logger.warning(
                    f"Full auto workflow for {opportunity_id} stopped: {reason}"
                )
                raise RuntimeError(reason)

            self.db_manager.update_opportunity_workflow_state(
                opportunity_id,
                "analysis_completed",
                "paused_for_approval",
                error_message="Awaiting user approval to proceed to content generation.",
            )
            self.job_manager.update_job_progress(job_id, "Paused", "Analysis complete. Awaiting user approval.", status="paused")
            self.logger.info(
                f"Full auto workflow for {opportunity_id} paused after analysis, awaiting user approval."
            )
            # This background job ends here. A new one is started by the 'approve_analysis' endpoint.
            # So, we set the job to a 'paused' but technically 'completed' state from the runner's perspective.
            self.job_manager.update_job_status(job_id, "paused", progress=50, result={"status": "paused", "message": "Awaiting approval."})

        except Exception as e:
            error_msg = f"Full auto workflow for {opportunity_id} failed: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            # The job's _run_job wrapper will catch this and handle the final 'failed' state.
            raise

    def run_full_auto_workflow(
        self, opportunity_id: int, override_validation: bool = False
    ) -> str:
        """Public method to initiate the full auto workflow asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Full Auto Workflow for Opportunity ID: {opportunity_id} (Async) with override: {override_validation} ---"
        )
        job_id = self.job_manager.create_job(
            self.client_id,
            target_function=self._run_full_auto_workflow_background,
            args=(opportunity_id, override_validation),
        )
        return job_id

    def _run_full_automation_workflow_background(
        self, job_id: str, opportunity_id: int, override_validation: bool
    ):
        """Internal method for the true 'fire and forget' full automation workflow."""
        try:
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "in_progress", "running"
            )
            self.job_manager.update_job_progress(job_id, "Workflow Started", "Starting full automation workflow.")

            if not override_validation:
                self.job_manager.update_job_progress(job_id, "Validation", "Running validation checks.")
                validation_result = self.run_validation_phase(opportunity_id)
                if validation_result.get("status") == "failed":
                    error_message = validation_result.get("message", "Unknown validation error.")
                    self.logger.warning(
                        f"Automation for opportunity {opportunity_id} stopped due to validation failure: {error_message}"
                    )
                    raise RuntimeError(f"Validation failed: {error_message}")
            else:
                self.logger.info(
                    f"Validation step skipped for opportunity {opportunity_id} due to override."
                )
                self.job_manager.update_job_progress(job_id, "Validation", "Validation skipped by user override.")
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id,
                    "validation_overridden",
                    "validated",
                    error_message="Validation manually overridden.",
                )

            self.job_manager.update_job_progress(job_id, "Analysis", "Starting in-depth analysis.")
            analysis_result = self.run_analysis_phase(
                opportunity_id, selected_competitor_urls=None
            )
            if analysis_result.get("status") == "failed":
                reason = analysis_result.get("message", "Unknown analysis error.")
                self.logger.warning(
                    f"Full automation for {opportunity_id} stopped: {reason}"
                )
                raise RuntimeError(reason)

            self.logger.info(
                f"Analysis complete for {opportunity_id}, proceeding directly to content generation."
            )
            self.job_manager.update_job_progress(job_id, "Content Generation", "Analysis complete, starting content generation.")

            # This now calls the content generation logic which also uses update_job_progress
            self._run_full_content_generation_background(job_id, opportunity_id, overrides=None)
            
            # The _run_job wrapper will mark the job as completed.
            # We add the redirect_url to the result.
            final_result = {
                "status": "success",
                "message": "Full automation workflow completed successfully.",
                "redirect_url": f"/opportunities/{opportunity_id}"
            }
            self.job_manager.update_job_status(job_id, "completed", progress=100, result=final_result)

        except Exception as e:
            error_msg = f"Full automation workflow for {opportunity_id} failed: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            # The job's _run_job wrapper will catch this and handle the final 'failed' state.
            raise
    
    # ... (keep existing public methods like run_full_automation_workflow) ...

    def continue_workflow_after_approval(self, job_id: str, opportunity_id: int, overrides: dict = None) -> str:
        """
        Continues a paused workflow job after user approval, proceeding to content generation.
        This now creates a NEW job for the content generation phase.
        """
        self.logger.info(
            f"Attempting to resume workflow from job {job_id} on opportunity {opportunity_id}."
        )

        job_info = self.job_manager.get_job_status(job_id)
        if not job_info or job_info.get("status") != "paused":
            error_msg = f"Cannot resume job {job_id}: Job not found or not in 'paused' state. Current status: {job_info.get('status') if job_info else 'Not Found'}."
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        self.db_manager.update_opportunity_status(opportunity_id, "in_progress")

        # Create a new job for the content generation part of the workflow
        new_job_id = self.job_manager.create_job(
            self.client_id,
            target_function=self._run_full_content_generation_background,
            args=(opportunity_id, overrides),
        )
        
        # Link the old job to the new one for traceability if needed
        self.job_manager.update_job_status(job_id, "completed", progress=100, result={"status": "resumed", "next_job_id": new_job_id})

        return new_job_id

    def _run_content_refresh_workflow_background(
        self, job_id: str, opportunity_id: int
    ):
        """Internal method to execute content refresh for a job."""
        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                error_msg = f"Opportunity {opportunity_id} not found for content refresh."
                self.logger.error(error_msg)
                raise ValueError(error_msg)

            self.logger.info(
                f"--- Orchestrator: Starting Content Refresh Workflow for '{opportunity.get('keyword')}' ---"
            )
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "refresh_started", "running"
            )
            self.job_manager.update_job_progress(job_id, "Refresh Started", "Re-analyzing content and SERP data.")

            analysis_result = self.run_analysis_phase(
                opportunity_id, selected_competitor_urls=None
            )
            if analysis_result.get("status") == "failed":
                error_message = analysis_result.get("message", "Unknown analysis error.")
                self.logger.error(f"Refresh failed during analysis for opportunity {opportunity_id}: {error_message}")
                raise RuntimeError(f"Refresh failed during analysis: {error_message}")

            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity or opportunity.get("status") != "analyzed":
                error_msg = f"Opportunity {opportunity_id} not in 'analyzed' state after refresh analysis. Current status: {opportunity.get('status') if opportunity else 'Not Found'}."
                self.logger.error(error_msg)
                raise RuntimeError(error_msg)

            self.job_manager.update_job_progress(job_id, "Content Generation", "Analysis complete, re-generating content.")
            
            self._run_full_content_generation_background(job_id, opportunity_id, overrides=None)

            self.db_manager.update_opportunity_workflow_state(
                opportunity_id,
                "refresh_completed",
                "generated",
                error_message="Content refreshed.",
            )
            
            final_result = {
                "status": "success",
                "message": "Content refresh completed successfully.",
                "redirect_url": f"/opportunities/{opportunity_id}"
            }
            self.job_manager.update_job_status(job_id, "completed", progress=100, result=final_result)
            self.logger.info(f"Content refresh for opportunity {opportunity_id} completed successfully.")

        except Exception as e:
            error_msg = f"Content refresh workflow failed for opportunity {opportunity_id}: {e}\n{traceback.format_exc()}"
            self.logger.error(error_msg)
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "refresh_failed", "failed", str(e)
            )
            # The job's _run_job wrapper will catch this and handle the final 'failed' state.
            raise

    def run_content_refresh_workflow(self, opportunity_id: int) -> str:
        """Public method to initiate an asynchronous content refresh workflow."""
        self.logger.info(
            f"--- Orchestrator: Initiating Content Refresh Workflow for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            self.client_id,
            target_function=self._run_content_refresh_workflow_background,
            args=(opportunity_id,),
        )
        return job_id
```

## File: pipeline/step_01_discovery/keyword_discovery/expander.py
```python
# pipeline/step_01_discovery/keyword_discovery/expander.py
import logging
from typing import List, Dict, Any, Optional

from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class NewKeywordExpander:
    def __init__(
        self,
        client: DataForSEOClientV2,
        config: Dict[str, Any],
        logger: Optional[logging.Logger] = None,
    ):
        self.client = client
        self.config = config
        self.logger = logger or logging.getLogger(self.__class__.__name__)

    def expand(
        self,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        existing_keywords: set,
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = False,
    ) -> Dict[str, Any]:
        if not discovery_modes:
            raise ValueError("At least one discovery mode must be selected.")

        # Filter out seed keywords that already exist
        original_seed_count = len(seed_keywords)
        seed_keywords = [
            kw for kw in seed_keywords if kw.lower() not in existing_keywords
        ]
        if not seed_keywords:
            self.logger.info(
                "All seed keywords already exist in the database. Skipping expansion."
            )
            return {
                "total_cost": 0.0,
                "raw_counts": {},
                "total_raw_count": 0,
                "total_unique_count": 0,
                "final_keywords": [],
            }
        self.logger.info(
            f"Filtered seed keywords from {original_seed_count} to {len(seed_keywords)}."
        )

        location_code = self.config.get("location_code")
        language_code = self.config.get("language_code")
        if not location_code or not language_code:
            raise ValueError("Location and language codes must be set.")

        # The frontend provides filters with 'keyword_data.' prefix, suitable for 'related_keywords'.
        # We need to create versions of these filters without the prefix for other modes.

        related_filters = filters
        ideas_filters = []
        if filters:
            for f in filters:
                new_filter = f.copy()
                if "field" in new_filter and new_filter["field"].startswith(
                    "keyword_data."
                ):
                    new_filter["field"] = new_filter["field"][len("keyword_data.") :]
                ideas_filters.append(new_filter)

        # Suggestions filters are the same as ideas filters (no prefix)
        suggestions_filters = ideas_filters

        structured_filters = {
            "ideas": ideas_filters,
            "suggestions": suggestions_filters,
            "related": related_filters,
        }

        # W21 FIX: Set default order_by if not provided, now structured as a dict
        if not order_by:
            ideas_suggestions_orderby = [
                "keyword_info.search_volume,desc",
            ]
            related_orderby = [
                "keyword_data.keyword_info.search_volume,desc",
            ]
            structured_orderby = {
                "ideas": ideas_suggestions_orderby,
                "suggestions": ideas_suggestions_orderby,
                "related": related_orderby,
            }
        else:
            # If order_by is provided from frontend, structure it for each mode
            related_orderby = [f"keyword_data.{rule}" for rule in order_by]
            structured_orderby = {
                "ideas": order_by,
                "suggestions": order_by,
                "related": related_orderby,
            }

        # Make a single burst call to the DataForSEOClientV2
        all_ideas, total_cost = self.client.get_keyword_ideas(
            seed_keywords=seed_keywords,
            location_code=location_code,
            language_code=language_code,
            client_cfg=self.config,
            discovery_modes=discovery_modes,
            filters=structured_filters,  # Use the structured filters directly
            order_by=structured_orderby,
            limit=limit,
            depth=depth,
            ignore_synonyms_override=ignore_synonyms,
        )
        self.logger.info(
            f"Burst discovery completed. Found {len(all_ideas)} raw keyword ideas. Cost: ${total_cost:.4f}"
        )

        # Filter out any duplicates and existing keywords from the burst results
        final_keywords_deduplicated = []
        seen_keywords = set(
            existing_keywords
        )  # Start with already existing to prevent re-adding

        # Recalculate raw counts per source based on `discovery_source` field added by get_keyword_ideas
        raw_counts = {"keyword_ideas": 0, "suggestions": 0, "related": 0}
        for item in all_ideas:
            kw_text = item.get("keyword", "").lower()
            if kw_text and kw_text not in seen_keywords:
                final_keywords_deduplicated.append(item)
                seen_keywords.add(kw_text)
                source = item.get("discovery_source")
                if source in raw_counts:
                    raw_counts[source] += 1
            elif kw_text:
                self.logger.debug(
                    f"Skipping duplicate or existing keyword: {item.get('keyword')}"
                )

        self.logger.info(
            f"Total unique new keywords after deduplication: {len(final_keywords_deduplicated)}"
        )

        return {
            "total_cost": total_cost,
            "raw_counts": raw_counts,
            "total_raw_count": len(all_ideas),  # Total raw from API before processing
            "total_unique_count": len(final_keywords_deduplicated),
            "final_keywords": final_keywords_deduplicated,
        }
```

## File: pipeline/step_01_discovery/keyword_discovery/filters.py
```python
# pipeline/step_01_discovery/keyword_discovery/filters.py
import json
import logging
from typing import List, Any, Tuple, Dict

logger = logging.getLogger(__name__)

FORBIDDEN_API_FILTER_FIELDS = [
    "relevance",
    "sv_bing",
    "sv_clickstream",
]  # Define forbidden fields


def sanitize_filters_for_api(filters: List[Any]) -> List[Any]:
    """
    Removes any filters attempting to use forbidden internal metrics or data sources.
    """
    sanitized = []
    for item in filters:
        if isinstance(item, list) and len(item) >= 1 and isinstance(item[0], str):
            field_path = item[0].lower()
            if any(
                forbidden in field_path for forbidden in FORBIDDEN_API_FILTER_FIELDS
            ):
                logger.warning(
                    f"Forbidden field '{field_path}' detected in API filter. Removing it."
                )
                continue
        sanitized.append(item)
    return sanitized


def build_discovery_filters(config: Dict[str, Any]) -> Tuple[List[Any], List[Any]]:
    """
    Builds filter lists for API-side filtering for KD, SV, Competition, and Intent.
    """
    std_api_filters = []
    rel_api_filters = []

    min_sv = config.get("min_search_volume")
    if min_sv is not None:
        std_api_filters.extend([["keyword_info.search_volume", ">=", min_sv], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword_info.search_volume", ">=", min_sv], "and"]
        )

    max_kd = config.get("max_keyword_difficulty")
    if max_kd is not None:
        std_api_filters.extend(
            [["keyword_properties.keyword_difficulty", "<=", max_kd], "and"]
        )
        rel_api_filters.extend(
            [
                ["keyword_data.keyword_properties.keyword_difficulty", "<=", max_kd],
                "and",
            ]
        )

    allowed_comp_levels = config.get("allowed_competition_levels")
    if allowed_comp_levels:
        std_api_filters.extend(
            [["keyword_info.competition_level", "in", allowed_comp_levels], "and"]
        )
        rel_api_filters.extend(
            [
                [
                    "keyword_data.keyword_info.competition_level",
                    "in",
                    allowed_comp_levels,
                ],
                "and",
            ]
        )

    allowed_intents = config.get("allowed_intents")
    if config.get("enforce_intent_filter", False) and allowed_intents:
        std_api_filters.extend(
            [["search_intent_info.main_intent", "in", allowed_intents], "and"]
        )
        rel_api_filters.extend(
            [
                ["keyword_data.search_intent_info.main_intent", "in", allowed_intents],
                "and",
            ]
        )

    # NEW: Closely Variants
    closely_variants = config.get("closely_variants")
    if closely_variants is not None:
        std_api_filters.extend(
            [["closely_variants", "=", closely_variants], "and"]
        )  # This param is at top level
        # Related keywords endpoint does not have closely_variants

    # NEW: CPC Range Filters
    min_cpc_filter = config.get("min_cpc_filter")
    max_cpc_filter = config.get("max_cpc_filter")
    if min_cpc_filter is not None:
        std_api_filters.extend([["keyword_info.cpc", ">=", min_cpc_filter], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword_info.cpc", ">=", min_cpc_filter], "and"]
        )
    if max_cpc_filter is not None:
        std_api_filters.extend([["keyword_info.cpc", "<=", max_cpc_filter], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword_info.cpc", "<=", max_cpc_filter], "and"]
        )

    # NEW: Competition Range Filters
    min_competition = config.get("min_competition")
    max_competition = config.get("max_competition")
    if min_competition is not None:
        std_api_filters.extend(
            [["keyword_info.competition", ">=", min_competition], "and"]
        )
        rel_api_filters.extend(
            [["keyword_data.keyword_info.competition", ">=", min_competition], "and"]
        )
    if max_competition is not None:
        std_api_filters.extend(
            [["keyword_info.competition", "<=", max_competition], "and"]
        )
        rel_api_filters.extend(
            [["keyword_data.keyword_info.competition", "<=", max_competition], "and"]
        )

    # NEW: Max Competition Level Filter
    max_competition_level = config.get("max_competition_level")
    if max_competition_level:
        levels = ["LOW", "MEDIUM", "HIGH"]
        allowed_levels = levels[: levels.index(max_competition_level) + 1]
        std_api_filters.extend(
            [["keyword_info.competition_level", "in", allowed_levels], "and"]
        )
        rel_api_filters.extend(
            [
                ["keyword_data.keyword_info.competition_level", "in", allowed_levels],
                "and",
            ]
        )

    # NEW: Regex Filter (from Task 34)
    search_phrase_regex = config.get("search_phrase_regex")
    if search_phrase_regex and search_phrase_regex.strip():
        std_api_filters.extend([["keyword", "regex", search_phrase_regex], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword", "regex", search_phrase_regex], "and"]
        )

    if std_api_filters:
        std_api_filters.pop()
    if rel_api_filters:
        rel_api_filters.pop()

    # Apply sanitation (Weakness 3.7 Fix)
    std_api_filters = sanitize_filters_for_api(std_api_filters)
    rel_api_filters = sanitize_filters_for_api(rel_api_filters)

    logger.info(f"Built standard API filters: {json.dumps(std_api_filters)}")
    logger.info(f"Built related API filters: {json.dumps(rel_api_filters)}")

    return std_api_filters, rel_api_filters
```

## File: pipeline/step_01_discovery/__init__.py
```python
# pipeline/step_01_discovery/__init__.py
```

## File: pipeline/step_01_discovery/blog_content_qualifier.py
```python
# pipeline/step_01_discovery/blog_content_qualifier.py
from typing import Dict, Any, Tuple
from .disqualification_rules import apply_disqualification_rules


def assign_status_from_score(
    opportunity: Dict[str, Any], score: float, client_cfg: Dict[str, Any]
) -> Tuple[str, str]:
    """
    Assigns a final status to a keyword based on its score and hard disqualification rules.
    """
    # First, check for hard-stop, non-negotiable disqualification rules.
    is_disqualified, reason, is_hard_stop = apply_disqualification_rules(
        opportunity, client_cfg, cannibalization_checker=None
    )

    if is_disqualified and is_hard_stop:
        return "rejected", reason

    # If not hard-stopped, categorize based on the strategic score.
    if score >= client_cfg.get("qualified_threshold", 70):
        return "qualified", "Qualified: High strategic score."
    elif score >= client_cfg.get("review_threshold", 50):
        return "review", "Review: Moderate strategic score."
    else:
        return "rejected", f"Rejected: Low strategic score ({score:.1f})."
```

## File: pipeline/step_01_discovery/cannibalization_checker.py
```python
import logging
from typing import List, Dict, Any
from urllib.parse import urlparse

from backend.data_access.database_manager import DatabaseManager


class CannibalizationChecker:
    def __init__(
        self,
        target_domain: str,
        dataforseo_client: Any,
        client_cfg: Dict[str, Any],
        db_manager: DatabaseManager,
    ):
        self.target_domain = (
            target_domain.lower().replace("www.", "") if target_domain else None
        )
        self.logger = logging.getLogger(self.__class__.__name__)
        self.dataforseo_client = dataforseo_client
        self.client_cfg = client_cfg
        self.db_manager = db_manager

    def is_url_in_serp(
        self, serp_results: List[Dict[str, Any]], keyword: str, client_id: str
    ) -> bool:
        """
        Returns True if the target domain is found in the list of SERP results
        OR if the keyword already exists in the opportunities database for the client.
        """
        if self.db_manager.check_existing_keywords(client_id, [keyword]):
            self.logger.warning(
                f"Cannibalization detected: Keyword '{keyword}' already exists in the database for client '{client_id}'."
            )
            return True

        if not self.target_domain:
            return False

        for result in serp_results:
            try:
                url = result.get("url")
                if not url:
                    continue
                url_domain = urlparse(url).netloc.lower().replace("www.", "")
                if url_domain == self.target_domain or url_domain.endswith(
                    f".{self.target_domain}"
                ):
                    self.logger.warning(
                        f"Cannibalization detected: Found '{url}' in SERP for '{keyword}'."
                    )
                    return True
            except Exception:
                continue
        return False
```

## File: pipeline/step_01_discovery/disqualification_rules.py
```python
# pipeline/step_01_discovery/disqualification_rules.py
import logging
import re
from typing import Dict, Any, Tuple, Optional
from datetime import datetime
import numpy as np
from core import utils

from .cannibalization_checker import CannibalizationChecker


def apply_disqualification_rules(
    opportunity: Dict[str, Any],
    client_cfg: Dict[str, Any],
    cannibalization_checker: CannibalizationChecker,
) -> Tuple[bool, Optional[str], bool]:
    """
    Applies the comprehensive 20-rule set to disqualify a keyword based on data from the discovery phase.
    Reads all thresholds from client_cfg.
    Returns (is_disqualified, reason, is_hard_stop).
    """
    keyword = opportunity.get("keyword", "Unknown Keyword")

    # --- Failsafe Validation ---
    required_keys = [
        "keyword_info",
        "keyword_properties",
        "serp_info",
        "search_intent_info",
    ]
    for key in required_keys:
        if key not in opportunity or opportunity[key] is None:
            logging.getLogger(__name__).warning(
                f"Disqualifying '{keyword}' due to missing or null '{key}' data."
            )
            return True, f"Rule 1: Missing critical data structure ({key}).", True

    serp_info = opportunity.get("serp_info", {})
    if not serp_info:
        logging.getLogger(__name__).warning(
            f"Disqualifying '{keyword}' due to empty 'serp_info' data."
        )
        return True, "Rule 1: Missing SERP info data.", True

    keyword_info = opportunity.get("keyword_info") or {}
    keyword_props = opportunity.get("keyword_properties") or {}
    avg_backlinks = opportunity.get("avg_backlinks_info") or {}
    intent_info = opportunity.get("search_intent_info") or {}

    # New Rule: Reject if SV or KD is 0 or null
    search_volume = keyword_info.get("search_volume")
    keyword_difficulty = keyword_props.get("keyword_difficulty")

    if search_volume is None or search_volume == 0:
        return True, "Rule 0: Rejected due to zero or null Search Volume.", True
    
    if keyword_difficulty is None or keyword_difficulty == 0:
        return True, "Rule 0: Rejected due to zero or null Keyword Difficulty.", True

    # Tier 1: Foundational Checks
    if not all([keyword_info, keyword_props, intent_info]):
        return (
            True,
            "Rule 1: Missing critical data structures (keyword_info, keyword_properties, or search_intent_info).",
            True,
        )

    # Rule 2: Check primary intent
    allowed_intents = client_cfg.get("allowed_intents", ["informational"])
    main_intent = intent_info.get("main_intent")
    foreign_intents = intent_info.get("foreign_intent", [])

    if main_intent not in allowed_intents:
        return True, f"Rule 2: Non-target main intent ('{main_intent}').", True

    # Rule 2b (NEW): Check secondary intents for prohibitive types
    prohibited_intents = set(client_cfg.get("prohibited_intents", ["navigational"]))
    foreign_intents = intent_info.get("foreign_intent", []) or []
    if not prohibited_intents.isdisjoint(set(foreign_intents)):
        offending_intents = prohibited_intents.intersection(set(foreign_intents))
        return (
            True,
            f"Rule 2b: Contains a prohibited secondary intent ({', '.join(offending_intents)}).",
            True,
        )

    if keyword_props.get("is_another_language"):
        return True, "Rule 3: Language mismatch.", True

    negative_keywords = set(
        kw.lower() for kw in client_cfg.get("negative_keywords", [])
    )
    core_keyword = keyword_props.get("core_keyword")
    if any(neg_kw in keyword.lower() for neg_kw in negative_keywords) or (
        core_keyword
        and any(neg_kw in core_keyword.lower() for neg_kw in negative_keywords)
    ):
        return True, "Rule 4: Contains a negative keyword.", True

    # Tier 2: Volume & Trend Analysis
    if utils.safe_compare(
        keyword_info.get("search_volume"), client_cfg.get("min_search_volume"), "lt"
    ):
        return (
            True,
            f"Rule 5: Below search volume floor (minimum: {client_cfg.get('min_search_volume', 100)} SV). Current: {keyword_info.get('search_volume', 0)} SV.",
            False,
        )

    trends = keyword_info.get("search_volume_trend", {})
    try:
        yearly_trend = trends.get("yearly")
        quarterly_trend = trends.get("quarterly")

        yearly_threshold = client_cfg.get("yearly_trend_decline_threshold", -25)
        quarterly_threshold = client_cfg.get("quarterly_trend_decline_threshold", 0)

        yearly_check = utils.safe_compare(yearly_trend, yearly_threshold, "lt")
        quarterly_check = utils.safe_compare(quarterly_trend, quarterly_threshold, "lt")

        if yearly_check and quarterly_check:
            return (
                True,
                f"Rule 6: Consistently declining trend. Yearly trend: {yearly_trend}% (below {yearly_threshold}% threshold), Quarterly trend: {quarterly_trend}% (below {quarterly_threshold}% threshold). Consider manual review for seasonality.",
                False,
            )
    except TypeError:
        logging.getLogger(__name__).error(
            f"TypeError during trend analysis for keyword '{keyword}'. "
            f"trends.get('yearly') value: {trends.get('yearly')}, type: {type(trends.get('yearly'))}. "
            f"trends.get('quarterly') value: {trends.get('quarterly')}, type: {type(trends.get('quarterly'))}."
        )
        return (
            True,
            "Rule 6: Failed to process trend data due to invalid format.",
            False,
        )

    monthly_searches = keyword_info.get("monthly_searches", [])
    if monthly_searches and len(monthly_searches) > 1:
        volumes = [
            ms["search_volume"]
            for ms in monthly_searches
            if ms.get("search_volume") is not None and ms["search_volume"] > 0
        ]
        if len(volumes) > 1 and np.mean(volumes) > 0:
            volatility_threshold = client_cfg.get(
                "search_volume_volatility_threshold", 1.5
            )
            std_dev_to_mean_ratio = np.std(volumes) / np.mean(volumes)
            if std_dev_to_mean_ratio > volatility_threshold:
                return (
                    True,
                    f"Rule 7: Extreme search volume volatility. Std Dev / Mean ratio: {std_dev_to_mean_ratio:.2f} (above {volatility_threshold} threshold). Could indicate a fleeting trend or strong seasonality. Manual review recommended.",
                    False,
                )

    # Rule 7b: Check for recent sharp decline using raw monthly searches
    monthly_searches = opportunity.get(
        "monthly_searches", []
    )  # Get from opportunity object, which is deserialized
    if monthly_searches and len(monthly_searches) >= 4:
        # Sort by year and month to ensure correctness (most recent first for trend)
        try:
            sorted_searches = sorted(
                monthly_searches, key=lambda x: (x["year"], x["month"]), reverse=True
            )
            if len(sorted_searches) >= 4:
                # Compare latest month with 3 months prior (index 0 vs index 3)
                latest_vol = sorted_searches[0].get("search_volume")
                past_vol = sorted_searches[3].get("search_volume")

                if latest_vol is not None and past_vol is not None and past_vol > 0:
                    if (
                        latest_vol / past_vol
                    ) < 0.5:  # If volume has dropped by more than 50% in 3 months
                        return (
                            True,
                            "Rule 7b: Recent sharp decline in search volume (>50% drop in last 3 months).",
                            False,
                        )
        except (TypeError, KeyError):
            logging.getLogger(__name__).warning(
                f"Could not parse monthly_searches for recent trend analysis on keyword '{keyword}'."
            )

    # Tier 3: Commercial & Competitive Analysis
    if utils.safe_compare(
        keyword_info.get("competition"),
        client_cfg.get("max_paid_competition_score", 0.8),
        "gt",
    ) and (keyword_info.get("competition_level") == "HIGH"):
        return True, "Rule 8: Excessive paid competition.", False

    if utils.safe_compare(
        keyword_info.get("high_top_of_page_bid"),
        client_cfg.get("max_high_top_of_page_bid", 15.0),
        "gt",
    ):
        return (
            True,
            f"Rule 9: Prohibitively high CPC bids (${client_cfg.get('max_high_top_of_page_bid', 15.00)}).",
            False,
        )

    if utils.safe_compare(
        keyword_props.get("keyword_difficulty"),
        client_cfg.get("max_kd_hard_limit", 70),
        "gt",
    ):
        return (
            True,
            f"Rule 10: Extreme keyword difficulty (>{client_cfg.get('max_kd_hard_limit', 70)}).",
            False,
        )

    if utils.safe_compare(
        avg_backlinks.get("referring_main_domains"),
        client_cfg.get("max_referring_main_domains_limit", 100),
        "gt",
    ):
        return (
            True,
            f"Rule 11: Overly authoritative competitor domains (>{client_cfg.get('max_referring_main_domains_limit', 100)} referring main domains).",
            False,
        )

    if utils.safe_compare(
        avg_backlinks.get("main_domain_rank"),
        client_cfg.get("max_avg_domain_rank_threshold", 500),
        "lt",
    ):
        return (
            True,
            f"Rule 12: SERP dominated by high-authority domains (avg rank < {client_cfg.get('max_avg_domain_rank_threshold', 500)}).",
            False,
        )

    if (avg_backlinks.get("referring_domains") or 0) > 0:
        pages_to_domain_ratio = (avg_backlinks.get("referring_pages") or 0) / (
            avg_backlinks.get("referring_domains") or 1
        )
        if pages_to_domain_ratio > client_cfg.get("max_pages_to_domain_ratio", 15):
            return (
                True,
                "Rule 13: Potential spammy competitor profile (high page/domain ratio).",
                False,
            )

    # Tier 4: Content, SERP & Keyword Structure

    # Rule: Check for hostile SERP environment
    is_hostile, hostile_reason = _check_hostile_serp_environment(opportunity)
    if is_hostile:
        return True, hostile_reason, True

    non_evergreen_pattern = _get_non_evergreen_year_pattern()
    if non_evergreen_pattern and re.search(non_evergreen_pattern, keyword):
        return (
            True,
            "Rule 14: Non-evergreen temporal keyword (matches pattern for past/current years).",
            False,
        )

    word_count = len(keyword.split())
    is_question = utils.is_question_keyword(keyword)  # This now exists

    min_wc = client_cfg.get("min_keyword_word_count", 2)
    max_wc = client_cfg.get("max_keyword_word_count", 8)

    is_outside_range = word_count < min_wc or word_count > max_wc

    # Rule 15 (Refined with override): Check word count and potentially override for high-value keywords
    if is_outside_range and not is_question:
        sv = keyword_info.get("search_volume", 0)
        cpc = keyword_info.get("cpc")  # Get the value, which could be None
        if cpc is None:
            cpc = 0.0  # Default to 0.0 if it's None

        high_sv_override = client_cfg.get("high_value_sv_override_threshold", 10000)
        high_cpc_override = client_cfg.get("high_value_cpc_override_threshold", 5.0)

        if sv >= high_sv_override or cpc >= high_cpc_override:
            logging.getLogger(__name__).info(
                f"Override: High value SV/CPC bypasses word count rule for '{keyword}'."
            )
            pass  # Allow the keyword to proceed
        else:
            return (
                True,
                f"Rule 15: Non-question keyword word count ({word_count}) is outside the acceptable range ({min_wc}-{max_wc} words).",
                False,
            )

    serp_info = opportunity.get("serp_info", {})
    serp_types = set(serp_info.get("serp_item_types", []))

    crowded_features = {
        "video",
        "images",
        "people_also_ask",
        "carousel",
        "featured_snippet",
        "short_videos",
    }
    if len(serp_types.intersection(crowded_features)) > client_cfg.get(
        "crowded_serp_features_threshold", 4
    ):
        return (
            True,
            f"Rule 17: SERP is overly crowded (>{client_cfg.get('crowded_serp_features_threshold', 4)} attention-grabbing features).",
            False,
        )

    # Rule 18: Check for navigational intent safely
    is_navigational = False
    if intent_info:
        if intent_info.get("main_intent") == "navigational":
            is_navigational = True
        else:
            foreign_intent = intent_info.get("foreign_intent")
            if foreign_intent and "navigational" in foreign_intent:
                is_navigational = True
    if is_navigational:
        return True, "Rule 18: Strong navigational intent.", True

    if serp_info.get("last_updated_time") and serp_info.get("previous_updated_time"):
        try:
            last_update = datetime.fromisoformat(
                serp_info["last_updated_time"].replace(" +00:00", "")
            )
            prev_update = datetime.fromisoformat(
                serp_info["previous_updated_time"].replace(" +00:00", "")
            )
            days_between_updates = (last_update - prev_update).days
            if days_between_updates < client_cfg.get("min_serp_stability_days", 14):
                return (
                    True,
                    f"Rule 19: Unstable SERP (updated every {days_between_updates} days).",
                    False,
                )
        except ValueError:
            logging.getLogger(__name__).warning(
                f"Could not parse SERP update times for '{keyword}': {serp_info.get('last_updated_time')}, {serp_info.get('previous_updated_time')}"
            )

    cpc_value = keyword_info.get("cpc")
    if cpc_value is None:
        cpc_value = 0.0
    if (
        intent_info.get("main_intent") in ["commercial", "transactional"]
        and cpc_value == 0
    ):
        return True, "Rule 20: Low-value commercial intent (zero CPC).", False

    return False, None, False


def _check_hostile_serp_environment(
    opportunity: Dict[str, Any],
) -> Tuple[bool, Optional[str]]:
    """
    Rule 16: Disqualifies keywords where the SERP is dominated by features hostile to blog content.
    """
    serp_info = opportunity.get("serp_info", {})
    if not serp_info:
        return False, None  # Cannot analyze if SERP info is missing

    serp_types = set(serp_info.get("serp_item_types", []))

    # Define hostile features based on detailed SERP analysis
    HOSTILE_FEATURES = {
        # Strong transactional/e-commerce intent
        "shopping",
        "popular_products",
        "refine_products",
        "explore_brands",
        # Strong local intent
        "local_pack",
        "map",
        "local_services",
        # Purely transactional/utility intent (Google-owned tools)
        "google_flights",
        "google_hotels",
        "hotels_pack",
        # App-related intent
        "app",
        # Job-seeking intent
        "jobs",
        # Direct utility/tool intent
        "math_solver",
        "currency_box",
        "stocks_box",
    }

    found_hostile_features = serp_types.intersection(HOSTILE_FEATURES)

    if found_hostile_features:
        return (
            True,
            f"Rule 16: SERP is hostile to blog content. Contains dominant non-article features: {', '.join(found_hostile_features)}.",
        )

    return False, None


def _get_non_evergreen_year_pattern() -> str:
    """
    Generates a regex pattern to find past years up to the current year,
    dynamically adjusting to avoid disqualifying valid keywords in the future.
    Example for current year 2024: \b(201\d|202[0-4])\b
    """
    current_year = datetime.now().year

    patterns = []
    # Handle decades before the current one (e.g., 2010s)
    for decade_start in range(2010, (current_year // 10) * 10, 10):
        patterns.append(
            f"{decade_start}|{decade_start + 1}|{decade_start + 2}|{decade_start + 3}|{decade_start + 4}|{decade_start + 5}|{decade_start + 6}|{decade_start + 7}|{decade_start + 8}|{decade_start + 9}"
        )

    # Handle years in the current decade up to the current year
    current_decade_start_year = (current_year // 10) * 10
    current_decade_years = [
        str(year) for year in range(current_decade_start_year, current_year + 1)
    ]
    if current_decade_years:
        patterns.append("|".join(current_decade_years))

    if not patterns:
        return ""  # Should not happen unless current_year is before 2010

    return r"\b(" + "|".join(patterns) + r")\b"
```

## File: pipeline/step_01_discovery/keyword_expander.py
```python
# pipeline/step_01_discovery/keyword_expander.py
import logging
from typing import List, Dict, Any, Optional
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from .keyword_discovery.expander import NewKeywordExpander


class KeywordExpander:
    """
    A wrapper class that uses the new modular keyword expansion system.
    """

    def __init__(
        self,
        client: DataForSEOClientV2,
        config: Dict[str, Any],
        run_logger: Optional[logging.Logger] = None,
    ):
        self.client = client
        self.config = config
        self.logger = run_logger or logging.getLogger(self.__class__.__name__)
        self.expander = NewKeywordExpander(client, config, self.logger)

    def expand_seed_keyword(
        self,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        existing_keywords: set,
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = False,
    ) -> Dict[str, Any]:
        """
        Delegates the keyword expansion to the new NewKeywordExpander.
        """
        self.logger.info(
            f"Starting keyword expansion with {len(seed_keywords)} seeds and modes: {discovery_modes}"
        )

        results = self.expander.expand(
            seed_keywords,
            discovery_modes,
            filters,
            order_by,
            existing_keywords,
            limit,
            depth,
            ignore_synonyms,
        )

        self.logger.info(
            f"Keyword expansion complete. Found {results['total_unique_count']} unique keywords."
        )

        return results
```

## File: pipeline/step_01_discovery/run_discovery.py
```python
import logging
from typing import List, Dict, Any, Optional

from data_access.database_manager import DatabaseManager
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from pipeline.step_01_discovery.keyword_expander import KeywordExpander
from pipeline.step_01_discovery.disqualification_rules import (
    apply_disqualification_rules,
)
from pipeline.step_01_discovery.cannibalization_checker import CannibalizationChecker
from pipeline.step_03_prioritization.scoring_engine import ScoringEngine
from pipeline.step_01_discovery.blog_content_qualifier import assign_status_from_score
from backend.services.serp_analysis_service import SerpAnalysisService


def run_discovery_phase(
    seed_keywords: List[str],
    dataforseo_client: DataForSEOClientV2,
    db_manager: "DatabaseManager",
    client_id: str,
    client_cfg: Dict[str, Any],
    discovery_modes: List[str],
    filters: Optional[List[Any]],
    order_by: Optional[List[str]],
    limit: Optional[int] = None,
    depth: Optional[int] = None,
    ignore_synonyms: Optional[bool] = False,
    include_clickstream_data: Optional[bool] = None,
    closely_variants: Optional[bool] = None,
    run_logger: Optional[logging.Logger] = None,
) -> Dict[str, Any]:
    logger = run_logger or logging.getLogger(__name__)
    logger.info("--- Starting Consolidated Keyword Discovery & Scoring Phase ---")

    expander = KeywordExpander(dataforseo_client, client_cfg, logger)
    cannibalization_checker = CannibalizationChecker(
        client_cfg.get("target_domain"), dataforseo_client, client_cfg, db_manager
    )
    scoring_engine = ScoringEngine(client_cfg)

    # 1. Get keywords that already exist for this client to avoid API calls for them.
    existing_keywords = set(db_manager.get_all_processed_keywords_for_client(client_id))
    logger.info(
        f"Found {len(existing_keywords)} existing keywords to exclude from API request."
    )

    # 2. Expand seed keywords into a large list of opportunities.
    expansion_result = expander.expand_seed_keyword(
        seed_keywords,
        discovery_modes,
        filters,
        order_by,
        existing_keywords,
        limit,
        depth,
        ignore_synonyms,
    )

    all_expanded_keywords = expansion_result.get("final_keywords", [])
    total_cost = expansion_result.get("total_cost", 0.0)

    # --- Scoring and Disqualification Loop (Consolidated Logic) ---
    processed_opportunities = []
    disqualification_reasons = {}
    status_counts = {"qualified": 0, "review": 0, "rejected": 0}
    required_keys = [
        "keyword_info",
        "keyword_properties",
        "serp_info",
        "search_intent_info",
    ]

    for opp in all_expanded_keywords:
        # Pre-validation of opportunity structure
        missing_keys = [
            key for key in required_keys if key not in opp or opp[key] is None
        ]
        if missing_keys:
            logger.warning(
                f"Skipping opportunity '{opp.get('keyword')}' due to missing required data: {', '.join(missing_keys)}"
            )
            continue

        # 3. Apply Hard Disqualification Rules (Cannibalization, Negative Keywords, etc.)
        is_disqualified, reason, is_hard_stop = apply_disqualification_rules(
            opp, client_cfg, cannibalization_checker
        )

        if is_disqualified and is_hard_stop:
            opp["status"] = "rejected"
            opp["blog_qualification_status"] = "rejected"
            opp["blog_qualification_reason"] = reason
            status_counts["rejected"] += 1
            disqualification_reasons[reason] = (
                disqualification_reasons.get(reason, 0) + 1
            )
        else:
            # 4. Score the remaining keywords
            score, breakdown = scoring_engine.calculate_score(opp)
            opp["strategic_score"] = score
            opp["score_breakdown"] = breakdown

            # 5. Assign Status based on Strategic Score
            status, reason = assign_status_from_score(opp, score, client_cfg)
            opp["status"] = status
            opp["blog_qualification_status"] = status
            opp["blog_qualification_reason"] = reason
            status_counts[status.split("_")[0]] = (
                status_counts.get(status.split("_")[0], 0) + 1
            )  # count qualified/review/rejected

        processed_opportunities.append(opp)

    disqualified_count = status_counts.get("rejected", 0)
    passed_count = status_counts.get("qualified", 0) + status_counts.get("review", 0)

    logger.info(
        f"Scoring and Qualification complete. Passed: {passed_count}, Rejected: {disqualified_count}."
    )

    stats = {
        **expansion_result,
        "disqualification_reasons": disqualification_reasons,
        "disqualified_count": disqualified_count,
        "final_qualified_count": passed_count,
    }

    return {
        "stats": stats,
        "total_cost": total_cost,
        "opportunities": processed_opportunities,
    }
```

## File: pipeline/step_02_qualification/__init__.py
```python
# pipeline/step_02_qualification/__init__.py
```

## File: pipeline/step_02_qualification/competitor_analyzer.py
```python
import logging
from typing import List, Dict, Any, Tuple

from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class CompetitorAnalyzer:
    """
    Analyzes top organic competitors from SERP data.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.min_word_count = self.config.get("min_competitor_word_count", 300)

    def analyze_competitors(
        self, top_results: List[Dict[str, Any]]
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Fetches OnPage data for top competitors and performs a basic analysis.
        """
        if not top_results:
            return [], 0.0

        # Limit the number of competitors to analyze based on config
        num_to_analyze = self.config.get("num_competitors_to_analyze", 5)
        competitor_urls = [
            res["url"] for res in top_results[:num_to_analyze] if res.get("url")
        ]

        onpage_results, cost = self.client.get_onpage_data_for_urls(competitor_urls)

        if not onpage_results:
            return [], cost

        analyzed_competitors = []
        for result in onpage_results:
            if "error" in result:
                self.logger.warning(
                    f"Could not analyze competitor {result.get('url')}: {result.get('error')}"
                )
                continue

            content_meta = result.get("meta", {}).get("content", {})
            word_count = content_meta.get("plain_text_word_count")
            if word_count and word_count >= self.min_word_count:
                analyzed_competitors.append(
                    {
                        "url": result.get("url"),
                        "word_count": word_count,
                        "readability_score": content_meta.get(
                            "flesch_kincaid_readability_index"
                        ),
                        "onpage_score": result.get("onpage_score"),
                        "internal_links": result.get("meta", {}).get(
                            "internal_links_count"
                        ),
                        "external_links": result.get("meta", {}).get(
                            "external_links_count"
                        ),
                        "headings": result.get("meta", {}).get("htags"),
                    }
                )

        return analyzed_competitors, cost
```

## File: pipeline/step_02_qualification/serp_analyzer.py
```python
import logging
from typing import Dict, Any, Tuple, Optional

from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from datetime import datetime


class SerpAnalyzer:
    """
    Analyzes the SERP for a given keyword to extract key insights.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def analyze_serp(self, keyword: str) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Fetches SERP data and extracts insights like featured snippets, PAA, etc.
        """
        location_code = self.config.get("location_code")
        language_code = self.config.get("language_code")

        serp_results, cost = self.client.get_serp_results(
            keyword, location_code, language_code
        )

        if not serp_results:
            return None, cost

        analysis = {
            "serp_has_featured_snippet": False,
            "serp_has_video_results": False,
            "serp_has_ai_overview": False,
            "people_also_ask": [],
            "top_organic_results": [],
            "serp_last_updated_days_ago": None,
        }

        item_types = serp_results.get("item_types", [])
        if "featured_snippet" in item_types:
            analysis["serp_has_featured_snippet"] = True
        if "video" in item_types:
            analysis["serp_has_video_results"] = True
        if "ai_overview" in item_types:
            analysis["serp_has_ai_overview"] = True

        # Extract PAA and top organic results
        for item in serp_results.get("items", []):
            if item.get("type") == "people_also_ask":
                analysis["people_also_ask"] = [
                    q.get("title") for q in item.get("items", []) if q.get("title")
                ]
            elif item.get("type") == "organic":
                analysis["top_organic_results"].append(
                    {
                        "rank": item.get("rank_absolute"),
                        "url": item.get("url"),
                        "title": item.get("title"),
                        "domain": item.get("domain"),
                        "main_domain_rank": item.get(
                            "main_domain_rank", 1000
                        ),  # Default to low rank
                    }
                )

        # Calculate SERP freshness
        datetime_str = serp_results.get("datetime")
        if datetime_str:
            try:
                serp_date = datetime.strptime(datetime_str, "%Y-%m-%d %H:%M:%S +00:00")
                analysis["serp_last_updated_days_ago"] = (
                    datetime.utcnow() - serp_date
                ).days
            except ValueError:
                self.logger.warning(f"Could not parse SERP datetime: {datetime_str}")

        return analysis, cost
```

## File: pipeline/step_03_prioritization/scoring_components/__init__.py
```python
# pipeline/step_03_prioritization/scoring_components/__init__.py
from .ease_of_ranking import calculate_ease_of_ranking_score
from .traffic_potential import calculate_traffic_potential_score
from .commercial_intent import calculate_commercial_intent_score
from .growth_trend import calculate_growth_trend_score
from .serp_features import calculate_serp_features_score
from .serp_volatility import calculate_serp_volatility_score
from .competitor_weakness import calculate_competitor_weakness_score
from .serp_crowding import calculate_serp_crowding_score
from .keyword_structure import calculate_keyword_structure_score
from .serp_threat import calculate_serp_threat_score
from .volume_volatility import calculate_volume_volatility_score
from .serp_freshness import calculate_serp_freshness_score
from .competitor_performance import calculate_competitor_performance_score

__all__ = [
    "calculate_ease_of_ranking_score",
    "calculate_traffic_potential_score",
    "calculate_commercial_intent_score",
    "calculate_growth_trend_score",
    "calculate_serp_features_score",
    "calculate_serp_volatility_score",
    "calculate_competitor_weakness_score",
    "calculate_serp_crowding_score",
    "calculate_keyword_structure_score",
    "calculate_serp_threat_score",
    "calculate_volume_volatility_score",
    "calculate_serp_freshness_score",
    "calculate_competitor_performance_score",  # ADDED THIS LINE
]
```

## File: pipeline/step_03_prioritization/scoring_components/commercial_intent.py
```python
# pipeline/step_03_prioritization/scoring_components/commercial_intent.py
from typing import Dict, Any, Tuple
from backend.core import utils  # NEW: Import the utils module


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_commercial_intent_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the keyword's strategic value for blog content,
    balancing commercial indicators with the type of user intent.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    intent_info = (
        data.get("search_intent_info")
        if isinstance(data.get("search_intent_info"), dict)
        else {}
    )
    keyword = data.get("keyword", "")

    cpc = keyword_info.get("cpc", 0.0)
    if cpc is None:
        cpc = 0.0
    max_cpc = config.get("max_cpc_for_scoring", 10.0)
    cpc_score = _normalize_value(cpc, max_cpc)

    # Add bonus for wide CPC bid spread, indicating market inefficiency
    low_bid = keyword_info.get("low_top_of_page_bid", 0.0) or 0.0
    high_bid = keyword_info.get("high_top_of_page_bid", 0.0) or 0.0
    if low_bid > 0 and high_bid > low_bid:
        bid_spread_ratio = high_bid / low_bid
        if bid_spread_ratio > 5:  # e.g., low is $1, high is >$5
            cpc_score = min(100, cpc_score + 15)

    main_intent = intent_info.get("main_intent", "informational")
    foreign_intents = intent_info.get("foreign_intent", []) or []

    intent_scores = {
        "informational": 75,
        "commercial": 60,
        "transactional": 10,
        "navigational": 0,
    }
    intent_score = intent_scores.get(main_intent, 75)
    explanation = f"Base score for '{main_intent}' intent is {intent_score}."

    if main_intent == "informational" and (
        "commercial" in foreign_intents or "transactional" in foreign_intents
    ):
        intent_score = min(100, intent_score + 25)
        explanation += " Bonus for commercial secondary intent."

    # REPLACED: Use the centralized utility function
    if utils.is_question_keyword(keyword):
        intent_score = min(100, intent_score + 15)
        explanation += " Bonus for being a question keyword."

    competition_level = keyword_info.get("competition_level")
    if competition_level == "LOW":
        cpc_score = min(100, cpc_score + 20)

    final_score = (cpc_score * 0.5) + (intent_score * 0.5)
    breakdown = {
        "CPC & Competition": {
            "value": f"${cpc:.2f} ({competition_level})",
            "score": round(cpc_score),
            "explanation": "Normalized CPC, with bonuses for low competition and wide bid spread.",
        },
        "Strategic Intent": {
            "value": main_intent.title(),
            "score": round(intent_score),
            "explanation": explanation,
        },
    }
    return round(final_score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/competitor_performance.py
```python
from typing import Dict, Any, Tuple
import logging

logger = logging.getLogger(__name__)


def _normalize_value(
    value: float, target_value: float, is_lower_better: bool = True
) -> float:
    """Helper to normalize a value to a 0-100 scale, with target_value being ideal."""
    if value is None or target_value is None or target_value == 0:
        return 50.0  # Neutral score if data is missing or target is zero

    if is_lower_better:
        # Example: LCP target 2500ms.
        # If value is 1250, score = 100. If value is 5000, score = 0.
        # This formula provides 100 at 0, 50 at target, 0 at 2*target
        score = max(0.0, min(100.0, 100 * (1 - (value / (2 * target_value)))))
    else:
        # Example: High metric, higher is better. e.g. High security score
        score = max(
            0.0, min(100.0, 100 * (value / (2 * target_value)))
        )  # Max out at 2*target for 100, linear

    return score


def calculate_competitor_performance_score(
    opportunity: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the technical performance (e.g., Core Web Vitals)
    of top organic competitors. Weak competitor performance indicates a higher opportunity.
    """
    if not isinstance(opportunity, dict) or not opportunity.get("blueprint"):
        return 50.0, {"message": "Invalid opportunity data for scoring."}

    competitor_analysis = opportunity["blueprint"].get("competitor_analysis", [])
    if not competitor_analysis:
        return 50.0, {
            "message": "No competitor analysis available for performance scoring."
        }

    lcp_times = []
    for comp in competitor_analysis:
        if (
            comp.get("page_timing")
            and comp["page_timing"].get("largest_contentful_paint") is not None
        ):
            lcp_times.append(comp["page_timing"]["largest_contentful_paint"])

    if not lcp_times:
        return 50.0, {"message": "No LCP data available from competitors."}

    avg_lcp_ms = sum(lcp_times) / len(lcp_times)

    # Get the target LCP from client config (lower is better for performance)
    # This target defines what "good" performance is. Competitors worse than this are an opportunity.
    target_good_lcp_ms = config.get(
        "max_avg_lcp_time", 2500
    )  # Default to 2.5s as a good target

    # Score: higher if competitors' LCP is high (poor performance)
    # We want to invert the normalization: a higher LCP (worse performance) means higher score (better opportunity)
    # If avg_lcp_ms is 2*target_good_lcp_ms, score is 100. If it's target_good_lcp_ms, score is 50.
    score = _normalize_value(avg_lcp_ms, target_good_lcp_ms, is_lower_better=False)

    explanation = f"Average competitor LCP is {avg_lcp_ms:.0f}ms. Higher value indicates worse competitor performance, which is a better opportunity. Target LCP for good performance is {target_good_lcp_ms}ms."
    breakdown = {
        "Avg. Competitor LCP": {
            "value": f"{avg_lcp_ms:.0f}ms",
            "score": round(score),
            "explanation": explanation,
        }
    }

    return round(score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/competitor_weakness.py
```python
# pipeline/step_03_prioritization/scoring_components/competitor_weakness.py
from typing import Dict, Any, Tuple


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_competitor_weakness_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the authority of ranking competitors using
    data available at the discovery phase (avg_backlinks_info).
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    breakdown = {}
    avg_backlinks = (
        data.get("avg_backlinks_info")
        if isinstance(data.get("avg_backlinks_info"), dict)
        else {}
    )

    # 1. Average Main Domain Rank of competitors
    domain_rank = avg_backlinks.get("main_domain_rank", 500)
    max_rank = config.get("max_domain_rank_for_scoring", 1000)
    domain_rank_score = _normalize_value(domain_rank, max_rank, invert=True)
    breakdown["Avg. Domain Rank"] = {
        "value": f"{domain_rank:.0f}",
        "score": round(domain_rank_score),
        "explanation": f"Normalized against a max of {max_rank}. Lower is better.",
    }

    # 2. Average Referring Main Domains
    ref_domains = avg_backlinks.get("referring_main_domains", 50)
    max_ref_domains = config.get("max_referring_domains_for_scoring", 100)
    ref_domains_score = _normalize_value(ref_domains, max_ref_domains, invert=True)
    breakdown["Avg. Referring Domains"] = {
        "value": f"{ref_domains:.1f}",
        "score": round(ref_domains_score),
        "explanation": f"Normalized against a max of {max_ref_domains}. Lower is better.",
    }

    # Weighted average for final score
    final_score = (domain_rank_score * 0.6) + (ref_domains_score * 0.4)
    return round(final_score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/ease_of_ranking.py
```python
# pipeline/step_03_prioritization/scoring_components/ease_of_ranking.py
import math
from typing import Dict, Any, Tuple


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_ease_of_ranking_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """Calculates a score based on how easy it is to rank for the keyword."""
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    breakdown = {}
    keyword_props = (
        data.get("keyword_properties")
        if isinstance(data.get("keyword_properties"), dict)
        else {}
    )
    avg_backlinks = (
        data.get("avg_backlinks_info")
        if isinstance(data.get("avg_backlinks_info"), dict)
        else {}
    )
    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}

    # 1. Keyword Difficulty (KD)
    kd = keyword_props.get("keyword_difficulty", 50)
    kd_score = _normalize_value(kd, 100, invert=True)
    breakdown["Keyword Difficulty"] = {
        "value": kd,
        "score": round(kd_score),
        "explanation": "Lower is better.",
    }

    # 2. Average Main Domain Rank of competitors
    domain_rank = avg_backlinks.get("main_domain_rank", 500)
    max_rank = config.get("max_domain_rank_for_scoring", 1000)
    domain_rank_score = _normalize_value(domain_rank, max_rank, invert=True)
    breakdown["Avg. Domain Rank"] = {
        "value": f"{domain_rank:.0f}",
        "score": round(domain_rank_score),
        "explanation": f"Normalized against a max of {max_rank}. Lower is better.",
    }

    # 3. Average Page Rank of top competing pages
    page_rank = avg_backlinks.get("rank", 50)
    page_rank_score = _normalize_value(page_rank, 100, invert=True)
    breakdown["Avg. Page Rank"] = {
        "value": f"{page_rank:.0f}",
        "score": round(page_rank_score),
        "explanation": "Represents page-level authority. Lower is better.",
    }

    # 4. Dofollow Ratio
    total_backlinks = avg_backlinks.get("backlinks", 0)
    dofollow_backlinks = avg_backlinks.get("dofollow", 0)
    dofollow_ratio = dofollow_backlinks / total_backlinks if total_backlinks > 0 else 0
    dofollow_score = _normalize_value(
        dofollow_ratio, 1, invert=True
    )  # Lower ratio is better
    breakdown["Dofollow Ratio"] = {
        "value": f"{dofollow_ratio:.1%}",
        "score": round(dofollow_score),
        "explanation": "Ratio of dofollow links. A lower ratio indicates weaker competitor backlink profiles.",
    }

    # 5. Search Engine Results Count
    results_count = serp_info.get("se_results_count") or 0
    if results_count > 0:
        log_score = _normalize_value(
            math.log(results_count + 1), math.log(1_000_000_000 + 1), invert=True
        )
    else:
        log_score = 100.0
    breakdown["Total Results"] = {
        "value": f"{results_count:,}",
        "score": round(log_score),
        "explanation": "Log-normalized. Fewer competing pages is better.",
    }

    # Weighted average for final score
    final_score = (
        (kd_score * 0.40)
        + (domain_rank_score * 0.25)
        + (page_rank_score * 0.20)
        + (dofollow_score * 0.10)
        + (log_score * 0.05)
    )
    return round(final_score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/growth_trend.py
```python
# pipeline/step_03_prioritization/scoring_components/growth_trend.py
from typing import Dict, Any, Tuple
import math


def calculate_growth_trend_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a volume-weighted score based on the keyword's search volume trend.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    trends = (
        keyword_info.get("search_volume_trend")
        if isinstance(keyword_info.get("search_volume_trend"), dict)
        else {}
    )
    sv = keyword_info.get("search_volume", 0) or 0

    yearly = trends.get("yearly", 0)
    quarterly = trends.get("quarterly", 0)
    monthly = trends.get("monthly", 0)

    def score_trend(value):
        if value is None:
            return 50  # Neutral score for missing data
        if value > 25:
            return 100
        if value > 10:
            return 75
        if value < -25:
            return 0
        if value < -10:
            return 25
        return 50

    yearly_score = score_trend(yearly)
    quarterly_score = score_trend(quarterly)
    monthly_score = score_trend(monthly)

    base_trend_score = (
        (yearly_score * 0.3) + (quarterly_score * 0.4) + (monthly_score * 0.3)
    )

    # Weight the trend score by search volume magnitude
    # A log scale helps moderate the effect of massive search volumes
    sv_weight = min(
        math.log(sv + 1) / math.log(100000), 1.0
    )  # Normalize against 100k SV

    # Final score is a blend: 70% trend, 30% volume weight. This prevents tiny keywords with huge trends from dominating.
    final_score = (base_trend_score * 0.7) + (sv_weight * 100 * 0.3)

    explanation = f"Weighted score from trends (Y:{yearly}%, Q:{quarterly}%, M:{monthly}%) and search volume."
    breakdown = {
        "Growth Trend": {
            "value": f"{yearly}% YoY",
            "score": round(final_score),
            "explanation": explanation,
        }
    }
    return round(final_score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/keyword_structure.py
```python
# pipeline/step_03_prioritization/scoring_components/keyword_structure.py
from typing import Dict, Any, Tuple


def calculate_keyword_structure_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Scores the keyword based on its structure, rewarding the "long-tail sweet spot"
    and adding a bonus for search depth.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword = data.get("keyword", "")
    word_count = len(keyword.split())
    depth = data.get("depth", 0)  # From related_keywords endpoint

    # Score is based on word count, with a peak at the 4-6 word sweet spot
    if word_count >= 4 and word_count <= 6:
        score = 100.0
    elif word_count == 3 or word_count == 7:
        score = 75.0
    elif word_count == 2 or word_count == 8:
        score = 50.0
    else:  # 1 word or 9+ words
        score = 25.0

    # Add a bonus for depth, rewarding more specific queries
    if depth > 0:
        score = min(100, score + (depth * 5))  # +5 points per depth level

    explanation = f"Keyword has {word_count} words and search depth of {depth}. The 4-6 word range is the sweet spot."
    breakdown = {
        "Keyword Structure": {
            "value": f"{word_count} words (Depth: {depth})",
            "score": score,
            "explanation": explanation,
        }
    }

    return score, breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/serp_crowding.py
```python
# pipeline/step_03_prioritization/scoring_components/serp_crowding.py
from typing import Dict, Any, Tuple


def calculate_serp_crowding_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on how crowded the SERP is with attention-grabbing features.
    A less crowded SERP is a better opportunity.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    serp_types = set(serp_info.get("serp_item_types", []))

    # Define features that compete for user attention
    CROWDING_FEATURES = {
        "video",
        "short_videos",
        "images",
        "people_also_ask",
        "carousel",
        "multi_carousel",
        "featured_snippet",
        "ai_overview",
    }

    crowding_feature_count = len(serp_types.intersection(CROWDING_FEATURES))

    # The score is inverted: more features = lower score
    if crowding_feature_count >= 5:
        score = 0.0
    elif crowding_feature_count == 4:
        score = 25.0
    elif crowding_feature_count == 3:
        score = 50.0
    elif crowding_feature_count == 2:
        score = 75.0
    elif crowding_feature_count == 1:
        score = 90.0
    else:
        score = 100.0

    explanation = f"{crowding_feature_count} attention-grabbing features found. A lower count is better."
    breakdown = {
        "SERP Crowding": {
            "value": crowding_feature_count,
            "score": score,
            "explanation": explanation,
        }
    }

    return score, breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/serp_features.py
```python
# pipeline/step_03_prioritization/scoring_components/serp_features.py
from typing import Dict, Any, Tuple


def calculate_serp_features_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the SERP environment, rewarding opportunities
    and penalizing attention-grabbing distractions.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    serp_types = set(serp_info.get("serp_item_types", []))

    score = 50.0  # Start with a neutral base score
    notes = []

    # Positive modifiers for high-value features
    if "featured_snippet" in serp_types:
        score += config.get("featured_snippet_bonus", 40)
        notes.append("Featured Snippet (+40)")
    if "people_also_ask" in serp_types:
        score += 25
        notes.append("People Also Ask (+25)")

    # Negative modifiers for threats and attention-grabbing features
    if "ai_overview" in serp_types:
        score -= config.get("ai_overview_penalty", 20)
        notes.append("AI Overview (-20)")
    if "video" in serp_types or "short_videos" in serp_types:
        score -= 15
        notes.append("Video Results (-15)")
    if "images" in serp_types:
        score -= 10
        notes.append("Image Carousel (-10)")

    final_score = max(0, min(100.0, score))
    explanation = (
        "Score reflects SERP opportunities. " + ", ".join(notes)
        if notes
        else "Neutral SERP environment."
    )

    breakdown = {
        "SERP Opportunity": {
            "value": len(notes),
            "score": final_score,
            "explanation": explanation.strip(),
        }
    }
    return final_score, breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/serp_freshness.py
```python
# pipeline/step_03_prioritization/scoring_components/serp_freshness.py
from typing import Dict, Any, Tuple
from datetime import datetime


def calculate_serp_freshness_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on SERP freshness. An older SERP is a better opportunity.
    """
    if not isinstance(data, dict):
        return 50.0, {
            "Freshness": {
                "value": "N/A",
                "score": 50,
                "explanation": "Invalid data format.",
            }
        }

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    last_update_str = serp_info.get("last_updated_time")

    if not last_update_str:
        return 50.0, {
            "Freshness": {
                "value": "N/A",
                "score": 50,
                "explanation": "No freshness data.",
            }
        }

    try:
        last_update = datetime.fromisoformat(last_update_str.replace(" +00:00", ""))
        days_since_update = (datetime.now() - last_update).days

        # Score increases as the SERP gets older
        if days_since_update > 90:
            score = 100.0
        elif days_since_update > 60:
            score = 80.0
        elif days_since_update > 30:
            score = 60.0
        elif days_since_update > 14:
            score = 40.0
        else:
            score = 20.0

        explanation = f"SERP last updated {days_since_update} days ago. Older SERPs are better opportunities."
        breakdown = {
            "Freshness": {
                "value": f"{days_since_update} days",
                "score": score,
                "explanation": explanation,
            }
        }
        return score, breakdown

    except (ValueError, TypeError):
        return 50.0, {
            "Freshness": {
                "value": "Error",
                "score": 50,
                "explanation": "Could not parse update timestamp.",
            }
        }
```

## File: pipeline/step_03_prioritization/scoring_components/serp_threat.py
```python
# pipeline/step_03_prioritization/scoring_components/serp_threat.py
from typing import Dict, Any, Tuple


def calculate_serp_threat_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a unified "threat" score for the SERP. A lower score is better.
    This score is inverted for the final calculation (higher threat = lower opportunity).
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    serp_types = set(serp_info.get("serp_item_types", []))

    threat_level = 0
    notes = []

    # Threat 1: Hostile, non-blog features
    HOSTILE_FEATURES = {
        "shopping",
        "popular_products",
        "local_pack",
        "google_flights",
        "google_hotels",
        "app",
        "jobs",
        "math_solver",
        "currency_box",
    }
    found_hostile = serp_types.intersection(HOSTILE_FEATURES)
    if found_hostile:
        threat_level += 50
        notes.append(f"Hostile features found ({', '.join(found_hostile)})")

    # Threat 2: AI Overview
    if "ai_overview" in serp_types:
        threat_level += config.get("ai_overview_penalty", 25)
        notes.append("AI Overview is present")

    # Threat 3: Paid Ads (implicit threat)
    if "paid" in serp_types:
        threat_level += 10
        notes.append("Paid ads are present")

    # Normalize the threat level to a 0-100 score
    final_threat_score = min(100, threat_level)

    # The final score is inverted: 100 is low threat, 0 is high threat.
    opportunity_score = 100 - final_threat_score

    explanation = (
        "Score reflects threats to organic CTR. " + "; ".join(notes)
        if notes
        else "No major threats found."
    )
    breakdown = {
        "SERP Threat": {
            "value": f"{final_threat_score}%",
            "score": opportunity_score,
            "explanation": explanation,
        }
    }

    return opportunity_score, breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/serp_volatility.py
```python
# pipeline/step_03_prioritization/scoring_components/serp_volatility.py
from typing import Dict, Any, Tuple
from datetime import datetime


def calculate_serp_volatility_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on SERP stability. A more volatile SERP can be an opportunity.
    """
    if not isinstance(data, dict):
        return 50.0, {
            "SERP Stability": {
                "value": "N/A",
                "score": 50,
                "explanation": "Invalid data format.",
            }
        }

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    breakdown = {}

    last_update_str = serp_info.get("last_updated_time")
    prev_update_str = serp_info.get("previous_updated_time")

    if not last_update_str or not prev_update_str:
        return 50.0, {
            "SERP Stability": {
                "value": "N/A",
                "score": 50,
                "explanation": "Insufficient data to calculate SERP volatility.",
            }
        }

    try:
        last_update = datetime.fromisoformat(last_update_str.replace(" +00:00", ""))
        prev_update = datetime.fromisoformat(prev_update_str.replace(" +00:00", ""))
        days_between_updates = (last_update - prev_update).days

        stable_threshold = config.get("serp_volatility_stable_threshold_days", 30)

        score = 0.0
        if days_between_updates < 7:  # Highly volatile
            score = 100.0
        elif days_between_updates < 21:  # Moderately volatile
            score = 75.0
        elif days_between_updates < stable_threshold:  # Relatively stable
            score = 50.0
        else:  # Very stable
            score = 25.0

        explanation = f"SERP updated every {days_between_updates} days. More frequent updates can signal an opportunity."
        breakdown["SERP Stability"] = {
            "value": f"{days_between_updates} days",
            "score": score,
            "explanation": explanation,
        }
        return score, breakdown

    except (ValueError, TypeError):
        return 50.0, {
            "SERP Stability": {
                "value": "Error",
                "score": 50,
                "explanation": "Could not parse SERP update timestamps.",
            }
        }
```

## File: pipeline/step_03_prioritization/scoring_components/traffic_potential.py
```python
# pipeline/step_03_prioritization/scoring_components/traffic_potential.py
from typing import Dict, Any, Tuple


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_traffic_potential_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a blended score based on both commercial traffic value and raw audience size.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    sv = keyword_info.get("search_volume", 0) or 0
    cpc = keyword_info.get("cpc", 0.0) or 0.0

    # 1. Calculate Traffic Value Score
    traffic_value = sv * cpc
    max_traffic_value = config.get("max_traffic_value_for_scoring", 50000)
    traffic_value_score = _normalize_value(traffic_value, max_traffic_value)

    # 2. Calculate Raw Search Volume Score
    max_sv = config.get("max_sv_for_scoring", 100000)
    raw_sv_score = _normalize_value(sv, max_sv)

    # 3. Blend the scores to balance commercial value and audience size
    final_score = (traffic_value_score * 0.7) + (raw_sv_score * 0.3)

    explanation = f"Blended score: 70% from Est. Traffic Value (${traffic_value:,.0f}) and 30% from Raw SV ({sv})."
    breakdown = {
        "Traffic Potential": {
            "value": f"{sv} SV | ${cpc:.2f} CPC",
            "score": round(final_score),
            "explanation": explanation,
        }
    }

    return round(final_score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/volume_volatility.py
```python
# pipeline/step_03_prioritization/scoring_components/volume_volatility.py
import numpy as np
from typing import Dict, Any, Tuple


def calculate_volume_volatility_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the stability of monthly search volume.
    Lower volatility is generally better for long-term planning.
    """
    if not isinstance(data, dict):
        return 50.0, {
            "Volatility": {
                "value": "N/A",
                "score": 50,
                "explanation": "Invalid data format.",
            }
        }

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    monthly_searches = keyword_info.get("monthly_searches", [])

    if not monthly_searches or len(monthly_searches) < 3:
        return 50.0, {
            "Volatility": {
                "value": "N/A",
                "score": 50,
                "explanation": "Insufficient data.",
            }
        }

    volumes = [
        ms["search_volume"]
        for ms in monthly_searches
        if isinstance(ms, dict)
        and ms.get("search_volume") is not None
        and ms["search_volume"] > 0
    ]
    if len(volumes) < 3:
        return 50.0, {
            "Volatility": {
                "value": "N/A",
                "score": 50,
                "explanation": "Insufficient data.",
            }
        }

    mean_volume = np.mean(volumes)
    std_dev = np.std(volumes)

    if mean_volume == 0:
        return 0.0, {
            "Volatility": {"value": "0", "score": 0, "explanation": "No search volume."}
        }

    coeff_of_variation = std_dev / mean_volume

    # Score is inverted: higher volatility = lower score
    # A CoV of 0.5 (50%) is considered moderately high.
    score = max(0, 100 - (coeff_of_variation * 150))  # Scale the penalty

    explanation = (
        f"Coefficient of Variation: {coeff_of_variation:.2%}. Lower is more stable."
    )
    breakdown = {
        "Volatility": {
            "value": f"{coeff_of_variation:.2%}",
            "score": round(score),
            "explanation": explanation,
        }
    }

    return round(score), breakdown
```

## File: pipeline/step_03_prioritization/__init__.py
```python
# pipeline/step_03_prioritization/__init__.py
```

## File: pipeline/step_03_prioritization/run_prioritization.py
```python
import logging
from typing import List, Dict, Any

from .scoring_engine import ScoringEngine


def run_prioritization_phase(
    opportunities: List[Dict[str, Any]], client_cfg: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Orchestrates the prioritization phase.

    1. Scores each opportunity based on a weighted formula.
    2. Sorts the opportunities by their calculated score.

    Returns a sorted list of opportunities with scoring data.
    """
    logger = logging.getLogger(__name__)
    logger.info("--- Starting Prioritization Phase ---")

    scoring_engine = ScoringEngine(client_cfg)

    # 1. Score Opportunities
    scored_opportunities = []
    for opp in opportunities:
        score, score_breakdown = scoring_engine.calculate_score(opp)
        opp["strategic_score"] = score
        # Add the focused competition score directly into the breakdown for persistence
        if "low_competition_score" in score_breakdown:
            opp["low_competition_score"] = score_breakdown["low_competition_score"][
                "score"
            ]
        opp["score_breakdown"] = score_breakdown
        scored_opportunities.append(opp)

    # 2. Sort Opportunities
    sorted_opportunities = sorted(
        scored_opportunities, key=lambda x: x["strategic_score"], reverse=True
    )

    logger.info(f"  -> Scored and sorted {len(sorted_opportunities)} opportunities.")
    logger.info("--- Prioritization Phase Complete ---")

    return sorted_opportunities
```

## File: pipeline/step_03_prioritization/scoring_engine.py
```python
import logging
from typing import Dict, Any, Tuple
from .scoring_components import (
    calculate_ease_of_ranking_score,
    calculate_traffic_potential_score,
    calculate_commercial_intent_score,
    calculate_growth_trend_score,
    calculate_serp_features_score,
    calculate_serp_volatility_score,
    calculate_competitor_weakness_score,
    calculate_serp_crowding_score,
    calculate_keyword_structure_score,
    calculate_serp_threat_score,
    calculate_volume_volatility_score,
    calculate_serp_freshness_score,
    calculate_competitor_performance_score,  # ADDED THIS IMPORT
)


class ScoringEngine:
    """
    Calculates a strategic score for each keyword opportunity by orchestrating
    a suite of modular scoring components.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def calculate_score(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[float, Dict[str, Any]]:
        """
        Calculates the final opportunity score by combining weighted scores
        from all registered scoring components.
        """
        if not isinstance(opportunity, dict):
            self.logger.warning(
                "Invalid data format passed to calculate_score. Expected a dictionary."
            )
            return 0.0, {"error": "Invalid data format."}

        breakdown = {}
        data_source = opportunity.get("full_data", opportunity)

        # --- Execute all scoring components ---
        ease_score, ease_breakdown = calculate_ease_of_ranking_score(
            data_source, self.config
        )
        breakdown["ease_of_ranking"] = {
            "name": "Ease of Ranking",
            "score": ease_score,
            "breakdown": ease_breakdown,
        }

        traffic_score, traffic_breakdown = calculate_traffic_potential_score(
            data_source, self.config
        )
        breakdown["traffic_potential"] = {
            "name": "Traffic Potential",
            "score": traffic_score,
            "breakdown": traffic_breakdown,
        }

        intent_score, intent_breakdown = calculate_commercial_intent_score(
            data_source, self.config
        )
        breakdown["commercial_intent"] = {
            "name": "Commercial Intent",
            "score": intent_score,
            "breakdown": intent_breakdown,
        }

        trend_score, trend_breakdown = calculate_growth_trend_score(
            data_source, self.config
        )
        breakdown["growth_trend"] = {
            "name": "Growth Trend",
            "score": trend_score,
            "breakdown": trend_breakdown,
        }

        features_score, features_breakdown = calculate_serp_features_score(
            data_source, self.config
        )
        breakdown["serp_features"] = {
            "name": "SERP Opportunity",
            "score": features_score,
            "breakdown": features_breakdown,
        }

        volatility_score, volatility_breakdown = calculate_serp_volatility_score(
            data_source, self.config
        )
        breakdown["serp_volatility"] = {
            "name": "SERP Volatility",
            "score": volatility_score,
            "breakdown": volatility_breakdown,
        }

        weakness_score, weakness_breakdown = calculate_competitor_weakness_score(
            data_source, self.config
        )
        breakdown["competitor_weakness"] = {
            "name": "Competitor Weakness",
            "score": weakness_score,
            "breakdown": weakness_breakdown,
        }

        crowding_score, crowding_breakdown = calculate_serp_crowding_score(
            data_source, self.config
        )
        breakdown["serp_crowding"] = {
            "name": "SERP Crowding",
            "score": crowding_score,
            "breakdown": crowding_breakdown,
        }

        structure_score, structure_breakdown = calculate_keyword_structure_score(
            data_source, self.config
        )
        breakdown["keyword_structure"] = {
            "name": "Keyword Structure",
            "score": structure_score,
            "breakdown": structure_breakdown,
        }

        threat_score, threat_breakdown = calculate_serp_threat_score(
            data_source, self.config
        )
        breakdown["serp_threat"] = {
            "name": "SERP Threat",
            "score": threat_score,
            "breakdown": threat_breakdown,
        }

        volume_volatility_score, volume_volatility_breakdown = (
            calculate_volume_volatility_score(data_source, self.config)
        )
        breakdown["volume_volatility"] = {
            "name": "Volume Volatility",
            "score": volume_volatility_score,
            "breakdown": volume_volatility_breakdown,
        }

        freshness_score, freshness_breakdown = calculate_serp_freshness_score(
            data_source, self.config
        )
        breakdown["serp_freshness"] = {
            "name": "SERP Freshness",
            "score": freshness_score,
            "breakdown": freshness_breakdown,
        }

        performance_score, performance_breakdown = (
            calculate_competitor_performance_score(opportunity, self.config)
        )
        breakdown["competitor_performance"] = {
            "name": "Competitor Tech Performance",
            "score": performance_score,
            "breakdown": performance_breakdown,
        }
        # --- Apply weights from config and calculate final score ---
        weights = {
            "ease": self.config.get("ease_of_ranking_weight", 25),
            "traffic": self.config.get("traffic_potential_weight", 20),
            "intent": self.config.get("commercial_intent_weight", 15),
            "weakness": self.config.get("competitor_weakness_weight", 10),
            "structure": self.config.get("keyword_structure_weight", 5),
            "trend": self.config.get("growth_trend_weight", 5),
            "features": self.config.get("serp_features_weight", 5),
            "crowding": self.config.get("serp_crowding_weight", 5),
            "volatility": self.config.get("serp_volatility_weight", 5),
            "threat": self.config.get("serp_threat_weight", 5),
            "freshness": self.config.get("serp_freshness_weight", 0),
            "competitor_performance": self.config.get(
                "competitor_performance_weight", 5
            ),  # ADDED THIS LINE
            "volume_volatility": self.config.get("volume_volatility_weight", 0),
        }

        total_weight = sum(weights.values())
        if total_weight == 0:
            return 0.0, breakdown  # Avoid division by zero

        final_score = (
            (ease_score * weights["ease"])
            + (traffic_score * weights["traffic"])
            + (intent_score * weights["intent"])
            + (weakness_score * weights["weakness"])
            + (structure_score * weights["structure"])
            + (trend_score * weights["trend"])
            + (features_score * weights["features"])
            + (crowding_score * weights["crowding"])
            + (volatility_score * weights["volatility"])
            + (threat_score * weights["threat"])
            + (freshness_score * weights["freshness"])
            + (volume_volatility_score * weights["volume_volatility"])
            + (performance_score * weights["competitor_performance"])  # ADDED THIS LINE
        ) / total_weight

        for key, breakdown_data in breakdown.items():
            # Map breakdown key to weight key
            weight_key_map = {
                "ease_of_ranking": "ease",
                "traffic_potential": "traffic",
                "commercial_intent": "intent",
                "competitor_weakness": "weakness",
                "keyword_structure": "structure",
                "growth_trend": "trend",
                "serp_features": "features",
                "serp_crowding": "crowding",
                "serp_volatility": "volatility",
                "serp_threat": "threat",
                "volume_volatility": "volume_volatility",
                "serp_freshness": "freshness",
                "competitor_performance": "competitor_performance",  # ADDED THIS LINE
            }
            weight_key = weight_key_map.get(key, "")
            breakdown_data["weight"] = weights.get(weight_key, 0)

        return round(final_score, 2), breakdown
```

## File: pipeline/step_04_analysis/content_analysis_modules/ai_intelligence_caller.py
```python
from typing import List, Dict, Any, Tuple
from external_apis.openai_client import OpenAIClientWrapper


def get_ai_content_analysis(
    openai_client: OpenAIClientWrapper,
    messages: List[Dict[str, str]],
    model: str,
    max_completion_tokens: int,
) -> Tuple[Dict[str, Any], str]:
    """
    Calls the OpenAI API to get content analysis and returns the response and any error.
    """
    schema = {
        "name": "extract_deep_content_insights",
        "type": "object",
        "properties": {
            "unique_angles_to_include": {"type": "array", "items": {"type": "string"}},
            "key_entities_from_competitors": {
                "type": "array",
                "items": {"type": "string"},
            },
            "core_questions_answered_by_competitors": {
                "type": "array",
                "items": {"type": "string"},
            },
            "identified_content_gaps": {"type": "array", "items": {"type": "string"}},
        },
        "required": [
            "unique_angles_to_include",
            "key_entities_from_competitors",
            "core_questions_answered_by_competitors",
            "identified_content_gaps",
        ],
        "additionalProperties": False,
    }

    response, error = openai_client.call_chat_completion(
        messages=messages,
        schema=schema,
        model=model,
        max_completion_tokens=max_completion_tokens,
    )

    return response, error
```

## File: pipeline/step_04_analysis/content_analysis_modules/heading_analyzer.py
```python
from typing import List, Dict, Any
from collections import Counter


def extract_common_headings(
    competitor_analysis: List[Dict[str, Any]], num_headings: int
) -> List[str]:
    """Extracts the most common H2 and H3 headings from competitor data."""
    all_headings = Counter(
        h
        for c in competitor_analysis
        if c.get("headings")
        for h_type in ["h2", "h3"]
        for h in c["headings"].get(h_type, [])
    )
    return [h for h, count in all_headings.most_common(num_headings)]
```

## File: pipeline/step_04_analysis/content_analysis_modules/metric_analyzer.py
```python
from typing import List, Dict, Any, Optional


def calculate_average_word_count(competitor_analysis: List[Dict[str, Any]]) -> int:
    """Calculates the average word count from a list of competitor data."""
    word_counts = [
        c.get("word_count") for c in competitor_analysis if c and c.get("word_count")
    ]
    return int(sum(word_counts) / len(word_counts)) if word_counts else 1500


def calculate_average_readability(
    competitor_analysis: List[Dict[str, Any]],
) -> Optional[float]:
    """Calculates the average readability score from a list of competitor data."""
    readability_scores = [
        c.get("readability_score")
        for c in competitor_analysis
        if c.get("readability_score") is not None
    ]
    return (
        sum(readability_scores) / len(readability_scores)
        if readability_scores
        else None
    )
```

## File: pipeline/step_04_analysis/__init__.py
```python
# pipeline/step_04_analysis/__init__.py
```

## File: pipeline/step_04_analysis/competitor_analyzer.py
```python
import logging
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import urlparse
import textstat

from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class FullCompetitorAnalyzer:
    """
    Performs a deep-dive analysis of top organic competitors using the OnPage Instant Pages API.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.min_word_count = self.config.get("min_competitor_word_count", 300)

        # Combine all blacklists dynamically

        excluded_domains_config = self.config.get(
            "competitor_analysis_excluded_domains", []
        )

        if isinstance(excluded_domains_config, str):
            excluded_domains = set(
                d.strip() for d in excluded_domains_config.split(",")
            )

        else:
            excluded_domains = set(excluded_domains_config)

        self.blacklist_domains = excluded_domains.union(
            set(self.config.get("ugc_and_parasite_domains", []))
        )

    def analyze_competitors(
        self, competitor_urls: List[str], selected_urls: Optional[List[str]] = None
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Fetches and analyzes competitor data using a two-tier, adaptive fetching strategy.
        First attempts a cheap scan without JS, then retries failures with JS enabled.
        """
        urls_to_scan = selected_urls or competitor_urls
        if not urls_to_scan:
            return [], 0.0

        total_api_cost = 0.0
        successful_results = []
        urls_that_need_js_retry = []

        # --- Tier 1: Fast, cheap scan with JavaScript DISABLED ---
        self.logger.info(
            f"Starting Tier 1 analysis for {len(urls_to_scan)} URLs (JS disabled)."
        )
        try:
            initial_tasks, initial_cost = self.client.get_content_onpage_data(
                urls_to_scan, self.config, enable_javascript=False
            )
            total_api_cost += initial_cost

            for task in initial_tasks:
                task_url = task.get("data", {}).get("url")

                if task.get("result") is None:
                    self.logger.warning(
                        f"Tier 1 scan for {task_url} returned a null result. Queuing for JS-enabled retry."
                    )
                    urls_that_need_js_retry.append(task_url)
                    continue

                result = task.get("result", [{}])[0]

                if (
                    task.get("status_code") == 20000
                    and result.get("crawl_status") != "Page content is empty"
                    and result.get("items_count", 0) > 0
                ):
                    successful_results.extend(result.get("items", []))
                else:
                    self.logger.warning(
                        f"Tier 1 scan failed for {task_url}. Reason: {result.get('crawl_status', task.get('status_message'))}. Queuing for JS-enabled retry."
                    )
                    urls_that_need_js_retry.append(task_url)
        except Exception as e:
            self.logger.error(
                f"Error during Tier 1 competitor analysis: {e}", exc_info=True
            )

        # --- Tier 2: Slower, more expensive scan with JavaScript ENABLED for failures ---
        if urls_that_need_js_retry:
            self.logger.info(
                f"Starting Tier 2 analysis for {len(urls_that_need_js_retry)} failed URLs (JS enabled)."
            )
            try:
                retry_tasks, retry_cost = self.client.get_content_onpage_data(
                    urls_that_need_js_retry, self.config, enable_javascript=True
                )
                total_api_cost += retry_cost

                for task in retry_tasks:
                    task_url = task.get("data", {}).get("url")

                    if task.get("result") is None:
                        self.logger.error(
                            f"Tier 2 retry FAILED for {task_url}. Reason: API returned a null result. This URL will be excluded from analysis."
                        )
                        continue

                    result = task.get("result", [{}])[0]

                    if (
                        task.get("status_code") == 20000
                        and result.get("items_count", 0) > 0
                    ):
                        self.logger.info(
                            f"Tier 2 JS-enabled retry SUCCEEDED for {task_url}."
                        )
                        successful_results.extend(result.get("items", []))
                    else:
                        self.logger.error(
                            f"Tier 2 retry FAILED for {task_url}. Reason: {result.get('crawl_status', task.get('status_message'))}. This URL will be excluded from analysis."
                        )
            except Exception as e:
                self.logger.error(
                    f"Error during Tier 2 competitor analysis: {e}", exc_info=True
                )

        # --- Final Processing ---
        final_competitor_list = self._process_content_parsing_results(
            successful_results
        )

        return final_competitor_list, total_api_cost

    def _process_content_parsing_results(
        self, results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Processes the successful results from the Content Parsing API call into a standardized competitor object.
        """
        final_competitors = []
        for result in results:
            url = result.get("url")  # URL is at the top level in the new API response
            if not url or result.get("status_code") != 200:
                continue

            domain = urlparse(url).netloc
            if domain in self.blacklist_domains:
                self.logger.info(f"Skipping blacklisted competitor: {domain}")
                continue

            page_content = result.get("page_content", {})
            main_topic_content = ""
            headings = {"h1": [], "h2": [], "h3": [], "h4": [], "h5": [], "h6": []}

            # Extract main content and headings from the structured 'main_topic' array
            if page_content and page_content.get("main_topic"):
                for topic in page_content["main_topic"]:
                    h_level = topic.get("level")
                    h_title = topic.get("h_title")
                    if h_level and h_title:
                        tag = f"h{h_level}"
                        if tag in headings:
                            headings[tag].append(h_title)

                    if topic.get("primary_content"):
                        for pc in topic["primary_content"]:
                            if pc and pc.get("text"):
                                main_topic_content += pc["text"] + " "

            main_topic_content = main_topic_content.strip()

            # Manually calculate word count and readability
            word_count = len(main_topic_content.split())
            readability_score = None
            if (
                word_count > 100
            ):  # textstat needs a reasonable amount of text to be accurate
                try:
                    readability_score = textstat.flesch_kincaid_grade(
                        main_topic_content
                    )
                except Exception as e:
                    self.logger.warning(
                        f"Could not calculate readability for {url}: {e}"
                    )

            if word_count >= self.min_word_count:
                processed_competitor = {
                    "url": url,
                    "title": headings["h1"][0] if headings.get("h1") else None,
                    "word_count": word_count,
                    "readability_score": readability_score,
                    "headings": headings,
                    "main_content_text": main_topic_content,  # Clean text for readability calculation
                    "full_content_markdown": result.get(
                        "page_as_markdown"
                    ),  # Clean markdown for AI analysis
                    # Set technical fields to defaults as they are not available from this endpoint
                    "technical_warnings": [],
                    "page_timing": {},
                    "onpage_score": None,
                }
                final_competitors.append(processed_competitor)
            else:
                self.logger.info(
                    f"Skipping competitor {url} due to low parsed word count: {word_count}"
                )

        return final_competitors
```

## File: pipeline/step_04_analysis/content_analyzer.py
```python
import logging
from typing import List, Dict, Any, Tuple

from external_apis.openai_client import OpenAIClientWrapper

# Keep these imports if you want to reuse them for the deep-dive path
from .content_analysis_modules.ai_intelligence_caller import get_ai_content_analysis


class ContentAnalyzer:
    """
    Orchestrates the analysis of competitor content (or SERP data) to synthesize intelligence.
    """

    def __init__(self, openai_client: OpenAIClientWrapper, config: Dict[str, Any]):
        self.openai_client = openai_client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.num_common_headings = self.config.get("num_common_headings", 8)
        self.num_unique_angles = self.config.get("num_unique_angles", 5)
        self.max_words_for_ai_analysis = self.config.get(
            "max_words_for_ai_analysis", 2000
        )
        self.num_competitors_for_ai_analysis = self.config.get(
            "num_competitors_for_ai_analysis", 3
        )

    # --- START MODIFICATION ---
    def synthesize_content_intelligence(
        self,
        keyword: str,
        serp_overview: Dict[str, Any],
        competitor_analysis: List[
            Dict[str, Any]
        ],  # This will be empty if deep analysis is skipped
    ) -> Tuple[Dict[str, Any], float]:
        """
        Synthesizes content intelligence by orchestrating data preparation and AI analysis.
        Conditionally uses deep competitor content or rich SERP data.
        """
        if competitor_analysis:
            from .content_analysis_modules.ai_content_preparer import (
                prepare_competitor_content_for_ai,
            )
            from .content_analysis_modules.ai_prompt_builder import (
                get_ai_prompt_messages,
            )

            self.logger.info(
                "Synthesizing intelligence from deep competitor content analysis (legacy path)."
            )

            # 1. Prepare Competitor Content for AI
            content_for_ai, using_markdown = prepare_competitor_content_for_ai(
                competitor_analysis,
                self.num_competitors_for_ai_analysis,
                self.max_words_for_ai_analysis,
            )

            # 2. Build AI Prompt (legacy)
            ai_prompt_messages = get_ai_prompt_messages(
                keyword, content_for_ai, using_markdown
            )

        else:
            self.logger.info(
                "Synthesizing intelligence from rich SERP data (new path)."
            )
            # 1. Prepare SERP Data for AI (already extracted by FullSerpAnalyzer)
            # We just need to ensure it's structured for the prompt.
            # All the new fields are already in serp_overview.

            # 2. Build AI Prompt (new, SERP-only)
            ai_prompt_messages = self._build_synthesis_prompt_from_serp(
                keyword, serp_overview
            )

        # 3. Call AI for Analysis (common to both paths)
        ai_analysis_response, error = get_ai_content_analysis(
            openai_client=self.openai_client,
            messages=ai_prompt_messages,
            model=self.config.get("default_model", "gpt-5-nano"),
            max_completion_tokens=self.config.get(
                "max_completion_tokens_for_generation", 4096
            ),
        )
        total_ai_cost = self.openai_client.latest_cost

        if error or not ai_analysis_response:
            self.logger.error(f"Failed to get deep content analysis from AI: {error}")
            return {
                "analysis_error": f"AI-powered content intelligence failed. Reason: {error}"
            }, total_ai_cost

        # 4. Assemble Final Intelligence Object
        ai_analysis_response["unique_angles_to_include"] = list(
            set(ai_analysis_response.get("unique_angles_to_include", []))
        )[: self.num_unique_angles]

        # --- NEW: Incorporate AI Overview Sources into AI Content Brief
        if (
            serp_overview.get("ai_overview_sources") and not competitor_analysis
        ):  # Only for SERP-only mode
            if "source_and_inspiration_content" not in ai_analysis_response:
                ai_analysis_response["source_and_inspiration_content"] = {}
            ai_analysis_response["source_and_inspiration_content"][
                "ai_overview_sources"
            ] = serp_overview["ai_overview_sources"]

        return ai_analysis_response, total_ai_cost

    # New private method for SERP-only prompt building
    def _build_synthesis_prompt_from_serp(
        self, keyword: str, serp_data: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """
        Builds a comprehensive prompt for AI content intelligence synthesis
        based purely on rich SERP data.
        """
        system_prompt = "You are a world-class SEO content strategist. Your task is to analyze structured SERP data to reverse-engineer a winning content strategy. Your insights must be actionable and highly specific."

        prompt_sections = [f'**Primary Keyword:** "{keyword}"\n']

        if serp_data.get("knowledge_graph_facts"):
            facts_list = '\n- '.join(serp_data['knowledge_graph_facts'])
            prompt_sections.append(
                f"**Verified Facts from Knowledge Graph (Incorporate these as core facts):**\n- {facts_list}\n"
            )

        if serp_data.get("paid_ad_copy"):
            paid_titles = [ad["title"] for ad in serp_data["paid_ad_copy"]]
            paid_descriptions = [ad["description"] for ad in serp_data["paid_ad_copy"]]
            prompt_sections.append(
                f"**High-Conversion Language from Top Paid Ads (Analyze for compelling headlines/intro/CTAs):**\n- Titles: {paid_titles}\n- Descriptions: {paid_descriptions}\n"
            )

        if serp_data.get("top_organic_sitelinks"):
            sitelinks_list = '\n- '.join(serp_data['top_organic_sitelinks'])
            prompt_sections.append(
                f"**High-Priority Subtopics from Competitor Sitelinks (Must include these as H2/H3s):**\n- {sitelinks_list}\n"
            )

        if serp_data.get("top_organic_faqs"):
            faqs_list = '\n- '.join(serp_data['top_organic_faqs'])
            prompt_sections.append(
                f"**High-Priority Questions from Competitor FAQ Snippets (Must include these in a dedicated FAQ section):**\n- {faqs_list}\n"
            )

        if serp_data.get("ai_overview_sources"):
            sources_list = '\n- '.join(serp_data['ai_overview_sources'])
            prompt_sections.append(
                f"**Authoritative Sources Used by Google's AI Overview (Give analytical priority to concepts from these sources):**\n- {sources_list}\n"
            )

        if serp_data.get("discussion_snippets"):
            snippets_list = '\n- '.join(serp_data['discussion_snippets'])
            prompt_sections.append(
                f"**Voice of the Customer from Discussions/Forums (Analyze for tone, pain points, and authentic perspective):**\n- {snippets_list}\n"
            )

        # Add basic organic results for general context
        if serp_data.get("top_organic_results"):
            org_titles_desc_list = '\n- '.join([
                f"Title: {r['title']}\nDescription: {r['description']}"
                for r in serp_data["top_organic_results"]
            ])
            prompt_sections.append(
                f"**Top Organic Result Snippets (for general content analysis):**\n- {org_titles_desc_list}\n"
            )

        if serp_data.get("people_also_ask"):
            paa_list = '\n- '.join(serp_data['people_also_ask'])
            prompt_sections.append(
                f"**People Also Ask Questions:**\n- {paa_list}\n"
            )
        if serp_data.get("related_searches"):
            related_list = '\n- '.join(serp_data['related_searches'])
            prompt_sections.append(
                f"**Related Searches:**\n- {related_list}\n"
            )
        if serp_data.get("ai_overview_content"):
            prompt_sections.append(
                f"**Google's AI Overview Content:**\n{serp_data['ai_overview_content']}\n"
            )
        if serp_data.get("featured_snippet_content"):
            prompt_sections.append(
                f"**Featured Snippet Content:**\n{serp_data['featured_snippet_content']}\n"
            )

        user_prompt_content = f"""
        Analyze the following comprehensive SERP intelligence report to generate a content strategy blueprint.

        {"".join(prompt_sections)}

        **Your Analysis Task:**
        1.  **Unique Angles & Insights:** Based on ALL the provided SERP data (Knowledge Graph, Paid Ads, FAQs, Sitelinks, AI Overview sources, discussions, organic snippets), identify 2-3 truly unique value propositions or content differentiation angles. Where are the gaps and opportunities for our content to stand out as superior?
        2.  **Key Entities:** List the 5-10 most critical entities (people, products, brands, concepts) from the entire SERP. These must be central to the topic.
        3.  **Core Questions Answered:** Synthesize the 5-7 most fundamental user questions that this keyword intends to answer, drawing from PAA, FAQ snippets, and top organic descriptions. These should form the backbone of the article's problem-solving narrative.
        4.  **Identified Content Gaps:** What specific sub-topics are implied or partially covered in the SERP, but could be expanded into full, authoritative sections in our article? What related long-tail questions (from PAA or Related Searches) are not adequately addressed by top results?

        Provide your analysis in the required structured JSON format.
        """
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt_content.strip()},
        ]

    # --- END MODIFICATION ---

    def generate_ai_outline(
        self,
        keyword: str,
        serp_overview: Dict[str, Any],
        content_intelligence: Dict[str, Any],
    ) -> Tuple[Dict[str, List[str]], float]:
        """
        Uses OpenAI to generate a structured content outline with H2s and corresponding H3s.
        (This can also be refactored into a separate module if desired)
        """
        prompt_messages = self._build_outline_prompt(
            keyword, serp_overview, content_intelligence
        )

        schema = {
            "name": "generate_structured_content_outline",
            "type": "object",
            "properties": {
                "article_structure": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "h2": {
                                "type": "string",
                                "description": "The H2 heading of the section.",
                            },
                            "h3s": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "A list of H3 subheadings for this H2 section.",
                            },
                        },
                        "required": ["h2", "h3s"],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["article_structure"],
            "additionalProperties": False,
        }

        response, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
            max_completion_tokens=self.config.get(
                "max_completion_tokens_for_generation", 4096
            ),
        )
        total_ai_cost = self.openai_client.latest_cost

        if error or not response or not response.get("article_structure"):
            self.logger.error(f"Failed to generate structured AI outline: {error}")
            return {"article_structure": []}, total_ai_cost

        return response, total_ai_cost

    def _build_outline_prompt(
        self,
        keyword: str,
        serp_overview: Dict[str, Any],
        content_intelligence: Dict[str, Any],
    ) -> List[Dict[str, str]]:
        """Builds the prompt for the AI structured outline generation."""
        prompt = f"""
        You are an expert SEO content strategist. Create a logical and comprehensive content outline for an article about "{keyword}". The output must be a structured list of sections, each with an H2 and a list of corresponding H3 subheadings.

        **Analysis Data:**
        - **Common Competitor Headings to Incorporate:** {", ".join(content_intelligence.get("common_headings_to_cover", []))}
        - **Unique Angles & Gaps to Address:** {", ".join(content_intelligence.get("unique_angles_to_include", []))}
        - **Key Entities to Mention:** {", ".join(content_intelligence.get("key_entities_from_competitors", []))}
        - **People Also Ask Questions to Answer:** {", ".join(serp_overview.get("paa_questions", []))}

        **Instructions:**
        1. Create a logical flow for the article.
        2. The first section must be titled 'Introduction'.
        3. The last section must be titled 'Conclusion'.
        4. If there are 'People Also Ask' questions, create a dedicated H2 section titled 'Frequently Asked Questions' and use the questions as H3s.
        5. Structure the entire output as a JSON object matching the requested schema.
        """
        return [{"role": "user", "content": prompt}]
```

## File: pipeline/step_04_analysis/run_analysis.py
```python
import logging
from typing import Dict, Any, Optional, List, Tuple

from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from external_apis.openai_client import OpenAIClientWrapper
from core.blueprint_factory import BlueprintFactory
from core.serp_analyzer import FullSerpAnalyzer
from .competitor_analyzer import FullCompetitorAnalyzer
from .content_analyzer import ContentAnalyzer
from pipeline.step_05_strategy.decision_engine import StrategicDecisionEngine
from pipeline.step_03_prioritization.scoring_engine import ScoringEngine

# --- NEW FUNCTION: run_final_validation ---
from urllib.parse import urlparse

# Then, replace the entire `run_final_validation` function with this new version.


def run_final_validation(
    live_serp_data: Dict[str, Any],
    opportunity: Dict[str, Any],
    client_cfg: Dict[str, Any],
    dataforseo_client: Any,
) -> Tuple[bool, Optional[str]]:
    full_data = opportunity.get("full_data", {})
    if not isinstance(full_data, dict):
        full_data = {}
    cached_serp_info = full_data.get("serp_info", {})
    cached_features = set(cached_serp_info.get("serp_item_types", []))
    live_features = set(live_serp_data.get("item_types", []))

    # NEW: Definitive Cannibalization Check (FIXED LOGIC)
    target_domain = client_cfg.get("target_domain", "").lower().replace("www.", "")
    if target_domain:
        for result in live_serp_data.get("top_organic_results", []):
            try:
                url_domain = (
                    urlparse(result.get("url", "")).netloc.lower().replace("www.", "")
                )
                # CRITICAL FIX: Check for exact match or subdomain suffix to avoid 'pet.com' matching 'competitor.com'
                if url_domain == target_domain or url_domain.endswith(
                    f".{target_domain}"
                ):
                    return (
                        False,
                        f"Final Validation Failed (Cannibalization): Target domain '{target_domain}' found in live SERP at URL '{result.get('url')}'.",
                    )
            except Exception:
                continue

    # Hostile features (configurable)
    hostile_features = set(
        client_cfg.get(
            "hostile_serp_features",
            [
                "shopping",
                "local_pack",
                "google_flights",
                "google_hotels",
                "popular_products",
            ],
        )
    )
    newly_added_hostile = live_features.intersection(
        hostile_features
    ) - cached_features.intersection(hostile_features)
    if newly_added_hostile:
        return (
            False,
            f"Final Validation Failed: Live SERP contains new hostile features: {', '.join(newly_added_hostile)}.",
        )

    # Non-blog content check (configurable domains & threshold)
    top_5_organic = live_serp_data.get("top_organic_results", [])[:5]
    non_blog_domains_cfg = set(client_cfg.get("final_validation_non_blog_domains", []))
    ugc_domains_cfg = set(
        client_cfg.get("ugc_and_parasite_domains", [])
    )  # Get from config

    hostile_domains = non_blog_domains_cfg.union(ugc_domains_cfg).union(
        client_cfg.get("competitor_blacklist_domains", [])
    )  # Combine all relevant hostile domains

    non_blog_count = sum(
        1
        for item in top_5_organic
        if any(domain in item.get("url", "") for domain in hostile_domains)
    )
    if non_blog_count >= client_cfg.get("max_non_blog_results", 4):
        return (
            False,
            "Final Validation Failed: Live SERP is dominated by non-blog/UGC/blacklisted content.",
        )

    # AI Overview comprehensiveness check (configurable threshold)
    disable_ai_overview_check = client_cfg.get("disable_ai_overview_check", False)
    if not disable_ai_overview_check:
        ai_overview_content = live_serp_data.get("ai_overview_content", "")
        if ai_overview_content and len(ai_overview_content.split()) > client_cfg.get(
            "max_ai_overview_words", 250
        ):
            return (
                False,
                "Final Validation Failed: AI Overview is too comprehensive, making a blog post redundant.",
            )

    # Organic visibility check (pixel ranking, configurable threshold)
    max_pixel_y = client_cfg.get("max_first_organic_y_pixel")
    if max_pixel_y is not None:
        first_organic_y = live_serp_data.get("first_organic_y_pixel")
        if first_organic_y is None:
            # Cannot check visibility without pixel data, proceed if other checks pass
            pass
        elif first_organic_y > max_pixel_y:
            return (
                False,
                f"Final Validation Failed: First organic result is too far down ({first_organic_y}px > {max_pixel_y}px).",
            )

    # NEW: LCP Check
    avg_lcp = live_serp_data.get("avg_page_timing", {}).get("largest_contentful_paint")
    if avg_lcp is not None and avg_lcp > client_cfg.get("max_avg_lcp_time", 4000):
        return (
            False,
            f"Final Validation Failed: Live SERP indicates poor page speed (Avg LCP: {avg_lcp}ms).",
        )

    return True, "Final validation passed."


# --- END NEW FUNCTION ---


def run_analysis_phase(
    opportunity: Dict[str, Any],
    openai_client: OpenAIClientWrapper,
    dataforseo_client: DataForSEOClientV2,
    client_cfg: Dict[str, Any],
    blueprint_factory: BlueprintFactory,
    scoring_engine: ScoringEngine,
    selected_competitor_urls: Optional[List[str]] = None,
) -> Tuple[Dict[str, Any], float]:
    logger = logging.getLogger(__name__)
    keyword = opportunity.get("keyword")
    logger.info(f"--- Starting Deep-Dive Analysis Phase for '{keyword}' ---")

    total_api_cost = 0.0

    serp_analyzer = FullSerpAnalyzer(dataforseo_client, client_cfg)
    competitor_analyzer = FullCompetitorAnalyzer(dataforseo_client, client_cfg)
    content_analyzer = ContentAnalyzer(openai_client, client_cfg)
    strategy_engine = StrategicDecisionEngine(client_cfg)
    # Blueprint factory is passed in, no need to re-initialize

    # 1. Make the single expensive, live SERP call for analysis
    logger.info(f"Making live SERP call for analysis of '{keyword}'...")
    serp_overview, serp_api_cost = serp_analyzer.analyze_serp(keyword)
    total_api_cost += serp_api_cost
    if not serp_overview:
        raise ValueError("Failed to retrieve live SERP data for analysis.")

    # VALIDATION GATE IS NOW REMOVED FROM THIS FUNCTION
    logger.info(f"Proceeding with full analysis for '{keyword}'.")

    # 2. On-Page competitor metadata and content analysis
    top_organic_urls = [
        result["url"]
        for result in serp_overview.get("top_organic_results", [])[
            : client_cfg.get("num_competitors_to_analyze", 5)
        ]
    ]
    competitor_analysis, competitor_api_cost = competitor_analyzer.analyze_competitors(
        top_organic_urls, selected_competitor_urls
    )
    total_api_cost += competitor_api_cost

    # 3. Content Intelligence Synthesis using the full content
    content_intelligence, content_api_cost = (
        content_analyzer.synthesize_content_intelligence(
            competitor_analysis,
            keyword,
            serp_overview.get("dominant_content_format", "Comprehensive Article"),
        )
    )
    total_api_cost += content_api_cost

    # 4. Determine Strategy
    recommended_strategy = strategy_engine.determine_strategy(
        serp_overview, competitor_analysis, content_intelligence
    )

    # 5. AI Content Outline Generation
    ai_outline, outline_api_cost = content_analyzer.generate_ai_outline(
        keyword, serp_overview, content_intelligence
    )
    total_api_cost += outline_api_cost
    content_intelligence.update(ai_outline)

    # 6. Assemble the final Blueprint
    analysis_data = {
        "serp_overview": serp_overview,
        "competitor_analysis": competitor_analysis,
        "content_intelligence": content_intelligence,
        "recommended_strategy": recommended_strategy,
    }

    blueprint = blueprint_factory.create_blueprint(
        seed_topic=keyword,
        winning_keyword_data=opportunity.get("full_data", {}).copy(),
        analysis_data=analysis_data,
        total_api_cost=total_api_cost,
        client_id=opportunity.get("client_id"),
    )

    opportunity["blueprint"] = blueprint

    # --- RE-SCORING ---
    # Re-calculate the strategic score with the new, rich data from the live SERP call
    # This ensures the score is based on the most accurate, up-to-date information
    final_score, final_score_breakdown = scoring_engine.calculate_score(opportunity)
    opportunity["strategic_score"] = final_score
    opportunity["score_breakdown"] = final_score_breakdown
    opportunity["full_data"]["strategic_score"] = final_score
    opportunity["full_data"]["score_breakdown"] = final_score_breakdown

    logger.info(f"  -> Final, updated strategic score: {final_score}")
    logger.info(f"  -> Total API Cost for Blueprint Generation: ${total_api_cost:.4f}")
    logger.info("--- Deep-Dive Analysis Phase Complete ---")

    return opportunity, total_api_cost
```

## File: pipeline/step_05_strategy/decision_engine.py
```python
import logging
from typing import Dict, Any, List
import json


class StrategicDecisionEngine:
    """
    Analyzes SERP and competitor data to recommend a specific content strategy.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def determine_strategy(
        self,
        serp_overview: Dict[str, Any],
        competitor_analysis: List[Dict[str, Any]],
        content_intelligence: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Determines the optimal content format and strategic goal based on the analysis data.
        """
        content_format = serp_overview.get(
            "dominant_content_format", "Comprehensive Article"
        )
        strategic_goal = "Create a definitive guide that outranks competitors through superior depth and quality."

        top_results = serp_overview.get("top_organic_results", [])

        # --- START MODIFICATION ---
        # Check if deep analysis was performed to adjust logic
        deep_analysis_enabled = self.config.get(
            "enable_deep_competitor_analysis", False
        )

        # NEW: Repurpose focus_competitors for SERP-only mode
        focus_competitors_info = []
        if deep_analysis_enabled and competitor_analysis:
            focus_competitors_info = [
                {"url": c.get("url"), "onpage_score": c.get("onpage_score")}
                for c in competitor_analysis
                if c and c.get("url")
            ][:3]
        else:
            # In SERP-only mode, we can show top organic results as "focus competitors"
            focus_competitors_info = [
                {"url": r.get("url"), "title": r.get("title")} for r in top_results[:3]
            ]

        # NEW: Detect rating-heavy SERPs based on serp_overview data
        rating_count = sum(
            1
            for r in top_results
            if r.get("rating") and r["rating"].get("value") is not None
        )
        avg_rating_value = (
            sum(
                r["rating"]["value"]
                for r in top_results
                if r.get("rating") and r["rating"].get("value")
            )
            / rating_count
            if rating_count > 0
            else 0
        )

        if rating_count >= 3 and avg_rating_value >= 4.0:
            content_format = "Review Article"
            strategic_goal = "Produce an authoritative review or comparison that leverages strong social proof and clearly outlines pros/cons, aiming for rich snippets."
            # Prioritize this strategy by returning early after setting it
            return {
                "content_format": content_format,
                "strategic_goal": strategic_goal,
                "focus_competitors": focus_competitors_info,
                "final_qualification_assessment": {
                    "scorecard": self.generate_qualification_scorecard(
                        {
                            "serp_overview": serp_overview,
                            "competitor_analysis": competitor_analysis,
                            "content_intelligence": content_intelligence,
                        }
                    ),
                    **self._determine_final_recommendation(
                        self.generate_qualification_scorecard(
                            {
                                "serp_overview": serp_overview,
                                "competitor_analysis": competitor_analysis,
                                "content_intelligence": content_intelligence,
                            }
                        )
                    ),
                },
            }

        # ... (existing dynamic content format recommendations, e.g., Recipe, Scholarly, etc. - no change) ...

        # Rule: Weak competition (applies only if deep analysis was performed)
        if (
            content_format == "Comprehensive Article"
            and deep_analysis_enabled
            and competitor_analysis
        ):
            onpage_scores = [
                c.get("onpage_score")
                for c in competitor_analysis
                if c and c.get("onpage_score")
            ]
            if onpage_scores and (sum(onpage_scores) / len(onpage_scores)) < 60:
                strategic_goal = "Exploit the technical weaknesses of competitors by creating a fast, well-structured, and technically superior article."

        # FINAL QUALIFICATION GATE
        scorecard = self.generate_qualification_scorecard(
            {
                "serp_overview": serp_overview,
                "competitor_analysis": competitor_analysis,
                "content_intelligence": content_intelligence,
            }
        )
        recommendation = self._determine_final_recommendation(scorecard)
        # --- END MODIFICATION ---

        return {
            "content_format": content_format,
            "strategic_goal": strategic_goal,
            "focus_competitors": focus_competitors_info,  # Use the conditionally populated info
            "final_qualification_assessment": {
                "scorecard": scorecard,
                **recommendation,
            },
        }

    def generate_qualification_scorecard(self, analysis_data: dict) -> dict:
        """Generates a scorecard of qualification factors, adapted for SERP-only mode."""
        serp_overview = analysis_data.get("serp_overview", {})
        competitor_analysis = analysis_data.get("competitor_analysis", [])

        # --- START MODIFICATION ---
        deep_analysis_enabled = self.config.get(
            "enable_deep_competitor_analysis", False
        )

        hostility_score = 0
        for item in serp_overview.get("items", []):  # Iterate through raw SERP items
            if item.get("rank_absolute", 99) <= 10:
                # Count known hostile/attention-grabbing features
                if item.get("type") in [
                    "video",
                    "local_pack",
                    "carousel",
                    "twitter",
                    "shopping",
                    "app",
                    "short_videos",
                    "images",
                ]:
                    hostility_score += 1
                # Knowledge Graph with AI Overview is a stronger signal
                elif item.get("type") == "knowledge_graph" and (
                    "ai_overview_item" in json.dumps(item)
                ):  # Check for AI overview within KG items
                    hostility_score += 2
                elif item.get("type") == "ai_overview":  # Direct AI overview item
                    hostility_score += 2

        is_hostile_serp_environment = hostility_score > 5
        has_ai_overview = serp_overview.get("serp_has_ai_overview", False) or (
            "ai_overview_content" in serp_overview
            and serp_overview["ai_overview_content"] is not None
        )

        # Average competitor weaknesses calculation
        average_competitor_weaknesses = 0
        if deep_analysis_enabled and competitor_analysis:
            technical_warnings = [
                w
                for comp in competitor_analysis
                for w in comp.get("technical_warnings", [])
            ]
            average_competitor_weaknesses = (
                (len(technical_warnings) / len(competitor_analysis))
                if competitor_analysis
                else 0
            )
        else:
            # If deep analysis is disabled, we cannot assess technical weaknesses directly,
            # so we could default to a neutral or slightly positive value to avoid premature disqualification.
            average_competitor_weaknesses = 2  # Assume a moderate level if unknown

        # Has clear content angle (now based purely on content_intelligence from SERP)
        content_intelligence = analysis_data.get("content_intelligence", {})
        has_clear_content_angle = bool(
            content_intelligence.get("unique_angles_to_include")
            or content_intelligence.get("core_questions_answered_by_competitors")
        )

        # Is intent well-defined (now based on all SERP features)
        is_intent_well_defined = bool(
            serp_overview.get("paa_questions")
            or serp_overview.get("extracted_serp_features")
            or serp_overview.get("top_organic_faqs")  # NEW
            or serp_overview.get("top_organic_sitelinks")  # NEW
        )

        return {
            "hostility_score": hostility_score,
            "is_hostile_serp_environment": is_hostile_serp_environment,
            "has_ai_overview": has_ai_overview,
            "average_competitor_weaknesses": average_competitor_weaknesses,
            "has_clear_content_angle": has_clear_content_angle,
            "is_intent_well_defined": is_intent_well_defined,
        }
        # --- END MODIFICATION ---

    def _determine_final_recommendation(self, scorecard: dict) -> dict:
        """Determines the final go/no-go recommendation."""
        confidence_score = 100
        positive_factors = []
        negative_factors = []

        if scorecard["is_hostile_serp_environment"]:
            confidence_score -= 30
            negative_factors.append("SERP is dominated by non-article formats.")
        if scorecard["has_ai_overview"]:
            confidence_score -= 15
            negative_factors.append(
                "Google AI Overview is present, increasing ranking difficulty."
            )
        if scorecard["average_competitor_weaknesses"] < 2:
            confidence_score -= 20
            negative_factors.append("Competitors are technically strong.")
        if scorecard["average_competitor_weaknesses"] > 4:
            confidence_score += 10
            positive_factors.append(
                "Competitors show significant technical weaknesses."
            )
        if not scorecard["has_clear_content_angle"]:
            confidence_score -= 40
            negative_factors.append("No clear content differentiation angle was found.")
        if scorecard["has_clear_content_angle"]:
            positive_factors.append("A unique content angle has been identified.")
        if scorecard["is_intent_well_defined"]:
            positive_factors.append("User intent is well-defined by SERP features.")

        if confidence_score >= 80:
            recommendation = "Proceed"
        elif 50 <= confidence_score < 80:
            recommendation = "Proceed with Caution"
        else:
            recommendation = "Reject"

        return {
            "recommendation": recommendation,
            "confidence_score": confidence_score,
            "positive_factors": positive_factors,
            "negative_factors": negative_factors,
        }
```

## File: pipeline/step_06_content_creation/__init__.py
```python
# This file marks the directory as a Python package.
```

## File: pipeline/__init__.py
```python
# backend/pipeline/__init__.py
from .orchestrator.main import WorkflowOrchestrator as WorkflowOrchestrator
```

## File: pipeline/orchestrator.py
```python
# backend/pipeline/orchestrator.py
import logging

from app_config.manager import ConfigManager
from data_access.database_manager import DatabaseManager
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from external_apis.openai_client import OpenAIClientWrapper
from agents.image_generator import ImageGenerator
from agents.social_media_crafter import SocialMediaCrafter
from agents.internal_linking_suggester import InternalLinkingSuggester
from agents.html_formatter import HtmlFormatter
from core.blueprint_factory import BlueprintFactory
from agents.content_auditor import ContentAuditor
from agents.prompt_assembler import DynamicPromptAssembler
from pipeline.step_03_prioritization.scoring_engine import ScoringEngine
from pipeline.step_01_discovery.cannibalization_checker import (
    CannibalizationChecker,
)
from jobs import JobManager

from .orchestrator.discovery_orchestrator import DiscoveryOrchestrator
from .orchestrator.analysis_orchestrator import AnalysisOrchestrator
from .orchestrator.content_orchestrator import ContentOrchestrator
from .orchestrator.image_orchestrator import ImageOrchestrator
from .orchestrator.social_orchestrator import SocialOrchestrator
from .orchestrator.validation_orchestrator import ValidationOrchestrator
from .orchestrator.workflow_orchestrator import WorkflowOrchestrator
from .orchestrator.cost_estimator import CostEstimator

logger = logging.getLogger(__name__)


class WorkflowOrchestrator(
    DiscoveryOrchestrator,
    AnalysisOrchestrator,
    ContentOrchestrator,
    ImageOrchestrator,
    SocialOrchestrator,
    ValidationOrchestrator,
    WorkflowOrchestrator,
    CostEstimator,
):
    def __init__(
        self,
        global_cfg_manager: ConfigManager,
        db_manager: DatabaseManager,
        client_id: str,
        job_manager: "JobManager",
    ):
        self.global_cfg_manager = global_cfg_manager
        self.db_manager = db_manager
        self.client_id = client_id
        self.job_manager = job_manager
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = self.global_cfg_manager.load_client_config(
            self.client_id, self.db_manager
        )

        self.openai_client = OpenAIClientWrapper(
            api_key=self.client_cfg.get("openai_api_key"), client_cfg=self.client_cfg
        )
        self.dataforseo_client = DataForSEOClientV2(
            login=self.client_cfg["dataforseo_login"],
            password=self.client_cfg["dataforseo_password"],
            config=self.client_cfg,
            db_manager=self.db_manager,
            enable_cache=self.client_cfg.get("enable_cache", True),
        )

        self.image_generator = ImageGenerator(self.client_cfg)
        self.social_crafter = SocialMediaCrafter(self.openai_client, self.client_cfg)
        self.internal_linking_suggester = InternalLinkingSuggester(
            self.openai_client, self.client_cfg, self.db_manager
        )
        self.html_formatter = HtmlFormatter()
        self.blueprint_factory = BlueprintFactory(
            self.openai_client, self.client_cfg, self.dataforseo_client, self.db_manager
        )
        self.scoring_engine = ScoringEngine(self.client_cfg)
        self.cannibalization_checker = CannibalizationChecker(
            self.client_cfg.get("target_domain"),
            self.dataforseo_client,
            self.client_cfg,
            self.db_manager,
        )
        self.content_auditor = ContentAuditor()
        self.prompt_assembler = DynamicPromptAssembler(self.db_manager)
```

## File: services/discovery_service.py
```python
# services/discovery_service.py

from typing import Dict, Any
from data_access.database_manager import DatabaseManager


class DiscoveryService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def create_discovery_run(self, client_id: str, parameters: Dict[str, Any]) -> int:
        """Creates a new discovery run record and returns its ID."""
        return self.db_manager.create_discovery_run(client_id, parameters)

    def get_disqualification_reasons(self, run_id: int) -> Dict[str, int]:
        """
        Retrieves a summary of disqualification reasons for a specific discovery run.
        """
        keywords = self.db_manager.get_keywords_for_run(run_id)

        disqualification_reasons = {}
        for keyword in keywords:
            if keyword.get("blog_qualification_status") == "rejected":
                reason = keyword.get("blog_qualification_reason")
                if reason:
                    disqualification_reasons[reason] = (
                        disqualification_reasons.get(reason, 0) + 1
                    )

        return disqualification_reasons
```

## File: services/disqualification_service.py
```python
# services/disqualification_service.py
import json
from typing import List, Dict, Any
from data_access.database_manager import DatabaseManager


class DisqualificationService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def disqualify(
        self, client_id: str, keywords: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Applies disqualification rules to a list of keywords.
        """
        qualification_settings = self.db_manager.get_qualification_settings(client_id)
        disqualification_rules = json.loads(
            qualification_settings.get("disqualification_rules", "[]")
        )

        brand_keywords = qualification_settings.get("brand_keywords", [])
        competitor_brand_keywords = qualification_settings.get(
            "competitor_brand_keywords", []
        )

        qualified_keywords = []
        for keyword in keywords:
            disqualified = False
            keyword_text = keyword.get("keyword", "").lower()

            if any(brand_kw in keyword_text for brand_kw in brand_keywords):
                continue

            if any(brand_kw in keyword_text for brand_kw in competitor_brand_keywords):
                continue

            for rule in disqualification_rules:
                field = rule.get("field")
                operator = rule.get("operator")
                value = rule.get("value")

                field_value = keyword
                for key in field.split("."):
                    field_value = field_value.get(key, {})

                if operator == "=" and field_value == value:
                    disqualified = True
                    break
                elif operator == ">" and field_value > value:
                    disqualified = True
                    break
                elif operator == "<" and field_value < value:
                    disqualified = True
                    break

            if not disqualified:
                qualified_keywords.append(keyword)

        return qualified_keywords
```

## File: services/keyword_data_aggregator.py
```python
# services/keyword_data_aggregator.py

from typing import List, Dict, Any, Optional
from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class KeywordDataAggregator:
    def __init__(
        self, dataforseo_client: DataForSEOClientV2, client_cfg: Dict[str, Any]
    ):
        self.dataforseo_client = dataforseo_client
        self.client_cfg = client_cfg

    def get_keyword_data(
        self,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        limit: Optional[int],
        depth: Optional[int],
        ignore_synonyms: Optional[bool],
    ) -> List[Dict[str, Any]]:
        """
        Calls the keyword discovery endpoints, deduplicates the results, and returns a unified list of keyword data objects.
        """
        # W21 FIX: Set default order_by if not provided, now structured as a dict
        if not order_by:
            ideas_suggestions_orderby = [
                "keyword_properties.keyword_difficulty,asc",
                "keyword_info.search_volume,desc",
            ]
            related_orderby = [
                "keyword_data.keyword_properties.keyword_difficulty,asc",
                "keyword_data.keyword_info.search_volume,desc",
            ]
            structured_orderby = {
                "ideas": ideas_suggestions_orderby,
                "suggestions": ideas_suggestions_orderby,
                "related": related_orderby,
            }
        else:
            # If order_by is provided from frontend, structure it for each mode
            related_orderby = [f"keyword_data.{rule}" for rule in order_by]
            structured_orderby = {
                "ideas": order_by,
                "suggestions": order_by,
                "related": related_orderby,
            }

        # Structure filters for the client
        ideas_suggestions_filters = []
        if filters:
            for f in filters:
                new_filter = f.copy()
                if "field" in new_filter and new_filter["field"].startswith(
                    "keyword_data."
                ):
                    new_filter["field"] = new_filter["field"][len("keyword_data.") :]
                ideas_suggestions_filters.append(new_filter)

        structured_filters = {
            "ideas": ideas_suggestions_filters,
            "suggestions": ideas_suggestions_filters,
            "related": filters,  # Related keeps the prefix
        }

        all_ideas, _ = self.dataforseo_client.get_keyword_ideas(
            seed_keywords=seed_keywords,
            location_code=self.client_cfg.get("location_code"),
            language_code=self.client_cfg.get("language_code"),
            client_cfg=self.client_cfg,
            discovery_modes=discovery_modes,
            filters=structured_filters,
            order_by=structured_orderby,
            limit=limit,
            depth=depth,
            ignore_synonyms=ignore_synonyms,
        )

        final_keywords_deduplicated = []
        seen_keywords = set()
        for item in all_ideas:
            kw_text = item.get("keyword", "").lower()
            if kw_text and kw_text not in seen_keywords:
                final_keywords_deduplicated.append(item)
                seen_keywords.add(kw_text)

        return final_keywords_deduplicated
```

## File: services/opportunities_service.py
```python
# services/opportunities_service.py

from typing import List, Dict, Any, Tuple
from data_access.database_manager import DatabaseManager


class OpportunitiesService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def get_all_opportunities(
        self, client_id: str, params: Dict[str, Any]
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        Retrieves keyword opportunities for a client, supporting filtering, sorting, and pagination.
        Returns (opportunities_list, total_count).
        """
        return self.db_manager.get_all_opportunities(client_id, params)

    def get_all_opportunities_summary(
        self, client_id: str, params: Dict[str, Any], select_columns: str = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        Retrieves a lightweight summary of keyword opportunities for a client.
        Returns (opportunities_list, total_count).
        """
        return self.db_manager.get_all_opportunities(
            client_id, params, summary=True, select_columns=select_columns
        )

    def get_opportunities_by_category(
        self, client_id: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Retrieves all opportunities for a client, grouped by category.
        """
        opportunities, _ = self.db_manager.get_all_opportunities(client_id, {})

        opportunities_by_category = {}
        for opportunity in opportunities:
            categories = opportunity.get("keyword_info", {}).get("categories", [])
            for category in categories:
                if category not in opportunities_by_category:
                    opportunities_by_category[category] = []
                opportunities_by_category[category].append(opportunity)

        return opportunities_by_category

    def get_opportunities_by_cluster(
        self, client_id: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Retrieves all opportunities for a client, grouped by cluster.
        """
        opportunities, _ = self.db_manager.get_all_opportunities(client_id, {})

        opportunities_by_cluster = {}
        for opportunity in opportunities:
            cluster_name = opportunity.get("cluster_name")
            if cluster_name:
                if cluster_name not in opportunities_by_cluster:
                    opportunities_by_cluster[cluster_name] = []
                opportunities_by_cluster[cluster_name].append(opportunity)

        return opportunities_by_cluster
```

## File: services/qualification_service.py
```python
# services/qualification_service.py

from typing import List, Dict, Any, Optional
from .keyword_data_aggregator import KeywordDataAggregator
from .disqualification_service import DisqualificationService
from .scoring_service import ScoringService
from .serp_analysis_service import SerpAnalysisService


class QualificationService:
    def __init__(
        self,
        keyword_data_aggregator: KeywordDataAggregator,
        disqualification_service: DisqualificationService,
        scoring_service: ScoringService,
        serp_analysis_service: SerpAnalysisService,
    ):
        self.keyword_data_aggregator = keyword_data_aggregator
        self.disqualification_service = disqualification_service
        self.scoring_service = scoring_service
        self.serp_analysis_service = serp_analysis_service

    def qualify_keywords(
        self,
        client_id: str,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        limit: Optional[int],
        depth: Optional[int],
        ignore_synonyms: Optional[bool],
    ) -> List[Dict[str, Any]]:
        """
        Orchestrates the entire qualification flow.
        """
        keyword_data = self.keyword_data_aggregator.get_keyword_data(
            seed_keywords,
            discovery_modes,
            filters,
            order_by,
            limit,
            depth,
            ignore_synonyms,
        )

        analyzed_keywords = self.serp_analysis_service.analyze_keywords_serp(
            keyword_data
        )

        qualified_keywords = self.disqualification_service.disqualify(
            client_id, analyzed_keywords
        )

        scored_keywords = []
        for keyword in qualified_keywords:
            score, breakdown = self.scoring_service.calculate_score(client_id, keyword)
            keyword["strategic_score"] = score
            keyword["score_breakdown"] = breakdown
            scored_keywords.append(keyword)

        return scored_keywords
```

## File: services/scoring_service.py
```python
# services/scoring_service.py

from typing import Dict, Any, Tuple
from data_access.database_manager import DatabaseManager


class ScoringService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def calculate_score(
        self, client_id: str, keyword_data: Dict[str, Any]
    ) -> Tuple[float, Dict[str, Any]]:
        """
        Calculates a strategic score for a keyword based on the client's qualification settings.
        """
        qualification_settings = self.db_manager.get_qualification_settings(client_id)

        traffic_potential_weight = qualification_settings.get(
            "traffic_potential_weight", 0
        )
        cpc_weight = qualification_settings.get("cpc_weight", 0)
        search_intent_weight = qualification_settings.get("search_intent_weight", 0)
        competitor_strength_weight = qualification_settings.get(
            "competitor_strength_weight", 0
        )
        serp_features_weight = qualification_settings.get("serp_features_weight", 0)
        trend_weight = qualification_settings.get("trend_weight", 0)
        seasonality_weight = qualification_settings.get("seasonality_weight", 0)
        serp_volatility_weight = qualification_settings.get("serp_volatility_weight", 0)

        search_volume = keyword_data.get(
            "keyword_info_normalized_with_clickstream", {}
        ).get("search_volume", 0)
        keyword_difficulty = keyword_data.get("keyword_properties", {}).get(
            "keyword_difficulty", 0
        )
        cpc = keyword_data.get("keyword_info", {}).get("cpc", 0)
        main_intent = keyword_data.get("search_intent_info", {}).get("main_intent")
        avg_referring_domains = keyword_data.get("avg_backlinks_info", {}).get(
            "referring_domains", 0
        )
        serp_item_types = keyword_data.get("serp_info", {}).get("serp_item_types", [])
        monthly_searches = keyword_data.get("keyword_info", {}).get(
            "monthly_searches", []
        )
        serp_last_updated_days_ago = keyword_data.get("serp_overview", {}).get(
            "serp_last_updated_days_ago"
        )
        serp_update_interval_days = keyword_data.get("serp_overview", {}).get(
            "serp_update_interval_days"
        )

        traffic_potential_score = search_volume * (1 - (keyword_difficulty / 100))
        cpc_score = cpc * 100
        competitor_strength_score = 100 - (avg_referring_domains / 10)
        serp_features_score = 0
        if "featured_snippet" in serp_item_types:
            serp_features_score += 20
        if "video" in serp_item_types:
            serp_features_score += 10
        if "ai_overview" in serp_item_types:
            serp_features_score -= 10

        trend_score = 0
        if len(monthly_searches) > 1:
            latest_search_volume = monthly_searches[0]["search_volume"]
            oldest_search_volume = monthly_searches[-1]["search_volume"]
            if oldest_search_volume > 0:
                trend_score = (
                    (latest_search_volume - oldest_search_volume) / oldest_search_volume
                ) * 100

        seasonality_score = 0
        if len(monthly_searches) > 11:
            # Calculate the average search volume for each month
            monthly_averages = [0] * 12
            for i in range(12):
                monthly_averages[i] = monthly_searches[i]["search_volume"]

            # Calculate the standard deviation of the monthly averages
            mean = sum(monthly_averages) / 12
            variance = sum([((x - mean) ** 2) for x in monthly_averages]) / 12
            std_dev = variance**0.5

            # Normalize the standard deviation to a score between 0 and 100
            if mean > 0:
                seasonality_score = 100 - (std_dev / mean) * 100

        serp_volatility_score = 0
        if (
            serp_last_updated_days_ago is not None
            and serp_update_interval_days is not None
        ):
            if serp_update_interval_days > 0:
                serp_volatility_score = (
                    100 - (serp_last_updated_days_ago / serp_update_interval_days) * 100
                )

        search_intent_score = 0
        if main_intent == "informational":
            search_intent_score = 100 * qualification_settings.get(
                "informational_intent_weight", 0
            )
        elif main_intent == "navigational":
            search_intent_score = 50 * qualification_settings.get(
                "navigational_intent_weight", 0
            )
        elif main_intent == "commercial":
            search_intent_score = 75 * qualification_settings.get(
                "commercial_intent_weight", 0
            )
        elif main_intent == "transactional":
            search_intent_score = 90 * qualification_settings.get(
                "transactional_intent_weight", 0
            )

        score = (
            (traffic_potential_score * traffic_potential_weight)
            + (cpc_score * cpc_weight)
            + (search_intent_score * search_intent_weight)
            + (competitor_strength_score * competitor_strength_weight)
            + (serp_features_score * serp_features_weight)
            + (trend_score * trend_weight)
            + (seasonality_score * seasonality_weight)
            + (serp_volatility_score * serp_volatility_weight)
        )

        breakdown = {
            "traffic_potential_score": traffic_potential_score,
            "cpc_score": cpc_score,
            "search_intent_score": search_intent_score,
            "competitor_strength_score": competitor_strength_score,
            "serp_features_score": serp_features_score,
            "trend_score": trend_score,
            "seasonality_score": seasonality_score,
            "serp_volatility_score": serp_volatility_score,
        }

        return score, breakdown
```

## File: services/serp_analysis_service.py
```python
# backend/services/serp_analysis_service.py

from typing import Dict, Any, List
from backend.core.serp_analyzer import FullSerpAnalyzer
from backend.external_apis.dataforseo_client_v2 import DataForSEOClientV2


class SerpAnalysisService:
    def __init__(self, dataforseo_client: DataForSEOClientV2, config: Dict[str, Any]):
        self.serp_analyzer = FullSerpAnalyzer(dataforseo_client, config)
        self.dataforseo_client = dataforseo_client
        self.config = config

    def analyze_serp_for_blog_opportunity(
        self, serp_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Analyzes the SERP data to determine if there is a good opportunity for a blog article.
        """
        if not serp_results or not serp_results.get("top_organic_results"):
            return {
                "blog_opportunity": False,
                "opportunity_score": 0,
                "competitor_urls": [],
            }

        top_results = serp_results.get("top_organic_results", [])
        blog_count = 0
        competitor_urls = []

        for result in top_results[:10]:
            page_type = result.get("page_type")
            if page_type == "blog" or page_type == "news":
                blog_count += 1
                competitor_urls.append(result.get("url"))

        # Simple logic: if there are at least 3 blog/news articles in the top 10,
        # it's a good opportunity.
        blog_opportunity = blog_count >= 3
        opportunity_score = blog_count / 10.0

        return {
            "blog_opportunity": blog_opportunity,
            "opportunity_score": opportunity_score,
            "competitor_urls": competitor_urls,
        }

    def analyze_keywords_serp(
        self, keywords_data: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Analyzes the SERP data for a list of keywords.
        """
        for keyword_data in keywords_data:
            keyword = keyword_data.get("keyword")
            if keyword:
                serp_results, _ = self.serp_analyzer.analyze_serp(keyword)
                serp_analysis = self.analyze_serp_for_blog_opportunity(serp_results)

                # Add competitor content if it's a blog opportunity
                if serp_analysis["blog_opportunity"]:
                    competitor_content, _ = (
                        self.dataforseo_client.get_content_onpage_data(
                            serp_analysis["competitor_urls"], self.config
                        )
                    )
                    serp_analysis["competitor_content"] = competitor_content
                else:
                    serp_analysis["competitor_content"] = []

                keyword_data["serp_analysis"] = serp_analysis

        return keywords_data
```

## File: tests/test_content_generation.py
```python
import pytest
from unittest.mock import MagicMock, patch
from backend.external_apis.openai_client import OpenAIClientWrapper
from backend.pipeline.step_04_analysis.content_analyzer import ContentAnalyzer

@pytest.fixture
def mock_openai_client():
    """Mocks the OpenAIClientWrapper to avoid actual API calls."""
    client = MagicMock(spec=OpenAIClientWrapper)
    client.latest_cost = 0.1
    return client

@pytest.fixture
def content_analyzer(mock_openai_client):
    """Provides a ContentAnalyzer instance with a mocked OpenAI client."""
    config = {
        "default_model": "gpt-5-nano",
        "max_completion_tokens_for_generation": 8192,
    }
    return ContentAnalyzer(openai_client=mock_openai_client, config=config)

def test_openai_client_enforces_json_schema_mode():
    """
    Verifies that the OpenAI client wrapper correctly uses 'json_schema' mode
    for gpt-5-nano and gpt-5-mini when a schema is provided.
    """
    with patch('openai.OpenAI') as mock_openai:
        # Arrange
        mock_create = MagicMock()
        mock_openai.return_value.chat.completions.create = mock_create

        client_wrapper = OpenAIClientWrapper(api_key="fake_key", client_cfg={})
        messages = [{"role": "user", "content": "Test prompt"}]
        schema = {"type": "object", "properties": {"key": {"type": "string"}}}

        # Act
        client_wrapper.call_chat_completion(
            messages=messages,
            schema=schema,
            model='gpt-5-nano' # Test with one of the target models
        )

        # Assert
        mock_create.assert_called_once()
        call_args = mock_create.call_args.kwargs
        assert "response_format" in call_args
        assert call_args["response_format"]["type"] == "json_schema"
        assert "json_schema" in call_args["response_format"]

def test_full_content_analysis_and_outline_workflow(content_analyzer, mock_openai_client):
    """
    Tests the full content analysis and outline generation workflow,
    ensuring it handles mocked AI responses correctly and produces a valid output.
    """
    # Arrange: Mock the return values for the two AI calls
    mock_synthesis_response = {
        "unique_angles_to_include": ["Angle 1", "Angle 2"],
        "key_entities_from_competitors": ["Entity A", "Entity B"],
        "core_questions_answered_by_serp": ["Question 1?", "Question 2?"],
        "identified_content_gaps": ["Gap A", "Gap B"],
    }
    mock_outline_response = {
        "article_structure": [
            {"h2": "Introduction", "h3s": []},
            {"h2": "Main Topic", "h3s": ["Sub-topic 1", "Sub-topic 2"]},
            {"h2": "Conclusion", "h3s": []},
        ]
    }
    # The client will return these values in order for the two calls
    mock_openai_client.call_chat_completion.side_effect = [
        (mock_synthesis_response, None),
        (mock_outline_response, None)
    ]

    keyword = "test keyword"
    serp_overview = {"paa_questions": ["PAA Question 1?"]}

    # Act: Run the synthesis and outline generation
    content_intelligence, total_cost_synthesis = content_analyzer.synthesize_content_intelligence(
        keyword=keyword,
        serp_overview=serp_overview,
        competitor_analysis=[] # Use the SERP-only path
    )

    outline, total_cost_outline = content_analyzer.generate_ai_outline(
        keyword=keyword,
        serp_overview=serp_overview,
        content_intelligence=content_intelligence
    )

    # Assert
    assert total_cost_synthesis == 0.1
    assert total_cost_outline == 0.1
    assert "unique_angles_to_include" in content_intelligence
    assert "article_structure" in outline
    assert len(outline["article_structure"]) == 3
    assert outline["article_structure"][1]["h2"] == "Main Topic"
    assert "Sub-topic 1" in outline["article_structure"][1]["h3s"]
    assert mock_openai_client.call_chat_completion.call_count == 2

print("Test script created at backend/tests/test_content_generation.py")
print("You can run this test using pytest:")
print("pytest backend/tests/test_content_generation.py")
```

## File: tests/test_filter_transformation.py
```python
# tests/test_filter_transformation.py
from pipeline.step_01_discovery.keyword_discovery.expander import (
    _transform_filters_for_api,
)


def test_no_filters():
    """Test that None is returned when no filters are provided."""
    assert _transform_filters_for_api(None) is None


def test_single_filter():
    """Test that a single filter is transformed into a flat list."""
    filters = [{"field": "keyword_info.search_volume", "operator": ">", "value": 100}]
    expected = ["keyword_info.search_volume", ">", 100]
    assert _transform_filters_for_api(filters) == expected


def test_multiple_filters():
    """Test that multiple filters are correctly interleaved with the 'and' operator."""
    filters = [
        {"field": "keyword_info.search_volume", "operator": ">", "value": 100},
        {
            "field": "keyword_properties.keyword_difficulty",
            "operator": "<",
            "value": 50,
        },
    ]
    expected = [
        ["keyword_info.search_volume", ">", 100],
        "and",
        ["keyword_properties.keyword_difficulty", "<", 50],
    ]
    assert _transform_filters_for_api(filters) == expected


def test_three_filters():
    """Test that three filters are correctly interleaved with the 'and' operator."""
    filters = [
        {"field": "keyword_info.search_volume", "operator": ">", "value": 100},
        {
            "field": "keyword_properties.keyword_difficulty",
            "operator": "<",
            "value": 50,
        },
        {"field": "keyword_info.cpc", "operator": ">", "value": 0.5},
    ]
    expected = [
        ["keyword_info.search_volume", ">", 100],
        "and",
        ["keyword_properties.keyword_difficulty", "<", 50],
        "and",
        ["keyword_info.cpc", ">", 0.5],
    ]
    assert _transform_filters_for_api(filters) == expected


def test_empty_filter_list():
    """Test that an empty list of filters returns None."""
    assert _transform_filters_for_api([]) is None
```

## File: tests/test_onpage_instant_pages.py
```python
# tests/test_onpage_instant_pages.py
import os
import json
import logging
import base64
import requests
from dotenv import load_dotenv

TEST_URL = "https://www.wikipedia.org/"
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def test_instant_pages_workflow():
    load_dotenv()
    api_login = os.getenv("DATAFORSEO_LOGIN")
    api_password = os.getenv("DATAFORSEO_PASSWORD")
    assert api_login and api_password, "DATAFORSEO credentials not found in .env file."

    credentials = f"{api_login}:{api_password}"
    headers = {
        "Authorization": f"Basic {base64.b64encode(credentials.encode()).decode()}",
        "Content-Type": "application/json",
    }

    post_data = [{"url": TEST_URL, "enable_browser_rendering": True}]

    response = requests.post(
        "https://api.dataforseo.com/v3/on_page/instant_pages",
        headers=headers,
        data=json.dumps(post_data),
        timeout=120,
    )
    response.raise_for_status()
    response_json = response.json()

    assert response_json["status_code"] == 20000, "API call was not successful."
    task_result = response_json["tasks"][0]["result"][0]
    item = task_result["items"][0]

    assert "meta" in item, "Response missing 'meta' object."
    assert "content" in item["meta"], "Response missing 'meta.content' object."
    assert "plain_text_word_count" in item["meta"]["content"], (
        "Content parsing failed: word count is missing."
    )

    word_count = item["meta"]["content"]["plain_text_word_count"]
    assert isinstance(word_count, int) and word_count > 50, (
        f"Expected a valid word count, got {word_count}."
    )

    logging.info(
        f"SUCCESS: 'instant_pages' test passed. Found word count: {word_count}."
    )
    print(json.dumps(item["meta"]["content"], indent=2))


if __name__ == "__main__":
    test_instant_pages_workflow()
```

## File: tests/test_scoring_engine.py
```python
import pytest
from pipeline.step_03_prioritization.scoring_engine import ScoringEngine

@pytest.fixture
def scoring_engine():
    # Provide a mock config with all necessary keys for scoring
    config = {
        "max_sv_for_scoring": 100000, "max_cpc_for_scoring": 10.0,
        "max_domain_rank_for_scoring": 1000, "max_referring_domains_for_scoring": 100,
        "ease_of_ranking_weight": 40, "traffic_potential_weight": 20,
        "commercial_intent_weight": 15, "competitor_weakness_weight": 10,
        # Add all other weights and config keys used by scoring components
    }
    return ScoringEngine(config)

def test_basic_scoring(scoring_engine):
    opportunity_data = {
        "keyword_info": {"search_volume": 5000, "cpc": 2.5},
        "keyword_properties": {"keyword_difficulty": 30},
        "avg_backlinks_info": {"main_domain_rank": 600, "referring_main_domains": 20},
        "search_intent_info": {"main_intent": "informational"},
        "serp_info": {"serp_item_types": ["featured_snippet"]}
    }
    score, breakdown = scoring_engine.calculate_score({"full_data": opportunity_data})
    assert 0 <= score <= 100
    assert "ease_of_ranking" in breakdown
    assert "traffic_potential" in breakdown
```

## File: Dockerfile
```dockerfile
# Use an official Python runtime as a parent image
FROM python:3.9-slim

# Set the working directory in the container
WORKDIR /app

# Copy the dependencies file to the working directory
COPY requirements.txt .

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copy the content of the local src directory to the working directory
COPY . ./backend/

# Make port 8000 available to the world outside this container
EXPOSE 8000

# Command to run the application
CMD ["uvicorn", "backend.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## File: export_db.py
```python
import json
from datetime import datetime
import os
import sys

# Add project root to sys.path to resolve imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from backend.app_config.manager import ConfigManager
from backend.data_access.database_manager import DatabaseManager

def json_serial(obj):
    """JSON serializer for objects not serializable by default json code"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Type {type(obj)} not serializable")

def export_database_to_json():
    """
    Connects to the database, fetches all opportunities, and exports them to a JSON file.
    """
    print("Starting database export...")
    try:
        config_manager = ConfigManager()
        db_manager = DatabaseManager(cfg_manager=config_manager)
        
        print("Fetching all opportunities from the database...")
        all_opportunities = db_manager.get_all_opportunities_for_export()
        print(f"Found {len(all_opportunities)} opportunities to export.")
        
        # Define the output file path in the project root
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
        output_file_path = os.path.join(project_root, "database_export.json")
        
        print(f"Exporting data to {output_file_path}...")
        with open(output_file_path, "w") as f:
            json.dump(all_opportunities, f, indent=4, default=json_serial)
            
        print("Database export completed successfully.")
        
    except Exception as e:
        print(f"An error occurred during the export process: {e}")

if __name__ == "__main__":
    export_database_to_json()
```

## File: jobs.py
```python
# jobs.py
import threading
import time
import uuid
import logging
from typing import Dict, Any, Callable, Optional
from datetime import datetime
from backend.data_access import queries

# Import DatabaseManager
from backend.data_access.database_manager import DatabaseManager

logger = logging.getLogger(__name__)


class JobManager:
    """Manages asynchronous jobs, their status, and results, backed by a database."""

    # MODIFIED: __init__ now requires a db_manager
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.db_manager.fail_stale_jobs()
        # The in-memory job store and lock are no longer needed.
        # self.jobs: Dict[str, Dict[str, Any]] = {}
        # self.lock = threading.Lock()

    def create_job(
        self, client_id: str, target_function: Callable, args: tuple = (), kwargs: dict = {}
    ) -> str:
        """
        Creates a new job, saves its initial state to the DB, starts it in a
        separate thread, and returns its ID.
        """
        job_id = str(uuid.uuid4())
        job_info = {
            "id": job_id,
            "client_id": client_id,
            "status": "pending",
            "progress": 0,
            "result": None,
            "error": None,
            "started_at": time.time(),
            "finished_at": None,
            "function_name": target_function.__name__,
        }

        # MODIFIED: Save job to DB instead of in-memory dict
        self.db_manager.update_job(job_info)

        logger.info(f"Job {job_id} created for client {client_id} and function {target_function.__name__}")
        thread = threading.Thread(
            target=self._run_job, args=(job_id, target_function, args, kwargs)
        )
        thread.daemon = True
        thread.start()
        return job_id

    def update_job_progress(self, job_id: str, step: str, message: str, status: Optional[str] = None):
        """Appends a progress log to the job record in the database."""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "message": message,
        }
        
        # This operation needs to be atomic to prevent race conditions.
        # We'll fetch the current job, update the log, and save it back.
        # A more advanced setup might use a database transaction or a JSON_APPEND function.
        job_info = self.get_job_status(job_id)
        if job_info:
            progress_log = job_info.get("progress_log", [])
            if isinstance(progress_log, str): # Handle case where it might be a JSON string
                try:
                    progress_log = json.loads(progress_log)
                except json.JSONDecodeError:
                    progress_log = []
            
            progress_log.append(log_entry)
            job_info["progress_log"] = progress_log

            # Optionally update the overall job status at the same time
            if status:
                job_info["status"] = status

            self.db_manager.update_job(job_info)

    def _run_job(
        self, job_id: str, target_function: Callable, args: tuple, kwargs: dict
    ):
        """Internal method to execute the target function and update job status in the DB."""
        logger.info(f"Job {job_id} starting. DB manager: {self.db_manager}")
        try:
            # Initialize the progress log
            self.update_job_status(job_id, "running", progress=5)
            self.update_job_progress(job_id, "Job Started", "The workflow is initializing.")
            
            result = target_function(job_id, *args, **kwargs)
            
            self.update_job_progress(job_id, "Job Finished", "The workflow completed successfully.")
            self.update_job_status(job_id, "completed", progress=100, result=result)
            logger.info(f"Job {job_id} completed successfully.")
        except Exception as e:
            error_message = f"Job {job_id} failed: {e}"
            logger.error(error_message, exc_info=True)
            self.update_job_progress(job_id, "Job Failed", str(e))
            self.update_job_status(job_id, "failed", progress=100, error=str(e))


    def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """Retrieves the current status of a job from the database."""
        # MODIFIED: Fetch from DB
        return self.db_manager.get_job(job_id)

    def update_job_status(
        self,
        job_id: str,
        status: str,
        progress: int,
        result: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None,
    ):
        """
        Updates job status using a direct UPDATE query (W10 FIX).
        """
        conn = self.db_manager._get_conn()
        finished_at = (
            datetime.now().timestamp() if status in ["completed", "failed"] else None
        )

        if result or error:
            # If result/error is present, use the original UPDATE_JOB (INSERT OR REPLACE)
            # that handles all fields.
            job_info = self.db_manager.get_job(job_id)
            if job_info:
                job_info["status"] = status
                job_info["progress"] = progress
                job_info["result"] = result
                job_info["error"] = error
                job_info["finished_at"] = finished_at
                self.db_manager.update_job(job_info)
        else:
            # Execute the direct, optimized status/progress update
            # This avoids fetching the entire job record first. (W10 FIX)
            with conn:
                conn.execute(
                    queries.UPDATE_JOB_STATUS_DIRECT,
                    (status, progress, finished_at, job_id),
                )

    # Global job manager instance is no longer initialized here.
    # It will be initialized in api/main.py where it has access to the db_manager.
    # job_manager = JobManager()

    def cancel_job(self, job_id: str) -> bool:
        """Marks a job as 'failed' with a 'cancelled by user' message."""
        job_info = self.get_job_status(job_id)
        if job_info and job_info["status"] in ["pending", "running", "paused"]:
            # The crucial part: mark as failed in the DB so the running thread sees it
            self.update_job_status(
                job_id,
                "failed",
                job_info.get("progress", 0),
                error="Cancelled by user.",
            )
            logger.info(f"Job {job_id} was marked as 'failed' (cancelled by user).")
            return True
        return False
```

## File: requirements.txt
```
fastapi
uvicorn
python-dotenv
scikit-learn
sentence-transformers
requests
textstat
bleach
openai
beautifulsoup4
markdown
```
