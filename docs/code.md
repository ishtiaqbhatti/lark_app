CODE:

This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-10-21T21:33:35.803Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
backend/
  agents/
    __init__.py
    article_generator.py
    brief_assembler.py
    content_auditor.py
    html_formatter.py
    image_generator.py
    internal_linking_suggester.py
    prompt_assembler.py
    social_media_crafter.py
    summary_generator.py
  api/
    routers/
      auth.py
      client_settings.py
      clients.py
      discovery.py
      jobs.py
      opportunities.py
      orchestrator.py
      qualification_settings.py
      qualification_strategies.py
      settings.py
    dependencies.py
    globals.py
    main.py
    models.py
  app_config/
    __init__.py
    manager.py
    settings.ini
  core/
    serp_analyzers/
      disqualification_analyzer.py
      featured_snippet_analyzer.py
      pixel_ranking_analyzer.py
      video_analyzer.py
    __init__.py
    blueprint_factory.py
    page_classifier.py
    serp_analyzer.py
    utils.py
  data_access/
    migrations/
      001_add_new_tables.sql
      001_initial_schema.sql
      002_add_keywords_table.sql
      002_remove_json_columns.sql
      003_add_indexes.sql
      003_add_total_api_cost.sql
      004_add_qualification_settings_table.sql
      005_add_qualification_columns.sql
      006_add_intent_weights.sql
      007_add_competitor_strength_weight.sql
      008_add_serp_features_weight.sql
      009_add_trend_weight.sql
      010_add_history_columns.sql
      012_add_seasonality_weight.sql
      013_add_serp_volatility_weight.sql
      014_add_disqualification_rules.sql
      015_add_brand_keywords.sql
      016_add_review_threshold.sql
      017_add_strategies_table.sql
      018_add_job_id_and_cluster_name.sql
      019_add_core_keyword_and_competitor_metrics_to_opportunities.sql
      020_backfill_core_keyword_metrics.sql
      021_add_unique_keyword_constraint.sql
      023_add_run_id_to_opportunities.sql
      024_add_total_api_cost_to_opportunities.sql
      025_add_cost_to_discovery_runs.sql
    __init__.py
    database_manager.py
    initialize.py
    models.py
    queries.py
  data_mappers/
    dataforseo_mapper.py
    keyword_data_mapper.py
    serp_overview_mapper.py
  external_apis/
    dataforseo_client_v2.py
    on-page-api.py
    openai_client.py
    pexels_client.py
  pipeline/
    orchestrator/
      __init__.py
      analysis_orchestrator.py
      content_orchestrator.py
      cost_estimator.py
      discovery_orchestrator.py
      image_orchestrator.py
      main.py
      social_orchestrator.py
      validation_orchestrator.py
      workflow_orchestrator.py
    step_01_discovery/
      keyword_discovery/
        expander.py
        filters.py
      __init__.py
      blog_content_qualifier.py
      cannibalization_checker.py
      disqualification_rules.py
      keyword_expander.py
      run_discovery.py
    step_02_qualification/
      __init__.py
      competitor_analyzer.py
      serp_analyzer.py
    step_03_prioritization/
      scoring_components/
        __init__.py
        commercial_intent.py
        competitor_performance.py
        competitor_weakness.py
        ease_of_ranking.py
        growth_trend.py
        keyword_structure.py
        serp_crowding.py
        serp_features.py
        serp_freshness.py
        serp_threat.py
        serp_volatility.py
        traffic_potential.py
        volume_volatility.py
      __init__.py
      run_prioritization.py
      scoring_engine.py
    step_04_analysis/
      content_analysis_modules/
        ai_intelligence_caller.py
        heading_analyzer.py
        metric_analyzer.py
      __init__.py
      competitor_analyzer.py
      content_analyzer.py
      run_analysis.py
    step_05_strategy/
      decision_engine.py
    step_06_content_creation/
      __init__.py
    __init__.py
    orchestrator.py
  services/
    discovery_service.py
    disqualification_service.py
    keyword_data_aggregator.py
    opportunities_service.py
    qualification_service.py
    scoring_service.py
    serp_analysis_service.py
  tests/
    test_content_generation.py
    test_filter_transformation.py
    test_onpage_instant_pages.py
  Dockerfile
  jobs.py
  requirements.txt
client/
  my-content-app/
    src/
      components/
        layout/
          AppSidebar.jsx
          MainLayout.jsx
        ContentDiffViewer.jsx
        CostConfirmationModal.jsx
        GlobalJobTracker.jsx
        JobStatusIndicator.jsx
        PromptTemplateEditor.jsx
      context/
        AuthContext.jsx
        ClientContext.jsx
        JobContext.jsx
        NotificationContext.jsx
      hooks/
        useClient.js
        useDebounce.js
      pages/
        ActivityLog/
          ActivityLogPage.jsx
        Auth/
          LoginPage.jsx
        BlogPage/
          BlogPage.css
          BlogPage.jsx
        ClientDashboard/
          AddNewClientModal.jsx
          ClientDashboardPage.jsx
        ClientSettings/
          ClientSettingsPage.jsx
        Dashboard/
          DashboardPage.jsx
        DiscoveryPage/
          components/
            DiscoveryForm.jsx
            DiscoveryHistory.jsx
            DiscoveryStatsBreakdown.jsx
            FilterBuilder.jsx
            FunnelChart.css
            FunnelChart.jsx
            PieChartCard.jsx
            RunDetailsModal.jsx
            RunDetailsPage.jsx
          hooks/
            useDiscoveryFilters.js
            useDiscoveryRuns.js
          DiscoveryPage.jsx
        NotFoundPage/
          NotFoundPage.jsx
        OpportunitiesPage/
          components/
            ScoreBreakdownModal.jsx
          hooks/
            useOpportunities.js
            useOpportunities.refactored.js
          OpportunitiesPage.css
          OpportunitiesPage.jsx
        opportunity-detail-page/
          components/
            ActionCenter.jsx
            AdditionalInsights.jsx
            ArticlePreview.jsx
            CompetitorBacklinks.jsx
            ContentAuditCard.jsx
            ContentBlueprint.jsx
            ErrorMessage.jsx
            ExecutiveSummary.jsx
            FactorsCard.jsx
            FeaturedSnippetCard.jsx
            GrowthTrend.jsx
            IntentAnalysis.jsx
            KeywordMetrics.jsx
            MetaInfo.jsx
            NoData.jsx
            OpportunityHeader.jsx
            QualificationInfo.jsx
            RecommendedStrategyCard.jsx
            SerpAnalysis.jsx
            SerpVitals.jsx
            SocialMediaTab.jsx
            StrategicNotes.jsx
            StrategicScoreBreakdown.jsx
            VerdictCard.jsx
            WorkflowStatusAlert.jsx
            WorkflowTracker.jsx
          hooks/
            useOpportunityData.js
          index.jsx
        RunDetailsPage/
          RunDetailsPage.jsx
        Settings/
          tabs/
            AiContentSettingsTab.jsx
            DiscoverySettingsTab.jsx
            ScoringWeightsTab.jsx
          SettingsPage.jsx
      services/
        apiClient.js
        authService.js
        clientService.js
        clientSettingsService.js
        discoveryService.js
        jobsService.js
        opportunitiesService.js
        orchestratorService.js
        settingsService.js
      App.jsx
      index.css
      main.jsx
    .dockerignore
    .eslintignore
    .eslintrc.json
    Dockerfile
    index.html
    nginx.conf
    package.json
    vite.config.js
docs/
  plan.md
  serp.md
.gitignore
docker-compose.yml
get_opportunity.py
openai_audit.md
opportunity_3.json
run_dev.sh
workflow_analysis.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="backend/agents/__init__.py">
# agents/__init__.py
</file>

<file path="backend/agents/article_generator.py">
import logging
from typing import Dict, Any, Tuple, Optional, List

from external_apis.openai_client import OpenAIClientWrapper
from agents.prompt_assembler import DynamicPromptAssembler


class SectionalArticleGenerator:
    """
    An agentic generator that creates content for specific sections of an article
    based on a highly contextual prompt.
    """

    def __init__(
        self, openai_client: OpenAIClientWrapper, config: Dict[str, Any], db_manager
    ):
        self.openai_client = openai_client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.prompt_assembler = DynamicPromptAssembler(db_manager)

    def _generate_component(
        self, messages: List[Dict[str, str]], model: str, temperature: float
    ) -> Tuple[Optional[str], float]:
        """Internal helper to call the LLM and get a raw HTML string."""
        schema = {
            "name": "generate_html_content",
            "type": "object",
            "properties": {
                "content_html": {
                    "type": "string",
                    "description": "The generated content as a clean, well-structured HTML block. Do not include the main heading tag itself.",
                }
            },
            "required": ["content_html"],
            "additionalProperties": False,
        }
        response, error = self.openai_client.call_chat_completion(
            messages=messages,
            schema=schema,
            model=model,
            max_completion_tokens=2048,
        )
        cost = self.openai_client.latest_cost
        if error or not response:
            self.logger.error(f"Failed to generate content component: {error}")
            return None, cost
        return response.get("content_html"), cost

    def generate_introduction(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[Optional[str], float]:
        brief = opportunity.get("blueprint", {}).get("ai_content_brief", {})
        prompt = f"""
        You are an expert copywriter. Write a compelling and hook-driven introduction for a blog post titled "{opportunity["keyword"]}".
        The introduction should be 2-3 paragraphs.
        - Immediately grab the reader's attention with a relatable problem or surprising statistic.
        - Briefly state the core problem or question the article will solve and why it matters.
        - End with a transition that clearly outlines what the reader will learn.
        - Target Audience: {brief.get("target_audience_persona")}
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}
        Return a JSON object with a single key "content_html" containing the HTML for the introduction (e.g., {{"content_html": "<p>...</p><p>...</p>"}}).
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.75,
        )

    def generate_conclusion(
        self, opportunity: Dict[str, Any], full_article_context: str
    ) -> Tuple[Optional[str], float]:
        cta_url = opportunity.get("client_cfg", {}).get("default_cta_url", "#")
        prompt = f"""
        You are an expert copywriter. Write a powerful conclusion for the following blog post.
        The conclusion should be 2 paragraphs.
        - Briefly summarize the most important takeaways from the article.
        - Provide a final, actionable thought or encouragement for the reader.
        - End with a compelling call-to-action that encourages the reader to visit {cta_url}.
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}

        **Full Article Context (for summary):**
        {full_article_context}

        Return a JSON object with a single key "content_html" containing the HTML for the conclusion (e.g., {{"content_html": "<p>...</p><p>...</p>"}}).
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.7,
        )

    def generate_section(
        self,
        opportunity: Dict[str, Any],
        section_title: str,
        section_sub_points: List[str],
        previous_section_content: str,
    ) -> Tuple[Optional[str], float]:
        brief = opportunity.get("blueprint", {}).get("ai_content_brief", {})
        prompt = f"""
        You are an expert SEO content writer and subject matter expert. Your task is to write a single, detailed section for a blog post about "{opportunity["keyword"]}".

        **Current Section to Write:** "{section_title}"
        **Key Sub-points to cover in this section:** {", ".join(section_sub_points) if section_sub_points else "N/A"}
        **Content from the Previous Section (for transition and context):**
        ...{previous_section_content[-1000:]}...

        **Instructions:**
        - Write a comprehensive, in-depth section covering the topic "{section_title}".
        - If provided, elaborate on all key sub-points, using them to structure the section's content.
        - Ensure a smooth, logical transition from the previous section's content.
        - Incorporate relevant entities and demonstrate expertise by using practical examples or insights.
        - Persona: {brief.get("target_audience_persona")}
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}
        
        Return a JSON object with a single key "content_html" containing the HTML for this section (e.g., using <p>, <ul>, <h3> tags). Do NOT include the main <h2> tag for "{section_title}" itself.
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.7,
        )
</file>

<file path="backend/agents/brief_assembler.py">
import logging
from typing import Dict, Any


class BriefAssembler:
    """
    Assembles the final AI content brief from the blueprint data.
    This agent acts as a transformation layer between the analysis blueprint
    and the actionable brief for the content generation AI.
    """

    def __init__(self, openai_client):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.openai_client = openai_client

    def _generate_dynamic_brief_attributes(self, blueprint: Dict[str, Any], client_cfg: Dict[str, Any]) -> Dict[str, Any]:
        """Uses an AI call to generate dynamic persona and goal."""
        try:
            target_keyword = blueprint.get("winning_keyword", {}).get("keyword", "the target topic")
            serp_overview = blueprint.get("serp_overview", {})
            
            prompt = f"""
            Based on the following SERP data for the keyword '{target_keyword}', define a target audience persona and a primary goal for a new piece of content.

            SERP Data:
            - Dominant Content Format: {serp_overview.get('dominant_content_format', 'N/A')}
            - People Also Ask: {', '.join(serp_overview.get('people_also_ask', []))}
            - Related Searches: {', '.join(serp_overview.get('related_searches', []))}

            Client's Brand Voice: "{client_cfg.get('brand_voice', 'expert and informative')}"

            Return a JSON object with two keys: 'target_audience_persona' and 'primary_goal'.
            The persona should be a brief, descriptive summary of the ideal reader.
            The goal should be a concise statement about what the content aims to achieve for that reader.
            """
            
            messages = [{"role": "user", "content": prompt}]
            schema = {
                "name": "generate_brief_attributes",
                "description": "Generates a target audience persona and primary goal for a piece of content.",
                "type": "object",
                "properties": {
                    "target_audience_persona": {
                        "type": "string",
                        "description": "A brief, descriptive summary of the ideal reader."
                    },
                    "primary_goal": {
                        "type": "string",
                        "description": "A concise statement about what the content aims to achieve for that reader."
                    }
                },
                "required": ["target_audience_persona", "primary_goal"],
                "additionalProperties": False
            }

            response_json, error = self.openai_client.call_chat_completion(
                messages=messages,
                model="gpt-5-nano",
                schema=schema
            )

            if error:
                raise Exception(f"AI call failed: {error}")

            return response_json

        except Exception as e:
            self.logger.error(f"Failed to generate dynamic brief attributes: {e}")
            # Fallback to static values
            return {
                "target_audience_persona": self._determine_persona("Blog Post", client_cfg),
                "primary_goal": f"To provide a comprehensive and helpful resource that ranks for '{target_keyword}'.",
            }

    def assemble_brief(
        self, blueprint: Dict[str, Any], client_id: str, client_cfg: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Constructs the AI content brief by extracting and structuring
        information from the blueprint.
        """
        winning_keyword_data = blueprint.get("winning_keyword", {})
        serp_overview = blueprint.get("serp_overview", {})
        content_intelligence = blueprint.get("content_intelligence", {})
        content_type = blueprint.get("recommended_strategy", {}).get(
            "content_format", "Blog Post"
        )
        
        dynamic_attrs = self._generate_dynamic_brief_attributes(blueprint, client_cfg)
        
        word_count_multiplier = self._get_word_count_multiplier(
            content_type, client_cfg
        )
        base_word_count = content_intelligence.get("recommended_word_count", 1500)
        target_word_count = int(base_word_count * word_count_multiplier)

        brief = {
            "target_keyword": winning_keyword_data.get("keyword"),
            "content_type": content_type,
            "target_audience_persona": dynamic_attrs.get("target_audience_persona"),
            "primary_goal": dynamic_attrs.get("primary_goal"),
            "target_word_count": target_word_count,
            "mandatory_sections": content_intelligence.get(
                "common_headings_to_cover", []
            ),
            "unique_angles_to_cover": content_intelligence.get(
                "unique_angles_to_include", []
            ),
            "questions_to_answer_directly": serp_overview.get("paa_questions", []),
            "key_entities_to_mention": content_intelligence.get(
                "key_entities_from_competitors", []
            ),
            "compelling_arguments_to_integrate": content_intelligence.get(
                "unique_arguments_from_competitors", []
            ),
            "core_questions_competitors_answer": content_intelligence.get(
                "core_questions_answered_by_competitors", []
            ),
            "related_topics_to_include": serp_overview.get("related_searches", []),
            "google_preferred_answers": serp_overview.get(
                "extracted_serp_features", []
            ),
            "dynamic_serp_instructions": self._get_dynamic_serp_instructions(
                serp_overview, content_intelligence, blueprint
            ),
            "source_and_inspiration_content": {
                "competitors_urls": [
                    comp["url"]
                    for comp in blueprint.get("competitor_analysis", [])
                    if "url" in comp
                ]
            },
            "client_id": client_id,
        }

        # --- START MODIFICATION ---
        # NEW: Add all the rich SERP data to the brief
        if serp_overview.get("knowledge_graph_facts"):
            brief["knowledge_graph_facts"] = serp_overview["knowledge_graph_facts"]
        if serp_overview.get("paid_ad_copy"):
            brief["paid_ad_copy"] = serp_overview["paid_ad_copy"]
        if serp_overview.get("ai_overview_sources"):
            brief["ai_overview_sources"] = serp_overview["ai_overview_sources"]
        if serp_overview.get("top_organic_faqs"):
            brief["top_organic_faqs"] = serp_overview["top_organic_faqs"]
        if serp_overview.get("top_organic_sitelinks"):
            brief["top_organic_sitelinks"] = serp_overview["top_organic_sitelinks"]
        if serp_overview.get("discussion_snippets"):
            brief["discussion_snippets"] = serp_overview["discussion_snippets"]

        all_about_search_terms = []
        all_about_related_terms = []
        for res in blueprint.get("serp_overview", {}).get("top_organic_results", []):
            if res.get("about_this_result_search_terms"):
                all_about_search_terms.extend(res["about_this_result_search_terms"])
            if res.get("about_this_result_related_terms"):
                all_about_related_terms.extend(res["about_this_result_related_terms"])

        if all_about_search_terms:
            brief["google_understanding_search_terms"] = list(
                set(all_about_search_terms)
            )
        if all_about_related_terms:
            brief["google_understanding_related_terms"] = list(
                set(all_about_related_terms)
            )
        # --- END MODIFICATION ---

        self.logger.info(f"Assembled AI content brief for '{brief['target_keyword']}'.")
        return brief

    def _get_dynamic_serp_instructions(
        self,
        serp_overview: Dict[str, Any],
        content_intelligence: Dict[str, Any],
        blueprint: Dict[str, Any],
    ) -> list[str]:
        """Generates specific instructions for the AI based on SERP features and content intelligence."""
        instructions = []
        if serp_overview.get("extracted_serp_features"):
            instructions.append(
                "Pay close attention to the 'google_preferred_answers' section. This contains content that Google has already featured in rich snippets, so it's a strong indication of what Google considers a good answer. Use it as a primary source of inspiration."
            )
        if serp_overview.get("serp_has_featured_snippet"):
            instructions.append(
                "Create a concise, clear paragraph early in the article that directly answers the main query to target the featured snippet."
            )
        if serp_overview.get("serp_has_ai_overview"):
            instructions.append(
                "The content must be exceptionally high-quality, original, and provide unique insights to stand out against Google's AI Overview."
            )
        if serp_overview.get("has_video_carousel"):
            instructions.append(
                "Structure content in a way that is easily adaptable into a video script, as video is a key format for this topic."
            )
        if serp_overview.get("knowledge_graph_facts"):
            instructions.append(
                f"Incorporate the key facts from the Knowledge Graph: {', '.join(serp_overview['knowledge_graph_facts'])}."
            )

        top_results_with_context = [
            r
            for r in serp_overview.get("top_organic_results", [])
            if r.get("about_this_result_source_info")
        ]
        if top_results_with_context:
            context_info = top_results_with_context[0]["about_this_result_source_info"]
            instructions.append(
                f"Google's 'About this Result' panel says the top result is relevant because: '{context_info}'. Use this to understand the core topic."
            )

        if serp_overview.get("discussions_and_forums_snippets"):
            instructions.append(
                f"Address user pain points from forum discussions: {' | '.join(serp_overview['discussions_and_forums_snippets'])}."
            )

        days_ago = serp_overview.get("serp_last_updated_days_ago")
        if days_ago is not None:
            if days_ago <= 30:
                instructions.append(
                    f"The SERP is fresh ({days_ago} days old). Ensure content reflects the latest information."
                )
            elif days_ago > 180:
                instructions.append(
                    f"The SERP is outdated ({days_ago} days old). This is an opportunity to publish a more current and comprehensive article."
                )

        # Instructions from content intelligence (including competitor weaknesses)
        if content_intelligence.get("key_entities_from_competitors"):
            instructions.append(
                f"Mention key entities identified from competitors: {', '.join(content_intelligence['key_entities_from_competitors'])}."
            )

            # Summarize competitor weaknesses based on new granular scores
            competitor_analysis = blueprint.get("competitor_analysis", [])
            if competitor_analysis and all(
                "overall_strength_score" in c
                for c in competitor_analysis
                if c.get("url")
            ):
                tech_scores = [
                    c.get("technical_strength_score", 100)
                    for c in competitor_analysis
                    if c.get("url")
                ]
                content_scores = [
                    c.get("content_quality_score", 100)
                    for c in competitor_analysis
                    if c.get("url")
                ]

                # NEW: Add specific technical weaknesses if identified
                all_tech_warnings = []
                for comp in competitor_analysis:
                    all_tech_warnings.extend(comp.get("technical_warnings", []))

                unique_issues_to_highlight = list(set(all_tech_warnings))[
                    :3
                ]  # Limit to top 3
                if unique_issues_to_highlight:
                    formatted_warnings = ", ".join(
                        [w.replace("_", " ") for w in unique_issues_to_highlight]
                    )
                    instructions.append(
                        f"EXPLOIT WEAKNESS: Top competitors exhibit specific technical flaws such as: {formatted_warnings}. Your content must be technically superior."
                    )
                elif tech_scores:  # Fallback to general if no specific warnings
                    avg_tech_score = sum(tech_scores) / len(tech_scores)
                    if avg_tech_score < 65:
                        instructions.append(
                            "EXPLOIT WEAKNESS: Top competitors are generally technically poor (e.g., slow page speed, render-blocking resources). A fast, well-built page has a strong advantage."
                        )

                if content_scores:
                    avg_content_score = sum(content_scores) / len(content_scores)
                    if avg_content_score < 65:
                        instructions.append(
                            "EXPLOIT WEAKNESS: Ranking content is low-quality, thin, or outdated. Win by providing significantly more depth and fresh information."
                        )

        return instructions

    def _determine_persona(self, content_type: str, client_cfg: Dict[str, Any]) -> str:
        """Determines the persona to use based on client config."""
        base_persona = client_cfg.get("expert_persona", "an expert writer")
        return f"{base_persona} who writes like a human"

    def _get_word_count_multiplier(
        self, content_type: str, client_cfg: Dict[str, Any]
    ) -> float:
        """
        Gets the word count multiplier for a given content type from the client config,
        falling back to a default if not found.
        """
        # Sanitize the content_type to match the keys in settings.ini
        # e.g., "Comprehensive Article" -> "comprehensive_article"
        sanitized_format = content_type.lower().replace(" ", "_")

        # Look up the specific multiplier, or use the default multiplier if not found
        return client_cfg.get(
            sanitized_format, client_cfg.get("default_multiplier", 1.2)
        )
</file>

<file path="backend/agents/content_auditor.py">
import logging
import textstat
from typing import Dict, Any, List, Optional  # ADD List
from bs4 import BeautifulSoup  # ADD this for HTML parsing
import re  # ADD this for regex checks
import requests


class ContentAuditor:
    """
    Audits the generated content for SEO and readability metrics,
    and checks for "publish-readiness" issues.
    """

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def _check_for_broken_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """Checks all <a> tags for 4xx or 5xx status codes."""
        issues = []
        links = soup.find_all("a", href=True)
        for link in links:
            href = link["href"]
            # Skip internal/anchor links and javascript links
            if (
                not href
                or href.startswith("#")
                or href.startswith("/")
                or href.startswith("javascript:")
            ):
                continue
            try:
                # Use a HEAD request for efficiency with a timeout
                response = requests.head(href, timeout=5, allow_redirects=True)
                if response.status_code >= 400:
                    issues.append(
                        {
                            "issue": "broken_link",
                            "context": f"URL '{href}' returned status {response.status_code}.",
                        }
                    )
            except requests.exceptions.Timeout:
                issues.append(
                    {
                        "issue": "link_timeout",
                        "context": f"Could not get response from '{href}' within 5 seconds.",
                    }
                )
            except requests.RequestException:
                issues.append(
                    {
                        "issue": "unreachable_link",
                        "context": f"Could not connect to URL '{href}'.",
                    }
                )
        return issues

    def audit_content(
        self,
        article_html: str,
        primary_keyword: str,
        blueprint: Dict[str, Any],
        client_cfg: Dict[str, Any],
        avg_competitor_readability: Optional[float] = None,
    ) -> Dict[str, Any]:
        """
        Audits the HTML content and returns a dictionary of metrics.
        """
        # Extract plain text for text-based analysis
        soup = BeautifulSoup(article_html, "html.parser")
        plain_text = soup.get_text(separator=" ", strip=True)
        html_issues = self._check_html_publish_readiness(article_html)
        if html_issues is None:
            html_issues = []

        # Add broken link check results
        broken_link_issues = self._check_for_broken_links(soup)
        html_issues.extend(broken_link_issues)

        word_count = len(plain_text.split())
        target_word_count = blueprint.get("ai_content_brief", {}).get(
            "target_word_count", 0
        )
        if target_word_count > 0:
            deviation = abs(word_count - target_word_count) / target_word_count
            if deviation > 0.20:
                html_issues.append(
                    {
                        "issue": "word_count_deviation",
                        "context": f"Actual count ({word_count}) deviates from target ({target_word_count}) by more than 20%.",
                    }
                )

        readability_score = textstat.flesch_kincaid_grade(plain_text)
        # Calculate additional metrics for comprehensive audit (Task 9.1)
        smog_score = textstat.smog_index(plain_text)
        coleman_liau_score = textstat.coleman_liau_index(plain_text)

        persona = blueprint.get("ai_content_brief", {}).get(
            "target_audience_persona", "General audience"
        )

        # W13 FIX: Determine Readability Mismatch and Required Refinement Command
        refinement_command = None
        readability_assessment = f"Flesch-Kincaid Grade Level: {readability_score:.1f}."

        if avg_competitor_readability is not None:
            readability_assessment += (
                f" (Avg. Competitor F-K: {avg_competitor_readability:.1f})."
            )
            if (
                abs(readability_score - avg_competitor_readability) > 3.0
            ):  # If our score is more than 3 grades off
                if readability_score < avg_competitor_readability:
                    readability_assessment += " Assessment: CRITICAL: Content is significantly simpler than competitors. Consider increasing complexity."
                    refinement_command = f"Increase the complexity to match competitor average of Flesch-Kincaid Grade Level {avg_competitor_readability:.1f}."
                else:
                    readability_assessment += " Assessment: CRITICAL: Content is significantly more complex than competitors. Consider simplifying."
                    refinement_command = f"Simplify the content to match competitor average of Flesch-Kincaid Grade Level {avg_competitor_readability:.1f}."
            else:
                readability_assessment += (
                    " Assessment: Readability is consistent with top competitors."
                )
        else:
            # Fallback to persona-based assessment if no competitor average is provided
            if "expert" in persona.lower() or "planner" in persona.lower():
                if readability_score < 9.5:
                    readability_assessment += " Assessment: CRITICAL: Content is likely oversimplified (Grade < 9.5)."
                    refinement_command = "Increase the complexity and authoritative tone of the writing to target a Flesch-Kincaid Grade Level of 10 or higher."
                else:
                    readability_assessment += (
                        " Assessment: Appropriate complexity for an expert audience."
                    )
            else:
                if readability_score > 12 or smog_score > 10.0:
                    readability_assessment += " Assessment: CRITICAL: Content is too academic or complex (Grade > 12 or SMOG > 10.0)."
                    refinement_command = "Simplify the complexity and reduce sentence length to target a Flesch-Kincaid Grade Level between 7 and 9, and a SMOG Index under 8."
                else:
                    readability_assessment += (
                        " Assessment: Appropriate complexity for a general audience."
                    )

        entity_metrics = self._check_entity_coverage(plain_text, blueprint)
        if entity_metrics.get("score", 100) < 75.0 and entity_metrics.get(
            "missing"
        ):  # Only flag if there are actually missing entities
            missing_entities_str = ", ".join(entity_metrics.get("missing", []))
            html_issues.append(
                {
                    "issue": "critical_entity_gap",
                    "context": f"Entity coverage is below 75%. Missing: {missing_entities_str}",
                }
            )

        # Ensure the final return object from audit_content includes all new data:
        return {
            "flesch_kincaid_grade": readability_score,
            "smog_index": smog_score,
            "coleman_liau_index": coleman_liau_score,
            "readability_assessment": readability_assessment,
            "refinement_command": refinement_command,
            "entity_coverage_score": entity_metrics.get("score", 0),
            "missing_entities": entity_metrics.get("missing", []),
            "covered_sections": None,
            "publish_readiness_issues": html_issues,  # This now includes broken links and critical entity gaps
        }

    def _check_entity_coverage(
        self, article_text: str, blueprint: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Checks if the key entities from the blueprint are present in the article text.
        """
        # ... (existing logic)
        entities = blueprint.get("ai_content_brief", {}).get(
            "key_entities_to_mention", []
        )
        if not entities:
            return {"score": 100, "missing": []}

        missing_entities = []
        for entity in entities:
            # Heuristic check: Look for exact match or simple pluralization (Task 11.1)
            if entity.endswith("s"):
                # If entity is already plural (e.g., 'tools'), check for exact match only
                pattern = r"\b" + re.escape(entity) + r"\b"
            else:
                # Check for singular or plural form (e.g., 'tool' or 'tools')
                pattern = r"\b" + re.escape(entity) + r"s?\b"
            if not re.search(pattern, article_text, re.IGNORECASE):
                missing_entities.append(entity)

        coverage_score = (
            100 - (len(missing_entities) / len(entities) * 100) if entities else 100
        )
        return {
            "score": coverage_score,
            "missing": missing_entities,
            "found_count": len(entities) - len(missing_entities),
            "total_expected": len(entities),
        }

    def _check_html_publish_readiness(self, article_html: str) -> List[Dict[str, Any]]:
        """
        Performs specific checks on the final HTML for publish-readiness, returning structured issues.
        """
        issues = []
        soup = BeautifulSoup(article_html, "html.parser")

        # Check for unresolved image placeholders
        placeholder_pattern = r"\[\[IMAGE_ID:\s*(.*?)\s*PROMPT:\s*(.*?)\s*\]\]"
        placeholders_found = re.findall(placeholder_pattern, article_html)
        if placeholders_found:
            issues.append(
                {
                    "issue": "unresolved_placeholder",
                    "context": f"{len(placeholders_found)} image placeholders remain in the text.",
                }
            )

        # Check for empty headings
        for h_tag in soup.find_all(re.compile(r"^h[1-6]$")):
            if not h_tag.get_text(strip=True):
                issues.append({"issue": "empty_heading", "context": str(h_tag)})

        # Check for extremely short paragraphs
        for p_tag in soup.find_all("p"):
            text = p_tag.get_text(strip=True)
            if 0 < len(text.split()) < 5:
                issues.append({"issue": "short_paragraph", "context": str(p_tag)})

        return issues
</file>

<file path="backend/agents/html_formatter.py">
import logging
from typing import Dict, Any, List, Optional
import os
import re
import markdown
from bs4 import BeautifulSoup
from datetime import datetime
from backend.core import utils


class HtmlFormatter:
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def _convert_markdown_tables_to_html(self, html_body: str) -> str:
        """Converts Markdown tables to HTML using markdown library directly."""
        return markdown.markdown(html_body, extensions=["tables", "fenced_code"])

    def _insert_internal_links(
        self, soup: BeautifulSoup, internal_links: List[Dict[str, str]]
    ) -> None:
        """
        Inserts internal links into the BeautifulSoup object based on specific contextual suggestions from an AI agent.
        """
        if not internal_links:
            return

        linked_anchors = set()

        for link_data in internal_links:
            anchor_text = link_data.get("anchor_text")
            url = link_data.get("url")
            context_paragraph = link_data.get("context_paragraph_text")

            if (
                not all([anchor_text, url, context_paragraph])
                or anchor_text.lower() in linked_anchors
            ):
                continue

            # Find all paragraphs that contain the exact context text
            potential_paragraphs = soup.find_all(
                "p", string=re.compile(re.escape(context_paragraph))
            )

            for p_tag in potential_paragraphs:
                # Find the text node within this paragraph that contains the anchor
                text_node = p_tag.find(
                    string=re.compile(re.escape(anchor_text), re.IGNORECASE)
                )

                if text_node and not text_node.find_parent(
                    "a"
                ):  # Ensure it's not already linked
                    match = re.search(
                        re.escape(anchor_text), str(text_node), re.IGNORECASE
                    )
                    if match:
                        before_text = str(text_node)[: match.start()]
                        matched_text = match.group(0)
                        after_text = str(text_node)[match.end() :]

                        link_tag = soup.new_tag("a", href=url)
                        link_tag.string = matched_text

                        new_content = []
                        if before_text:
                            new_content.append(before_text)
                        new_content.append(link_tag)
                        if after_text:
                            new_content.append(after_text)

                        text_node.replace_with(*new_content)
                        linked_anchors.add(anchor_text.lower())
                        break  # Move to the next link suggestion once placed

    def _generate_toc(self, soup: BeautifulSoup) -> None:
        """Generates and inserts a Table of Contents from H2 tags into the BeautifulSoup object."""
        toc_list = soup.new_tag("ul", **{"class": "toc-list"})
        h2_tags = soup.find_all("h2")

        if len(h2_tags) < 2:
            return  # No TOC needed for less than 2 headings

        # Add unique IDs to H2 tags and build TOC
        for i, h2 in enumerate(h2_tags):
            slug = utils.slugify(h2.text)
            if not slug:  # Fallback for empty/unsluggable H2s
                slug = f"section-{i + 1}"
            h2["id"] = slug  # Add ID to H2 for linking

            toc_item = soup.new_tag("li")
            toc_link = soup.new_tag("a", href=f"#{slug}")
            toc_link.string = h2.text
            toc_item.append(toc_link)
            toc_list.append(toc_item)

        toc_header = soup.new_tag("h2")
        toc_header.string = "Table of Contents"
        toc_header["id"] = "table-of-contents"  # Give TOC its own ID

        first_h2 = soup.find("h2")
        if first_h2:
            first_h2.insert_before(toc_list)
            first_h2.insert_before(toc_header)
        else:
            # Fallback if no H2s exist, place it after the first paragraph or at the start
            first_p = soup.find("p")
            if first_p:
                first_p.insert_after(toc_header)
                first_p.insert_after(toc_list)
            else:
                soup.insert(0, toc_list)
                soup.insert(0, toc_header)

    def _generate_schema_org(
        self, soup: BeautifulSoup, opportunity: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Dynamically generates Schema.org JSON-LD from the final HTML soup.
        """
        schema_graph: List[Dict[str, Any]] = []
        client_cfg = opportunity.get("client_cfg", {})
        slug = opportunity.get("blueprint", {}).get("slug", "default-slug")

        domain = client_cfg.get("target_domain", "")
        article_url = f"https://{domain}/{slug}" if domain else slug
        publisher_name = domain or "Publisher Name"

        h1_tag = soup.find("h1")
        article_headline = (
            h1_tag.get_text(strip=True)
            if h1_tag
            else opportunity.get("keyword", "Article")
        )

        article_schema = {
            "@type": "BlogPosting",
            "@id": f"{article_url}#article",
            "mainEntityOfPage": {"@id": article_url},
            "headline": article_headline,
            "author": {
                "@type": client_cfg.get("schema_author_type", "Organization"),
                "name": client_cfg.get("default_author_name", "Author"),
            },
            "publisher": {"@type": "Organization", "name": publisher_name},
            "datePublished": datetime.now().isoformat(),
            "dateModified": datetime.now().isoformat(),
            "additionalProperties": False,
        }
        schema_graph.append(article_schema)

        # Dynamic HowTo Schema
        how_to_headings = soup.find_all(
            ["h2", "h3"], string=re.compile(r"how to", re.IGNORECASE)
        )
        for heading in how_to_headings:
            ol = heading.find_next("ol")
            if ol:
                steps = [
                    {"@type": "HowToStep", "text": li.get_text(strip=True)}
                    for li in ol.find_all("li")
                ]
                if steps:
                    schema_graph.append(
                        {
                            "@type": "HowTo",
                            "name": heading.get_text(strip=True),
                            "step": steps,
                        }
                    )

        return {"@context": "https://schema.org", "@graph": schema_graph}

    def format_final_package(
        self,
        opportunity: Dict[str, Any],
        internal_linking_suggestions: Optional[List[Dict[str, str]]] = None,
        in_article_images_data: Optional[List[Dict[str, Any]]] = None,
    ) -> Dict[str, Any]:
        """
        Constructs the final content package, now including schema generation.
        """
        # ... (existing code for html_body_str, soup creation, internal linking, ToC, etc.) ...
        ai_content = opportunity.get("ai_content", {})
        client_cfg = opportunity.get("client_cfg", {})
        html_body_str = ai_content.get("article_body_html", "")
        soup = BeautifulSoup(f"<div>{html_body_str}</div>", "html.parser")

        if internal_linking_suggestions:
            self._insert_internal_links(soup, internal_linking_suggestions)

        if client_cfg.get("generate_toc", True):
            self._generate_toc(soup)

        # ... (image replacement logic) ...

        article_html_final = (
            str(soup.body.decode_contents()) if soup.body else str(soup)
        )

        # --- NEW: Call the schema generator ---
        schema_org_json = self._generate_schema_org(soup, opportunity)

        featured_image = opportunity.get("featured_image_data", {})
        featured_image_relative_path = (
            f"/api/images/{os.path.basename(featured_image.get('local_path'))}"
            if featured_image and featured_image.get("local_path")
            else None
        )

        return {
            "meta_title": ai_content.get("meta_title", "No Title"),
            "meta_description": ai_content.get("meta_description", ""),
            "article_html_final": article_html_final,
            "schema_org_json": schema_org_json,  # Add the generated schema to the final package
            "featured_image_path": featured_image.get("local_path"),
            "featured_image_relative_path": featured_image_relative_path,
            "social_media_posts": opportunity.get("social_media_posts_json", []),
        }
</file>

<file path="backend/agents/image_generator.py">
import logging
import os
from typing import Dict, Any, Tuple, Optional, List

from backend.external_apis.pexels_client import PexelsClient, download_image_from_url
from backend.core import utils

from PIL import Image, ImageDraw, ImageFont, ImageColor


class ImageGenerator:
    """
    Agent for finding featured and in-article images from Pexels.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

        self.pexels_client = None
        if self.config.get("pexels_api_key"):
            try:
                self.pexels_client = PexelsClient(self.config["pexels_api_key"])
            except ValueError as e:
                self.logger.warning(
                    f"Pexels client could not be initialized: {e}. Image generation will be skipped."
                )

    def _add_text_overlay(self, image_path: str, text: str) -> str:
        """Adds a text overlay to the image based on configured settings."""
        if not self.config.get("overlay_text_enabled", False):
            return image_path  # If disabled, return original path

        try:
            image = Image.open(image_path).convert(
                "RGBA"
            )  # Convert to RGBA for alpha channel in overlay background
            draw = ImageDraw.Draw(image)

            text_color = ImageColor.getrgb(
                self.config.get("overlay_text_color", "#FFFFFF")
            )
            bg_color_hex = self.config.get("overlay_background_color", "#00000080")
            # Extract RGB and alpha from RGBA hex
            bg_color = ImageColor.getrgb(
                bg_color_hex
            )  # This returns (R,G,B) for #RRGGBB or (R,G,B,A) for #RRGGBBAA
            # Ensure bg_color is (R,G,B,A) if alpha is specified
            if len(bg_color) == 3 and len(bg_color_hex) == 9:  # #RRGGBBAA format
                bg_alpha = int(bg_color_hex[7:9], 16)
                bg_color = bg_color + (bg_alpha,)
            elif len(bg_color) == 3:  # default to some alpha if only RGB is given
                bg_color = bg_color + (128,)  # Default 50% opacity

            font_size = self.config.get("overlay_font_size", 40)

            try:
                # Use a reliable path to a bundled font file.
                # Assumes a `resources/fonts` directory exists relative to the project root.
                font_path = os.path.join(
                    os.path.dirname(__file__),
                    "..",
                    "..",
                    "resources",
                    "fonts",
                    "DejaVuSans-Bold.ttf",
                )
                if not os.path.exists(font_path):
                    raise IOError("Bundled font file not found.")
                font = ImageFont.truetype(font_path, font_size)
            except IOError:
                self.logger.warning(
                    f"Could not load the bundled font at {font_path}. "
                    "Falling back to default bitmap font. Text quality will be poor. "
                    "Ensure the font file exists."
                )
                font = ImageFont.load_default()

            # Use textbbox (or textsize for older PIL versions)
            # draw.textbbox is preferred
            try:
                text_bbox = draw.textbbox((0, 0), text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_height = text_bbox[3] - text_bbox[1]
            except (
                AttributeError
            ):  # Fallback for older PIL where textbbox might not exist
                text_width, text_height = draw.textsize(text, font=font)

            # Position the text based on configuration
            position = self.config.get("overlay_position", "bottom_center")
            padding = 20  # Padding around text

            x, y = 0, 0
            if "center" in position:
                x = (image.width - text_width) / 2
            elif "left" in position:
                x = padding
            elif "right" in position:
                x = image.width - text_width - padding

            if "bottom" in position:
                y = image.height - text_height - padding
            elif "top" in position:
                y = padding
            elif "center" in position:  # Vertical center
                y = (image.height - text_height) / 2

            # Create a transparent layer for the background
            overlay = Image.new("RGBA", image.size, (255, 255, 255, 0))
            draw_overlay = ImageDraw.Draw(overlay)

            # Draw semi-transparent background
            draw_overlay.rectangle(
                (
                    x - padding / 2,
                    y - padding / 2,
                    x + text_width + padding / 2,
                    y + text_height + padding / 2,
                ),
                fill=bg_color,
            )

            image = Image.alpha_composite(image, overlay)  # Composite the background
            draw = ImageDraw.Draw(image)  # Re-get draw object for updated image

            draw.text((x, y), text, font=font, fill=text_color)

            # Save the modified image
            new_image_path = image_path.replace(".jpeg", "-overlay.jpeg")
            image.convert("RGB").save(new_image_path)
            return new_image_path
        except Exception as e:
            self.logger.error(f"Failed to add text overlay to image: {e}")
            return image_path

    def generate_featured_image(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Finds and saves the featured image for the article from Pexels.
        """
        if not self.pexels_client:
            self.logger.warning(
                "Pexels client not initialized. Cannot generate featured image."
            )
            return None, 0.0

        search_query = opportunity["keyword"]
        self.logger.info(f"Searching Pexels for featured image for '{search_query}'...")

        pexels_photos, cost = self.pexels_client.search_photos(
            query=search_query, orientation="landscape", size="large", per_page=1
        )

        if not pexels_photos:
            self.logger.warning(
                f"No suitable featured image found on Pexels for '{search_query}'."
            )
            return None, cost

        best_photo = pexels_photos[0]
        best_photo_url = (
            best_photo["src"].get("landscape")
            or best_photo["src"].get("large2x")
            or best_photo["src"].get("original")
        )

        if not best_photo_url:
            self.logger.warning(
                f"Pexels photo found, but no usable URL for '{search_query}'."
            )
            return None, cost

        image_dir = "generated_images"
        os.makedirs(image_dir, exist_ok=True)
        file_path = os.path.join(
            image_dir,
            f"pexels-featured-{utils.slugify(search_query)}-{best_photo['id']}.jpeg",
        )

        local_path = download_image_from_url(best_photo_url, file_path)

        if not local_path:
            self.logger.error(
                f"Failed to download featured image from Pexels: {best_photo_url}"
            )
            return None, cost

        # Add text overlay
        meta_title = opportunity.get("ai_content", {}).get(
            "meta_title", opportunity["keyword"]
        )
        local_path = self._add_text_overlay(local_path, meta_title)

        self.logger.info(
            f"Successfully sourced featured image from Pexels: {local_path}"
        )
        return {
            "type": "featured",
            "search_query": search_query,
            "local_path": local_path,
            "remote_url": best_photo_url,  # Store Pexels URL directly
            "alt_text": best_photo["alt"],
            "source_id": best_photo["id"],
            "source": "Pexels",
        }, cost

    def _simplify_prompt_for_pexels(self, descriptive_prompt: str) -> str:
        """
        Uses an LLM to extract 3-5 high-impact keywords suitable for a stock photo search
        from a more descriptive AI image prompt.
        """
        if not descriptive_prompt or not self.openai_client:
            return descriptive_prompt  # Fallback to original if no client or prompt

        self.logger.info(
            f"Refining image prompt for Pexels search: '{descriptive_prompt}'"
        )

        prompt_messages = [
            {
                "role": "system",
                "content": "You are a concise keyword extractor for stock photo sites. Extract 3-5 key nouns, adjectives, or short phrases from the user's descriptive image prompt that would be most effective for searching a stock photo library like Pexels. Return only a comma-separated list of keywords.",
            },
            {"role": "user", "content": f"Descriptive prompt: '{descriptive_prompt}'"},
        ]

        # Use a low temperature for predictable, factual output
        extracted_keywords_str, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
                            model=self.config.get("default_model", "gpt-5-nano"),  # Use a cost-effective model for this            temperature=0.1,
            max_completion_tokens=50,  # Keep output very short
            schema={
                "name": "extract_keywords",
                "type": "object",
                "properties": {
                    "keywords": {
                        "type": "string",
                        "description": "Comma-separated keywords.",
                    }
                },
                "required": ["keywords"],
                "additionalProperties": False
            },
        )

        if error or not extracted_keywords_str:
            self.logger.warning(
                f"Failed to extract keywords for Pexels. Falling back to original prompt. Error: {error}"
            )
            return descriptive_prompt  # Fallback to original prompt

        # The AI should return a dictionary with a 'keywords' key
        if (
            isinstance(extracted_keywords_str, dict)
            and "keywords" in extracted_keywords_str
        ):
            return extracted_keywords_str["keywords"]
        elif isinstance(
            extracted_keywords_str, str
        ):  # Fallback if AI doesn't follow schema perfectly
            return extracted_keywords_str

        return descriptive_prompt  # Final fallback

    # In class ImageGenerator, replace the generate_images_from_prompts method
    def generate_images_from_prompts(
        self, prompts: List[str]
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Finds and saves in-article images from Pexels based on a list of specific prompts.
        """
        if not self.pexels_client:
            self.logger.warning(
                "Pexels client not initialized. Cannot generate images from prompts."
            )
            return [], 0.0

        images_data = []
        total_cost = 0.0

        for i, prompt in enumerate(prompts):
            search_query = self._simplify_prompt_for_pexels(prompt)
            self.logger.info(
                f"Searching Pexels for in-article image with simplified query: '{search_query}' (from prompt: '{prompt}')..."
            )

            pexels_photos, cost = self.pexels_client.search_photos(
                query=search_query, orientation="landscape", size="large", per_page=1
            )
            total_cost += cost

            if pexels_photos:
                photo = pexels_photos[0]
                photo_url = photo["src"].get("large") or photo["src"].get("original")

                if photo_url:
                    image_dir = "generated_images"
                    os.makedirs(image_dir, exist_ok=True)
                    file_path = os.path.join(
                        image_dir,
                        f"pexels-in-article-{utils.slugify(search_query)}-{photo['id']}.jpeg",
                    )
                    local_path = download_image_from_url(photo_url, file_path)

                    if local_path:
                        images_data.append(
                            {
                                "type": f"in_article_{i + 1}",
                                "search_query": search_query,
                                "original_prompt": prompt,
                                "local_path": local_path,
                                "remote_url": photo_url,
                                "alt_text": photo.get("alt") or prompt,
                                "source_id": photo["id"],
                                "source": "Pexels",
                            }
                        )
                        self.logger.info(
                            f"Successfully sourced in-article image from Pexels: {local_path}"
                        )
                        continue

            self.logger.warning(
                f"Could not find a suitable Pexels image for prompt: '{prompt}'."
            )

        return images_data, total_cost
</file>

<file path="backend/agents/internal_linking_suggester.py">
import logging
from typing import Dict, Any, List, Tuple

from backend.external_apis.openai_client import OpenAIClientWrapper
from backend.data_access.database_manager import DatabaseManager


class InternalLinkingSuggester:
    def __init__(
        self,
        openai_client: OpenAIClientWrapper,
        config: Dict[str, Any],
        db_manager: DatabaseManager,
    ):
        self.openai_client = openai_client
        self.config = config
        self.db_manager = db_manager
        self.logger = logging.getLogger(self.__class__.__name__)

    def suggest_links(
        self,
        article_text: str,
        key_entities: List[str],
        target_domain: str,
        client_id: str,
    ) -> Tuple[List[Dict[str, str]], float]:
        existing_articles = self._fetch_existing_articles(client_id)
        if not existing_articles:
            return [], 0.0

        prompt_messages = self._build_suggestion_prompt(
            article_text, key_entities, existing_articles
        )

        schema = {
            "name": "suggest_contextual_internal_links",
            "type": "object",
            "properties": {
                "internal_links": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "anchor_text": {
                                "type": "string",
                                "description": "The exact, natural phrase from the paragraph to be used as anchor text.",
                            },
                            "url": {
                                "type": "string",
                                "description": "The corresponding relative URL from the available articles list.",
                            },
                            "context_paragraph_text": {
                                "type": "string",
                                "description": "The full, exact text of the paragraph where the anchor text was found. This will be used to pinpoint the link location.",
                            },
                        },
                        "required": ["anchor_text", "url", "context_paragraph_text"],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["internal_links"],
            "additionalProperties": False,
        }

        response, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
        )

        if error or not response:
            self.logger.error(
                f"Failed to get contextual internal linking suggestions from AI: {error}"
            )
            return [], self.openai_client.latest_cost

        return response.get("internal_links", []), self.openai_client.latest_cost

    def _fetch_existing_articles(self, client_id: str) -> List[Dict[str, str]]:
        """
        Fetches a list of already 'generated' articles for the given client from the local DB.
        """
        self.logger.info(
            f"Fetching existing published articles for internal linking for client: {client_id}."
        )

        if not self.config.get("enable_automated_internal_linking", False):
            self.logger.info(
                "Automated internal linking is disabled by client configuration."
            )
            return []

        existing_articles = self.db_manager.get_published_articles_for_linking(
            client_id
        )

        if not existing_articles:
            self.logger.warning(
                f"No existing 'published' articles found for client '{client_id}' to use for internal linking suggestions."
            )
            return []

        self.logger.info(
            f"Found {len(existing_articles)} published articles for internal linking."
        )
        return existing_articles

    def _build_suggestion_prompt(
        self,
        linking_context: str,
        key_entities: List[str],
        existing_articles: List[Dict[str, str]],
    ) -> List[Dict[str, str]]:
        """Builds the prompt for context-aware AI internal linking suggestion."""

        existing_articles_text = "\n".join(
            [
                f'- Title: "{article["title"]}", Relative URL: {article["url"]}'
                for article in existing_articles
            ]
        )

        prompt = f"""
        You are an expert SEO strategist. Your task is to analyze a new blog post and identify the most semantically relevant opportunities to link to existing articles on the site.

        **Main Article Text (as HTML):**
        ```html
        {linking_context}
        ```

        **Available Published Articles to Link To:**
        {existing_articles_text}

        **Instructions:**
        1. Read the main article HTML thoroughly to understand its context and structure.
        2. For each available published article, identify the single BEST paragraph in the main article to place a link. The best location is one that is highly contextually and semantically related to the title of the published article.
        3. From that best location, extract a natural, compelling phrase of 3-7 words to use as the anchor text.
        4. Suggest a maximum of 3-5 of the most relevant internal links.
        5. Return your suggestions in the required JSON format. For 'context_paragraph_text', provide the full, clean text content of the paragraph where the link should be placed.
        """
        return [{"role": "user", "content": prompt}]
</file>

<file path="backend/agents/prompt_assembler.py">
import logging
from typing import Dict, Any, List

from backend.data_access.database_manager import DatabaseManager


class DynamicPromptAssembler:
    def __init__(self, db_manager: DatabaseManager):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.db_manager = db_manager

    def build_prompt(self, opportunity: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        Constructs the detailed prompt for the article generation AI using a safe,
        user-configurable template, now with rich SERP data.
        """
        blueprint = opportunity.get("blueprint", {})
        brief = blueprint.get("ai_content_brief", {})
        strategy = blueprint.get("recommended_strategy", {})
        client_cfg = opportunity.get("client_cfg", {})
        num_images = client_cfg.get("num_in_article_images", 2)

        template = client_cfg.get("custom_prompt_template")
        if not template or not template.strip():
            template = """Write a comprehensive, helpful, and expert-level blog post on [TOPIC]. The article must demonstrate first-hand experience and deep expertise. Structure the content for maximum readability and SEO impact. The post must:

        - Be approximately [WORD_COUNT] words, providing authoritative depth on the topic.
        - Target the primary keyword "[PRIMARY KEYWORD]" and naturally incorporate related LSI keywords: [LSI/secondary keywords], along with relevant entities, synonyms, and contextually related concepts to ensure topical completeness.
        - **Demonstrate E-E-A-T (Experience, Expertise, Authoritativeness, Trustworthiness):**
            - Start with a clear, direct 1-2 sentence summary that immediately answers the user's core question.
            - Write from a first-person or expert perspective. Include at least one hypothetical scenario, relatable anecdote, or personal insight to signal direct experience.
            - Cite specific data or statistics and attribute them to a source (e.g., 'According to a 2023 study by...').
            - Include multiple answer formats: short direct responses, step-by-step instructions, and quick takeaway lists (bullet points) so AI models and users can easily extract information.
        - Structure the article with a logical flow using clear subheadings (H2s and H3s).
        - Include a "Frequently Asked Questions" (FAQ) section at the end using real-world questions users search for, written in a conversational Q&A style.
        - Naturally promote [CTA_URL] with a relevant call-to-action at the end of the post.
        - **AVOID:** Do not use generic filler, over-optimization, or unnatural keyword stuffing. Focus on topical relevance, not keyword density. Avoid making unsubstantiated claims.
        """

        default_cta_url = client_cfg.get(
            "default_cta_url", "https://profitparrot.com/contact/"
        )
        recommended_word_count = brief.get("target_word_count", 1000)
        expert_persona_from_brief = brief.get(
            "target_audience_persona",
            client_cfg.get("expert_persona", "an expert writer"),
        )
        replacements = {
            "[TOPIC]": brief.get("target_keyword", "the topic"),
            "[PRIMARY KEYWORD]": brief.get("target_keyword", "the topic"),
            "[LSI/secondary keywords]": ", ".join(brief.get("lsi_keywords", [])),
            "[WORD_COUNT]": str(recommended_word_count),
            "[CTA_URL]": default_cta_url,
            "%%NUM_IMAGES%%": str(num_images),
            "[PERSONA]": expert_persona_from_brief,
        }

        base_instructions = template
        for placeholder, value in replacements.items():
            base_instructions = base_instructions.replace(placeholder, str(value))

        persona = brief.get("target_audience_persona", "General audience")
        if "expert" in persona.lower() or "planner" in persona.lower():
            readability_instruction = "The tone must be highly sophisticated and authoritative. Maintain a Flesch-Kincaid Grade level of 10 or higher."
        elif "general" in persona.lower() or "beginner" in persona.lower():
            readability_instruction = "The tone must be clear and accessible. Maintain a Flesch-Kincaid Grade level between 7 and 9."
        else:
            readability_instruction = ""

        if readability_instruction:
            base_instructions += (
                f"\n- **Readability Target:** {readability_instruction}"
            )

        base_instructions += f"\n- Adopt the persona of {expert_persona_from_brief}."
        base_instructions += "\n- Include 1-2 'Expert Tips' in blockquotes."

        client_knowledge_base = client_cfg.get("client_knowledge_base")
        if client_knowledge_base and client_knowledge_base.strip():
            base_instructions += "\n\n**CLIENT KNOWLEDGE BASE (CRITICAL CONTEXT):**\n"
            base_instructions += "The following information is crucial for content accuracy and brand alignment. Incorporate relevant details naturally and factually:\n"
            base_instructions += f"{client_knowledge_base}\n"
            base_instructions += "Prioritize accuracy based on this knowledge base.\n"

        base_instructions += "\n- If the content contains data suitable for a table (e.g., comparisons, specifications, statistics), format it as a Markdown table."
        base_instructions += "\n- Include one link to a non-competing, high-authority external resource to back up a key statistic or claim."
        base_instructions += "\n- For in-article images, use a placeholder with the exact format `[[IMAGE: <A descriptive prompt for the image>]]`. For example: `[[IMAGE: A bar chart showing SEO growth over time]]`."

        # --- START MODIFICATION ---
        # NEW: Add instructions based on rich SERP data
        dynamic_serp_data_instructions = []

        if brief.get("knowledge_graph_facts"):
            facts_str = "\n- ".join(brief["knowledge_graph_facts"])
            dynamic_serp_data_instructions.append(
                f"**CRITICAL:** Incorporate these verified facts from Google's Knowledge Graph directly into the article to ensure factual accuracy and boost E-A-T:\n- {facts_str}"
            )

        if brief.get("paid_ad_copy"):
            paid_titles = [ad["title"] for ad in brief["paid_ad_copy"]]
            paid_descriptions = [ad["description"] for ad in brief["paid_ad_copy"]]
            dynamic_serp_data_instructions.append(
                f"**HIGH PRIORITY:** Analyze the following top paid ad copy to understand high-conversion language, primary value propositions, and key pain points. Use these insights to craft compelling article headlines, the introduction, and calls-to-action:\n- Titles: {paid_titles}\n- Descriptions: {paid_descriptions}"
            )

        if brief.get("top_organic_sitelinks"):
            sitelinks_str = "\n- ".join(brief["top_organic_sitelinks"])
            dynamic_serp_data_instructions.append(
                f"**MANDATORY:** Include dedicated sections (H2s or H3s) covering the following high-priority subtopics identified from competitor sitelinks:\n- {sitelinks_str}"
            )

        if brief.get("top_organic_faqs"):
            faqs_str = "\n- ".join(brief["top_organic_faqs"])
            dynamic_serp_data_instructions.append(
                f"**MANDATORY:** Create a dedicated 'Frequently Asked Questions' section (as an H2) at the end of the article, using these exact questions from competitor FAQ snippets:\n- {faqs_str}"
            )

        if brief.get("ai_overview_sources"):
            sources_str = "\n- ".join(brief["ai_overview_sources"])
            dynamic_serp_data_instructions.append(
                f"**STRATEGIC:** Give analytical priority to concepts and insights derived from these authoritative sources used by Google's own AI Overview:\n- {sources_str}"
            )

        if brief.get("discussion_snippets"):
            snippets_str = "\n- ".join(brief["discussion_snippets"])
            dynamic_serp_data_instructions.append(
                f"**TONE & EXPERIENCE:** Analyze the tone, specific pain points, and real-world language from these discussion snippets. Infuse the article with a personal, experience-driven, and authentic voice to directly address user concerns:\n- {snippets_str}"
            )

        # Append to dynamic instructions list
        dynamic_instructions = [
            f"**Primary Content Format:** Your output should be a '{strategy.get('content_format', 'Comprehensive Article')}'.",
            f"**Strategic Goal:** {strategy.get('strategic_goal', '')}",
        ]
        if brief.get("dynamic_serp_instructions"):
            dynamic_instructions.append(
                "**Tactical Guidance:**\n"
                + "\n".join(
                    [f"- {inst}" for inst in brief["dynamic_serp_instructions"]]
                )
            )

        # Combine all dynamic instructions
        all_dynamic_instructions = dynamic_instructions + dynamic_serp_data_instructions
        dynamic_instructions_str = "\n- ".join(all_dynamic_instructions)

        final_prompt_content = f"""You are an expert SEO writer. Generate a complete blog post package in JSON format based on the brief below.

        **Topic:** "{brief.get("target_keyword")}"
        **Core Instructions:**
        {base_instructions}
        **Dynamic Strategic Instructions:**
        - {dynamic_instructions_str}
        **Mandatory Information & Structure:**
        - **WORD COUNT: The final article body MUST be AT LEAST {recommended_word_count} words.** This is a strict requirement.
        - To meet the word count, elaborate on each of the following sections, providing detailed explanations, examples, and insights.
        - Must include sections on: {", ".join(blueprint.get("content_intelligence", {}).get("common_headings_to_cover", ["N/A"]))}
        - Must explore these unique angles: {", ".join(brief.get("unique_angles_to_cover", ["N/A"]))}
        - Mention these key entities: {", ".join(brief.get("key_entities_to_mention", ["N/A"]))}

        Generate a single, valid JSON object with three keys: "article_body_html", "meta_title", and "meta_description". The "article_body_html" must be well-structured HTML."""
        # --- END MODIFICATION ---

        feedback_examples_text = ""
        feedback_data = self.db_manager.get_content_feedback_examples(
            opportunity.get("client_id")
        )
        if feedback_data.get("good_examples") or feedback_data.get("bad_examples"):
            feedback_examples_text = "\n\n**Style Guide based on Past Feedback:**\n"
            if feedback_data.get("good_examples"):
                feedback_examples_text += (
                    "- **DO:** Emulate the style of these highly-rated articles:\n"
                )
                for ex in feedback_data["good_examples"]:
                    feedback_examples_text += f"  - Title: '{ex['keyword']}'. User Feedback: '{ex['comments']}' (Rated {ex['rating']}/5)\n"
            if feedback_data.get("bad_examples"):
                feedback_examples_text += "- **AVOID:** Avoid the issues found in these poorly-rated articles:\n"
                for ex in feedback_data["bad_examples"]:
                    feedback_examples_text += f"  - Title: '{ex['keyword']}'. User Feedback: '{ex['comments']}' (Rated {ex['rating']}/5)\n"

        system_message = "You are an expert SEO content strategist. Your output must be a single, valid JSON object."
        final_prompt_content = final_prompt_content + feedback_examples_text

        return [
            {"role": "system", "content": system_message},
            {"role": "user", "content": final_prompt_content},
        ]

    def flatten_prompt_for_display(self, messages: List[Dict[str, str]]) -> str:
        """Flattens the structured prompt messages into a single string for UI preview."""
        # ... (existing method, no change needed) ...
</file>

<file path="backend/agents/social_media_crafter.py">
# agents/social_media_crafter.py
import logging
from typing import Dict, Any, Tuple, Optional, List

from bs4 import BeautifulSoup

from backend.external_apis.openai_client import OpenAIClientWrapper


class SocialMediaCrafter:
    """
    AI agent for crafting social media posts based on the generated article.
    """

    def __init__(self, openai_client: OpenAIClientWrapper, config: Dict[str, Any]):
        self.openai_client = openai_client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def craft_posts(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[Optional[List[Dict[str, Any]]], float]:
        """
        Generates social media blurbs for different platforms.
        """
        prompt_messages = self._build_crafting_prompt(opportunity)

        schema = {
            "type": "object",
            "properties": {
                "social_media_posts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "platform": {
                                "type": "string",
                                "description": "The social media platform (e.g., 'Twitter', 'LinkedIn').",
                            },
                            "content": {
                                "type": "string",
                                "description": "The content of the social media post.",
                            },
                        },
                        "required": ["platform", "content"],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["social_media_posts"],
            "additionalProperties": False,
        }

        response, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
        )

        # Get the actual cost from the client after the API call
        cost = self.openai_client.latest_cost

        if error or not response:
            self.logger.error(f"Failed to craft social media posts: {error}")
            return None, cost  # Return the actual cost even on failure

        return response.get("social_media_posts"), cost

    def _build_crafting_prompt(
        self, opportunity: Dict[str, Any]
    ) -> list[Dict[str, str]]:
        """Constructs the prompt for the social media crafting AI."""
        ai_content = opportunity.get("ai_content", {})
        article_title = ai_content.get("meta_title", "Untitled")
        article_summary = ai_content.get("meta_description", "No summary available.")
        article_body_html = ai_content.get(
            "article_body_html", ""
        )  # NEW: Get full HTML for parsing

        client_cfg = opportunity.get("client_cfg", {})
        platforms = ", ".join(client_cfg.get("platforms", ["Twitter", "LinkedIn"]))

        # --- Extract key information from the generated article's HTML ---
        soup = BeautifulSoup(article_body_html, "html.parser")

        h1_tag = soup.find("h1")
        # W17 FIX: Ensure h1 extraction is guarded, falling back to a modified title if necessary
        h1_text = (
            h1_tag.get_text(strip=True)
            if h1_tag
            else f"The Article on {article_title.replace(':', ' - ')}"
        )

        # W17 FIX: Extract H2s robustly, filtering out any empty tags
        h2_texts = [
            h.get_text(strip=True)
            for h in soup.find_all("h2")
            if h.get_text(strip=True)
        ]

        key_entities = (
            opportunity.get("blueprint", {})
            .get("ai_content_brief", {})
            .get("key_entities_to_mention", [])
        )

        # --- Social Media Tag Analysis (existing logic) ---
        competitor_social_tags = []
        for competitor in opportunity.get("blueprint", {}).get(
            "competitor_analysis", []
        )[:3]:
            if competitor.get("social_media_tags"):
                tags = competitor["social_media_tags"]
                key_tags = {
                    "og:title": tags.get("og:title"),
                    "og:description": tags.get("og:description"),
                    "twitter:title": tags.get("twitter:title"),
                    "twitter:description": tags.get("twitter:description"),
                }
                competitor_social_tags.append(
                    {
                        "url": competitor["url"],
                        "tags": {k: v for k, v in key_tags.items() if v},
                    }
                )

        competitor_examples = ""
        if competitor_social_tags:
            competitor_examples = (
                "\n**Competitor Social Media Examples (for inspiration):**\n"
            )
            for item in competitor_social_tags:
                competitor_examples += f"- For {item['url']}:\n"
                for tag, value in item["tags"].items():
                    competitor_examples += f"  - {tag}: {value}\n"

        prompt = f"""
        You are a social media marketing expert. Your task is to create engaging social media posts to promote a new blog article.

        **Article Details:**
        - **Primary Headline (H1):** {h1_text} # Now guaranteed to be populated
        - **Key Sections (H2s):** {", ".join(h2_texts[:5]) if h2_texts else "N/A (No subheadings found)"} # Now guaranteed text if available
        - **Summary:** {article_summary}
        - **Key Entities/Topics:** {", ".join(key_entities) if key_entities else "N/A"}
        - **Link (use placeholder):** [LINK]
        {competitor_examples}
        **Instructions:**
        1.  Create a unique post for each of the following platforms: {platforms}.
        2.  Analyze the competitor examples to understand how they frame their content on social media.
        3.  Tailor the tone and length of each post to the specific platform.
        4.  Include relevant hashtags.
        5.  End each post with a call to action and the [LINK] placeholder.
        6.  Ensure posts directly reference information present in the article's headlines and key entities.

        Provide the output in the required JSON format.
        """
        return [{"role": "user", "content": prompt}]
</file>

<file path="backend/agents/summary_generator.py">
import logging
from typing import Dict, Any


class SummaryGenerator:
    """
    Generates a human-readable strategic summary of a keyword opportunity.
    """

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def generate_summary(self, opportunity: Dict[str, Any]) -> str:
        """Builds a narrative summary based on the opportunity's data."""
        full_data = opportunity.get("full_data", {})
        score_breakdown = full_data.get("score_breakdown", {})

        # Check qualification status first
        quality_status = full_data.get("quality_status", "passed")
        cannibal_status = full_data.get("cannibalization_status", "passed")
        if quality_status == "failed" or cannibal_status == "failed":
            reasons = ", ".join(full_data.get("reasons", ["Unknown reason"]))
            return f"**Disqualified:** This keyword was filtered out. Reason(s): {reasons}."

        # Build summary for qualified keywords using the new breakdown
        intent = full_data.get("search_intent_info", {}).get("main_intent", "unknown")
        summary_parts = [f"This is a promising **{intent.upper()}** opportunity."]

        ease_of_ranking_score = score_breakdown.get("ease_of_ranking", {}).get(
            "score", 0
        )
        if ease_of_ranking_score < 60:
            summary_parts.append(
                "However, the SERP shows some competitive challenges that will require a strong article."
            )
        elif ease_of_ranking_score < 85:
            summary_parts.append(
                "The competitive landscape appears favorable, with weaknesses to exploit."
            )
        else:  # 85+
            summary_parts.append(
                "The competitive landscape appears **extremely favorable**, with clear technical and content weaknesses among top competitors."
            )

        traffic_score = score_breakdown.get("traffic_potential", {}).get("score", 0)
        if traffic_score > 70:
            summary_parts.append("It has **excellent traffic potential**.")
        elif traffic_score > 40:
            summary_parts.append("It has solid traffic potential.")

        commercial_score = score_breakdown.get("commercial_intent", {}).get("score", 0)
        if commercial_score > 85:
            summary_parts.append(
                "The keyword shows **strong commercial value**, making it a high-priority target."
            )
        elif commercial_score > 60:
            summary_parts.append("It has good commercial value.")

        return " ".join(summary_parts)

    def generate_score_narrative(self, score_breakdown: Dict[str, Any]) -> str:
        narrative_parts = []
        if not score_breakdown:
            return "No score breakdown available to generate a narrative."

        # Ease of Ranking
        ease_score = score_breakdown.get("ease_of_ranking", {}).get("score", 0)
        if ease_score >= 80:
            narrative_parts.append(
                "Ranks highly due to a **very weak competitive landscape**."
            )
        elif ease_score >= 60:
            narrative_parts.append(
                "Has a good chance to rank because of a **favorable competitive landscape**."
            )
        else:
            narrative_parts.append(
                "Faces **strong competition**, making it a challenging keyword to rank for."
            )

        # Traffic Potential
        traffic_score = score_breakdown.get("traffic_potential", {}).get("score", 0)
        if traffic_score >= 75:
            narrative_parts.append("It has **excellent traffic potential**.")
        elif traffic_score >= 50:
            narrative_parts.append("It has **solid traffic potential**.")
        else:
            narrative_parts.append("Its traffic potential is moderate.")

        # Commercial Intent
        commercial_score = score_breakdown.get("commercial_intent", {}).get("score", 0)
        if commercial_score >= 80:
            narrative_parts.append("The keyword shows **strong commercial value**.")
        elif commercial_score >= 50:
            narrative_parts.append("It has **good commercial value**.")

        return "Overall: " + " ".join(narrative_parts)
</file>

<file path="backend/api/routers/auth.py">
from fastapi import APIRouter, HTTPException
from ..models import LoginRequest

router = APIRouter()


@router.post("/auth/login")
async def login(request: LoginRequest):
    """
    Dummy login endpoint for development.
    In a real app, verify a hashed password against a user database.
    """
    DUMMY_PASSWORD = "password123"
    if request.password == DUMMY_PASSWORD:
        dummy_user = {"username": "admin", "email": "admin@example.com"}
        dummy_token = "dummy-secret-token"
        return {"user": dummy_user, "token": dummy_token}
    else:
        raise HTTPException(status_code=401, detail="Incorrect password")


@router.post("/auth/logout")
async def logout():
    """Dummy logout endpoint."""
    return {"message": "Logged out successfully"}
</file>

<file path="backend/api/routers/client_settings.py">
# api/routers/client_settings.py
# NEW FILE
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from ..models import ClientSettings  # Assuming a Pydantic model exists
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()


@router.get("/settings/{client_id}", response_model=ClientSettings)
async def get_client_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    settings = db.get_client_settings(client_id)
    if not settings:
        raise HTTPException(
            status_code=404, detail="Settings not found for this client."
        )
    return settings


@router.put("/settings/{client_id}", response_model=Dict[str, str])
async def update_client_settings_endpoint(
    client_id: str,
    settings: ClientSettings,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        db.update_client_settings(client_id, settings.dict())
        return {"message": "Settings updated successfully."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
</file>

<file path="backend/api/routers/clients.py">
import logging
from pydantic import BaseModel
from typing import List
from fastapi import APIRouter, Depends, HTTPException
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from .. import globals as api_globals
from backend.pipeline import WorkflowOrchestrator


class NewClientRequest(BaseModel):
    client_id: str
    client_name: str


router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/clients")
async def get_all_clients(db: DatabaseManager = Depends(get_db)):
    logger.info("Received request for /clients")
    clients = db.get_clients()
    logger.info(f"Found clients: {clients}")
    if not clients:
        return []
    return clients


@router.get("/clients/{client_id}/settings")
async def get_client_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    settings = db.get_client_settings(client_id)
    if not settings:
        raise HTTPException(
            status_code=404, detail=f"Settings not found for client '{client_id}'"
        )
    return settings


@router.get("/clients/{client_id}/dashboard-stats")
async def get_dashboard_stats_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    stats = db.get_dashboard_stats(client_id)
    if not stats:
        raise HTTPException(
            status_code=404, detail=f"Stats not found for client '{client_id}'"
        )
    return stats


@router.get("/clients/{client_id}/dashboard")
async def get_dashboard_data_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to fetch aggregated data for the main dashboard."""
    logger.info(f"Dashboard endpoint called for client: {client_id}")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        logger.info("Fetching dashboard data from database...")
        dashboard_data = db.get_dashboard_data(client_id)
        logger.info("Successfully fetched dashboard data.")
        if not dashboard_data:
            logger.warning(f"No dashboard data found for client {client_id}")
            raise HTTPException(
                status_code=404, detail=f"Dashboard data not found for client '{client_id}'"
            )
        logger.info("Returning dashboard data.")
        return dashboard_data
    except Exception as e:
        logger.error(f"Error fetching dashboard data: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal Server Error")


@router.get("/clients/{client_id}/processed-keywords")
async def get_processed_keywords_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Retrieves all processed keywords for a client to prevent duplicates."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    keywords = db.get_all_processed_keywords_for_client(client_id)
    return keywords


@router.post("/clients/{client_id}/check-keywords")
async def check_existing_keywords_endpoint(
    client_id: str,
    keywords: List[str],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Checks a batch of keywords and returns which ones already exist."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    existing = db.check_existing_keywords(client_id, keywords)
    return {"existing_keywords": existing}


# ADD the new endpoint to the router:
@router.get("/clients/{client_id}/search-all-assets")
async def search_all_assets_endpoint(
    client_id: str,
    query: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    logger.info(
        f"Received search-all-assets request for client {client_id} with query: '{query}'"
    )
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    if len(query) < 3:
        return []

    results = []

    # Search Opportunities
    opportunities = db.search_opportunities(client_id, query)
    for opp in opportunities:
        results.append({"id": opp["id"], "name": opp["keyword"], "type": "opportunity"})

    # Search Discovery Runs (by seed keywords or run status/error)
    discovery_runs = db.search_discovery_runs(client_id, query)
    for run in discovery_runs:
        seed_keywords_str = ", ".join(
            run.get("parameters", {}).get("seed_keywords", [])
        )
        results.append(
            {
                "id": run["id"],
                "name": f"Discovery Run #{run['id']}: {seed_keywords_str[:50]}...",
                "type": "discovery_run",
            }
        )

    # Deduplicate results by type-id if necessary (optional, but good practice)
    unique_results = {}
    for item in results:
        key = f"{item['type']}-{item['id']}"
        if key not in unique_results:
            unique_results[key] = item

    return list(unique_results.values())


@router.get("/clients/{client_id}/opportunities/high-priority")
async def get_high_priority_opportunities_endpoint(
    client_id: str,
    limit: int = 5,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Retrieves a short list of the highest-scored, validated opportunities."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    opportunities = db.get_high_priority_opportunities(client_id, limit)
    return opportunities


@router.post("/clients")
async def add_new_client(
    request: NewClientRequest, db: DatabaseManager = Depends(get_db)
):
    """Adds a new client to the database and initializes their settings."""
    try:
        # Ensure the config manager is available
        if not api_globals.config_manager:
            raise HTTPException(
                status_code=500, detail="Configuration manager not initialized."
            )

        default_settings = (
            api_globals.config_manager.get_default_client_settings_template()
        )

        success = db.add_client(
            client_id=request.client_id,
            client_name=request.client_name,
            default_settings=default_settings,
        )

        if success:
            return {"message": "Client added successfully."}
        else:
            # This could happen if the client_id already exists (IntegrityError)
            raise HTTPException(
                status_code=409,
                detail=f"Client with ID '{request.client_id}' already exists.",
            )
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(
            f"Error adding new client '{request.client_name}': {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail="Failed to add new client.")
</file>

<file path="backend/api/routers/discovery.py">
import logging
from fastapi import APIRouter, Depends, HTTPException
from data_access.database_manager import DatabaseManager
from backend.pipeline import WorkflowOrchestrator
from services.discovery_service import DiscoveryService
from ..dependencies import get_db, get_orchestrator, get_discovery_service
from ..models import (
    JobResponse,
    DiscoveryRunRequest,
)  # Ensure DiscoveryRunRequest is imported

# --- NEW IMPORTS AND MODELS FOR FRONTEND FEATURES ---
from typing import Dict


# --- END NEW IMPORTS AND MODELS ---

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/discovery/available-filters")
async def get_available_filters():
    """
    Returns a curated list of available discovery modes, filters, and sorting options,
    structured to be easily consumable by the frontend.
    """
    base_filters = [
        {
            "name": "search_volume",
            "label": "Search Volume",
            "type": "number",
            "operators": [">", "<", "="],
        },
        {
            "name": "keyword_difficulty",
            "label": "Keyword Difficulty",
            "type": "number",
            "operators": [">", "<", "="],
        },
        {
            "name": "main_intent",
            "label": "Search Intent",
            "type": "select",
            "options": ["informational", "navigational", "commercial", "transactional"],
            "operators": ["="],
        },
        {
            "name": "competition_level",
            "label": "Competition Level",
            "type": "select",
            "options": ["LOW", "MEDIUM", "HIGH"],
            "operators": ["="],
        },
        {"name": "cpc", "label": "CPC", "type": "number", "operators": [">", "<", "="]},
    ]

    base_sorting = [
        {"name": "search_volume", "label": "Search Volume"},
        {"name": "keyword_difficulty", "label": "Keyword Difficulty"},
        {"name": "cpc", "label": "CPC"},
        {"name": "competition", "label": "Competition"},
    ]

    def construct_paths(prefix, items):
        new_items = []
        for item in items:
            new_item = item.copy()
            if "search_volume" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.search_volume"
            elif "keyword_difficulty" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_properties.keyword_difficulty"
            elif "main_intent" in new_item["name"]:
                new_item["name"] = f"{prefix}search_intent_info.main_intent"
            elif "competition_level" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.competition_level"
            elif "cpc" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.cpc"
            elif "competition" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.competition"
            new_items.append(new_item)
        return new_items

    return [
        {
            "id": "keyword_ideas",
            "name": "Broad Market Exploration",
            "description": "Discover a wide range of foundational keywords related to your core topics. Ideal for initial research and uncovering new content pillars.",
            "filters": construct_paths("", base_filters),
            "sorting": [{"name": "relevance", "label": "Relevance"}]
            + construct_paths("", base_sorting),
        },
        {
            "id": "keyword_suggestions",
            "name": "Targeted Query Expansion",
            "description": "Generate specific, long-tail variations of your seed keywords. Perfect for finding niche opportunities and targeted article ideas.",
            "filters": construct_paths("", base_filters),
            "sorting": construct_paths("", base_sorting),
        },
        {
            "id": "related_keywords",
            "name": "Semantic & Competitor Analysis",
            "description": "Find semantically related terms and phrases that your competitors may be ranking for. Excellent for expanding content depth and authority.",
            "filters": construct_paths("keyword_data.", base_filters),
            "sorting": construct_paths("keyword_data.", base_sorting),
        },
        {
            "id": "find_questions",
            "name": "Find Questions",
            "description": "Discover question-based keywords (e.g., 'how to...', 'what is...') related to your core topics.",
            "filters": construct_paths("", base_filters),
            "sorting": construct_paths("", base_sorting),
        },
    ]


@router.post("/clients/{client_id}/discovery-runs-async", response_model=JobResponse)
async def start_discovery_run_async(
    client_id: str,
    request: DiscoveryRunRequest,
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
    discovery_service: DiscoveryService = Depends(get_discovery_service),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        filters = request.filters
        limit = request.limit or 1000
        discovery_modes = ["keyword_ideas", "keyword_suggestions", "related_keywords"]

        if limit <= 500:
            depth = 2
        elif limit <= 2000:
            depth = 3
        else:
            depth = 4

        parameters = {
            "seed_keywords": request.seed_keywords,
            "discovery_modes": discovery_modes,
            "filters": filters,
            "order_by": request.order_by,
            "filters_override": request.filters_override,
            "limit": limit,
            "depth": depth,
            "include_clickstream_data": request.include_clickstream_data,  # NEW
            "closely_variants": request.closely_variants,  # NEW
            "ignore_synonyms": request.ignore_synonyms,  # NEW
        }
        run_id = discovery_service.create_discovery_run(
            client_id=client_id, parameters=parameters
        )

        job_id = orchestrator.run_discovery_and_save(
            run_id,
            request.seed_keywords,
            discovery_modes,
            filters,
            request.order_by,
            request.filters_override,
            limit,
            depth,
            request.ignore_synonyms,
            request.include_clickstream_data,  # NEW
            request.closely_variants,  # NEW
        )
        return {"job_id": job_id, "message": f"Discovery run job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start discovery run for client {client_id}: {e}", exc_info=True
        )
        raise HTTPException(
            status_code=500, detail=f"Failed to start discovery run: {e}"
        )


@router.get("/clients/{client_id}/discovery-runs")
async def get_discovery_runs(
    client_id: str,
    page: int = 1,
    limit: int = 10,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    runs, total_count = db.get_all_discovery_runs_paginated(client_id, page, limit)
    if not runs:
        return {"items": [], "total_items": 0, "page": page, "limit": limit}
    return {"items": runs, "total_items": total_count, "page": page, "limit": limit}


# def calculate_discovery_cost(request: KeywordListRequest) -> Dict[str, Any]:

# # ... (existing setup) ...

#     # ... lines 141-143 unchanged

#     # item_cost = num_items * cost_per_item

#     # estimated_cost = task_cost + item_cost


#     # explanation = [

#     #     f"{num_tasks} tasks @ ${cost_per_task:.4f} each: ${task_cost:.4f}",

#     #     f"{num_items} items @ ${cost_per_item:.4f} each: ${item_cost:.4f}"

#     # ]

#     pass

# if request.include_clickstream_data:

#     estimated_cost *= 2

#     explanation.append("Cost multiplied by 2x due to 'include_clickstream_data' flag.")

# return {"estimated_cost": round(estimated_cost, 2), "explanation": explanation}


@router.post("/discovery-runs/rerun/{run_id}")
async def rerun_discovery_run(
    run_id: int,
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
    db: DatabaseManager = Depends(get_db),  # current_client_id dependency removed here
):
    """
    Initiates a new discovery run using the parameters from a previous run.
    """
    previous_run = db.get_discovery_run_by_id(run_id)
    if not previous_run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")

    # Authorization check within the function (using orchestrator's client_id)
    if (
        previous_run["client_id"] != orchestrator.client_id
    ):  # Use orchestrator's client_id
        raise HTTPException(
            status_code=403, detail="You do not have permission to re-run this job."
        )

    try:
        parameters = previous_run.get("parameters", {})
        seed_keywords = parameters.get("seed_keywords", [])
        filters = parameters.get("filters")
        order_by = parameters.get("order_by")
        filters_override = parameters.get("filters_override", {})
        limit = parameters.get("limit")
        depth = parameters.get("depth")

        if not seed_keywords:
            raise HTTPException(
                status_code=400, detail="No seed keywords found in the original run."
            )

        # Dynamic discovery logic based on limit
        limit = limit or 1000
        discovery_modes = ["keyword_ideas", "keyword_suggestions", "related_keywords"]

        if depth is None:
            if limit <= 500:
                depth = 2
            elif limit <= 2000:
                depth = 3
            else:
                depth = 4

        # Reconstruct parameters for the new run to be created
        new_run_parameters = {
            "seed_keywords": seed_keywords,
            "discovery_modes": discovery_modes,
            "filters": filters,
            "order_by": order_by,
            "filters_override": filters_override,
            "limit": limit,
            "depth": depth,
        }

        new_run_id = orchestrator.db_manager.create_discovery_run(
            client_id=previous_run["client_id"], parameters=new_run_parameters
        )
        job_id = orchestrator.run_discovery_and_save(
            new_run_id,
            seed_keywords,
            discovery_modes,
            filters,
            order_by,
            filters_override,
            limit,
            depth,
        )

        return {
            "job_id": job_id,
            "message": f"Re-run of job {run_id} started as new job {job_id}.",
        }
    except Exception as e:
        logger.error(f"Failed to re-run discovery run {run_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to start re-run: {e}")


@router.get("/discovery-runs/{run_id}/keywords")
async def get_run_keywords(
    run_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all keywords that were added to the database as part of a specific discovery run.
    """
    run = db.get_discovery_run_by_id(run_id)
    if not run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")
    if (
        run["client_id"] != orchestrator.client_id
    ):  # Use orchestrator's client_id for auth
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access these keywords.",
        )

    try:
        keywords = db.get_keywords_for_run(run_id)
        return keywords
    except Exception as e:
        logger.error(
            f"Failed to retrieve keywords for run {run_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=f"Failed to retrieve keywords: {e}")


@router.get("/discovery-runs/{run_id}/keywords/{reason}")
async def get_run_keywords_by_reason(
    run_id: int,
    reason: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all keywords that were added to the database as part of a specific discovery run
    that were disqualified for a specific reason.
    """
    run = db.get_discovery_run_by_id(run_id)
    if not run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")
    if (
        run["client_id"] != orchestrator.client_id
    ):  # Use orchestrator's client_id for auth
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access these keywords.",
        )

    try:
        keywords = db.get_keywords_for_run_by_reason(run_id, reason)
        return keywords
    except Exception as e:
        logger.error(
            f"Failed to retrieve keywords for run {run_id} and reason {reason}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=f"Failed to retrieve keywords: {e}")


@router.get(
    "/discovery-runs/{run_id}/disqualification-reasons", response_model=Dict[str, int]
)
async def get_disqualification_reasons_endpoint(
    run_id: int, discovery_service: DiscoveryService = Depends(get_discovery_service)
):
    """
    Retrieves a summary of disqualification reasons for a specific discovery run.
    """
    logger.info(f"Received request for disqualification reasons for run {run_id}")
    reasons = discovery_service.get_disqualification_reasons(run_id)
    return reasons
</file>

<file path="backend/api/routers/jobs.py">
# api/routers/jobs.py

import logging
from fastapi import APIRouter, Depends, HTTPException
from jobs import JobManager
from ..dependencies import get_job_manager
from ..models import JobResponse

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/jobs/{job_id}", response_model=JobResponse)
async def get_job_status(
    job_id: str, job_manager: JobManager = Depends(get_job_manager)
):
    """
    Retrieves the status of a background job.
    """
    logger.info(f"Received request for job status for job_id: {job_id}")
    job = job_manager.get_job_status(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    return {
        "job_id": job["id"],
        "message": f"Status: {job['status']}",
        "status": job["status"],
        "progress": job["progress"],
        "result": job.get("result"),
        "error": job.get("error"),
        "progress_log": job.get("progress_log"),
    }
</file>

<file path="backend/api/routers/opportunities.py">
import logging
import bleach
import json
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException
from data_access.database_manager import DatabaseManager
from fastapi.concurrency import run_in_threadpool
from services.opportunities_service import OpportunitiesService
from ..dependencies import get_db, get_opportunities_service, get_orchestrator
from ..models import (
    OpportunityListResponse,
    ContentHistoryItem,
    RestoreRequest,
    SocialMediaPostsUpdate,
    ContentUpdatePayload,
)
from pydantic import BaseModel
from .. import globals as api_globals
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/settings/discovery-strategies", response_model=List[str])
async def get_discovery_strategies():
    """Returns the available discovery strategies from the global config."""
    strategies = api_globals.config_manager.get_global_config().get(
        "discovery_strategies", []
    )
    return strategies


class ContentFeedbackRequest(BaseModel):
    rating: int
    comments: Optional[str] = None


@router.get(
    "/clients/{client_id}/opportunities/summary", response_model=OpportunityListResponse
)
async def get_all_opportunities_summary_endpoint(
    client_id: str,
    status: Optional[str] = None,
    keyword: Optional[str] = None,
    page: int = 1,
    limit: int = 20,
    sort_by: str = "date_added",
    sort_direction: str = "desc",
    opportunities_service: OpportunitiesService = Depends(get_opportunities_service),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint for fetching a paginated summary of opportunities for the main table view."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    params = {
        "status": status,
        "keyword": keyword,
        "page": page,
        "limit": limit,
        "sort_by": sort_by,
        "sort_direction": sort_direction,
    }
    opportunities, total_count = await run_in_threadpool(
        opportunities_service.get_all_opportunities_summary,
        client_id,
        params,
        select_columns="id, keyword, status, date_added, strategic_score, cpc, competition, main_intent, blog_qualification_status, blog_qualification_reason, latest_job_id, cluster_name, full_data",
    )
    return {
        "items": opportunities,
        "total_items": total_count,
        "page": page,
        "limit": limit,
    }


@router.get(
    "/clients/{client_id}/opportunities/by-cluster",
    response_model=Dict[str, List[Dict[str, Any]]],
)
async def get_opportunities_by_cluster_endpoint(
    client_id: str,
    opportunities_service: OpportunitiesService = Depends(get_opportunities_service),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all opportunities for a client, grouped by cluster.
    """
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    opportunities_by_cluster = await run_in_threadpool(
        opportunities_service.get_opportunities_by_cluster, client_id
    )
    return opportunities_by_cluster


@router.put("/opportunities/{opportunity_id}/status", response_model=Dict[str, str])
async def update_opportunity_status_endpoint(
    opportunity_id: int, status: str, db: DatabaseManager = Depends(get_db)
):
    """
    Manually updates the status of an opportunity.
    """
    logger.info(
        f"Received request to update status for opportunity {opportunity_id} to {status}"
    )
    db.update_opportunity_status(opportunity_id, status)
    return {"message": "Opportunity status updated successfully."}


@router.post("/opportunities/bulk-action", response_model=Dict[str, str])
async def bulk_action_endpoint(
    action: str, opportunity_ids: List[int], db: DatabaseManager = Depends(get_db)
):
    """
    Performs a bulk action on a list of opportunities.
    """
    logger.info(
        f"Received request to perform bulk action '{action}' on {len(opportunity_ids)} opportunities"
    )
    for opportunity_id in opportunity_ids:
        if action == "reject":
            db.update_opportunity_status(opportunity_id, "rejected")
        elif action == "approve":
            db.update_opportunity_status(opportunity_id, "qualified")

    return {"message": "Bulk action completed successfully."}


@router.post("/opportunities/compare", response_model=List[Dict[str, Any]])
async def compare_opportunities_endpoint(
    opportunity_ids: List[int], db: DatabaseManager = Depends(get_db)
):
    """
    Retrieves a list of opportunities for comparison.
    """
    logger.info(f"Received request to compare {len(opportunity_ids)} opportunities")
    opportunities = []
    for opportunity_id in opportunity_ids:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if opportunity:
            opportunities.append(opportunity)

    return opportunities


@router.get(
    "/clients/{client_id}/opportunities/search", response_model=List[Dict[str, Any]]
)
async def search_opportunities_endpoint(
    client_id: str,
    query: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    logger.info(f"Received search request for client {client_id} with query: '{query}'")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    if len(query) < 3:
        return []

    opportunities = db.search_opportunities(client_id, query)
    return opportunities


@router.get("/opportunities/{opportunity_id}", response_model=Dict[str, Any])
async def get_opportunity_by_id_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),  # Add this
):
    logger.info(f"Received request for opportunity {opportunity_id}")
    opportunity = db.get_opportunity_by_id(opportunity_id)
    if not opportunity:
        raise HTTPException(status_code=404, detail="Opportunity not found")
    # Add authorization check
    if opportunity["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this opportunity.",
        )

    # W23 FIX: Manually parse the blueprint from full_data if it exists
    if opportunity.get("full_data") and isinstance(opportunity["full_data"], str):
        try:
            full_data_json = json.loads(opportunity["full_data"])
            if "blueprint" in full_data_json:
                opportunity["blueprint"] = full_data_json["blueprint"]
            if "serp_overview" in full_data_json:
                opportunity["serp_overview"] = full_data_json["serp_overview"]
        except json.JSONDecodeError:
            logger.warning(
                f"Could not decode full_data JSON for opportunity {opportunity_id}."
            )

    logger.info(f"Retrieved opportunity from DB: {opportunity}")
    return opportunity


@router.get(
    "/opportunities/{opportunity_id}/content-history",
    response_model=List[ContentHistoryItem],
)
async def get_content_history_endpoint(
    opportunity_id: int, db: DatabaseManager = Depends(get_db)
):
    logger.info(f"Fetching content history for opportunity {opportunity_id}")
    history = db.get_content_history(opportunity_id)
    if not history:
        return []
    return history


@router.post(
    "/opportunities/{opportunity_id}/restore-content", response_model=Dict[str, Any]
)
async def restore_content_version_endpoint(
    opportunity_id: int, request: RestoreRequest, db: DatabaseManager = Depends(get_db)
):
    logger.info(
        f"Restoring content version from {request.version_timestamp} for opportunity {opportunity_id}"
    )
    try:
        restored_content = db.restore_content_version(
            opportunity_id, request.version_timestamp
        )
        if restored_content:
            return {
                "message": "Content version restored successfully.",
                "restored_content": restored_content,
            }
        else:
            raise HTTPException(
                status_code=404, detail="Failed to restore content version."
            )
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(
            f"Error restoring content for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500,
            detail="An internal error occurred during content restoration.",
        )


@router.put(
    "/opportunities/{opportunity_id}/social-media-posts", response_model=Dict[str, str]
)
async def update_social_media_posts_endpoint(
    opportunity_id: int,
    payload: SocialMediaPostsUpdate,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),  # Add this
):
    logger.info(f"Updating social media posts for opportunity {opportunity_id}")
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        db.update_opportunity_social_posts(opportunity_id, payload.social_media_posts)
        return {"message": "Social media posts updated successfully."}
    except Exception as e:
        logger.error(
            f"Error updating social media posts for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Failed to update social media posts."
        )


@router.put("/opportunities/{opportunity_id}/content", response_model=Dict[str, str])
async def update_opportunity_content_endpoint(
    opportunity_id: int,
    payload: ContentUpdatePayload,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),  # Add this
):
    """Updates the main HTML content of an opportunity's ai_content blob with server-side sanitization."""
    logger.info(f"Received manual content update for opportunity {opportunity_id}")
    from datetime import datetime

    try:
        current_opp = db.get_opportunity_by_id(opportunity_id)
        if not current_opp:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if current_opp["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        # 1. SERVER-SIDE SANITIZATION (CRITICAL SECURITY FIX)
        ALLOWED_TAGS = bleach.sanitizer.ALLOWED_TAGS + [
            "h1",
            "h2",
            "h3",
            "h4",
            "h5",
            "h6",
            "p",
            "br",
            "a",
            "i",
            "u",
            "em",
            "strong",
            "blockquote",
            "li",
            "ul",
            "ol",
            "img",
            "div",
            "span",
            "table",
            "thead",
            "tbody",
            "tr",
            "td",
            "th",
            "code",
            "pre",
        ]
        ALLOWED_ATTRIBUTES_SAFE = {
            "*": ["id", "class"],
            "a": ["href", "title"],
            "img": ["src", "alt", "width", "height"],
        }

        clean_html = bleach.clean(
            payload.article_body_html,
            tags=ALLOWED_TAGS,
            attributes=ALLOWED_ATTRIBUTES_SAFE,
        )

        # 2. Save the current version to history before overwriting
        current_ai_content = current_opp.get("ai_content", {})
        if current_ai_content:
            db.save_content_version_to_history(
                opportunity_id,
                current_ai_content,
                timestamp=f"{datetime.now().isoformat()} (Before Manual Edit)",
            )

        # 3. Update the content with the new payload
        updated_ai_content = current_ai_content.copy()
        updated_ai_content["article_body_html"] = clean_html

        # Use the query that also updates status and timestamp
        db.update_opportunity_ai_content_and_status(
            opportunity_id,
            updated_ai_content,
            current_opp.get("ai_content_model"),
            "generated",  # Reset status to 'generated' to reflect it's ready
        )
        return {"message": "Content updated and previous version saved to history."}
    except Exception as e:
        logger.error(
            f"Error updating content for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Failed to update content due to a server error."
        )


@router.post(
    "/opportunities/{opportunity_id}/override-disqualification",
    response_model=Dict[str, str],
)
async def override_disqualification_endpoint(
    opportunity_id: int, db: DatabaseManager = Depends(get_db)
):
    """Manually overrides a 'failed' or 'rejected' qualification status."""
    success = db.override_disqualification(opportunity_id)
    if not success:
        raise HTTPException(
            status_code=404,
            detail="Opportunity not found or its status did not permit an override.",
        )
    return {
        "message": "Opportunity has been re-qualified and moved to the pending queue."
    }


@router.post("/opportunities/{opportunity_id}/feedback", response_model=Dict[str, str])
async def submit_content_feedback_endpoint(
    opportunity_id: int,
    request: ContentFeedbackRequest,
    db: DatabaseManager = Depends(get_db),
):
    """Submits user feedback for the generated content."""
    if not (1 <= request.rating <= 5):
        raise HTTPException(status_code=400, detail="Rating must be between 1 and 5.")
    try:
        db.save_content_feedback(opportunity_id, request.rating, request.comments)
        return {"message": "Feedback submitted successfully."}
    except Exception as e:
        logger.error(
            f"Error submitting feedback for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail="Failed to submit feedback.")
</file>

<file path="backend/api/routers/orchestrator.py">
import logging
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from typing import Optional, Dict, List
from data_access.database_manager import DatabaseManager
from jobs import JobManager
from ..dependencies import get_db, get_job_manager, get_orchestrator
from ..models import (
    JobResponse,
    AnalysisRequest,
    AutoWorkflowRequest,
    RefineContentRequest,
    ApproveAnalysisRequest,
)  # Add ApproveAnalysisRequest
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


class GenerationRequest(BaseModel):
    model_override: Optional[str] = None
    temperature: Optional[float] = None


@router.post(
    "/orchestrator/{opportunity_id}/run-generation-async", response_model=JobResponse
)
async def run_generation_async_endpoint(
    opportunity_id: int,
    request: GenerationRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        # Add authorization check
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        # orchestrator is already initialized with the correct client_id from the header
        job_id = orchestrator.run_full_content_generation(
            opportunity_id, request.model_override, request.temperature
        )
        return {
            "job_id": job_id,
            "message": f"Content generation job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start generation job for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/run-validation-async", response_model=JobResponse
)
async def run_validation_async_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        job_id = orchestrator.run_validation(opportunity_id)
        return {"job_id": job_id, "message": f"Validation job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start validation job for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/run-analysis-async", response_model=JobResponse
)
async def run_analysis_async_endpoint(
    opportunity_id: int,
    request: AnalysisRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        # --- START MODIFICATION ---
        # Pass selected_competitor_urls from the request
        job_id = orchestrator.run_full_analysis(
            opportunity_id, request.selected_competitor_urls
        )
        # --- END MODIFICATION ---
        return {"job_id": job_id, "message": f"Full analysis job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start analysis job for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/orchestrator/{opportunity_id}/full-prompt", response_model=str)
async def get_full_prompt_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to get the full, flattened prompt for an opportunity."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        full_prompt = orchestrator.get_full_prompt_for_display(opportunity_id)
        if not full_prompt:
            raise HTTPException(
                status_code=404,
                detail="Prompt has not been generated yet or opportunity data is missing.",
            )
        return full_prompt
    except Exception as e:
        logger.error(
            f"Failed to get full prompt for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/regenerate-social-async", response_model=JobResponse
)
async def regenerate_social_async_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to start a job for regenerating only the social media posts."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.regenerate_social_posts(opportunity_id)
        return {
            "job_id": job_id,
            "message": f"Social media post regeneration job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start social post regeneration for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


class DiscoveryCostParams(BaseModel):
    seed_keywords: List[str]
    discovery_modes: List[str]
    # We only need fields that affect cost calculation
    discovery_max_pages: Optional[int] = 1


class CostEstimationRequest(BaseModel):
    action_type: str
    discovery_params: Optional[DiscoveryCostParams] = None


@router.post("/orchestrator/estimate-cost")
async def estimate_cost_endpoint(
    request: CostEstimationRequest,
    opportunity_id: Optional[int] = None,  # Make opportunity_id optional
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Endpoint to estimate the cost of a workflow action.
    For 'discovery', it uses discovery_params.
    For other actions, it uses opportunity_id.
    """
    action = request.action_type.lower()
    ALLOWED_ACTIONS = {"analyze", "generate", "validate", "discovery"}

    if action not in ALLOWED_ACTIONS:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid action '{action}'. Must be one of: {', '.join(ALLOWED_ACTIONS)}.",
        )

    try:
        if action == "discovery":
            if not request.discovery_params:
                raise HTTPException(
                    status_code=400,
                    detail="discovery_params are required for 'discovery' action.",
                )
            # For discovery, we don't need to check for an opportunity
            cost_estimation = orchestrator.estimate_action_cost(
                action=action, discovery_params=request.discovery_params.dict()
            )
        else:
            if not opportunity_id:
                raise HTTPException(
                    status_code=400,
                    detail="opportunity_id is required for this action.",
                )

            opportunity = db.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                raise HTTPException(status_code=404, detail="Opportunity not found")

            if opportunity["client_id"] != orchestrator.client_id:
                raise HTTPException(
                    status_code=403,
                    detail="You do not have permission to access this opportunity.",
                )

            cost_estimation = orchestrator.estimate_action_cost(
                action=action, opportunity_id=opportunity_id
            )

        return cost_estimation
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to estimate cost for action {action}: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, detail="Failed to estimate cost due to a server error."
        )


@router.get("/jobs/{job_id}/status")
async def get_job_status_endpoint(
    job_id: str, jm: JobManager = Depends(get_job_manager)
):
    """Endpoint to get the status of a background job."""
    logger.info(f"API: Received request for job status: {job_id}")
    job_status = jm.get_job_status(job_id)
    if not job_status:
        logger.warning(f"API: Job with ID {job_id} not found in JobManager.")
        raise HTTPException(status_code=404, detail="Job not found")
    logger.info(f"API: Found job {job_id}, status: {job_status.get('status')}")
    return {"job_id": job_status["id"], "message": f"Status: {job_status['status']}", **job_status}


@router.get("/jobs")
async def get_all_jobs_endpoint(db: DatabaseManager = Depends(get_db)):
    """Endpoint to get all jobs for the activity log."""
    try:
        jobs = db.get_all_jobs()
        return jobs
    except Exception as e:
        logger.error(f"Failed to retrieve all jobs: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to retrieve jobs.")


@router.post("/jobs/{job_id}/cancel")
async def cancel_job_endpoint(job_id: str, jm: JobManager = Depends(get_job_manager)):
    """Endpoint to cancel a running job."""
    try:
        success = jm.cancel_job(job_id)
        if success:
            return {"message": "Job cancellation request sent."}
        else:
            raise HTTPException(
                status_code=404, detail="Job not found or already completed."
            )
    except Exception as e:
        logger.error(f"Failed to cancel job {job_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to cancel job.")


@router.post(
    "/orchestrator/{opportunity_id}/run-full-auto-async", response_model=JobResponse
)
async def run_full_auto_async_endpoint(
    opportunity_id: int,
    request: AutoWorkflowRequest,  # ADD request body
    db: DatabaseManager = Depends(
        get_db
    ),  # Ensure DatabaseManager is correctly imported and injected
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to start the full 'auto' workflow from validation to generation."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.run_full_auto_workflow(
            opportunity_id, request.override_validation
        )  # Pass override flag
        return {
            "job_id": job_id,
            "message": f"Full auto workflow job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start full auto workflow for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/rerun-analysis-async", response_model=JobResponse
)
async def clear_cache_and_analyze_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Clears API cache for the opportunity's keyword and starts a new analysis job."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.run_full_analysis(opportunity_id)
        return {
            "job_id": job_id,
            "message": f"Cache cleared and analysis job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to clear cache and analyze for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.get(
    "/orchestrator/{opportunity_id}/score-narrative", response_model=Dict[str, str]
)
async def get_score_narrative_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to generate a human-readable narrative for the score breakdown."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        narrative = orchestrator.summary_generator.generate_score_narrative(
            opportunity.get("full_data", {}).get("score_breakdown", {})
        )
        return {"narrative": narrative}
    except Exception as e:
        logger.error(
            f"Failed to generate score narrative for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/refresh-content-async", response_model=JobResponse
)
async def refresh_content_async_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to trigger a refresh of an existing opportunity's content."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.run_content_refresh_workflow(opportunity_id)
        return {"job_id": job_id, "message": f"Content refresh job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start content refresh for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


class FeaturedImageRequest(BaseModel):
    prompt: str


@router.post(
    "/orchestrator/{opportunity_id}/generate-featured-image-async",
    response_model=JobResponse,
)
async def generate_featured_image_async_endpoint(
    opportunity_id: int,
    request: FeaturedImageRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Starts a job to generate a new featured image for an opportunity."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.regenerate_featured_image(opportunity_id, request.prompt)
        return {
            "job_id": job_id,
            "message": f"Featured image generation job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start featured image generation for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/refine-content", response_model=Dict[str, str]
)
async def refine_content_endpoint(
    opportunity_id: int,
    request: RefineContentRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to refine a snippet of HTML content using an AI command."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        prompt_messages = [
            {
                "role": "system",
                "content": "You are an expert web content editor. You will receive a block of HTML and a specific command. Your task is to apply the command to the provided HTML segment and return ONLY the modified HTML block. You MUST preserve all original HTML tags, attributes (like IDs, classes, styles), and structure, only changing the text content or adding/removing tags as absolutely necessary to fulfill the command. Do not add any introductory or concluding remarks, just the refined HTML.",
            },
            {
                "role": "user",
                "content": f"Command: '{request.command}'\n\nHTML to modify:\n```html\n{request.html_content}\n```\n\nReturn ONLY the refined HTML:",
            },
        ]

        refined_html, error = orchestrator.openai_client.call_chat_completion(
            messages=prompt_messages,
            model=orchestrator.client_cfg.get("default_model", "gpt-5-nano"),
            temperature=0.4,
        )

        if error or not refined_html:
            raise HTTPException(
                status_code=500, detail=f"AI content refinement failed: {error}"
            )

        # Clean up potential markdown code block fences from the AI response
        refined_html = refined_html.strip()
        if refined_html.startswith("```html"):
            refined_html = refined_html[len("```html") :]
        if refined_html.endswith("```"):
            refined_html = refined_html[: -len("```")]

        return {"refined_html": refined_html.strip()}
    except Exception as e:
        logger.error(
            f"Failed to refine content for opp {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(
            status_code=500, detail="Failed to refine content due to a server error."
        )


@router.post(
    "/orchestrator/{opportunity_id}/generate-content-override",
    response_model=JobResponse,
)
async def generate_content_override(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to manually trigger content generation override"""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        job_id = orchestrator.run_full_auto_workflow(
            opportunity_id, True
        )  # Run with override = True
        return {
            "job_id": job_id,
            "message": f"Content generation override job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start content generation override for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/approve-analysis/{opportunity_id}", response_model=JobResponse
)
async def approve_analysis_endpoint(
    opportunity_id: int,
    request: ApproveAnalysisRequest,  # Use the new request body model
    db: DatabaseManager = Depends(get_db),
    jm: JobManager = Depends(get_job_manager),
    orchestrator: WorkflowOrchestrator = Depends(
        get_orchestrator
    ),  # Add orchestrator to get client_id
):
    """Endpoint to approve the analysis and continue the workflow by starting content generation with optional overrides."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        # Add authorization check
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        # Convert Pydantic model to dict if it exists, else pass None
        overrides_dict = request.overrides.dict() if request.overrides else None

        job_id = orchestrator.run_full_content_generation(
            opportunity_id, overrides=overrides_dict
        )

        return {
            "job_id": job_id,
            "message": f"Analysis approved. Started content generation job {job_id}.",
        }
    except ValueError as ve:
        logger.error(
            f"State mismatch trying to approve analysis for {opportunity_id}: {ve}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=409, detail=str(ve)
        )  # 409 Conflict for state issues
    except Exception as e:
        logger.error(
            f"Failed to approve analysis for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/reject-opportunity/{opportunity_id}", response_model=Dict[str, str]
)
async def reject_opportunity_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
):
    """Endpoint to reject the opportunity and set status to 'rejected'"""
    try:
        # This is a direct status update, no job manager needed for rejection itself
        db.update_opportunity_workflow_state(
            opportunity_id,
            "rejected_by_user",
            "rejected",
            error_message="Opportunity rejected by user.",
        )
        return {"message": "Opportunity rejected."}
    except Exception as e:
        logger.error(
            f"Failed to reject opportunity {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


class SocialMediaStatusUpdateRequest(BaseModel):
    new_status: str


# Add this new endpoint at the end of the file:
@router.post(
    "/orchestrator/{opportunity_id}/social-media-status", response_model=Dict[str, str]
)
async def update_social_media_status_endpoint(
    opportunity_id: int,
    request: SocialMediaStatusUpdateRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(
        get_orchestrator
    ),  # For client_id auth
):
    """Endpoint to update the status of social media posts (e.g., 'approved', 'rejected')."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        valid_statuses = ["draft", "approved", "rejected", "scheduled", "published"]
        if request.new_status not in valid_statuses:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid status: {request.new_status}. Must be one of {valid_statuses}.",
            )

        db.update_social_media_posts_status(opportunity_id, request.new_status)
        return {
            "message": "Social media posts status updated successfully.",
            "new_status": request.new_status,
        }
    except Exception as e:
        logger.error(
            f"Failed to update social media status for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))
</file>

<file path="backend/api/routers/qualification_settings.py">
# api/routers/qualification_settings.py

import logging
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get(
    "/clients/{client_id}/qualification-settings", response_model=Dict[str, Any]
)
async def get_qualification_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves the qualification settings for a specific client.
    """
    logger.info(f"Received request for qualification settings for client {client_id}")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    settings = db.get_qualification_settings(client_id)
    if not settings:
        raise HTTPException(status_code=404, detail="Qualification settings not found")
    return settings


@router.put(
    "/clients/{client_id}/qualification-settings", response_model=Dict[str, Any]
)
async def update_qualification_settings_endpoint(
    client_id: str,
    settings: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Updates the qualification settings for a specific client.
    """
    logger.info(
        f"Received request to update qualification settings for client {client_id}"
    )
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    db.update_qualification_settings(client_id, settings)
    return {"message": "Qualification settings updated successfully."}
</file>

<file path="backend/api/routers/qualification_strategies.py">
# api/routers/qualification_strategies.py

import logging
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any, List
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get(
    "/clients/{client_id}/qualification-strategies", response_model=List[Dict[str, Any]]
)
async def get_qualification_strategies_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all qualification strategies for a specific client.
    """
    logger.info(f"Received request for qualification strategies for client {client_id}")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    strategies = db.get_qualification_strategies(client_id)
    return strategies


@router.post(
    "/clients/{client_id}/qualification-strategies", response_model=Dict[str, Any]
)
async def create_qualification_strategy_endpoint(
    client_id: str,
    strategy: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Creates a new qualification strategy for a specific client.
    """
    logger.info(
        f"Received request to create qualification strategy for client {client_id}"
    )
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    strategy_id = db.create_qualification_strategy(client_id, strategy)
    return {"id": strategy_id}


@router.put("/qualification-strategies/{strategy_id}", response_model=Dict[str, str])
async def update_qualification_strategy_endpoint(
    strategy_id: int,
    strategy: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Updates a qualification strategy.
    """
    logger.info(f"Received request to update qualification strategy {strategy_id}")
    # Auth check
    strat_to_update = db.get_qualification_strategy_by_id(strategy_id)
    if not strat_to_update:
        raise HTTPException(status_code=404, detail="Strategy not found.")
    if strat_to_update["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to modify this resource.",
        )

    db.update_qualification_strategy(strategy_id, strategy)
    return {"message": "Qualification strategy updated successfully."}


@router.delete("/qualification-strategies/{strategy_id}", response_model=Dict[str, str])
async def delete_qualification_strategy_endpoint(
    strategy_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Deletes a qualification strategy.
    """
    logger.info(f"Received request to delete qualification strategy {strategy_id}")
    # Auth check
    strat_to_delete = db.get_qualification_strategy_by_id(strategy_id)
    if not strat_to_delete:
        raise HTTPException(status_code=404, detail="Strategy not found.")
    if strat_to_delete["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to delete this resource.",
        )

    db.delete_qualification_strategy(strategy_id)
    return {"message": "Qualification strategy deleted successfully."}
</file>

<file path="backend/api/routers/settings.py">
# api/routers/settings.py
import logging
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)

@router.get("/settings/{client_id}", response_model=Dict[str, Any])
async def get_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint for fetching all client-specific settings."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        settings = db.get_client_settings(client_id)
        if not settings:
            raise HTTPException(status_code=404, detail="Settings not found for this client.")
        return settings
    except Exception as e:
        logger.error(f"Failed to retrieve settings for client {client_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to retrieve settings.")

@router.put("/settings/{client_id}", response_model=Dict[str, str])
async def update_settings_endpoint(
    client_id: str,
    settings: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint for updating client-specific settings."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        db.update_client_settings(client_id, settings)
        return {"message": "Settings updated successfully."}
    except Exception as e:
        logger.error(f"Failed to update settings for client {client_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to update settings.")
</file>

<file path="backend/api/dependencies.py">
# api/dependencies.py
import os
from fastapi import Depends, HTTPException, Security, Request
from fastapi.security import APIKeyHeader
from data_access.database_manager import DatabaseManager
from backend.pipeline import WorkflowOrchestrator
from jobs import JobManager
from services.opportunities_service import OpportunitiesService
from services.discovery_service import DiscoveryService
from . import globals as api_globals


def get_db() -> DatabaseManager:
    """Dependency injector for DatabaseManager."""
    return api_globals.db_manager


def get_opportunities_service(
    db: DatabaseManager = Depends(get_db),
) -> OpportunitiesService:
    """Dependency injector for OpportunitiesService."""
    return OpportunitiesService(db)


def get_discovery_service(db: DatabaseManager = Depends(get_db)) -> DiscoveryService:
    """Dependency injector for DiscoveryService."""
    return DiscoveryService(db)


def get_job_manager() -> JobManager:
    """Dependency injector for JobManager."""
    return api_globals.job_manager


# Replace the entire `get_current_client_id` function with this:
def get_current_client_id(request: Request) -> str:
    """
    Dependency to get the current client_id from the X-Client-ID header.
    In a real multi-tenant application, this would also be validated against user's permissions.
    """
    client_id = request.headers.get("X-Client-ID")
    if not client_id:
        # Fallback to default if header is missing, or raise HTTPException
        # For development, we might fallback. For production, raising is safer.
        # raise HTTPException(status_code=400, detail="X-Client-ID header is required")
        return "Lark_Main_Site"  # Fallback for local dev/testing
    return client_id


# Update the `get_orchestrator` dependency to *not* directly use `get_current_client_id` within its signature
# because it will be passed explicitly to the endpoint if needed.
# Modify `get_orchestrator` signature from `get_orchestrator(client_id: str, ...)` to:
def get_orchestrator(
    request: Request,  # Add Request to get client_id
    db: DatabaseManager = Depends(get_db),
    jm: JobManager = Depends(get_job_manager),
) -> WorkflowOrchestrator:
    """
    Dependency injector for WorkflowOrchestrator.
    Creates a new instance for each request, configured for the specific client_id.
    """
    client_id = request.headers.get("X-Client-ID")  # Get client_id from request headers
    if not client_id:
        # Fallback to default for orchestrator initialization if header is missing
        client_id = "Lark_Main_Site"

    if not db.get_client_settings(client_id):
        raise HTTPException(
            status_code=404, detail=f"Client with ID '{client_id}' not found."
        )

    return WorkflowOrchestrator(api_globals.config_manager, db, client_id, jm)


API_KEY = os.getenv("INTERNAL_API_KEY")
API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)


async def get_api_key(api_key: str = Security(api_key_header)):
    if api_key == API_KEY:
        return api_key
    else:
        raise HTTPException(status_code=403, detail="Could not validate credentials")
</file>

<file path="backend/api/globals.py">
# api/globals.py
from typing import Optional
from app_config.manager import ConfigManager
from data_access.database_manager import DatabaseManager
from jobs import JobManager

config_manager: Optional[ConfigManager] = None
db_manager: Optional[DatabaseManager] = None
job_manager: Optional[JobManager] = None
</file>

<file path="backend/api/main.py">
# api/main.py
# api/main.py (New File, or existing FastAPI entry point)
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles  # ADD THIS for Task 3
import logging
import os
import sys

# Add project root to sys.path to resolve imports from agents, pipeline, etc.
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# Import from your existing project structure
from app_config.manager import ConfigManager
from data_access.database_manager import DatabaseManager
from jobs import JobManager  # Import the class

from . import globals as api_globals


logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI()

# Mount the static directory for generated images
# Images will be accessible at /api/images/{filename}
static_images_path = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "generated_images")
)
app.mount(
    "/api/images", StaticFiles(directory=static_images_path), name="static_images"
)


# --- Global Dependency Initialization (simplified for example) ---
# In a real app, use @lru_cache or proper dependency injection
@app.on_event("startup")
async def startup_event():
    api_globals.config_manager = ConfigManager()
    api_globals.db_manager = DatabaseManager(cfg_manager=api_globals.config_manager)
    api_globals.db_manager.initialize()  # Ensure DB tables are created/migrated
    api_globals.job_manager = JobManager(
        db_manager=api_globals.db_manager
    )  # Initialize JobManager with db_manager

    logger.info("FastAPI application startup complete. Dependencies initialized.")

    from .routers import (
        auth,
        clients,
        opportunities,
        discovery,
        orchestrator,
        jobs,
        qualification_settings,
        qualification_strategies,
        settings,
    )

    app.include_router(auth.router, prefix="/api")
    app.include_router(clients.router, prefix="/api")
    app.include_router(opportunities.router, prefix="/api")
    app.include_router(discovery.router, prefix="/api")
    app.include_router(orchestrator.router, prefix="/api")
    app.include_router(jobs.router, prefix="/api")
    app.include_router(qualification_settings.router, prefix="/api")
    app.include_router(qualification_strategies.router, prefix="/api")
    app.include_router(settings.router, prefix="/api")
</file>

<file path="backend/api/models.py">
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any


class DiscoveryCostParams(BaseModel):
    seed_keywords: List[str]
    discovery_modes: Optional[List[str]] = ["ideas"]
    limit: Optional[int] = 1000
    include_clickstream_data: Optional[bool] = False
    people_also_ask_click_depth: Optional[int] = 0


# Define ContentUpdateRequest (W18 FIX)
class ContentUpdatePayload(BaseModel):
    article_body_html: str = Field(
        ..., description="The new HTML content for the article body."
    )


# Define ImageUpdatePayload (W18 FIX)
class ImageRegenRequest(BaseModel):
    original_prompt: str
    new_prompt: str


class DiscoveryRunRequest(BaseModel):
    seed_keywords: List[str]
    discovery_modes: Optional[List[str]] = ["ideas"]
    filters: Optional[List[Any]] = None
    order_by: Optional[List[str]] = None
    filters_override: Optional[Dict[str, Any]] = {}
    depth: Optional[int] = None
    limit: Optional[int] = None
    ignore_synonyms: Optional[bool] = False
    # NEW: Parameters for user flexibility
    include_clickstream_data: Optional[bool] = None
    closely_variants: Optional[bool] = None


class KeywordListRequest(BaseModel):
    seed_keywords: List[str]
    # NEW: Parameter for dynamic cost estimation
    include_clickstream_data: Optional[bool] = False


class JobResponse(BaseModel):
    job_id: str
    message: str
    status: Optional[str] = None
    progress: Optional[int] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    progress_log: Optional[List[Dict[str, Any]]] = None


class LoginRequest(BaseModel):
    password: str


class TemplateContent(BaseModel):
    name: str
    content: str
    description: Optional[str] = None


class TemplateResponse(BaseModel):
    name: str
    content: str
    description: Optional[str] = None
    last_updated: str


class PromptPreviewRequest(BaseModel):
    custom_template_content: Optional[str] = None


class PromptPreviewResponse(BaseModel):
    prompt: str


class ContentHistoryItem(BaseModel):
    id: int
    opportunity_id: int
    timestamp: str
    ai_content_json: Dict[str, Any]


class RestoreRequest(BaseModel):
    version_timestamp: str


class SingleImageRegenRequest(BaseModel):
    opportunity_id: int
    original_prompt: str
    new_prompt: str


class AutoWorkflowRequest(BaseModel):
    override_validation: bool = False


class SocialMediaPostsUpdate(BaseModel):
    social_media_posts: List[Dict[str, Any]]


class GlobalSettingsUpdate(BaseModel):
    settings: Dict[str, Any]


class OpportunityListResponse(BaseModel):
    items: List[Dict[str, Any]]
    total_items: int
    page: int
    limit: int


class AnalysisRequest(BaseModel):
    selected_competitor_urls: Optional[List[str]] = None


class RefineContentRequest(BaseModel):
    html_content: str
    command: str


class ClientSettings(BaseModel):
    brand_tone: Optional[str] = None
    target_audience: Optional[str] = None
    terms_to_avoid: Optional[str] = None


class GenerationOverrides(BaseModel):
    target_word_count: Optional[int] = None
    expert_persona: Optional[str] = None
    additional_instructions: Optional[str] = None


class ApproveAnalysisRequest(BaseModel):
    overrides: Optional[GenerationOverrides] = None
</file>

<file path="backend/app_config/__init__.py">
# app_config/__init__.py
# This file marks the directory as a Python package.
</file>

<file path="backend/app_config/manager.py">
import os
import configparser
from dotenv import load_dotenv
from typing import Dict, Any, List, Optional
import logging


class ConfigManager:
    """
    Manages loading, validating, and providing configuration settings.
    Handles global defaults from settings.ini and client-specific overrides.
    """

    _setting_types = {
        # Integers
        "location_code": int,
        "max_sv_for_scoring": int,
        "max_domain_rank_for_scoring": int,
        "max_referring_domains_for_scoring": int,
        "max_avg_referring_domains_filter": int,
        "serp_freshness_old_threshold_days": int,
        "serp_volatility_stable_threshold_days": int,
        "min_competitor_word_count": int,
        "max_competitor_technical_warnings": int,
        "num_competitors_to_analyze": int,
        "num_common_headings": int,
        "num_unique_angles": int,
        "max_initial_serp_urls_to_analyze": int,
        "people_also_ask_click_depth": int,
        "min_search_volume": int,
        "max_keyword_difficulty": int,
        "num_in_article_images": int,
        "onpage_max_domains_per_request": int,
        "onpage_max_tasks_per_request": int,
        "deep_dive_top_n_keywords": int,
        "max_completion_tokens_for_generation": int,
        "discovery_max_pages": int,
        "min_serp_results": int,
        "max_serp_results": int,
        "min_avg_backlinks": int,
        "max_avg_backlinks": int,
        "discovery_related_depth": int,
        "yearly_trend_decline_threshold": int,
        "quarterly_trend_decline_threshold": int,
        "max_kd_hard_limit": int,
        "max_referring_main_domains_limit": int,
        "max_avg_domain_rank_threshold": int,
        "min_keyword_word_count": int,
        "max_keyword_word_count": int,
        "crowded_serp_features_threshold": int,
        "min_serp_stability_days": int,
        "max_non_blog_results": int,
        "max_ai_overview_words": int,
        "max_first_organic_y_pixel": int,
        "max_words_for_ai_analysis": int,
        "num_competitors_for_ai_analysis": int,
        "max_avg_lcp_time": int,  # NEW
        "high_value_sv_override_threshold": int,
        "overlay_font_size": int,
        # Floats
        "informational_score": float,
        "commercial_score": float,
        "transactional_score": float,
        "navigational_score": float,
        "question_keyword_bonus": float,
        "max_cpc_for_scoring": float,
        "featured_snippet_bonus": float,
        "ai_overview_bonus": float,
        "serp_freshness_bonus_max": float,
        "min_cpc_filter": float,
        "min_yearly_trend_filter": float,
        "min_cpc": float,
        "max_cpc": float,
        "min_competition": float,
        "max_competition": float,
        "min_cpc_filter_api": float,
        "category_intent_bonus": float,
        "search_volume_volatility_threshold": float,
        "max_paid_competition_score": float,
        "max_high_top_of_page_bid": float,
        "max_pages_to_domain_ratio": float,
        "ai_generation_temperature": float,
        "recommended_word_count_multiplier": float,
        "default_multiplier": float,  # ADDED
        "comprehensive_article": float,  # ADDED
        "how_to_guide": float,  # ADDED
        "comparison_post": float,  # ADDED
        "review_article": float,  # ADDED
        "video_led_article": float,  # ADDED
        "forum_summary_post": float,  # ADDED
        "recipe_article": float,
        "scholarly_summary": float,
        "product_comparison": float,
        "high_value_cpc_override_threshold": float,
        # Booleans
        "require_question_keywords": bool,
        "enforce_intent_filter": bool,
        "calculate_rectangles": bool,
        "enable_cache": bool,
        "deep_dive_discovery": bool,
        "use_pexels_first": bool,
        "cleanup_local_images": bool,
        "onpage_enable_javascript": bool,
        "onpage_load_resources": bool,
        "onpage_disable_cookie_popup": bool,
        "onpage_return_despite_timeout": bool,
        "onpage_enable_browser_rendering": bool,
        "onpage_store_raw_html": bool,
        "onpage_validate_micromarkup": bool,
        "discovery_replace_with_core_keyword": bool,
        "discovery_ignore_synonyms": bool,
        "enable_automated_internal_linking": bool,
        "generate_toc": bool,
        "overlay_text_enabled": bool,
        "include_clickstream_data": bool,
        "load_async_ai_overview": bool,  # ADD THIS FOR W3
        "onpage_check_spell": bool,  # ADD THIS LINE (W5 FIX)
        "disable_ai_overview_check": bool,
        "onpage_accept_language": str,  # ADD THIS LINE (W7 FIX)
        "onpage_enable_switch_pool": bool,  # ADD THIS LINE (W13 FIX)
        "onpage_enable_custom_js": bool,  # ADD THIS LINE (W12 FIX)
        "onpage_custom_js": str,  # ADD THIS LINE (W12 FIX)
        "discovery_exact_match": bool,  # ADD THIS LINE (W7 FIX)
        "onpage_browser_screen_resolution_ratio": float,  # ADD THIS LINE (W7 FIX)
        # Lists (comma-separated strings)
        "allowed_intents": list,
        "negative_keywords": list,
        "competitor_blacklist_domains": list,
        "serp_feature_filters": list,
        "serp_features_exclude_filter": list,
        "platforms": list,
        "default_wordpress_categories": list,
        "default_wordpress_tags": list,
        "ugc_and_parasite_domains": list,
        "high_value_categories": list,
        "hostile_serp_features": list,
        "final_validation_non_blog_domains": list,
        # Weights
        "ease_of_ranking_weight": float,
        "traffic_potential_weight": float,
        "commercial_intent_weight": float,
        "serp_features_weight": float,
        "growth_trend_weight": float,
        "serp_freshness_weight": float,
        "serp_volatility_weight": float,
        "competitor_weakness_weight": float,
        "competitor_performance_weight": float,  # ADDED THIS LINE
        # Strings
        "max_competition_level": str,
        "non_evergreen_year_pattern": str,
        "db_file_name": str,  # NEW
        "db_type": str,  # NEW
        "overlay_text_color": str,
        "overlay_background_color": str,
        "overlay_position": str,
        "closely_variants": bool,
        "max_cpc_filter": float,
        "discovery_order_by_field": str,
        "discovery_order_by_direction": str,
        "search_phrase_regex": str,
        "onpage_custom_checks_thresholds": str,  # ADD THIS LINE (W9 FIX)
        "serp_remove_from_url_params": str,
        "schema_author_type": str,
        "client_knowledge_base": str,
        "wordpress_url": str,
        "wordpress_user": str,
        "wordpress_app_password": str,
        "wordpress_seo_plugin": str,
    }

    def __init__(self, settings_path: str = "backend/app_config/settings.ini"):
        load_dotenv()
        self.config_parser = configparser.ConfigParser(inline_comment_prefixes=(";",))
        if not os.path.exists(settings_path):
            raise FileNotFoundError(f"Configuration file not found at: {settings_path}")
        self.config_parser.read(settings_path)
        self.logger = logging.getLogger(self.__class__.__name__)
        self._configure_logging()
        self._global_settings = self._load_and_validate_global()

    def _configure_logging(self):
        """Sets up basic logging for the application."""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        logging.getLogger("urllib3").setLevel(logging.WARNING)
        self.logger.info("Logging configured.")

    def _get_int_from_config(
        self, section: str, key: str, fallback: Optional[int] = None
    ) -> int:
        try:
            value_str = self.config_parser.get(section, key)
            return int(value_str)
        except (configparser.NoOptionError, configparser.NoSectionError, ValueError):
            if fallback is not None:
                return fallback
            raise ValueError(
                f"Missing or invalid integer configuration for [{section}]{key}"
            )

    def _get_float_from_config(
        self, section: str, key: str, fallback: Optional[float] = None
    ) -> float:
        try:
            value_str = self.config_parser.get(section, key)
            return float(value_str)
        except (configparser.NoOptionError, configparser.NoSectionError, ValueError):
            if fallback is not None:
                return fallback
            raise ValueError(
                f"Missing or invalid float configuration for [{section}]{key}"
            )

    def _get_list_from_config(
        self, section: str, key: str, fallback: str = ""
    ) -> List[str]:
        try:
            value_str = self.config_parser.get(section, key)
            return [item.strip() for item in value_str.split(",") if item.strip()]
        except (configparser.NoOptionError, configparser.NoSectionError):
            return [item.strip() for item in fallback.split(",") if item.strip()]

    def _load_and_validate_global(self) -> Dict[str, Any]:
        """Loads all global settings from settings.ini and .env."""
        settings = {}

        # API Credentials (from .env)
        settings["dataforseo_login"] = os.getenv("DATAFORSEO_LOGIN")
        settings["dataforseo_password"] = os.getenv("DATAFORSEO_PASSWORD")
        if not settings["dataforseo_login"] or not settings["dataforseo_password"]:
            self.logger.critical(
                "DATAFORSEO_LOGIN and DATAFORSEO_PASSWORD must be set in the .env file."
            )
            raise ValueError(
                "DATAFORSEO_LOGIN and DATAFORSEO_PASSWORD must be set in the .env file."
            )

        settings["openai_api_key"] = os.getenv("OPENAI_API_KEY")
        if not settings["openai_api_key"]:
            self.logger.warning(
                "OPENAI_API_KEY is not set in the .env file. AI content generation will likely fail."
            )

        settings["pexels_api_key"] = os.getenv("PEXELS_API_KEY")
        if not settings["pexels_api_key"]:
            self.logger.warning(
                "PEXELS_API_KEY is not set in the .env file. Pexels integration will be unavailable."
            )

        # UI Password (from .env)
        settings["ui_password"] = os.getenv("UI_PASSWORD")
        if not settings["ui_password"]:
            self.logger.critical(
                "UI_PASSWORD must be set in the .env file for Streamlit authentication."
            )
            raise ValueError("UI_PASSWORD must be set in the .env file.")

        # Load all settings from settings.ini
        for section in self.config_parser.sections():
            for key, value in self.config_parser.items(section):
                try:
                    target_type = self._setting_types.get(key)
                    if target_type is bool:
                        settings[key] = self.config_parser.getboolean(section, key)
                    elif target_type is int:
                        settings[key] = self.config_parser.getint(section, key)
                    elif target_type is float:
                        settings[key] = self.config_parser.getfloat(section, key)
                    elif target_type is list:
                        raw_values = self._get_list_from_config(section, key)
                        if key == "serp_feature_filters":
                            parsed_filters = []
                            for f_str in raw_values:
                                if f_str.startswith("no_"):
                                    parsed_filters.append(
                                        {"type": "has_not", "feature": f_str[3:]}
                                    )
                                elif f_str.startswith("has_"):
                                    parsed_filters.append(
                                        {"type": "has", "feature": f_str[4:]}
                                    )
                            settings[key] = parsed_filters
                        else:
                            settings[key] = raw_values
                    else:  # Default to string if no type is mapped
                        settings[key] = value
                except Exception as e:
                    self.logger.critical(
                        f"FATAL CONFIG ERROR: Could not parse key [{section}]{key} with value '{value}' to expected type: {e}"
                    )
                    raise ValueError(
                        f"Configuration key parsing failed for [{section}]{key}. Value: '{value}'."
                    )

        self.logger.info("Global settings loaded.")
        return settings

    def get_global_config(self) -> Dict[str, Any]:
        """Returns the loaded global configuration."""
        return self._global_settings

    def get_default_client_settings_template(self) -> Dict[str, Any]:
        """Returns a template of client settings based on global config, for new client creation."""
        template = self._global_settings.copy()
        template.pop("dataforseo_login", None)
        template.pop("dataforseo_password", None)
        template.pop("openai_api_key", None)
        template.pop("pexels_api_key", None)
        template.pop("ui_password", None)

        for key, value in template.items():
            if isinstance(value, list):
                template[key] = value[:]
        return template

    def load_client_config(self, client_id: str, db_manager: Any) -> Dict[str, Any]:
        """
        Loads client-specific settings from the database and merges them with global settings.
        Scoring weights are always loaded from the global settings to ensure consistency.
        """
        client_settings_from_db = db_manager.get_client_settings(client_id)
        overridden_settings = self._global_settings.copy()

        # Define keys that should NOT be overridden by client-specific settings to ensure they are globally managed.
        globally_managed_keys = set(
            [
                "dataforseo_login",
                "dataforseo_password",
                "openai_api_key",
                "pexels_api_key",
                "ui_password",
                "db_file_name",
                "cache_file_name",
                "default_client_id",
                "db_type",
            ]
        )  # UPDATED

        for key, value in client_settings_from_db.items():
            if key in globally_managed_keys:
                continue  # Skip override for globally managed keys

            if value is not None and value != "":
                overridden_settings[key] = value

        self.logger.info(f"Loaded client-specific configuration for '{client_id}'.")
        return overridden_settings

    def save_client_settings(
        self, client_id: str, new_settings: Dict[str, Any], db_manager: Any
    ):
        """Saves specified client settings to the database."""
        db_manager.update_client_settings(client_id, new_settings)
        self.logger.info(f"Client settings for '{client_id}' saved to database.")

    def save_global_settings_to_file(self, updated_global_settings: Dict[str, Any]):
        """Saves specified settings back to the settings.ini file (for global defaults)."""
        for section in self.config_parser.sections():
            for key in self.config_parser.options(section):
                if (
                    key in updated_global_settings
                    and updated_global_settings[key] is not None
                ):
                    if isinstance(updated_global_settings[key], list):
                        self.config_parser.set(
                            section, key, ",".join(updated_global_settings[key])
                        )
                    elif isinstance(updated_global_settings[key], bool):
                        self.config_parser.set(
                            section, key, str(updated_global_settings[key]).lower()
                        )
                    else:
                        self.config_parser.set(
                            section, key, str(updated_global_settings[key])
                        )

        with open("backend/app_config/settings.ini", "w") as configfile:
            self.config_parser.write(configfile)

        self._global_settings = self._load_and_validate_global()
        self.logger.info("Global settings updated and reloaded from settings.ini.")
</file>

<file path="backend/app_config/settings.ini">
[SEO_CRITERIA]
location_code = 2840
language_code = en
target_domain = profitparrot.com
device = desktop ; NEW: Global default device for SERP and OnPage calls
os = windows ; NEW: Global default OS for SERP calls

[INTENT_SCORING]
informational_score = 100
commercial_score = 70
transactional_score = 50
navigational_score = 10
question_keyword_bonus = 5

[SCORING_WEIGHTS]
ease_of_ranking_weight = 40
traffic_potential_weight = 15
commercial_intent_weight = 5
serp_features_weight = 5
growth_trend_weight = 5
serp_freshness_weight = 5
serp_volatility_weight = 5
competitor_weakness_weight = 20
competitor_performance_weight = 5 ; NEW: Weight for competitor technical performance

[SCORING_NORMALIZATION]
max_cpc_for_scoring = 20.0
max_sv_for_scoring = 50000
max_domain_rank_for_scoring = 700
max_referring_domains_for_scoring = 200 ; NEW: For ease of ranking calculation
max_avg_referring_domains_filter = 20 ; NEW: Filter for discovery - max avg referring domains for top competitors

[SERP_FEATURE_SCORING]
featured_snippet_bonus = 15
ai_overview_bonus = 10
serp_freshness_bonus_max = 20
serp_freshness_old_threshold_days = 180
serp_volatility_stable_threshold_days = 90 ; NEW: Threshold for a "stable" SERP

[QUALITY_FILTERS]
require_question_keywords = true
enforce_intent_filter = true
allowed_intents = informational
negative_keywords = login, sign in, account, free, cheap
min_search_volume = 100
max_keyword_difficulty = 80
min_cpc = 0.0
max_cpc = 5.0
min_competition = 0.0
max_competition = 1.0
max_competition_level = LOW
min_serp_results = 100000
max_serp_results = 10000000
min_avg_backlinks = 0
max_avg_backlinks = 20
min_keyword_word_count = 2 ; NEW
max_keyword_word_count = 8 ; NEW
high_value_sv_override_threshold = 10000 ; NEW
high_value_cpc_override_threshold = 5.0 ; NEW


[DEFAULT]
enable_cache = true
cache_file_name = data/cache.json
max_completion_tokens_for_generation = 32768
db_file_name = data/opportunities.db
ai_generation_temperature = 0.7
include_clickstream_data = false

[Lark_Main_Site]
target_domain = profitparrot.com
# Other client-specific settings can go here
expert_persona = a certified financial planner with 15 years of experience

[DISCOVERY_SETTINGS]
discovery_strategies = Keyword Ideas, Related Keywords, Keyword Suggestions ; W20 FIX: Centralize strategies
deep_dive_discovery = false
deep_dive_top_n_keywords = 5
serp_feature_filters = no_ai_overview, has_featured_snippet
load_async_ai_overview = false ; ADD THIS (W3 FIX)
discovery_max_pages = 100
people_also_ask_click_depth = 2
serp_features_exclude_filter = popular_products,local_pack,shopping,app,jobs,refine_products
closely_variants = false
discovery_exact_match = false ; ADD THIS (W7 Default: Disable for broad match by default)
min_cpc_filter = 0.0
max_cpc_filter = 999.0
min_competition = 0.0
max_competition = 1.0
max_competition_level = HIGH
discovery_ignore_synonyms = false


search_phrase_regex =

[IMAGE_GENERATION]
num_in_article_images = 2
use_pexels_first = true
cleanup_local_images = true
overlay_text_enabled = true
overlay_text_color = #FFFFFF
overlay_background_color = #00000080 ; RGBA hex for semi-transparent black
overlay_font_size = 40 ; Pixels
overlay_position = bottom_center ; top_left, top_right, bottom_left, bottom_center, bottom_right

[CONTENT]
generate_toc = true
default_author_name = Profit Parrot Marketing Expert
schema_author_type = Organization ; ADD THIS (W12 Default)

[ONPAGE_API_CLIENT_CONFIG] ; NEW: Separate section for OnPage API client parameters
onpage_enable_javascript = true
onpage_load_resources = true ; Set to true if browser rendering is enabled, as it forces loading resources.
onpage_disable_cookie_popup = true
onpage_return_despite_timeout = false
onpage_enable_browser_rendering = true
onpage_store_raw_html = false
onpage_validate_micromarkup = true
onpage_check_spell = true ; ADD THIS (W5 FIX)
onpage_accept_language = en ; ADD THIS (W7 FIX)
onpage_custom_user_agent = Mozilla/5.0 (compatible; RSiteAuditor)
onpage_max_domains_per_request = 5 ; Max 5 identical domains in one batch
onpage_max_tasks_per_request = 20
onpage_custom_checks_thresholds = {"high_loading_time": 2500} ; ADD THIS (W9 Default: Target < 2.5s instead of > 3s)
onpage_enable_switch_pool = true
ip_pool_for_scan = us
onpage_enable_custom_js = false ; ADD THIS (W12 Default)
onpage_custom_js = meta = {}; meta.url = document.URL; meta; ; ADD THIS (W12 Example)
onpage_browser_screen_resolution_ratio = 1.0 ; ADD THIS (W7 Default)

[APP_SETTINGS]
default_client_id = Lark_Main_Site
recommended_word_count_multiplier = 1.2
enable_automated_internal_linking = false

[SOCIAL_MEDIA]
platforms = facebook,linkedin,twitter,google_business_profile

[WORDPRESS_SETTINGS]
wordpress_url = 
wordpress_user = 
wordpress_app_password = 
wordpress_seo_plugin = aioseo
default_wordpress_categories = Blog
default_wordpress_tags = SEO,Content Marketing

[PAGE_CLASSIFICATION]
forum_domains = reddit.com,quora.com,stackoverflow.com,forums.somethingawful.com
ecommerce_domains = amazon.com,ebay.com,walmart.com,target.com,bestbuy.com,etsy.com
news_domains = nytimes.com,bbc.com,cnn.com,theguardian.com,wsj.com
blog_url_patterns = /blog/,/post/,/article/,/\d{4}/\d{2}/
forum_url_patterns = /thread/,/forum/,/discussion/,/q/,/questions/
ugc_and_parasite_domains = linkedin.com, pinterest.com, amazon.com, ebay.com, wikipedia.org, reddit.com, facebook.com, youtube.com, medium.com, quora.com

[DISQUALIFICATION_RULES]
; Tier 1
allowed_intents = informational
negative_keywords = login, sign in, account, free, cheap, porn
min_search_volume = 100

; Tier 2
yearly_trend_decline_threshold = -25
quarterly_trend_decline_threshold = 0
search_volume_volatility_threshold = 1.5

; Tier 3
max_paid_competition_score = 0.8
max_high_top_of_page_bid = 15.00
max_kd_hard_limit = 70
max_referring_main_domains_limit = 100
max_avg_domain_rank_threshold = 500
max_pages_to_domain_ratio = 15

; Tier 4
non_evergreen_year_pattern = 20[12]\d ; e.g., 2010-2029
min_keyword_word_count = 2
max_keyword_word_count = 8
hostile_serp_features = shopping,local_pack,google_flights,google_hotels,popular_products,local_services
crowded_serp_features_threshold = 4
min_serp_stability_days = 14
max_y_pixel_threshold = 800
max_forum_results_in_top_10 = 3
max_ecommerce_results_in_top_10 = 2
disallowed_page_types_in_top_3 = E-commerce,Forum

[FINAL_VALIDATION]
; Thresholds for the final, live SERP validation gate before running a full analysis.
disable_ai_overview_check = true
max_non_blog_results = 3
max_ai_overview_words = 250
max_first_organic_y_pixel = 1500
final_validation_non_blog_domains = amazon.com,walmart.com,ebay.com,reddit.com,quora.com
max_avg_lcp_time = 4000 ; NEW: For Rule 21 in run_final_validation

[ANALYSIS]
enable_deep_competitor_analysis = false
num_competitors_for_ai_analysis = 3
serp_analysis_depth = 100
max_words_for_ai_analysis = 2000
serp_remove_from_url_params = srsltid,utm_source,ref_id ; ADD THIS (W11 Default)

[WORD_COUNT_MULTIPLIERS]
default_multiplier = 1.2
comprehensive_article = 1.3
how_to_guide = 1.5
comparison_post = 1.1
review_article = 1.2
video_led_article = 0.8
forum_summary_post = 1.0
recipe_article = 1.0
scholarly_summary = 1.1
product_comparison = 1.2

[OPENAI_PRICING]
# Prices are per 1 million tokens
gpt-4o_input = 5.00
gpt-4o_output = 15.00
gpt-3.5-turbo_input = 0.50
gpt-3.5-turbo_output = 1.50

[OpenAI]
default_model = gpt-5-nano
default_image_model = dall-e-3
api_key = ${OPENAI_API_KEY}
</file>

<file path="backend/core/serp_analyzers/disqualification_analyzer.py">
# core/serp_analyzers/disqualification_analyzer.py
from typing import Dict, Any


class DisqualificationAnalyzer:
    def analyze(
        self, analysis: Dict[str, Any], config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Applies a set of granular rules to determine if a keyword should be disqualified.
        """
        disqualification_rules = config.get("disqualification_rules", {})

        # Rule: SERP is too crowded (pixel ranking)
        max_y_pixel = disqualification_rules.get("max_y_pixel_threshold")
        if max_y_pixel and analysis.get("first_organic_y_pixel") is not None:
            if analysis["first_organic_y_pixel"] > max_y_pixel:
                return {
                    "is_disqualified": True,
                    "disqualification_reason": f"First organic result is pushed down by {analysis['first_organic_y_pixel']} pixels, exceeding the {max_y_pixel}px threshold.",
                }

        # Rules based on page types in top 10
        top_10_results = analysis.get("top_organic_results", [])[:10]
        page_types = [result.get("page_type") for result in top_10_results]

        max_forum_results = disqualification_rules.get("max_forum_results_in_top_10")
        if (
            max_forum_results is not None
            and page_types.count("Forum") > max_forum_results
        ):
            return {
                "is_disqualified": True,
                "disqualification_reason": f"SERP contains {page_types.count('Forum')} forum results in the top 10, exceeding the threshold of {max_forum_results}.",
            }

        max_ecommerce_results = disqualification_rules.get(
            "max_ecommerce_results_in_top_10"
        )
        if (
            max_ecommerce_results is not None
            and page_types.count("E-commerce") > max_ecommerce_results
        ):
            return {
                "is_disqualified": True,
                "disqualification_reason": f"SERP contains {page_types.count('E-commerce')} e-commerce results in the top 10, exceeding the threshold of {max_ecommerce_results}.",
            }

        # Rule: Disallowed page types in top 3
        disallowed_in_top_3 = disqualification_rules.get(
            "disallowed_page_types_in_top_3", []
        )
        if disallowed_in_top_3:
            top_3_page_types = [
                result.get("page_type") for result in top_10_results[:3]
            ]
            for page_type in disallowed_in_top_3:
                if page_type in top_3_page_types:
                    return {
                        "is_disqualified": True,
                        "disqualification_reason": f"A '{page_type}' result was found in the top 3, which is a disallowed page type for high-ranking positions.",
                    }

        return {"is_disqualified": False, "disqualification_reason": None}
</file>

<file path="backend/core/serp_analyzers/featured_snippet_analyzer.py">
# core/serp_analyzers/featured_snippet_analyzer.py

from typing import Dict, Any


class FeaturedSnippetAnalyzer:
    def analyze(self, serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyzes the featured snippet in the SERP."""
        analysis = {
            "serp_has_featured_snippet": "featured_snippet"
            in serp_data.get("item_types", []),
            "featured_snippet_content": None,
        }

        for item in serp_data.get("items", []):
            if item.get("type") == "featured_snippet":
                analysis["featured_snippet_content"] = item.get("description")
                break

        return analysis
</file>

<file path="backend/core/serp_analyzers/pixel_ranking_analyzer.py">
# core/serp_analyzers/pixel_ranking_analyzer.py

from typing import Dict, Any


class PixelRankingAnalyzer:
    def analyze(self, serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyzes the pixel ranking data in the SERP."""
        analysis = {
            "pixel_ranking_summary": None,
            "raw_pixel_ranking_data": [],
            "first_organic_y_pixel": None,
        }

        for item in serp_data.get("items", []):
            if item.get("rectangle"):
                analysis["raw_pixel_ranking_data"].append(
                    {
                        "type": item.get("type"),
                        "rank_group": item.get("rank_group"),
                        "rank_absolute": item.get("rank_absolute"),
                        "title": item.get("title"),
                        "rectangle": item.get("rectangle"),
                    }
                )

        if analysis["raw_pixel_ranking_data"]:
            top_organic_rects_y_coords = [
                r["rectangle"]["y"]
                for r in analysis["raw_pixel_ranking_data"]
                if r["type"] == "organic"
                and r.get("rank_absolute", 99) <= 3
                and "y" in r.get("rectangle", {})
            ]
            if top_organic_rects_y_coords:
                avg_y = sum(top_organic_rects_y_coords) / len(
                    top_organic_rects_y_coords
                )
                analysis["pixel_ranking_summary"] = (
                    f"Top 3 organic results start an average of {avg_y:.0f} pixels from the top of the page."
                )

        first_organic_result = next(
            (
                r
                for r in analysis["raw_pixel_ranking_data"]
                if r["type"] == "organic" and r.get("rank_absolute") == 1
            ),
            None,
        )
        if first_organic_result and "y" in first_organic_result.get("rectangle", {}):
            analysis["first_organic_y_pixel"] = first_organic_result["rectangle"]["y"]

        return analysis
</file>

<file path="backend/core/serp_analyzers/video_analyzer.py">
# core/serp_analyzers/video_analyzer.py

from typing import Dict, Any


class VideoAnalyzer:
    def analyze(self, serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyzes the video results in the SERP."""
        analysis = {
            "serp_has_video_results": "video" in serp_data.get("item_types", [])
        }

        return analysis
</file>

<file path="backend/core/__init__.py">
# core/__init__.py
# This file marks the directory as a Python package.
</file>

<file path="backend/core/blueprint_factory.py">
import json
from datetime import datetime
from typing import Dict, Any
import logging

from backend.agents.brief_assembler import BriefAssembler
from backend.agents.internal_linking_suggester import InternalLinkingSuggester
from backend.core import utils
from backend.data_access.database_manager import DatabaseManager


class BlueprintFactory:
    def __init__(
        self, openai_client, client_cfg, dataforseo_client, db_manager: DatabaseManager
    ):
        self.brief_assembler = BriefAssembler(openai_client)
        self.internal_linking_suggester = InternalLinkingSuggester(
            openai_client, client_cfg, db_manager
        )
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = client_cfg

    def _create_executive_summary(self, blueprint_data: Dict[str, Any]) -> str:
        # Placeholder until SummaryGenerator is integrated here
        return "The executive summary will be generated by the AI based on the full analysis when implementation is complete."

    def create_blueprint(
        self,
        seed_topic: str,
        winning_keyword_data: Dict[str, Any],
        analysis_data: Dict[str, Any],
        total_api_cost: float,
        client_id: str,
    ) -> Dict[str, Any]:
        """Assembles all data into the final, structured JSON blueprint."""

        analysis_notes = None
        competitor_analysis_data = analysis_data.get("competitor_analysis", [])
        if not competitor_analysis_data or (
            len(competitor_analysis_data) == 1
            and "message" in competitor_analysis_data[0]
        ):
            analysis_notes = "No qualified article-based competitors were found in the top results after rigorous qualification. This SERP may be dominated by social media, video, or other non-article formats, making it a challenging topic to rank for with a standard blog post."
            competitor_analysis_data = []  # Ensure it's always an empty list of competitors

        recommended_strategy_data = analysis_data.get("recommended_strategy", {})
        self.logger.info(
            f"STRATEGY DATA FOR BLUEPRINT: {json.dumps(recommended_strategy_data)}"
        )

        blueprint_data = {
            "metadata": {
                "seed_topic": seed_topic,
                "blueprint_version": "6.0",
                "generated_at": datetime.now().isoformat(),
                "total_api_cost": round(total_api_cost, 4),
                "client_id": client_id,
            },
            "winning_keyword": winning_keyword_data,
            "serp_overview": analysis_data.get("serp_overview", {}),
            "content_intelligence": analysis_data.get("content_intelligence", {}),
            "competitor_analysis": competitor_analysis_data,
            "recommended_strategy": recommended_strategy_data,
            "final_qualification_assessment": recommended_strategy_data.get(
                "final_qualification_assessment", {}
            ),
            "analysis_notes": analysis_notes,
        }

        blueprint_data["executive_summary"] = self._create_executive_summary(
            blueprint_data
        )

        # --- START MODIFICATION ---
        # Pass rich serp_overview data to brief_assembler
        blueprint_data["ai_content_brief"] = self.brief_assembler.assemble_brief(
            blueprint_data, client_id, self.client_cfg
        )

        brief_text_for_linking = json.dumps(blueprint_data["ai_content_brief"])
        target_domain = self.client_cfg.get("target_domain")
        key_entities = blueprint_data.get("ai_content_brief", {}).get(
            "key_entities_to_mention", []
        )

        if brief_text_for_linking and target_domain:
            suggestions, linking_cost = self.internal_linking_suggester.suggest_links(
                brief_text_for_linking, key_entities, target_domain, client_id
            )
            blueprint_data["internal_linking_suggestions"] = suggestions
            blueprint_data["metadata"]["total_api_cost"] = round(
                blueprint_data["metadata"]["total_api_cost"] + linking_cost, 4
            )

        keyword_for_slug = winning_keyword_data.get("keyword", seed_topic)
        # Ensure the slug is part of the blueprint
        opportunity_slug = (
            f"{utils.slugify(keyword_for_slug)}-{int(datetime.now().timestamp())}"
        )
        blueprint_data["slug"] = opportunity_slug
        # --- END MODIFICATION ---

        return blueprint_data
</file>

<file path="backend/core/page_classifier.py">
# core/page_classifier.py
import re
from typing import Dict, Any
from urllib.parse import urlparse


class PageClassifier:
    """
    Categorizes a webpage based on its URL, domain, title, and other attributes.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config.get("page_classification", {})
        self.forum_domains = self.config.get("forum_domains", [])
        self.ecommerce_domains = self.config.get("ecommerce_domains", [])
        self.news_domains = self.config.get("news_domains", [])

        # Pre-compile regex for efficiency
        self.blog_patterns = [
            re.compile(p) for p in self.config.get("blog_url_patterns", [])
        ]
        self.forum_patterns = [
            re.compile(p) for p in self.config.get("forum_url_patterns", [])
        ]

    def classify(self, url: str, domain: str, title: str) -> str:
        """
        Classifies the given URL into a specific page type.

        Args:
            url: The full URL of the page.
            domain: The domain of the page.
            title: The title of the page.

        Returns:
            A string representing the classified page type.
        """
        if domain in self.ecommerce_domains:
            return "E-commerce"
        if domain in self.forum_domains:
            return "Forum"
        if domain in self.news_domains:
            return "News"

        # Check URL patterns for more specific types
        parsed_url = urlparse(url)
        path = parsed_url.path

        for pattern in self.forum_patterns:
            if pattern.search(path) or pattern.search(title.lower()):
                return "Forum"

        for pattern in self.blog_patterns:
            if pattern.search(path):
                return "Blog/Article"

        # Check for homepage/landing page (short path)
        if len(path.strip("/").split("/")) <= 1:
            return "Homepage/Landing Page"

        return "Blog/Article"  # Default category
</file>

<file path="backend/core/serp_analyzer.py">
import logging
from typing import Dict, Any, Tuple, Optional
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from core import utils
from core.serp_analyzers.featured_snippet_analyzer import FeaturedSnippetAnalyzer
from core.serp_analyzers.video_analyzer import VideoAnalyzer
from core.serp_analyzers.pixel_ranking_analyzer import PixelRankingAnalyzer
from core.page_classifier import PageClassifier
from core.serp_analyzers.disqualification_analyzer import DisqualificationAnalyzer


class FullSerpAnalyzer:
    """
    Performs a comprehensive analysis of the SERP for a given keyword.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.featured_snippet_analyzer = FeaturedSnippetAnalyzer()
        self.video_analyzer = VideoAnalyzer()
        self.pixel_ranking_analyzer = PixelRankingAnalyzer()
        self.page_classifier = PageClassifier(config)
        self.disqualification_analyzer = DisqualificationAnalyzer()

    def analyze_serp(self, keyword: str) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Fetches SERP data and extracts a wide range of insights, including rich SERP elements.
        """
        high_cost_operators = [
            "allinanchor:",
            "allintext:",
            "allintitle:",
            "allinurl:",
            "define:",
            "filetype:",
            "id:",
            "inanchor:",
            "info:",
            "intext:",
            "intitle:",
            "inurl:",
            "link:",
            "site:",
        ]
        keyword_lower = keyword.lower()

        if any(op in keyword_lower for op in high_cost_operators):
            raise ValueError(
                f"Keyword '{keyword}' contains a high-cost search operator. Please remove it and try again."
            )

        location_code = self.config.get("location_code")
        language_code = self.config.get("language_code")
        serp_call_params = {}

        serp_call_params["depth"] = 10  # Default to depth 10 for quick fetch

        paa_click_depth = self.config.get("people_also_ask_click_depth", 0)
        if isinstance(paa_click_depth, int) and 1 <= paa_click_depth <= 4:
            serp_call_params["people_also_ask_click_depth"] = paa_click_depth

        device = self.config.get("device", "desktop")
        os_name = self.config.get("os", "windows")
        if device == "mobile" and os_name not in ["android", "ios"]:
            os_name = "android"
        serp_call_params["device"] = device
        serp_call_params["os"] = os_name

        serp_results, cost = self.client.get_serp_results(
            keyword,
            location_code,
            language_code,
            client_cfg=self.config,
            serp_call_params=serp_call_params,
        )

        if not serp_results:
            return None, cost

        serp_times = utils.calculate_serp_times(
            serp_results.get("datetime"), serp_results.get("previous_updated_time")
        )

        analysis = {
            "serp_has_ai_overview": "ai_overview" in serp_results.get("item_types", []),
            "has_popular_products": "popular_products"
            in serp_results.get("item_types", []),
            "people_also_ask": [],  # Kept for backward compatibility, new field below is preferred
            "paa_questions": [],  # Primary field for cleaned PAA questions
            "top_organic_results": [],
            "related_searches": [],
            "knowledge_graph_data": {},  # Kept for backward compatibility
            "knowledge_graph_facts": [],  # NEW: Structured facts from KG
            "paid_ad_copy": [],  # NEW: Top paid ad titles/descriptions
            "ai_overview_content": None,
            "ai_overview_sources": [],  # NEW: URLs from AI Overview references
            "top_organic_faqs": [],  # NEW: FAQ questions from organic results
            "top_organic_sitelinks": [],  # NEW: Sitelinks from organic results
            "discussion_snippets": [],  # NEW: Snippets from discussions/forums/perspectives
            "product_considerations_summary": None,
            "refinement_chips": [],
            "extracted_serp_features": [],
            "serp_last_updated_days_ago": serp_times.get("days_ago"),
            "serp_update_interval_days": serp_times.get("update_interval_days"),
            "dominant_content_format": "Article",
        }

        analysis.update(self.featured_snippet_analyzer.analyze(serp_results))
        analysis.update(self.video_analyzer.analyze(serp_results))
        analysis.update(self.pixel_ranking_analyzer.analyze(serp_results))

        for item in serp_results.get("items") or []:
            item_type = item.get("type")

            # Organic Results
            if item_type == "organic":
                organic_result = {
                    "rank": item.get("rank_absolute"),
                    "url": item.get("url"),
                    "title": item.get("title"),
                    "domain": item.get("domain"),
                    "description": item.get("description"),
                    "page_type": self.page_classifier.classify(
                        item.get("url"), item.get("domain"), item.get("title")
                    ),
                }
                if item.get("rating"):
                    organic_result["rating"] = {
                        "value": item["rating"].get("value"),
                        "votes_count": item["rating"].get("votes_count"),
                        "rating_max": item["rating"].get("rating_max"),
                    }
                if item.get("about_this_result"):
                    organic_result["about_this_result_source_info"] = item[
                        "about_this_result"
                    ].get("source_info")
                    organic_result["about_this_result_search_terms"] = item[
                        "about_this_result"
                    ].get("search_terms")
                    organic_result["about_this_result_related_terms"] = item[
                        "about_this_result"
                    ].get("related_terms")

                # NEW: Extract FAQ and Sitelinks directly from organic results
                if item.get("faq") and item["faq"].get("items"):
                    analysis["top_organic_faqs"].extend(
                        [
                            faq_item.get("title")
                            for faq_item in item["faq"]["items"]
                            if faq_item.get("title")
                        ]
                    )
                if item.get("links"):
                    analysis["top_organic_sitelinks"].extend(
                        [
                            link.get("title")
                            for link in item["links"]
                            if link.get("title")
                        ]
                    )

                analysis["top_organic_results"].append(organic_result)

            # Paid Ads
            elif item_type == "paid":
                if item.get("title") and item.get("description"):
                    analysis["paid_ad_copy"].append(
                        {
                            "title": item.get("title"),
                            "description": item.get("description"),
                            "url": item.get("url"),
                        }
                    )

            # People Also Ask (PAA)
            elif item_type == "people_also_ask":
                all_paa_questions = []
                for paa_item in item.get("items") or []:
                    if paa_item and paa_item.get("title"):
                        (all_paa_questions.append(paa_item.get("title")),)
                    if paa_item and paa_item.get("expanded_element"):
                        for expanded_item in paa_item.get("expanded_element") or []:
                            if expanded_item and expanded_item.get("title"):
                                (all_paa_questions.append(expanded_item.get("title")),)
                analysis["paa_questions"] = list(set(all_paa_questions))
                analysis["people_also_ask"] = analysis[
                    "paa_questions"
                ]  # For backward compatibility

            # Knowledge Graph
            elif item_type == "knowledge_graph":
                analysis["knowledge_graph_data"] = {  # For backward compatibility
                    "title": item.get("title"),
                    "description": item.get("description"),
                    "url": item.get("url"),
                    "image_url": item.get("image_url"),
                }
                # NEW: Deep parse Knowledge Graph structured items
                if item.get("items"):
                    for kg_sub_item in item["items"]:
                        if (
                            kg_sub_item
                            and kg_sub_item.get("type") == "knowledge_graph_row_item"
                        ):
                            analysis["knowledge_graph_facts"].append(
                                f"{kg_sub_item.get('title')}: {kg_sub_item.get('text')}"
                            )
                        elif (
                            kg_sub_item
                            and kg_sub_item.get("type")
                            == "knowledge_graph_carousel_item"
                            and kg_sub_item.get("items")
                        ):
                            analysis["knowledge_graph_facts"].extend(
                                [
                                    carousel_el.get("title")
                                    for carousel_el in kg_sub_item["items"]
                                    if carousel_el.get("title")
                                ]
                            )
                        elif (
                            kg_sub_item
                            and kg_sub_item.get("type") == "knowledge_graph_list_item"
                            and kg_sub_item.get("items")
                        ):
                            analysis["knowledge_graph_facts"].extend(
                                [
                                    list_el.get("title")
                                    for list_el in kg_sub_item["items"]
                                    if list_el.get("title")
                                ]
                            )

            # AI Overview
            elif item_type == "ai_overview":
                ai_items = item.get("items") or []
                ai_parts = [
                    sub_item.get("markdown")
                    for sub_item in ai_items
                    if sub_item and sub_item.get("markdown")
                ]
                analysis["ai_overview_content"] = "\n".join(ai_parts)
                # NEW: Extract AI Overview References
                for sub_item in ai_items:
                    if sub_item and sub_item.get("references"):
                        analysis["ai_overview_sources"].extend(
                            [
                                ref.get("url")
                                for ref in sub_item["references"]
                                if ref.get("url")
                            ]
                        )

            # Discussions and Forums / Perspectives
            elif item_type in [
                "discussions_and_forums",
                "perspectives",
            ]:  # Combined handling
                if item.get("items"):
                    analysis["discussion_snippets"].extend(
                        [
                            d_item.get("title")
                            for d_item in item["items"]
                            if d_item.get("title")
                        ]
                    )

            # Related Searches
            elif item_type == "related_searches":
                related_items = item.get("items") or []
                for s in related_items:
                    if isinstance(s, str):
                        analysis["related_searches"].append(s)
                    elif isinstance(s, dict) and s.get("title"):
                        (analysis["related_searches"].append(s.get("title")),)

            # Product Considerations (existing)
            elif item_type == "product_considerations":
                title = item.get("title")
                items = item.get("items") or []
                if title and items:
                    considerations = [
                        sub.get("title") for sub in items if sub and sub.get("title")
                    ]
                    analysis["product_considerations_summary"] = (
                        f"{title}: {', '.join(considerations)}"
                    )

        # Deduplicate all lists
        analysis["ai_overview_sources"] = list(set(analysis["ai_overview_sources"]))
        analysis["top_organic_faqs"] = list(set(analysis["top_organic_faqs"]))
        analysis["top_organic_sitelinks"] = list(set(analysis["top_organic_sitelinks"]))
        analysis["discussion_snippets"] = list(set(analysis["discussion_snippets"]))

        # Determine dominant content format (existing logic)
        # ...

        disqualification_results = self.disqualification_analyzer.analyze(
            analysis, self.config
        )
        analysis.update(disqualification_results)

        return analysis, cost
</file>

<file path="backend/core/utils.py">
# core/utils.py
import logging
import re
from typing import Optional, Union, Dict
from datetime import datetime


def slugify(text: str) -> str:
    """
    Convert a string to a URL-friendly slug.
    """
    if not text:
        return ""
    text = text.lower()
    # Remove special characters
    text = re.sub(r"[^\w\s-]", "", text)
    # Replace spaces with hyphens
    text = re.sub(r"\s+", "-", text)
    return text


def is_question_keyword(keyword: str) -> bool:
    """
    Checks if a keyword is likely a question.
    Covers common question formats and leading words.
    """
    if not keyword:
        return False

    keyword_lower = keyword.lower().strip()

    # Common question prefixes
    question_starters = [
        "what",
        "when",
        "where",
        "who",
        "why",
        "how",
        "which",
        "whose",
        "is",
        "are",
        "am",
        "was",
        "were",
        "do",
        "does",
        "did",
        "can",
        "could",
        "will",
        "would",
        "should",
        "may",
        "might",
        "have",
        "has",
        "had",
        "are there",
        "is there",
    ]

    # Check if the keyword starts with a question word or ends with a question mark
    if keyword_lower.endswith("?"):
        return True

    for starter in question_starters:
        if keyword_lower.startswith(starter + " "):
            return True

    return False


def safe_compare(
    value: Optional[Union[int, float]],
    threshold: Optional[Union[int, float]],
    operation: str,
) -> bool:
    """
    Safely compares a potentially None value against a potentially None threshold.
    Returns False if either value is None to prevent TypeErrors.

    :param value: The value to check (e.g., from API data).
    :param threshold: The threshold to compare against (e.g., from config).
    :param operation: The comparison to perform ('gt' for >, 'lt' for <).
    :return: Boolean result of the comparison, or False if unsafe.
    """
    if value is None or threshold is None:
        return False

    if operation == "gt":
        return value > threshold
    elif operation == "lt":
        return value < threshold

    return False


def parse_datetime_string(dt_str: Optional[str]) -> Optional[str]:
    """
    Parses a DataForSEO datetime string (e.g., "yyyy-mm-dd hh-mm-ss +00:00")
    into a consistent ISO format string or returns None.
    """
    if not dt_str:
        return None

    # Remove timezone offset for consistent parsing if it's always +00:00
    cleaned_dt_str = dt_str.replace(" +00:00", "").strip()

    formats = [
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%dT%H:%M:%S",  # Added ISO 8601 format
        "%Y-%m-%d %H:%M:%S.%f",  # With microseconds
        "%Y-%m-%d",  # Date only
    ]

    for fmt in formats:
        try:
            return datetime.strptime(cleaned_dt_str, fmt).isoformat()
        except ValueError:
            pass

    logging.getLogger(__name__).warning(
        f"Could not parse datetime string: {dt_str}. Returning None."
    )
    return None


def calculate_serp_times(
    datetime_str: Optional[str], previous_datetime_str: Optional[str]
) -> Dict[str, Optional[int]]:
    """
    Calculates the age of the SERP and the interval between the last two updates.
    """
    days_ago = None
    update_interval_days = None

    if datetime_str:
        parsed_date_iso = parse_datetime_string(datetime_str)
        if parsed_date_iso:
            serp_date = datetime.fromisoformat(parsed_date_iso)
            days_ago = (datetime.utcnow() - serp_date).days
        else:
            logging.getLogger(__name__).warning(
                f"Could not parse SERP datetime for days_ago: {datetime_str}"
            )

    if datetime_str and previous_datetime_str:
        parsed_last_update_iso = parse_datetime_string(datetime_str)
        parsed_prev_update_iso = parse_datetime_string(previous_datetime_str)

        if parsed_last_update_iso and parsed_prev_update_iso:
            last_update_dt = datetime.fromisoformat(parsed_last_update_iso)
            prev_update_dt = datetime.fromisoformat(parsed_prev_update_iso)
            update_interval_days = abs((last_update_dt - prev_update_dt).days)
        else:
            logging.getLogger(__name__).warning(
                f"Could not parse SERP previous update times for interval: {datetime_str}, {previous_datetime_str}"
            )

    return {"days_ago": days_ago, "update_interval_days": update_interval_days}
</file>

<file path="backend/data_access/migrations/001_add_new_tables.sql">
-- data_access/migrations/001_add_new_tables.sql

CREATE TABLE keyword_info (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    search_volume INTEGER,
    keyword_difficulty INTEGER,
    cpc REAL,
    competition REAL,
    search_volume_trend TEXT,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);

CREATE TABLE serp_overview (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    serp_has_featured_snippet BOOLEAN,
    serp_has_video_results BOOLEAN,
    serp_has_ai_overview BOOLEAN,
    people_also_ask TEXT,
    ai_overview_content TEXT,
    featured_snippet_content TEXT,
    avg_referring_domains_top5_organic REAL,
    avg_main_domain_rank_top5_organic REAL,
    serp_last_updated_days_ago INTEGER,
    dominant_content_format TEXT,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);
</file>

<file path="backend/data_access/migrations/001_initial_schema.sql">
-- data_access/migrations/001_initial_schema.sql
-- This script sets up the initial schema for existing tables.
-- It should only be run if the tables do not exist.
-- The database_manager already creates them, so this is mainly for tracking.

-- CREATE TABLE IF NOT EXISTS opportunities (
--     id INTEGER PRIMARY KEY AUTOINCREMENT,
--     keyword TEXT NOT NULL,
--     status TEXT NOT NULL DEFAULT 'pending',
--     client_id TEXT NOT NULL DEFAULT 'default',
--     date_added TEXT NOT NULL,
--     date_processed TEXT,
--     full_data TEXT NOT NULL,
--     blueprint_data TEXT,
--     ai_content_json TEXT,
--     ai_content_model TEXT,
--     featured_image_url TEXT,
--     featured_image_local_path TEXT,
--     in_article_images_data TEXT,
--     social_media_posts_json TEXT,
--     last_workflow_step TEXT,
--     error_message TEXT,
--     wordpress_payload_json TEXT,
--     final_package_json TEXT,
--     UNIQUE(keyword, client_id)
-- );

-- CREATE TABLE IF NOT EXISTS clients (
--     client_id TEXT PRIMARY KEY,
--     client_name TEXT NOT NULL,
--     date_created TEXT NOT NULL
-- );

-- CREATE TABLE IF NOT EXISTS client_settings (
--     client_id TEXT PRIMARY KEY,
--     openai_api_key TEXT,
--     pexels_api_key TEXT,
--     location_code INTEGER,
--     language_code TEXT,
--     target_domain TEXT,
--     device TEXT,
--     os TEXT,
--     informational_score REAL,
--     commercial_score REAL,
--     transactional_score REAL,
--     navigational_score REAL,
--     question_keyword_bonus REAL,
--     ease_of_ranking_weight INTEGER,
--     traffic_potential_weight INTEGER,
--     commercial_intent_weight INTEGER,
--     growth_trend_weight INTEGER,
--     serp_features_weight INTEGER,
--     serp_freshness_weight INTEGER,
--     serp_volatility_weight INTEGER,
--     competitor_weakness_weight INTEGER,
--     max_cpc_for_scoring REAL,
--     max_sv_for_scoring INTEGER,
--     max_domain_rank_for_scoring INTEGER,
--     max_referring_domains_for_scoring INTEGER,
--     max_avg_referring_domains_filter INTEGER,
--     featured_snippet_bonus REAL,
--     ai_overview_bonus REAL,
--     serp_freshness_bonus_max REAL,
--     serp_freshness_old_threshold_days INTEGER,
--     serp_volatility_stable_threshold_days INTEGER,
--     enforce_intent_filter INTEGER,
--     allowed_intents TEXT,
--     require_question_keywords INTEGER,
--     negative_keywords TEXT,
--     min_monthly_trend_percentage REAL,
--     min_competitor_word_count INTEGER,
--     max_competitor_technical_warnings INTEGER,
--     competitor_blacklist_domains TEXT,
--     ugc_and_parasite_domains TEXT,
--     num_competitors_to_analyze INTEGER,
--     num_common_headings INTEGER,
--     num_unique_angles INTEGER,
--     max_initial_serp_urls_to_analyze INTEGER,
--     calculate_rectangles INTEGER,
--     people_also_ask_click_depth INTEGER,
--     min_search_volume INTEGER,
--     max_keyword_difficulty INTEGER,
--     ai_content_model TEXT,
--     num_in_article_images INTEGER,
--     use_pexels_first INTEGER,
--     cleanup_local_images INTEGER,
--     onpage_enable_javascript INTEGER,
--     onpage_load_resources INTEGER,
--     onpage_disable_cookie_popup INTEGER,
--     onpage_return_despite_timeout INTEGER,
--     onpage_enable_browser_rendering INTEGER,
--     onpage_store_raw_html INTEGER,
--     onpage_validate_micromarkup INTEGER,
--     onpage_custom_user_agent TEXT,
--     onpage_max_domains_per_request INTEGER,
--     onpage_max_tasks_per_request INTEGER,
--     platforms TEXT,
--     custom_prompt_template TEXT,
--     wordpress_url TEXT,
--     wordpress_user TEXT,
--     wordpress_app_password TEXT,
--     wordpress_seo_plugin TEXT,
--     default_wordpress_categories TEXT,
--     default_wordpress_tags TEXT,
--     enable_automated_internal_linking INTEGER,
--     db_type TEXT,
--     max_words_for_ai_analysis INTEGER,
--     ai_generation_temperature REAL,
--     recommended_word_count_multiplier REAL,
--     max_avg_lcp_time INTEGER,
--     prohibited_intents TEXT,
--     last_updated TEXT NOT NULL,
--     FOREIGN KEY (client_id) REFERENCES clients (client_id)
-- );

-- CREATE TABLE IF NOT EXISTS discovery_runs (
--     id INTEGER PRIMARY KEY AUTOINCREMENT,
--     client_id TEXT NOT NULL,
--     start_time TEXT NOT NULL,
--     end_time TEXT,
--     status TEXT NOT NULL,
--     parameters TEXT,
--     results_summary TEXT,
--     log_file_path TEXT,
--     error_message TEXT,
--     FOREIGN KEY (client_id) REFERENCES clients (client_id)
-- );

-- These are initially created by `database_manager.py` before migrations start.
-- This file exists for migration tracking purposes.
</file>

<file path="backend/data_access/migrations/002_add_keywords_table.sql">
-- data_access/migrations/011_add_keywords_table.sql

CREATE TABLE keywords (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    keyword TEXT NOT NULL UNIQUE,
    search_volume INTEGER,
    keyword_difficulty INTEGER,
    cpc REAL,
    competition REAL,
    search_volume_trend TEXT,
    main_intent TEXT,
    core_keyword TEXT
);

ALTER TABLE opportunities ADD COLUMN keyword_id INTEGER REFERENCES keywords(id);
CREATE INDEX IF NOT EXISTS idx_opportunities_keyword_id ON opportunities (keyword_id);
</file>

<file path="backend/data_access/migrations/002_remove_json_columns.sql">
-- data_access/migrations/002_remove_json_columns.sql

-- This migration is intentionally left blank.
-- SQLite does not support dropping columns.
-- The data from the JSON columns will be migrated to the new tables in the application logic.
-- The old columns will be ignored by the application.
</file>

<file path="backend/data_access/migrations/003_add_indexes.sql">
-- data_access/migrations/003_add_indexes.sql

CREATE INDEX IF NOT EXISTS idx_opportunities_status ON opportunities (status);
CREATE INDEX IF NOT EXISTS idx_opportunities_client_id ON opportunities (client_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_strategic_score ON opportunities (strategic_score);
CREATE INDEX IF NOT EXISTS idx_opportunities_run_id ON opportunities (run_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_keyword_id ON opportunities (keyword_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_slug ON opportunities (slug);
</file>

<file path="backend/data_access/migrations/003_add_total_api_cost.sql">
-- Add a column to store the total API cost for the entire workflow
ALTER TABLE opportunities ADD COLUMN total_api_cost REAL DEFAULT 0.0;
</file>

<file path="backend/data_access/migrations/004_add_qualification_settings_table.sql">
-- data_access/migrations/004_add_qualification_settings_table.sql

CREATE TABLE qualification_settings (
    client_id TEXT PRIMARY KEY,
    ease_of_ranking_weight REAL,
    traffic_potential_weight REAL,
    commercial_intent_weight REAL,
    serp_features_weight REAL,
    growth_trend_weight REAL,
    serp_freshness_weight REAL,
    serp_volatility_weight REAL,
    competitor_weakness_weight REAL,
    competitor_performance_weight REAL,
    informational_intent_weight REAL,
    navigational_intent_weight REAL,
    transactional_intent_weight REAL,
    competitor_strength_weight REAL,
    trend_weight REAL,
    seasonality_weight REAL,
    review_threshold REAL,
    disqualification_rules TEXT,
    brand_keywords TEXT,
    competitor_brand_keywords TEXT,
    min_search_volume INTEGER,
    max_keyword_difficulty INTEGER,
    negative_keywords TEXT,
    prohibited_intents TEXT,
    max_y_pixel_threshold INTEGER,
    max_forum_results_in_top_10 INTEGER,
    max_ecommerce_results_in_top_10 INTEGER,
    disallowed_page_types_in_top_3 TEXT,
    FOREIGN KEY (client_id) REFERENCES clients (client_id)
);
</file>

<file path="backend/data_access/migrations/005_add_qualification_columns.sql">
-- data_access/migrations/005_add_qualification_columns.sql

ALTER TABLE opportunities ADD COLUMN strategic_score REAL;
ALTER TABLE opportunities ADD COLUMN score_breakdown TEXT;
ALTER TABLE opportunities ADD COLUMN qualification_status TEXT;
ALTER TABLE opportunities ADD COLUMN qualification_reason TEXT;
</file>

<file path="backend/data_access/migrations/006_add_intent_weights.sql">
-- data_access/migrations/006_add_intent_weights.sql

ALTER TABLE qualification_settings ADD COLUMN informational_intent_weight REAL;
ALTER TABLE qualification_settings ADD COLUMN navigational_intent_weight REAL;
ALTER TABLE qualification_settings ADD COLUMN commercial_intent_weight REAL;
ALTER TABLE qualification_settings ADD COLUMN transactional_intent_weight REAL;
</file>

<file path="backend/data_access/migrations/007_add_competitor_strength_weight.sql">
-- data_access/migrations/007_add_competitor_strength_weight.sql

ALTER TABLE qualification_settings ADD COLUMN competitor_strength_weight REAL;
</file>

<file path="backend/data_access/migrations/008_add_serp_features_weight.sql">
-- data_access/migrations/008_add_serp_features_weight.sql

ALTER TABLE qualification_settings ADD COLUMN serp_features_weight REAL;
</file>

<file path="backend/data_access/migrations/009_add_trend_weight.sql">
-- data_access/migrations/009_add_trend_weight.sql

ALTER TABLE qualification_settings ADD COLUMN trend_weight REAL;
</file>

<file path="backend/data_access/migrations/010_add_history_columns.sql">
-- data_access/migrations/010_add_history_columns.sql

ALTER TABLE opportunities ADD COLUMN last_seen_at TEXT;
ALTER TABLE opportunities ADD COLUMN metrics_history TEXT;
</file>

<file path="backend/data_access/migrations/012_add_seasonality_weight.sql">
-- data_access/migrations/012_add_seasonality_weight.sql

ALTER TABLE qualification_settings ADD COLUMN seasonality_weight REAL;
</file>

<file path="backend/data_access/migrations/013_add_serp_volatility_weight.sql">
-- data_access/migrations/013_add_serp_volatility_weight.sql

ALTER TABLE qualification_settings ADD COLUMN serp_volatility_weight REAL;
</file>

<file path="backend/data_access/migrations/014_add_disqualification_rules.sql">
-- data_access/migrations/014_add_disqualification_rules.sql

ALTER TABLE qualification_settings ADD COLUMN disqualification_rules TEXT;
</file>

<file path="backend/data_access/migrations/015_add_brand_keywords.sql">
-- data_access/migrations/015_add_brand_keywords.sql

ALTER TABLE qualification_settings ADD COLUMN brand_keywords TEXT;
ALTER TABLE qualification_settings ADD COLUMN competitor_brand_keywords TEXT;
</file>

<file path="backend/data_access/migrations/016_add_review_threshold.sql">
-- data_access/migrations/016_add_review_threshold.sql

ALTER TABLE qualification_settings ADD COLUMN review_threshold REAL;
</file>

<file path="backend/data_access/migrations/017_add_strategies_table.sql">
-- data_access/migrations/017_add_strategies_table.sql

CREATE TABLE qualification_strategies (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    client_id TEXT NOT NULL,
    name TEXT NOT NULL,
    traffic_potential_weight REAL,
    keyword_difficulty_weight REAL,
    cpc_weight REAL,
    search_intent_weight REAL,
    min_search_volume INTEGER,
    max_keyword_difficulty INTEGER,
    negative_keywords TEXT,
    prohibited_intents TEXT,
    informational_intent_weight REAL,
    navigational_intent_weight REAL,
    commercial_intent_weight REAL,
    transactional_intent_weight REAL,
    competitor_strength_weight REAL,
    serp_features_weight REAL,
    trend_weight REAL,
    seasonality_weight REAL,
    serp_volatility_weight REAL,
    review_threshold REAL,
    disqualification_rules TEXT,
    brand_keywords TEXT,
    competitor_brand_keywords TEXT,
    FOREIGN KEY (client_id) REFERENCES clients (client_id)
);
</file>

<file path="backend/data_access/migrations/018_add_job_id_and_cluster_name.sql">
-- data_access/migrations/018_add_job_id_and_cluster_name.sql

ALTER TABLE opportunities ADD COLUMN latest_job_id TEXT;
ALTER TABLE opportunities ADD COLUMN cluster_name TEXT;
</file>

<file path="backend/data_access/migrations/019_add_core_keyword_and_competitor_metrics_to_opportunities.sql">
-- data_access/migrations/019_add_core_keyword_and_competitor_metrics_to_opportunities.sql

-- Add new columns for direct access to frequently used keyword metrics
ALTER TABLE opportunities ADD COLUMN cpc REAL DEFAULT 0.0;
ALTER TABLE opportunities ADD COLUMN competition REAL DEFAULT 0.0;
ALTER TABLE opportunities ADD COLUMN main_intent TEXT DEFAULT 'informational';
ALTER TABLE opportunities ADD COLUMN search_volume_trend_json TEXT;

-- Add new columns for storing aggregated competitor data for easier access
ALTER TABLE opportunities ADD COLUMN competitor_social_media_tags_json TEXT;
ALTER TABLE opportunities ADD COLUMN competitor_page_timing_json TEXT;

-- Create indexes on these new columns for improved query performance
CREATE INDEX idx_opportunities_cpc ON opportunities (cpc);
CREATE INDEX idx_opportunities_competition ON opportunities (competition);
CREATE INDEX idx_opportunities_main_intent ON opportunities (main_intent);
</file>

<file path="backend/data_access/migrations/020_backfill_core_keyword_metrics.sql">
-- data_access/migrations/020_backfill_core_keyword_metrics.sql

-- Backfill cpc, competition, main_intent, and search_volume_trend_json from existing keyword_info and search_intent_info JSON blobs
UPDATE opportunities
SET
    cpc = CAST(JSON_EXTRACT(keyword_info, '$.cpc') AS REAL),
    competition = CAST(JSON_EXTRACT(keyword_info, '$.competition') AS REAL),
    main_intent = JSON_EXTRACT(search_intent_info, '$.main_intent'),
    search_volume_trend_json = JSON_EXTRACT(keyword_info, '$.search_volume_trend')
WHERE
    keyword_info IS NOT NULL AND search_intent_info IS NOT NULL;

-- Backfill aggregated competitor social media tags and page timing from blueprint_data
-- This requires iterating through the competitor_analysis array within blueprint_data
-- Note: SQLite's JSON functions can be limited for complex array aggregation directly in SQL.
-- This might require application-level backfill for more complex aggregations if `blueprint_data` is large.
-- For a simple direct copy of the *first* competitor's data (as an example), or an empty JSON if none:
UPDATE opportunities
SET
    competitor_social_media_tags_json = (
        SELECT CASE
            WHEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].social_media_tags') IS NOT NULL
            THEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].social_media_tags')
            ELSE '{}'
        END
    ),
    competitor_page_timing_json = (
        SELECT CASE
            WHEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].page_timing') IS NOT NULL
            THEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].page_timing')
            ELSE '{}'
        END
    )
WHERE
    blueprint_data IS NOT NULL;
</file>

<file path="backend/data_access/migrations/021_add_unique_keyword_constraint.sql">
CREATE UNIQUE INDEX IF NOT EXISTS idx_unique_client_keyword ON opportunities (client_id, keyword);
</file>

<file path="backend/data_access/migrations/023_add_run_id_to_opportunities.sql">
ALTER TABLE opportunities ADD COLUMN run_id INTEGER;
CREATE INDEX IF NOT EXISTS idx_opportunities_run_id ON opportunities (run_id);
</file>

<file path="backend/data_access/migrations/024_add_total_api_cost_to_opportunities.sql">
ALTER TABLE opportunities ADD COLUMN total_api_cost REAL DEFAULT 0.0;
</file>

<file path="backend/data_access/migrations/025_add_cost_to_discovery_runs.sql">
-- Add a column to store the total API cost for a discovery run
ALTER TABLE discovery_runs ADD COLUMN total_api_cost REAL DEFAULT 0.0;
</file>

<file path="backend/data_access/__init__.py">
# data_access/__init__.py
# This file marks the directory as a Python package.
</file>

<file path="backend/data_access/database_manager.py">
import sqlite3
import json
import threading
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
import logging
import bleach  # ADD THIS LINE
import os
from . import queries
from backend.app_config.manager import ConfigManager

ALLOWED_ATTRIBUTES_DB = {
    "*": ["id", "class"],
    "a": ["href", "title"],
    "img": ["src", "alt", "width", "height"],
}

# W20 FIX: Define sanitization constants globally or as module constants (based on router update logic)
ALLOWED_TAGS = list(bleach.sanitizer.ALLOWED_TAGS) + [
    "h1",
    "h2",
    "h3",
    "h4",
    "h5",
    "h6",
    "p",
    "br",
    "a",
    "i",
    "u",
    "em",
    "strong",
    "blockquote",
    "li",
    "ul",
    "ol",
    "img",
    "div",
    "span",
    "table",
    "thead",
    "tbody",
    "tr",
    "td",
    "th",
    "code",
    "pre",
]
ALLOWED_ATTRIBUTES = bleach.sanitizer.ALLOWED_ATTRIBUTES.copy()
ALLOWED_ATTRIBUTES.update(
    {
        "a": ["href", "title"],
        "img": ["src", "alt", "width", "height", "style"],
        "*": ["id", "class", "style"],
    }
)

DB_FILE = "data/opportunities.db"


class DatabaseManager:
    """Handles all interactions with the SQLite opportunity queue database."""

    def __init__(
        self, cfg_manager: Optional[ConfigManager] = None, db_path: Optional[str] = None
    ):  # db_path is now passed via cfg_manager
        self.cfg_manager = cfg_manager

        # Define project root relative to this file's location
        project_root = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "..")
        )

        if db_path:
            self.db_path = db_path
            self.db_type = "sqlite"
        elif cfg_manager:
            global_cfg = cfg_manager.get_global_config()
            db_file_name = global_cfg.get("db_file_name", DB_FILE)
            self.db_path = os.path.join(
                project_root, db_file_name
            )  # Ensure path is relative to project root
            self.db_type = global_cfg.get("db_type", "sqlite")
        else:
            db_file_name = DB_FILE
            self.db_path = os.path.join(
                project_root, db_file_name
            )  # Ensure path is relative to project root
            self.db_type = "sqlite"

        self.logger = logging.getLogger(self.__class__.__name__)
        self._thread_local = threading.local()

    def initialize(self):
        """Connects to the DB, creates tables, applies migrations, and ensures default client exists."""
        conn = self._get_conn()
        try:
            # Combine all CREATE TABLE statements into a single script for atomic execution
            full_schema_script = f"""
                {queries.CREATE_OPPORTUNITIES_TABLE}
                {queries.CREATE_CLIENTS_TABLE}
                {queries.CREATE_CLIENT_SETTINGS_TABLE}
                {queries.CREATE_DISCOVERY_RUNS_TABLE}
                {queries.CREATE_SCHEMA_VERSION_TABLE}
                {queries.CREATE_JOBS_TABLE}
                {queries.CREATE_CONTENT_HISTORY_TABLE}
                {queries.CREATE_CONTENT_FEEDBACK_TABLE}
                {queries.CREATE_API_CACHE_TABLE}
            """
            with conn:  # Ensure transaction for initial setup
                conn.executescript(full_schema_script)

            self._apply_migrations_from_files()
            self._ensure_default_client_exists(conn)  # Add this line
            self.logger.info("Database initialized.")
        finally:
            self._close_conn()

    def _get_conn(self):
        """Gets a connection from the thread-local storage or creates a new one."""
        if not hasattr(self._thread_local, "conn") or self._thread_local.conn is None:
            if self.db_type == "sqlite":
                self._thread_local.conn = sqlite3.connect(
                    self.db_path, check_same_thread=False
                )
                self._thread_local.conn.row_factory = sqlite3.Row
            else:
                raise NotImplementedError(
                    f"External database type '{self.db_type}' is not yet implemented."
                )
        return self._thread_local.conn

    def _close_conn(self):
        """Closes the connection for the current thread."""
        if hasattr(self._thread_local, "conn") and self._thread_local.conn is not None:
            self._thread_local.conn.close()
            self._thread_local.conn = None

    def _ensure_default_client_exists(self, conn):
        """Checks for and creates the default client if it doesn't exist in the database."""
        if not self.cfg_manager:
            return

        global_cfg = self.cfg_manager.get_global_config()
        default_id = global_cfg.get("default_client_id")

        if not default_id:
            self.logger.warning("No default_client_id found in configuration.")
            return

        conn = self._get_conn()
        cursor = conn.execute(
            "SELECT 1 FROM clients WHERE client_id = ?", (default_id,)
        )
        if cursor.fetchone() is None:
            self.logger.warning(
                f"Default client '{default_id}' not found in database. Creating it now."
            )
            default_settings_template = (
                self.cfg_manager.get_default_client_settings_template()
            )
            self.add_client(default_id, default_id, default_settings_template)

    def _get_current_schema_version(self, conn) -> int:
        """Retrieves the current schema version from the database."""
        cursor = conn.execute(queries.GET_SCHEMA_VERSION)
        result = cursor.fetchone()
        return result["version"] if result else 0

    def _apply_migrations_from_files(self):
        """Applies SQL migration scripts from the migrations directory."""
        conn = self._get_conn()
        try:
            current_version = self._get_current_schema_version(conn)
            migrations_dir = os.path.join(os.path.dirname(__file__), "migrations")

            if not os.path.exists(migrations_dir):
                self.logger.warning(
                    f"Migrations directory not found: {migrations_dir}. Skipping migrations."
                )
                return

            migration_files = sorted(
                [f for f in os.listdir(migrations_dir) if f.endswith(".sql")]
            )

            for filename in migration_files:
                version_str = filename.split("_")[0]
                if not version_str.isdigit():
                    self.logger.warning(
                        f"Skipping malformed migration file: {filename}"
                    )
                    continue

                version = int(version_str)

                if version > current_version:
                    self.logger.info(f"Applying migration {filename}...")
                    filepath = os.path.join(migrations_dir, filename)
                    with open(filepath, "r") as f:
                        sql_script = f.read()

                    print(f"Executing migration script: {filename}")
                    print(sql_script)

                    try:
                        with conn:  # Execute in a transaction
                            conn.executescript(sql_script)
                            conn.execute(
                                queries.INSERT_SCHEMA_VERSION,
                                (version, datetime.now().isoformat()),
                            )
                        self.logger.info(f"Migration {filename} applied successfully.")
                        current_version = version
                    except sqlite3.OperationalError as e:
                        if "duplicate column name" in str(e):
                            self.logger.warning(
                                f"Migration {filename} failed because a column already exists. Assuming it was already applied and continuing."
                            )
                            conn.execute(
                                queries.INSERT_SCHEMA_VERSION,
                                (version, datetime.now().isoformat()),
                            )
                            current_version = version
                        else:
                            raise e
                else:
                    self.logger.debug(f"Migration {filename} already applied.")
            self.logger.info("Database migration check complete.")
        except Exception as e:
            self.logger.error(f"Error during database migration: {e}", exc_info=True)
            raise
        finally:
            self._close_conn()  # Ensure connection is closed after migrations

    def _deserialize_rows(self, rows: List[sqlite3.Row]) -> List[Dict[str, Any]]:
        """Deserializes JSON strings from database rows into a clean dictionary."""
        results = []

        json_keys = [
            "blueprint_data",
            "ai_content_json",
            "in_article_images_data",
            "social_media_posts_json",
            "final_package_json",
            "wordpress_payload_json",
            "keyword_info",
            "keyword_properties",
            "search_intent_info",
            "serp_overview",
            "score_breakdown",
            "keyword_info_normalized_with_bing",
            "keyword_info_normalized_with_clickstream",
            "monthly_searches",
            "full_data",
            "search_volume_trend_json",
            "competitor_social_media_tags_json",
            "competitor_page_timing_json",  # ADDED THIS LINE
        ]

        for row in rows:
            final_item = dict(row)

            # Deserialize all JSON fields first
            for key in json_keys:
                if key in final_item and isinstance(final_item[key], str):
                    try:
                        final_item[key] = json.loads(final_item[key])
                    except json.JSONDecodeError:
                        self.logger.warning(
                            f"Failed to parse JSON for key '{key}' on row ID {final_item.get('id')}. Leaving as raw string."
                        )

            # --- Data Unification and Renaming (with added safety checks and handling promoted columns) ---
            # Ensure direct columns are prioritized; if null, try to extract from old JSON blobs for backward compatibility
            if final_item.get("main_intent") is None and isinstance(
                final_item.get("search_intent_info"), dict
            ):
                final_item["main_intent"] = final_item["search_intent_info"].get(
                    "main_intent"
                )
            if final_item.get("cpc") is None and isinstance(
                final_item.get("keyword_info"), dict
            ):
                final_item["cpc"] = float(final_item["keyword_info"].get("cpc") or 0.0)
            if final_item.get("competition") is None and isinstance(
                final_item.get("keyword_info"), dict
            ):
                final_item["competition"] = float(
                    final_item["keyword_info"].get("competition") or 0.0
                )
            if final_item.get("search_volume") is None and isinstance(
                final_item.get("keyword_info"), dict
            ):
                final_item["search_volume"] = int(
                    final_item["keyword_info"].get("search_volume") or 0
                )
            if final_item.get("keyword_difficulty") is None and isinstance(
                final_item.get("keyword_properties"), dict
            ):
                final_item["keyword_difficulty"] = int(
                    final_item["keyword_properties"].get("keyword_difficulty") or 0
                )

            # Deserialize search_volume_trend_json if present in new column
            if isinstance(final_item.get("search_volume_trend_json"), str):
                try:
                    final_item["search_volume_trend"] = json.loads(
                        final_item["search_volume_trend_json"]
                    )
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse search_volume_trend_json for row ID {final_item.get('id')}. Resetting."
                    )
                    final_item["search_volume_trend"] = {}
            # Fallback to old keyword_info if new column is empty
            elif isinstance(final_item.get("keyword_info"), dict):
                final_item["search_volume_trend"] = final_item["keyword_info"].get(
                    "search_volume_trend"
                )

            # Deserialize competitor_social_media_tags_json
            if isinstance(final_item.get("competitor_social_media_tags_json"), str):
                try:
                    final_item["competitor_social_media_tags"] = json.loads(
                        final_item["competitor_social_media_tags_json"]
                    )
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse competitor_social_media_tags_json for row ID {final_item.get('id')}. Resetting."
                    )
                    final_item["competitor_social_media_tags"] = {}

            # Deserialize competitor_page_timing_json
            if isinstance(final_item.get("competitor_page_timing_json"), str):
                try:
                    final_item["competitor_page_timing"] = json.loads(
                        final_item["competitor_page_timing_json"]
                    )
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse competitor_page_timing_json for row ID {final_item.get('id')}. Resetting."
                    )
                    final_item["competitor_page_timing"] = {}

            # Ensure keyword_properties is a dict before assigning to it (for `intent` field that might be manually added)
            if not isinstance(final_item.get("keyword_properties"), dict):
                final_item["keyword_properties"] = {}
            if final_item.get("main_intent") and isinstance(
                final_item.get("keyword_properties"), dict
            ):
                final_item["keyword_properties"]["intent"] = final_item["main_intent"]

            # Simplify monthly_searches if stored as JSON string directly
            if isinstance(
                final_item.get("monthly_searches_json"), str
            ):  # This is from Task 1.2
                try:
                    final_item["monthly_searches"] = json.loads(
                        final_item["monthly_searches_json"]
                    )
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse monthly_searches_json for row ID {final_item.get('id')}. Resetting."
                    )
                    final_item["monthly_searches"] = []
            # Fallback to old keyword_info if new column is empty
            elif isinstance(
                final_item.get("keyword_info"), dict
            ):  # This is the old way, still in place for historical data
                final_item["monthly_searches"] = final_item["keyword_info"].get(
                    "monthly_searches"
                )

            if "blueprint_data" in final_item:
                final_item["blueprint"] = final_item.pop("blueprint_data")
            if "ai_content_json" in final_item:
                final_item["ai_content"] = final_item.pop("ai_content_json")

            results.append(final_item)
        return results

    def add_opportunity(self, client_id: str, opportunity_data: Dict[str, Any]):
        """Adds a new opportunity to the database from a dictionary."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO opportunities (
                    keyword, client_id, status, date_added, date_processed, 
                    strategic_score, keyword_info, keyword_properties, 
                    search_intent_info, serp_overview, score_breakdown, ai_content_json
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    opportunity_data.get("keyword"),
                    client_id,
                    opportunity_data.get("status", "pending"),
                    opportunity_data.get("date_added", datetime.now().isoformat()),
                    opportunity_data.get("date_processed"),
                    opportunity_data.get("strategic_score"),
                    json.dumps(opportunity_data.get("keyword_info")),
                    json.dumps(opportunity_data.get("keyword_properties")),
                    json.dumps(opportunity_data.get("search_intent_info")),
                    json.dumps(opportunity_data.get("serp_overview")),
                    json.dumps(opportunity_data.get("score_breakdown")),
                    json.dumps(opportunity_data.get("ai_content")),
                ),
            )
            return cursor.lastrowid

    def add_opportunities(
        self, opportunities: List[Dict[str, Any]], client_id: str, run_id: int
    ) -> int:
        """Adds multiple opportunities to the database in a single transaction, updating existing ones."""
        conn = self._get_conn()

        with conn:
            cursor = conn.cursor()
            for opp in opportunities:
                keyword = opp.get("keyword")
                cursor.execute("SELECT id FROM keywords WHERE keyword = ?", (keyword,))
                keyword_row = cursor.fetchone()

                keyword_info = opp.get("keyword_info", {})
                keyword_properties = opp.get("keyword_properties", {})
                search_intent_info = opp.get("search_intent_info", {})

                if keyword_row:
                    keyword_id = keyword_row["id"]
                    # Update existing keyword
                    cursor.execute(
                        """
                        UPDATE keywords
                        SET search_volume = ?, keyword_difficulty = ?, cpc = ?, competition = ?, search_volume_trend = ?, main_intent = ?, core_keyword = ?
                        WHERE id = ?
                    """,
                        (
                            keyword_info.get("search_volume"),
                            keyword_properties.get("keyword_difficulty"),
                            keyword_info.get("cpc"),
                            keyword_info.get("competition"),
                            json.dumps(keyword_info.get("search_volume_trend")),
                            search_intent_info.get("main_intent"),
                            keyword_properties.get("core_keyword"),
                            keyword_id,
                        ),
                    )
                else:
                    # Insert new keyword
                    cursor.execute(
                        """
                        INSERT INTO keywords (keyword, search_volume, keyword_difficulty, cpc, competition, search_volume_trend, main_intent, core_keyword)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            keyword,
                            keyword_info.get("search_volume"),
                            keyword_properties.get("keyword_difficulty"),
                            keyword_info.get("cpc"),
                            keyword_info.get("competition"),
                            json.dumps(keyword_info.get("search_volume_trend")),
                            search_intent_info.get("main_intent"),
                            keyword_properties.get("core_keyword"),
                        ),
                    )
                    keyword_id = cursor.lastrowid

                cursor.execute(
                    "SELECT id FROM opportunities WHERE client_id = ? AND keyword_id = ?",
                    (client_id, keyword_id),
                )
                opportunity_row = cursor.fetchone()

                if opportunity_row:
                    # Update existing opportunity
                    history = (
                        json.loads(opportunity_row["metrics_history"])
                        if opportunity_row["metrics_history"]
                        else []
                    )
                    history.append(
                        {
                            "date": datetime.now().isoformat(),
                            "search_volume": keyword_info.get("search_volume"),
                            "keyword_difficulty": keyword_properties.get(
                                "keyword_difficulty"
                            ),
                            "cpc": keyword_info.get("cpc"),
                        }
                    )
                    cursor.execute(
                        """
                        UPDATE opportunities
                        SET last_seen_at = ?, metrics_history = ?
                        WHERE id = ?
                    """,
                        (
                            datetime.now().isoformat(),
                            json.dumps(history),
                            opportunity_row["id"],
                        ),
                    )
                else:
                    # Insert new opportunity
                    # Extract values for direct columns, potentially nulling them out from JSON if no longer needed there
                    cpc_val = keyword_info.get("cpc")
                    competition_val = keyword_info.get("competition")
                    main_intent_val = search_intent_info.get("main_intent")
                    search_volume_trend_json_val = json.dumps(
                        keyword_info.get("search_volume_trend")
                    )

                    # Aggregate top competitor data for direct columns
                    top_competitor = next(
                        (
                            comp
                            for comp in opp.get("blueprint", {}).get(
                                "competitor_analysis", []
                            )
                            if comp.get("url")
                        ),
                        None,
                    )
                    competitor_social_media_tags_json_val = (
                        json.dumps(top_competitor.get("social_media_tags", {}))
                        if top_competitor
                        else None
                    )
                    competitor_page_timing_json_val = (
                        json.dumps(top_competitor.get("page_timing", {}))
                        if top_competitor
                        else None
                    )

                    cursor.execute(
                        """
                        INSERT INTO opportunities (
                            keyword, client_id, run_id, status, date_added, date_processed,
                            strategic_score, blog_qualification_status, blog_qualification_reason,
                            keyword_info, keyword_properties,
                            search_intent_info, serp_overview, score_breakdown, ai_content_json,
                            keyword_info_normalized_with_bing, keyword_info_normalized_with_clickstream, monthly_searches, traffic_value,
                            check_url, related_keywords, keyword_categories, core_keyword, last_seen_at, metrics_history, keyword_id,
                            full_data,
                            cpc, competition, main_intent, search_volume_trend_json,
                            competitor_social_media_tags_json, competitor_page_timing_json,
                        social_media_posts_status
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            keyword,
                            client_id,
                            run_id,
                            opp.get("status", "pending"),
                            opp.get("date_added", datetime.now().isoformat()),
                            opp.get("date_processed"),
                            opp.get("strategic_score"),
                            opp.get("blog_qualification_status"),
                            opp.get("blog_qualification_reason"),
                            json.dumps(keyword_info),
                            json.dumps(keyword_properties),
                            json.dumps(search_intent_info),
                            json.dumps(opp.get("serp_overview")),
                            json.dumps(opp.get("score_breakdown")),
                            json.dumps(opp.get("ai_content")),
                            json.dumps(opp.get("keyword_info_normalized_with_bing")),
                            json.dumps(
                                opp.get("keyword_info_normalized_with_clickstream")
                            ),
                            json.dumps(keyword_info.get("monthly_searches")),
                            opp.get("traffic_value", 0),
                            opp.get("serp_info", {}).get("check_url"),
                            json.dumps(opp.get("related_keywords")),
                            json.dumps(keyword_info.get("categories")),
                            keyword_properties.get("core_keyword"),
                            datetime.now().isoformat(),
                            json.dumps([]),
                            keyword_id,
                            json.dumps(opp),
                            cpc_val,  # NEW DIRECT COLUMN
                            competition_val,  # NEW DIRECT COLUMN
                            main_intent_val,  # NEW DIRECT COLUMN
                            search_volume_trend_json_val,  # NEW DIRECT COLUMN
                            competitor_social_media_tags_json_val,  # NEW DIRECT COLUMN
                            competitor_page_timing_json_val,  # NEW DIRECT COLUMN
                            opp.get("social_media_posts_status", "draft"),
                        ),
                    )

            return cursor.rowcount

    def get_opportunity_queue(self, client_id: str = "default") -> List[Dict[str, Any]]:
        """Retrieves all pending opportunities for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_PENDING_OPPORTUNITIES, (client_id,))
            return self._deserialize_rows(cursor.fetchall())

    def get_all_opportunities(
        self,
        client_id: str,
        params: Dict[str, Any],
        summary: bool = False,
        select_columns: str = None,
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        Retrieves keyword opportunities for a client, supporting filtering, sorting, and pagination.
        If summary is True, only essential fields for the table view are returned.
        """
        conn = self._get_conn()
        limit = int(params.get("limit", 20))
        page = int(params.get("page", 1))
        offset = (page - 1) * limit

        sort_by_map = {
            "strategic_score": "strategic_score",
            "date_added": "date_added",
            "keyword": "keyword",
            "status": "status",
            "search_volume": "JSON_EXTRACT(full_data, '$.keyword_info.search_volume')",
            "keyword_difficulty": "JSON_EXTRACT(full_data, '$.keyword_properties.keyword_difficulty')",
            "cpc": "JSON_EXTRACT(full_data, '$.keyword_info.cpc')",
        }
        sort_by = sort_by_map.get(params.get("sort_by"), "date_added")
        sort_direction = "ASC" if params.get("sort_direction") == "asc" else "DESC"

        where_parts = ["client_id = ?"]
        query_values = [client_id]

        status_filter = params.get("status")
        if status_filter:
            statuses = [s.strip() for s in status_filter.split(",")]
            placeholders = ",".join(["?"] * len(statuses))
            where_parts.append(f"status IN ({placeholders})")
            query_values.extend(statuses)

        where_clause = " AND ".join(where_parts)

        count_query = f"SELECT COUNT(*) FROM opportunities WHERE {where_clause}"
        with conn:
            cursor = conn.cursor()
            cursor.execute(count_query, query_values)
            total_count = cursor.fetchone()[0]

        select_columns = (
            select_columns
            if select_columns
            else "id, keyword, status, date_added, strategic_score, search_volume, keyword_difficulty, cpc, competition, main_intent, search_volume_trend_json, competitor_social_media_tags_json, competitor_page_timing_json, blog_qualification_status, latest_job_id, cluster_name, score_breakdown, full_data"
        )
        if summary and "full_data" not in select_columns:
            select_columns += ", full_data"

        final_query = f"SELECT {select_columns} FROM opportunities WHERE {where_clause} ORDER BY {sort_by} {sort_direction} LIMIT ? OFFSET ?"

        paged_values = query_values + [limit, offset]
        with conn:
            cursor = conn.cursor()
            cursor.execute(final_query, paged_values)
            opportunities = self._deserialize_rows(cursor.fetchall())

        # Manually extract and add search_volume and keyword_difficulty for the frontend
        for opp in opportunities:
            try:
                if opp.get("full_data"):
                    full_data = opp["full_data"]
                    opp["search_volume"] = full_data.get("keyword_info", {}).get(
                        "search_volume"
                    )
                    opp["keyword_difficulty"] = full_data.get(
                        "keyword_properties", {}
                    ).get("keyword_difficulty")
            except (KeyError, TypeError):
                opp["search_volume"] = None
                opp["keyword_difficulty"] = None

        return opportunities, total_count

    def get_opportunity_by_id(self, opportunity_id: int) -> Optional[Dict[str, Any]]:
        """Retrieves a single opportunity by its primary key ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_OPPORTUNITY_BY_ID, (opportunity_id,))
            row = cursor.fetchone()
            if row:
                return self._deserialize_rows([row])[0]
        return None

    def get_opportunity_summary_by_id(
        self, opportunity_id: int
    ) -> Optional[Dict[str, Any]]:
        """
        Retrieves only essential summary fields for an opportunity. (W13 FIX)
        """
        conn = self._get_conn()
        with conn:
            # Use a selective query to avoid fetching large JSON blobs
            cursor = conn.execute(
                "SELECT id, keyword, status, date_added, strategic_score, blog_qualification_status, featured_image_local_path FROM opportunities WHERE id = ?",
                (opportunity_id,),
            )
            row = cursor.fetchone()
            if row:
                # Need to manually or selectively deserialize, but for simplicity, we treat the row as the summary
                return dict(row)
            return None

    def get_opportunity_by_slug(self, slug: str) -> Optional[Dict[str, Any]]:
        """Retrieves a single opportunity by its URL slug."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_OPPORTUNITY_BY_SLUG, (slug,))
            row = cursor.fetchone()
            if row:
                return self._deserialize_rows([row])[0]
        return None

    def search_opportunities(self, client_id: str, query: str) -> List[Dict[str, Any]]:
        """Searches for opportunities by keyword for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            search_term = f"%{query}%"
            cursor.execute(
                queries.SEARCH_OPPORTUNITIES_BY_KEYWORD, (client_id, search_term)
            )
            # No need for full deserialization as we are fetching simple columns
            return [dict(row) for row in cursor.fetchall()]

    def get_published_articles_for_linking(
        self, client_id: str
    ) -> List[Dict[str, str]]:
        """
        Retrieves a list of published articles (title and slug) for internal linking suggestions.
        """
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            # Select only 'keyword' (as title) and 'slug' for published articles
            # 'full_data' is not needed here to save memory/processing
            cursor.execute(
                """
                SELECT keyword, slug FROM opportunities
                WHERE client_id = ? AND status IN ('generated', 'published') AND slug IS NOT NULL;
            """,
                (client_id,),
            )

            articles = []
            for row in cursor.fetchall():
                articles.append(
                    {
                        "title": row["keyword"],  # Use keyword as a proxy for title
                        "url": f"/article/{row['slug']}",  # Construct the relative URL
                    }
                )
            return articles

    def get_all_processed_keywords_for_client(self, client_id: str) -> List[str]:
        """Retrieves a flat list of all primary keywords for a client that are not in a 'rejected' or 'failed' state."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_ALL_PROCESSED_KEYWORDS, (client_id,))
            return [row["keyword"] for row in cursor.fetchall()]

    def check_existing_keywords(self, client_id: str, keywords: List[str]) -> List[str]:
        """Checks a list of keywords against the DB and returns those that exist."""
        if not keywords:
            return []
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            placeholders = ",".join("?" for _ in keywords)
            query = f"SELECT keyword FROM opportunities WHERE client_id = ? AND keyword IN ({placeholders})"
            cursor.execute(query, [client_id] + keywords)
            return [row["keyword"] for row in cursor.fetchall()]

    def update_opportunity_status(self, opportunity_id: int, new_status: str):
        """Updates a keyword's status in the database."""
        conn = self._get_conn()
        with conn:
            if new_status in ["generated", "analyzed", "failed"]:
                conn.execute(
                    queries.UPDATE_OPPORTUNITY_STATUS_WITH_DATE,
                    (new_status, datetime.now().isoformat(), opportunity_id),
                )
            else:
                conn.execute(
                    queries.UPDATE_OPPORTUNITY_STATUS, (new_status, opportunity_id)
                )

    def update_opportunity_workflow_state(
        self,
        opportunity_id: int,
        step: str,
        status: str = "in_progress",
        error_message: Optional[str] = None,
    ):
        """Updates the workflow step and status for a given opportunity.
        Possible statuses include: 'in_progress', 'completed', 'failed', 'pending', 'validated', 'analyzed', 'generated', 'published', 'rejected', 'paused_for_approval'."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Updating workflow state to step='{step}', status='{status}'"
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_WORKFLOW_STATE,
                (step, status, error_message, opportunity_id),
            )

    def update_opportunity_blueprint(
        self, opportunity_id: int, blueprint_data: Dict[str, Any], slug: str
    ):
        """Stores the generated blueprint data and the URL slug for a specific opportunity."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_BLUEPRINT_AND_SLUG,
                (json.dumps(blueprint_data), slug, opportunity_id),
            )

    def update_opportunity_ai_content(
        self, opportunity_id: int, ai_content_data: Dict[str, Any], ai_model: str
    ):
        """
        Stores the generated AI content package and model used for a specific opportunity.
        Applies server-side sanitization to the HTML body (W20 FIX).
        """
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving AI content generated by model '{ai_model}'."
        )

        # W20 FIX: Sanitize content before saving
        html_body = ai_content_data.get("article_body_html")
        if html_body:
            clean_html = bleach.clean(
                html_body, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES_DB
            )
            ai_content_data["article_body_html"] = clean_html

        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_AI_CONTENT,
                (
                    json.dumps(ai_content_data),
                    ai_model,
                    datetime.now().isoformat(),
                    opportunity_id,
                ),
            )

    def update_opportunity_images(
        self,
        opportunity_id: int,
        featured_image_url: Optional[str],
        featured_image_local_path: Optional[str],
        in_article_images_data: List[Dict[str, Any]],
    ):
        """Stores image generation details for a specific opportunity."""
        # W5 FIX: Ensure path is relative before storing, e.g., by checking if it starts with the expected API prefix
        if featured_image_local_path and os.path.isabs(featured_image_local_path):
            # Assuming the standard path is relative to the API's image mounting point,
            # extract the filename or the relative part required by the API
            # This is a simplified example; robust path management is needed.
            base_path = os.path.abspath(
                os.path.join(os.path.dirname(__file__), "..", "generated_images")
            )
            featured_image_local_path = os.path.relpath(
                featured_image_local_path, base_path
            )

        self.logger.info(
            f"Opportunity {opportunity_id}: Saving Pexels image data (featured URL: {featured_image_url})."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_IMAGES,
                (
                    featured_image_url,
                    featured_image_local_path,
                    json.dumps(in_article_images_data),
                    opportunity_id,
                ),
            )

    def update_opportunity_full_data(
        self, opportunity_id: int, full_data: Dict[str, Any]
    ):
        """Updates the full_data JSON blob for a specific opportunity."""
        self.logger.info(f"Opportunity {opportunity_id}: Updating full_data field.")
        conn = self._get_conn()
        with conn:
            conn.execute(
                "UPDATE opportunities SET full_data = ? WHERE id = ?",
                (json.dumps(full_data), opportunity_id),
            )

    def update_opportunity_scores(
        self,
        opportunity_id: int,
        strategic_score: float,
        score_breakdown: Dict[str, Any],
        blueprint_data: Optional[Dict[str, Any]] = None,
    ):
        """Updates the primary strategic score, breakdown, and blueprint for an opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Updating strategic_score to {strategic_score:.2f} and saving blueprint."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_SCORES,
                (
                    strategic_score,
                    json.dumps(score_breakdown),
                    json.dumps(blueprint_data) if blueprint_data else None,
                    opportunity_id,
                ),
            )

    def update_opportunity_final_package(
        self, opportunity_id: int, final_package: Dict[str, Any]
    ):
        """Stores the generated final content package JSON for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving final standalone content package."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_FINAL_PACKAGE,
                (json.dumps(final_package), opportunity_id),
            )

    def add_client(
        self, client_id: str, client_name: str, default_settings: Dict[str, Any]
    ) -> bool:
        """Adds a new client to the clients table and initializes their settings."""
        conn = self._get_conn()
        try:
            with conn:
                cursor = conn.execute("PRAGMA table_info(client_settings)")
                schema_columns = {row["name"] for row in cursor.fetchall()}

                conn.execute(
                    queries.INSERT_CLIENT,
                    (client_id, client_name, datetime.now().isoformat()),
                )

                settings_to_insert = {
                    k: v for k, v in default_settings.items() if k in schema_columns
                }
                if (
                    "client_knowledge_base" not in settings_to_insert
                    and "client_knowledge_base" in schema_columns
                ):
                    settings_to_insert["client_knowledge_base"] = (
                        ""  # Initialize with empty string
                    )
                settings_to_insert["client_id"] = client_id
                settings_to_insert["last_updated"] = datetime.now().isoformat()

                keys = ", ".join(settings_to_insert.keys())
                placeholders = ", ".join("?" * len(settings_to_insert))

                values = []
                for key in settings_to_insert.keys():
                    value = settings_to_insert[key]
                    if isinstance(value, list):
                        values.append(",".join(map(str, value)))
                    elif isinstance(value, bool):
                        values.append(1 if value else 0)
                    else:
                        values.append(value)

                insert_query = (
                    f"INSERT INTO client_settings ({keys}) VALUES ({placeholders})"
                )

                conn.execute(insert_query, tuple(values))
                self.logger.info(
                    f"Added new client '{client_name}' ({client_id}) and initialized settings."
                )

                # Also initialize qualification settings with a comprehensive set of default values
                conn.execute(
                    """
                    INSERT INTO qualification_settings (
                        client_id, ease_of_ranking_weight, traffic_potential_weight, commercial_intent_weight,
                        serp_features_weight, growth_trend_weight, serp_freshness_weight, serp_volatility_weight,
                        competitor_weakness_weight, competitor_performance_weight, min_search_volume, max_keyword_difficulty,
                        negative_keywords, prohibited_intents, max_y_pixel_threshold,
                        max_forum_results_in_top_10, max_ecommerce_results_in_top_10,
                        disallowed_page_types_in_top_3
                    ) VALUES (?, 40, 15, 5, 5, 5, 5, 5, 20, 5, 100, 80, 'login,free,cheap', 'navigational', 800, 3, 2, 'E-commerce,Forum')
                """,
                    (client_id,),
                )
                self.logger.info(
                    f"Initialized default qualification settings for client '{client_name}' ({client_id})."
                )
            return True
        except sqlite3.IntegrityError:
            self.logger.warning(f"Client with ID '{client_id}' already exists.")
            return False
        except Exception as e:
            self.logger.error(
                f"Error adding client '{client_name}': {e}", exc_info=True
            )
            return False

    def get_clients(self) -> List[Dict[str, str]]:
        """Retrieves all clients from the database."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_ALL_CLIENTS)
            return [dict(row) for row in cursor.fetchall()]

    def get_processed_opportunities(self, client_id: str) -> List[Dict[str, Any]]:
        """Retrieves all opportunities with a generated blueprint for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_PROCESSED_OPPORTUNITIES, (client_id,))
            return self._deserialize_rows(cursor.fetchall())

    def get_client_settings(self, client_id: str) -> Dict[str, Any]:
        """Retrieves settings for a specific client, converting from DB types."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_CLIENT_SETTINGS, (client_id,))
            row = cursor.fetchone()
            if row:
                settings = dict(row)
                settings.pop("client_id", None)
                settings.pop("last_updated", None)

                list_keys = [
                    "allowed_intents",
                    "negative_keywords",
                    "competitor_blacklist_domains",
                    "serp_feature_filters",
                    "serp_features_exclude_filter",
                    "platforms",
                    "default_wordpress_categories",
                    "default_wordpress_tags",
                    "ugc_and_parasite_domains",
                    "high_value_categories",
                    "hostile_serp_features",
                    "final_validation_non_blog_domains",
                    "prohibited_intents",  # NEW
                ]
                for key in list_keys:
                    if settings.get(key) is not None and isinstance(settings[key], str):
                        settings[key] = [
                            item.strip()
                            for item in settings[key].split(",")
                            if item.strip()
                        ]
                    elif settings.get(key) is None:
                        settings[key] = []

                bool_keys = [
                    "enforce_intent_filter",
                    "require_question_keywords",
                    "use_pexels_first",
                    "cleanup_local_images",
                    "onpage_enable_javascript",
                    "onpage_load_resources",
                    "calculate_rectangles",
                    "onpage_disable_cookie_popup",
                    "onpage_return_despite_timeout",
                    "onpage_enable_browser_rendering",
                    "onpage_store_raw_html",
                    "onpage_validate_micromarkup",
                    "discovery_replace_with_core_keyword",
                    "discovery_ignore_synonyms",
                    "enable_automated_internal_linking",  # NEW
                ]
                for key in bool_keys:
                    if settings.get(key) is not None:
                        settings[key] = bool(settings[key])

                int_keys = [
                    "num_in_article_images",
                    "location_code",
                    "serp_freshness_old_threshold_days",
                    "min_competitor_word_count",
                    "max_competitor_technical_warnings",
                    "num_competitors_to_analyze",
                    "num_common_headings",
                    "num_unique_angles",
                    "max_initial_serp_urls_to_analyze",
                    "min_search_volume",
                    "max_keyword_difficulty",
                    "people_also_ask_click_depth",
                    "onpage_max_domains_per_request",
                    "onpage_max_tasks_per_request",
                    "ease_of_ranking_weight",
                    "traffic_potential_weight",
                    "commercial_intent_weight",
                    "growth_trend_weight",
                    "serp_features_weight",
                    "serp_freshness_weight",
                    "serp_volatility_weight",
                    "competitor_weakness_weight",
                    "max_sv_for_scoring",
                    "max_domain_rank_for_scoring",
                    "max_referring_domains_for_scoring",
                    "serp_volatility_stable_threshold_days",
                    "discovery_related_depth",
                    "max_avg_referring_domains_filter",
                    "yearly_trend_decline_threshold",
                    "quarterly_trend_decline_threshold",
                    "max_kd_hard_limit",
                    "max_referring_main_domains_limit",
                    "max_avg_domain_rank_threshold",
                    "min_keyword_word_count",
                    "max_keyword_word_count",
                    "crowded_serp_features_threshold",
                    "min_serp_stability_days",
                    "max_non_blog_results",
                    "max_ai_overview_words",
                    "max_first_organic_y_pixel",
                    "max_words_for_ai_analysis",  # NEW
                    "max_avg_lcp_time",  # NEW
                ]
                for key in int_keys:
                    if settings.get(key) is not None:
                        try:
                            settings[key] = int(settings[key])
                        except (ValueError, TypeError):
                            pass

                float_keys = [
                    "informational_score",
                    "commercial_score",
                    "transactional_score",
                    "navigational_score",
                    "question_keyword_bonus",
                    "max_cpc_for_scoring",
                    "min_monthly_trend_percentage",
                    "featured_snippet_bonus",
                    "ai_overview_bonus",
                    "serp_freshness_bonus_max",
                    "min_cpc_filter_api",
                    "category_intent_bonus",
                    "search_volume_volatility_threshold",
                    "max_paid_competition_score",
                    "max_high_top_of_page_bid",
                    "max_pages_to_domain_ratio",
                    "ai_generation_temperature",  # NEW
                    "recommended_word_count_multiplier",  # NEW
                ]
                for key in float_keys:
                    if settings.get(key) is not None:
                        try:
                            settings[key] = float(settings[key])
                        except (ValueError, TypeError):
                            pass

                return settings
            return {}

    def update_client_settings(self, client_id: str, settings: Dict[str, Any]):
        """Updates client-specific settings, handling type conversions for DB storage."""
        self.logger.info(f"Updating client settings for {client_id}.")
        conn = self._get_conn()
        with conn:
            cursor = conn.execute("PRAGMA table_info(client_settings)")
            schema_columns = {row["name"] for row in cursor.fetchall()}

            current_time = datetime.now().isoformat()

            set_clauses = ["last_updated = ?"]
            values = [current_time]

            for key, value in settings.items():
                if key in schema_columns and key not in ["client_id", "last_updated"]:
                    db_value = value
                    if isinstance(value, list):
                        db_value = ",".join(map(str, value))
                    elif isinstance(value, bool):
                        db_value = 1 if value else 0

                    set_clauses.append(f"{key} = ?")
                    values.append(db_value)

            if len(set_clauses) > 1:
                update_query = f"UPDATE client_settings SET {', '.join(set_clauses)} WHERE client_id = ?"
                values.append(client_id)
                conn.execute(update_query, values)
                self.logger.info(f"Client settings for {client_id} updated in DB.")
            else:
                self.logger.info(
                    f"No valid client settings found to update for {client_id}."
                )

    def get_all_opportunities_for_export(self) -> List[Dict[str, Any]]:
        """Retrieves all opportunities for all clients from the database."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_ALL_OPPORTUNITIES_FOR_EXPORT)
            return self._deserialize_rows(cursor.fetchall())

    def get_dashboard_stats(self, client_id: str) -> Dict[str, Any]:
        """Retrieves statistics for the dashboard UI."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # Get counts by status
            cursor.execute(queries.COUNT_OPPORTUNITIES_BY_STATUS, (client_id,))
            status_counts = {row["status"]: row["count"] for row in cursor.fetchall()}

            # Get recent items
            cursor.execute(queries.SELECT_RECENTLY_GENERATED, (client_id,))
            recent_items = [dict(row) for row in cursor.fetchall()]

            return {"status_counts": status_counts, "recent_items": recent_items}

    def get_total_api_cost(self, client_id: str) -> float:
        """Calculates the total API cost for a client by summing costs from both opportunities and discovery runs."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            
            # Sum cost from opportunities
            cursor.execute(
                "SELECT SUM(total_api_cost) FROM opportunities WHERE client_id = ?",
                (client_id,),
            )
            opportunities_cost = cursor.fetchone()[0] or 0.0
            
            # Sum cost from discovery runs
            cursor.execute(
                "SELECT SUM(total_api_cost) FROM discovery_runs WHERE client_id = ?",
                (client_id,),
            )
            runs_cost = cursor.fetchone()[0] or 0.0
            
            return opportunities_cost + runs_cost

    def get_dashboard_data(self, client_id: str) -> Dict[str, Any]:
        """Retrieves aggregated data for the main dashboard UI."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # 1. KPIs
            cursor.execute(
                "SELECT COUNT(*) FROM opportunities WHERE client_id = ?", (client_id,)
            )
            total_opportunities = cursor.fetchone()[0]
            cursor.execute(
                "SELECT COUNT(*) FROM opportunities WHERE client_id = ? AND status = 'generated'",
                (client_id,),
            )
            content_generated = cursor.fetchone()[0]
            cursor.execute(
                "SELECT SUM(traffic_value) FROM opportunities WHERE client_id = ?",
                (client_id,),
            )
            total_traffic_value = cursor.fetchone()[0] or 0
            total_api_cost = self.get_total_api_cost(client_id)

            kpis = {
                "totalOpportunities": total_opportunities,
                "contentGenerated": content_generated,
                "totalTrafficValue": total_traffic_value,
                "totalApiCost": total_api_cost,
            }

            # 2. Funnel and Stats Data
            dashboard_stats = self.get_dashboard_stats(client_id)
            status_counts = dashboard_stats.get("status_counts", {})
            recent_items = dashboard_stats.get("recent_items", [])

            funnel_data = [
                {"stage": "Total", "count": total_opportunities},
                {"stage": "Validated", "count": status_counts.get("validated", 0)},
                {"stage": "Analyzed", "count": status_counts.get("analyzed", 0)},
                {"stage": "Generated", "count": content_generated},
                {"stage": "Disqualified", "count": (status_counts.get("rejected", 0) or 0) + (status_counts.get("failed", 0) or 0)},
            ]

            # 3. Action Items
            cursor.execute(queries.SELECT_ACTION_ITEMS, (client_id,))
            action_items_raw = [dict(row) for row in cursor.fetchall()]
            action_items = {
                "awaitingApproval": [
                    item
                    for item in action_items_raw
                    if item["status"] == "paused_for_approval"
                ],
                "failed": [
                    item for item in action_items_raw if item["status"] == "failed"
                ],
            }

            return {
                "kpis": kpis,
                "funnelData": funnel_data,
                "actionItems": action_items,
                "status_counts": status_counts,
                "recent_items": recent_items,
            }

    def update_opportunity_wordpress_payload(
        self, opportunity_id: int, wordpress_payload: Dict[str, Any]
    ):
        """Stores the generated WordPress JSON payload for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving final WordPress JSON payload."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_WORDPRESS_PAYLOAD,
                (json.dumps(wordpress_payload), opportunity_id),
            )

    def get_api_cache(self, key: str) -> Optional[Dict[str, Any]]:
        """Retrieves a cached item from the api_cache table."""
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(queries.SELECT_API_CACHE, (key,))
            row = cursor.fetchone()
            if row:
                # Check TTL during retrieval
                if row["timestamp"] + (row["ttl_days"] * 86400) > time.time():
                    return json.loads(row["data"])
                else:
                    self.logger.debug(f"Cache STALE for key: {key}")
                    self.delete_api_cache_by_key(key)  # Clean up stale entry
            return None

    def set_api_cache(self, key: str, value: Any, ttl_days: int = 7):
        """Stores an item in the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.INSERT_API_CACHE,
                (key, json.dumps(value), time.time(), ttl_days),
            )
        self.logger.debug(f"Cache SET for key: {key}")

    def delete_api_cache_by_key(self, key: str):
        """Deletes a specific item from the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.DELETE_API_CACHE_BY_KEY, (key,))

    def clear_api_cache(self):
        """Clears all items from the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.TRUNCATE_API_CACHE)
        self.logger.info("API cache cleared.")

    def clear_expired_api_cache(self):
        """Deletes all expired items from the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.DELETE_EXPIRED_API_CACHE, (time.time(),))
        self.logger.debug("Expired API cache entries cleaned up.")

    # --- Discovery Run Methods ---

    def create_discovery_run(self, client_id: str, parameters: Dict[str, Any]) -> int:
        """Creates a new discovery run record and returns its ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                queries.INSERT_DISCOVERY_RUN,
                (
                    client_id,
                    datetime.now().isoformat(),
                    "running",
                    json.dumps(parameters),
                ),
            )
            return cursor.lastrowid

    def update_discovery_run_status(self, run_id: int, status: str):
        """Updates the status of a discovery run."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.UPDATE_DISCOVERY_RUN_STATUS, (status, run_id))

    def update_discovery_run_completed(
        self, run_id: int, results_summary: Dict[str, Any]
    ):
        """Marks a discovery run as completed and stores the results summary."""
        conn = self._get_conn()
        total_cost = results_summary.get("total_cost", 0.0)
        with conn:
            conn.execute(
                queries.UPDATE_DISCOVERY_RUN_COMPLETED,
                (
                    datetime.now().isoformat(),
                    json.dumps(results_summary),
                    total_cost,
                    run_id,
                ),
            )

    def update_discovery_run_failed(self, run_id: int, error_message: str):
        """Marks a discovery run as failed and stores the error message."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_DISCOVERY_RUN_FAILED,
                (datetime.now().isoformat(), error_message, run_id),
            )

    def get_all_discovery_runs_paginated(
        self, client_id: str, page: int, limit: int
    ) -> Tuple[List[Dict[str, Any]], int]:
        """Retrieves all discovery runs for a specific client with pagination."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # Get total count
            cursor.execute(
                "SELECT COUNT(*) FROM discovery_runs WHERE client_id = ?", (client_id,)
            )
            total_count = cursor.fetchone()[0]

            cursor.execute(
                "SELECT * FROM discovery_runs WHERE client_id = ? ORDER BY start_time DESC LIMIT ? OFFSET ?",
                (client_id, limit, (page - 1) * limit),
            )
            runs = []
            for row in cursor.fetchall():
                run = dict(row)
                try:
                    if run.get("parameters"):
                        run["parameters"] = json.loads(run["parameters"])
                    if run.get("results_summary"):
                        run["results_summary"] = json.loads(run["results_summary"])
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse JSON for discovery run ID {run.get('id')}."
                    )
                runs.append(run)
            return runs, total_count

    def get_discovery_run_by_id(self, run_id: int) -> Optional[Dict[str, Any]]:
        """Retrieves a single discovery run by its ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_DISCOVERY_RUN_BY_ID, (run_id,))
            row = cursor.fetchone()
            if row:
                run = dict(row)
                try:
                    if run.get("parameters"):
                        run["parameters"] = json.loads(run["parameters"])
                    if run.get("results_summary"):
                        run["results_summary"] = json.loads(run["results_summary"])
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse JSON for discovery run ID {run.get('id')}."
                    )
                return run
        return None

    def update_discovery_run_log_path(self, run_id: int, log_path: str):
        """Updates the log file path for a discovery run."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.UPDATE_DISCOVERY_RUN_LOG_PATH, (log_path, run_id))

    def get_keywords_for_run(self, run_id: int) -> List[Dict[str, Any]]:
        """Retrieves all opportunities associated with a specific discovery run ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM opportunities WHERE run_id = ?", (run_id,))
            return self._deserialize_rows(cursor.fetchall())

    def get_keywords_for_run_by_reason(
        self, run_id: int, reason: str
    ) -> List[Dict[str, Any]]:
        """Retrieves all opportunities associated with a specific discovery run ID that were disqualified for a specific reason."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_KEYWORDS_FOR_RUN_BY_REASON, (run_id, reason))
            return self._deserialize_rows(cursor.fetchall())

    def search_discovery_runs(self, client_id: str, query: str) -> List[Dict[str, Any]]:
        """Searches for discovery runs by seed keywords or status for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            search_term = f"%{query}%"
            cursor.execute(
                """
                SELECT id, client_id, start_time, status, parameters, results_summary
                FROM discovery_runs
                WHERE client_id = ? 
                  AND (parameters LIKE ? OR status LIKE ? OR error_message LIKE ?)
                ORDER BY start_time DESC
                LIMIT 10;
            """,
                (client_id, search_term, search_term, search_term),
            )

            runs = []
            for row in cursor.fetchall():
                run = dict(row)
                if run.get("parameters"):
                    run["parameters"] = json.loads(run["parameters"])
                if run.get("results_summary"):
                    run["results_summary"] = json.loads(run["results_summary"])
                runs.append(run)
            return runs

    def get_job(self, job_id: str) -> Optional[Dict[str, Any]]:
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(queries.GET_JOB, (job_id,))
            row = cursor.fetchone()
            if row:
                job_data = dict(row)
                if job_data.get("result"):
                    job_data["result"] = json.loads(job_data["result"])
                return job_data
        return None

    def get_all_jobs(self) -> List[Dict[str, Any]]:
        """Retrieves all jobs from the database, ordered by start time."""
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(queries.GET_ALL_JOBS)
            jobs = []
            for row in cursor.fetchall():
                job_data = dict(row)
                if job_data.get("result"):
                    try:
                        job_data["result"] = json.loads(job_data["result"])
                    except json.JSONDecodeError:
                        job_data["result"] = {"raw_result": job_data["result"]}
                jobs.append(job_data)
            return jobs

    def update_job(self, job_info: Dict[str, Any]):
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_JOB,
                (
                    job_info["id"],
                    job_info["status"],
                    job_info["progress"],
                    json.dumps(job_info["result"]) if job_info.get("result") else None,
                    job_info.get("error"),
                    job_info["started_at"],
                    job_info.get("finished_at"),
                ),
            )

    def get_client_prompt_templates(self, client_id: str) -> List[Dict[str, Any]]:
        """MOCK: Retrieves all prompt templates for a client."""
        return []

    def save_client_prompt_template(
        self, client_id: str, template_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """MOCK: Saves or updates a client's prompt template."""
        template_data["last_updated"] = datetime.now().isoformat()
        return template_data

    def delete_client_prompt_template(self, client_id: str, template_name: str):
        """MOCK: Deletes a client's prompt template."""
        pass

    def update_opportunity_ai_content_and_status(
        self,
        opportunity_id: int,
        ai_content_data: Dict[str, Any],
        ai_model: str,
        status: str,
    ):
        """Stores the generated AI content package, model, and status used for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving AI content generated by model '{ai_model}' and setting status to '{status}'."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_AI_CONTENT_AND_STATUS,
                (
                    json.dumps(ai_content_data),
                    ai_model,
                    datetime.now().isoformat(),
                    status,
                    opportunity_id,
                ),
            )

    def save_content_version_to_history(
        self,
        opportunity_id: int,
        ai_content_json: Dict[str, Any],
        timestamp: Optional[str] = None,
    ):
        """Saves the current content package of an opportunity to the history table."""
        conn = self._get_conn()
        timestamp = timestamp or datetime.now().isoformat()
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving content version to history at {timestamp}."
        )
        with conn:
            conn.execute(
                queries.INSERT_CONTENT_HISTORY,
                (opportunity_id, timestamp, json.dumps(ai_content_json)),
            )

    def save_content_feedback(
        self, opportunity_id: int, rating: int, comments: Optional[str] = None
    ):
        """Saves user feedback for generated content."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.INSERT_CONTENT_FEEDBACK,
                (opportunity_id, rating, comments, datetime.now().isoformat()),
            )
        self.logger.info(
            f"Saved content feedback for opportunity {opportunity_id} (Rating: {rating})."
        )

    def get_content_history(self, opportunity_id: int) -> List[Dict[str, Any]]:
        """Retrieves all historical content versions for an opportunity."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_CONTENT_HISTORY_BY_OPP_ID, (opportunity_id,))
            rows = cursor.fetchall()

            results = []
            for row in rows:
                row_dict = dict(row)
                if row_dict.get("ai_content_json"):
                    row_dict["ai_content_json"] = json.loads(
                        row_dict["ai_content_json"]
                    )
                results.append(row_dict)
            return results

    def restore_content_version(
        self, opportunity_id: int, version_timestamp: str
    ) -> Optional[Dict[str, Any]]:
        """
        Restores a content version from history. Before restoring, it saves the current
        'generated' content to the history table to prevent data loss.
        """
        conn = self._get_conn()
        with conn:
            # First, fetch the current opportunity to save its content
            cursor = conn.execute(queries.SELECT_OPPORTUNITY_BY_ID, (opportunity_id,))
            current_opp_row = cursor.fetchone()
            if not current_opp_row:
                self.logger.error(
                    f"Cannot restore: Opportunity {opportunity_id} not found."
                )
                return None

            current_opp = self._deserialize_rows([current_opp_row])[0]
            current_content = current_opp.get("ai_content")

            # Save the current 'generated' content to history before overwriting
            if current_content and current_opp.get("status") == "generated":
                self.save_content_version_to_history(opportunity_id, current_content)

            # Now, find the historical version to restore
            cursor = conn.execute(
                "SELECT ai_content_json FROM content_history WHERE opportunity_id = ? AND timestamp = ?",
                (opportunity_id, version_timestamp),
            )
            version_to_restore_row = cursor.fetchone()

            if (
                not version_to_restore_row
                or not version_to_restore_row["ai_content_json"]
            ):
                self.logger.error(
                    f"Version not found or content missing for timestamp: {version_timestamp}"
                )
                raise ValueError(f"Content version at {version_timestamp} not found.")

            restored_content_str = version_to_restore_row["ai_content_json"]
            restored_content = json.loads(restored_content_str)

            # Update the main opportunities table with the restored content
            conn.execute(
                queries.UPDATE_OPPORTUNITY_AI_CONTENT_AND_STATUS,
                (
                    restored_content_str,
                    current_opp.get(
                        "ai_content_model", "gpt-4o"
                    ),  # Keep the last used model
                    datetime.now().isoformat(),
                    "generated",  # Reset status to 'generated'
                    opportunity_id,
                ),
            )
            self.logger.info(
                f"Successfully restored content from {version_timestamp} for opportunity {opportunity_id}."
            )

        return restored_content

    def update_opportunity_social_posts(
        self, opportunity_id: int, social_media_posts: List[Dict[str, Any]]
    ):
        """Stores the updated social media posts JSON for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving updated social media posts."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_SOCIAL_POSTS,
                (json.dumps(social_media_posts), opportunity_id),
            )

    def update_social_media_posts_status(self, opportunity_id: int, new_status: str):
        """Updates the status of social media posts for an opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Updating social media posts status to '{new_status}'."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                "UPDATE opportunities SET social_media_posts_status = ? WHERE id = ?",
                (new_status, opportunity_id),
            )

    def save_full_content_package(
        self,
        opportunity_id: int,
        ai_content_data: Dict[str, Any],
        ai_model: str,
        featured_image_data: Optional[Dict[str, Any]],
        in_article_images_data: List[Dict[str, Any]],
        social_posts: Optional[List[Dict[str, Any]]],
        final_package: Dict[str, Any],
        total_api_cost: float,
    ):
        """
        Saves the entire generated content package and updates the status to 'generated' in a single transaction.
        Applies server-side sanitization to the main HTML body. (W20 FIX)
        """
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving full content package and finalizing status."
        )

        # W20 FIX: Sanitize the final package HTML content before saving
        html_body = final_package.get("article_html_final")
        if html_body:
            clean_html = bleach.clean(
                html_body, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES_DB
            )
            final_package["article_html_final"] = clean_html

        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_GENERATED_CONTENT_AND_STATUS,
                (
                    json.dumps(ai_content_data),
                    ai_model,
                    featured_image_data.get("remote_url")
                    if featured_image_data
                    else None,
                    featured_image_data.get("local_path")
                    if featured_image_data
                    else None,
                    json.dumps(in_article_images_data),
                    json.dumps(social_posts) if social_posts else None,
                    json.dumps(final_package),
                    datetime.now().isoformat(),
                    total_api_cost,
                    opportunity_id,
                ),
            )
        self.logger.info(
            f"Opportunity {opportunity_id}: Successfully saved full content package and set status to 'generated'."
        )

        def update_opportunity_published_url(self, opportunity_id: int, url: str):
            """Updates the published_url for a specific opportunity."""
            self.logger.info(
                f"Opportunity {opportunity_id}: Storing published URL: {url}"
            )
            conn = self._get_conn()
            with conn:
                conn.execute(
                    "UPDATE opportunities SET published_url = ? WHERE id = ?",
                    (url, opportunity_id),
                )

    def get_high_priority_opportunities(
        self, client_id: str, limit: int = 5
    ) -> List[Dict[str, Any]]:
        """Retrieves the top N validated opportunities with the highest strategic score."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                queries.SELECT_HIGH_PRIORITY_OPPORTUNITIES, (client_id, limit)
            )
            return self._deserialize_rows(cursor.fetchall())

    def get_content_feedback_examples(
        self, client_id: str, limit: int = 2
    ) -> Dict[str, List]:
        """Retrieves examples of good and bad content based on user feedback."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # Get highest rated
            cursor.execute(
                """
                SELECT o.keyword, cf.comments, cf.rating
                FROM content_feedback cf
                JOIN opportunities o ON cf.opportunity_id = o.id
                WHERE o.client_id = ? AND cf.rating >= 4
                ORDER BY cf.rating DESC, cf.timestamp DESC
                LIMIT ?;
            """,
                (client_id, limit),
            )
            good_examples = [dict(row) for row in cursor.fetchall()]

            # Get lowest rated
            cursor.execute(
                """
                SELECT o.keyword, cf.comments, cf.rating
                FROM content_feedback cf
                JOIN opportunities o ON cf.opportunity_id = o.id
                WHERE o.client_id = ? AND cf.rating <= 2
                ORDER BY cf.rating ASC, cf.timestamp DESC
                LIMIT ?;
            """,
                (client_id, limit),
            )
            bad_examples = [dict(row) for row in cursor.fetchall()]

        return {"good_examples": good_examples, "bad_examples": bad_examples}

    def get_qualification_settings(self, client_id: str) -> Dict[str, Any]:
        """Retrieves qualification settings for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT * FROM qualification_settings WHERE client_id = ?", (client_id,)
            )
            row = cursor.fetchone()
            if row:
                return dict(row)
            return {}

    def get_qualification_strategies(self, client_id: str) -> List[Dict[str, Any]]:
        """Retrieves all qualification strategies for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT * FROM qualification_strategies WHERE client_id = ?",
                (client_id,),
            )
            return [dict(row) for row in cursor.fetchall()]

    def get_qualification_strategy_by_id(
        self, strategy_id: int
    ) -> Optional[Dict[str, Any]]:
        """Retrieves a single qualification strategy by its ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT * FROM qualification_strategies WHERE id = ?", (strategy_id,)
            )
            row = cursor.fetchone()
            if row:
                return dict(row)
            return None

    def create_qualification_strategy(
        self, client_id: str, strategy: Dict[str, Any]
    ) -> int:
        """Creates a new qualification strategy for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            keys = ", ".join(strategy.keys())
            placeholders = ", ".join("?" * len(strategy))
            values = list(strategy.values())
            cursor.execute(
                f"INSERT INTO qualification_strategies (client_id, {keys}) VALUES (?, {placeholders})",
                [client_id] + values,
            )
            return cursor.lastrowid

    def update_qualification_strategy(self, strategy_id: int, strategy: Dict[str, Any]):
        """Updates a qualification strategy."""
        conn = self._get_conn()
        with conn:
            set_clauses = []
            values = []
            for key, value in strategy.items():
                set_clauses.append(f"{key} = ?")
                values.append(value)

            if set_clauses:
                update_query = f"UPDATE qualification_strategies SET {', '.join(set_clauses)} WHERE id = ?"
                values.append(strategy_id)
                conn.execute(update_query, values)

    def delete_qualification_strategy(self, strategy_id: int):
        """Deletes a qualification strategy."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                "DELETE FROM qualification_strategies WHERE id = ?", (strategy_id,)
            )

    def update_qualification_settings(self, client_id: str, settings: Dict[str, Any]):
        """Updates qualification settings for a specific client."""
        conn = self._get_conn()
        with conn:
            set_clauses = []
            values = []
            for key, value in settings.items():
                set_clauses.append(f"{key} = ?")
                values.append(value)

            if set_clauses:
                update_query = f"UPDATE qualification_settings SET {', '.join(set_clauses)} WHERE client_id = ?"
                values.append(client_id)
                conn.execute(update_query, values)

    def get_content_snippet_by_slug(self, slug: str) -> Optional[Dict[str, Any]]:
        """Retrieves the title and a content snippet for an opportunity by its slug."""
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(
                """
                SELECT keyword as title,
                       SUBSTR(JSON_EXTRACT(ai_content_json, '$.meta_description'), 1, 200) as snippet_desc
                FROM opportunities WHERE slug = ?;
            """,
                (slug,),
            )
            row = cursor.fetchone()
            if row:
                return dict(row)
        return None

    def fail_stale_jobs(self):
        """Finds all jobs with a 'running' status and marks them as 'failed' on startup."""
        self.logger.info("Scanning for stale jobs from previous sessions...")
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            error_message = "Job failed due to application restart."
            finished_time = time.time()
            cursor.execute(
                """
                UPDATE jobs
                SET status = 'failed', error = ?, finished_at = ?
                WHERE status = 'running';
            """,
                (error_message, finished_time),
            )

            if cursor.rowcount > 0:
                self.logger.warning(
                    f"Marked {cursor.rowcount} stale 'running' jobs as 'failed'."
                )
            else:
                self.logger.info("No stale jobs found.")

        def override_disqualification(self, opportunity_id: int) -> bool:
            """Manually overrides a failed qualification, resetting status to pending."""
            self.logger.info(
                f"Overriding disqualification for opportunity ID: {opportunity_id}"
            )
            conn = self._get_conn()
            with conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    UPDATE opportunities
                    SET status = 'pending', blog_qualification_status = 'passed_manual_override', blog_qualification_reason = 'Manually overridden by user.'
                    WHERE id = ? AND status IN ('failed', 'rejected');
                """,
                    (opportunity_id,),
                )

                if cursor.rowcount > 0:
                    self.logger.info(
                        f"Successfully overrode disqualification for opportunity ID: {opportunity_id}"
                    )
                    return True
                else:
                    self.logger.warning(
                        f"Could not override disqualification for ID: {opportunity_id}. Status was not 'failed' or 'rejected'."
                    )
                    return False
</file>

<file path="backend/data_access/initialize.py">
import logging
import os
import sys

# Ensure the project root is in sys.path for module imports
# Correctly identify the project root, which is two levels above the 'backend' directory
project_root = os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
backend_root = os.path.join(project_root, "backend")

# Add the backend directory to sys.path to allow for absolute imports from the backend module
if backend_root not in sys.path:
    sys.path.insert(0, backend_root)

# Import necessary classes from your project
try:
    from app_config.manager import ConfigManager
    from data_access.database_manager import DatabaseManager
except ImportError as e:
    print(f"Error importing project modules: {e}")
    print(
        "Please ensure you are running this script from the project's root directory or that paths are set up correctly."
    )
    print(f"Current working directory: {os.getcwd()}")
    print(f"Project root: {project_root}")
    print(f"Backend root added to sys.path: {backend_root}")
    sys.exit(1)

# Configure logging for this script run
logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")
logger = logging.getLogger(__name__)


def main():
    """Initializes the database, runs migrations, and seeds the default client."""

    logger.info(
        "Starting database initialization and default client seeding process..."
    )

    # Construct the absolute path to the settings.ini file
    settings_path = os.path.join(backend_root, "app_config", "settings.ini")
    if not os.path.exists(settings_path):
        logger.error(f"settings.ini file not found at expected path: {settings_path}")
        sys.exit(1)

    # Instantiate ConfigManager with the correct path
    config_manager = ConfigManager(settings_path=settings_path)
    global_cfg = config_manager.get_global_config()
    db_file_name = global_cfg.get(
        "db_file_name", "data/opportunities.db"
    )  # Get DB file name from config

    # The database path is relative to the project root
    db_path = os.path.join(project_root, db_file_name)
    db_data_dir = os.path.dirname(db_path)

    # --- Force re-creation of the database ---
    if os.path.exists(db_path):
        logger.info(
            f"Deleting existing database file at {db_path} to ensure a clean slate."
        )
        os.remove(db_path)
    # Ensure the data directory exists
    if not os.path.exists(db_data_dir):
        os.makedirs(db_data_dir)
        logger.info(f"Created data directory: {db_data_dir}")
    # --- End ---

    try:
        # 1. Initialize DatabaseManager
        db_manager = DatabaseManager(cfg_manager=config_manager, db_path=db_path)

        # 2. Ensure database and all tables are created, and migrations are applied
        db_manager.initialize()
        logger.info("Database initialized and all migrations applied.")

        # 3. Define default client details (using values from settings.ini)
        client_id = global_cfg.get("default_client_id", "Lark_Main_Site")
        client_name = client_id.replace(
            "_", " "
        ).title()  # Simple conversion for display name

        # 4. Get default settings template (which includes brand voice settings from settings.ini)
        default_settings = config_manager.get_default_client_settings_template()

        if not default_settings:
            logger.error("Could not load default client settings template. Aborting.")
            return

        # 5. Add the default client
        success = db_manager.add_client(client_id, client_name, default_settings)

        if success:
            logger.info(
                f"Successfully seeded the database with default client: '{client_name}' ({client_id})"
            )
        else:
            # This would only happen if the client_id already existed, which shouldn't after deleting the DB.
            logger.warning(
                f"Default client '{client_name}' ({client_id}) already existed in the database."
            )

        logger.info("Database setup process complete. No keyword data has been added.")

    except Exception as e:
        logger.error(f"An error occurred during database setup: {e}", exc_info=True)


if __name__ == "__main__":
    main()
</file>

<file path="backend/data_access/models.py">
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional


@dataclass
class KeywordData:
    keyword: str
    search_volume: int
    keyword_difficulty: int
    main_intent: str
    cpc: float
    search_volume_trend: Optional[Dict[str, Any]] = None
    core_keyword: Optional[str] = None
    # Add other fields as they come from DataForSEO API


@dataclass
class CompetitorPage:
    url: str
    rank: int
    word_count: int
    readability_score: float
    technical_warnings: List[str] = field(default_factory=list)
    headings: Dict[str, List[str]] = field(default_factory=dict)
    full_content_plain_text: Optional[str] = None
    error: Optional[str] = None


@dataclass
class SerpOverview:
    serp_has_featured_snippet: bool
    serp_has_video_results: bool
    serp_has_ai_overview: bool
    people_also_ask: List[str] = field(default_factory=list)
    ai_overview_content: Optional[str] = None
    featured_snippet_content: Optional[str] = None
    avg_referring_domains_top5_organic: Optional[float] = None
    avg_main_domain_rank_top5_organic: Optional[float] = None
    serp_last_updated_days_ago: Optional[int] = None
    dominant_content_format: Optional[str] = None


@dataclass
class ContentIntelligence:
    recommended_word_count: int
    average_readability_score: float
    common_headings_to_cover: List[str] = field(default_factory=list)
    unique_angles_to_include: List[str] = field(default_factory=list)
    ai_generated_outline_h2: List[str] = field(default_factory=list)
    ai_generated_outline_h3: List[str] = field(default_factory=list)


@dataclass
class AIBrief:
    target_keyword: str
    content_type: str
    target_audience_persona: str
    primary_goal: str
    target_word_count: int
    mandatory_sections: List[str] = field(default_factory=list)
    unique_angles_to_cover: List[str] = field(default_factory=list)
    questions_to_answer_directly: List[str] = field(default_factory=list)
    internal_linking_suggestions: List[Dict[str, str]] = field(default_factory=list)
    dynamic_serp_instructions: List[str] = field(default_factory=list)
    source_and_inspiration_content: Dict[str, Any] = field(default_factory=dict)
    client_id: str = "default"


@dataclass
class Blueprint:
    metadata: Dict[str, Any]
    winning_keyword: Dict[
        str, Any
    ]  # Full keyword data for the selected winning keyword
    serp_overview: SerpOverview
    content_intelligence: ContentIntelligence
    competitor_analysis: List[CompetitorPage]
    executive_summary: str
    ai_content_brief: AIBrief


@dataclass
class AIContentPackage:
    article_body_html: str
    meta_title: str
    meta_description: str
    social_blurbs: List[Dict[str, Any]]
    editor_notes: str
    ai_focus_keyword: str


@dataclass
class GeneratedImage:
    type: str  # 'featured' or 'in_article_N'
    original_prompt: str
    enhanced_prompt: str
    revised_prompt: str
    local_path: str
    model: str
    wordpress_id: Optional[int] = None
    remote_url: Optional[str] = None
    alt_text: Optional[str] = None
    insertion_marker: Optional[str] = None
    error: Optional[str] = None


@dataclass
class Opportunity:
    id: Optional[int]
    keyword: str  # This is the cluster_topic
    status: str
    client_id: str
    date_added: datetime
    date_processed: Optional[datetime]
    scheduled_for: Optional[datetime]
    full_data: Dict[str, Any]  # Original keyword data
    blueprint_data: Optional[Blueprint]
    ai_content_json: Optional[AIContentPackage]
    ai_content_model: Optional[str]
    featured_image_prompt: Optional[str]
    featured_image_model: Optional[str]
    featured_image_url: Optional[str]
    featured_image_local_path: Optional[str]
    in_article_images_data: List[GeneratedImage] = field(default_factory=list)
    wordpress_post_url: Optional[str]
    wordpress_post_id: Optional[int]
    social_media_posts_json: List[Dict[str, Any]] = field(default_factory=list)
    last_workflow_step: Optional[str]
    error_message: Optional[str]
    search_volume: Optional[int] = None
    keyword_difficulty: Optional[int] = None


@dataclass
class Client:
    client_id: str
    client_name: str
    date_created: datetime


@dataclass
class ClientSettings:
    client_id: str
    openai_api_key: Optional[str] = None
    pexels_api_key: Optional[str] = None  # NEW, was missing from dataclass
    wordpress_url: Optional[str] = None
    wordpress_user: Optional[str] = None
    wordpress_app_password: Optional[str] = None
    hootsuite_api_key: Optional[str] = None
    custom_ai_prompt_template: Optional[str] = None
    ai_image_style_formula: Optional[str] = None
    featured_image_base_prompt: Optional[str] = None
    wordpress_seo_plugin: Optional[str] = None
    ai_content_model: Optional[str] = None
    image_ai_model: Optional[str] = None
    num_in_article_images: Optional[int] = None
    image_quality: Optional[str] = None
    enable_automated_internal_linking: Optional[bool] = False
    default_wordpress_categories: List[str] = field(default_factory=list)
    default_wordpress_tags: List[str] = field(default_factory=list)
    negative_keywords: List[str] = field(default_factory=list)
    competitor_blacklist_domains: List[str] = field(default_factory=list)
    require_question_keywords: Optional[bool] = None
    enforce_intent_filter: Optional[bool] = None
    allowed_intents: List[str] = field(default_factory=list)
    max_competition_level: Optional[str] = None
    discovery_order_by: Optional[str] = None
    db_type: Optional[str] = "sqlite"  # NEW
    max_words_for_ai_analysis: Optional[int] = 1500  # NEW
    ai_generation_temperature: Optional[float] = 0.7  # NEW
    recommended_word_count_multiplier: Optional[float] = 1.2  # NEW
    max_avg_lcp_time: Optional[int] = 4000  # NEW
    prohibited_intents: List[str] = field(default_factory=list)  # NEW
    last_updated: datetime = field(default_factory=datetime.now)
</file>

<file path="backend/data_access/queries.py">
# data_access/queries.py

# --- Table Creation ---
CREATE_OPPORTUNITIES_TABLE = """
CREATE TABLE IF NOT EXISTS opportunities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    keyword TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending',
    client_id TEXT NOT NULL DEFAULT 'default',
    date_added TEXT NOT NULL,
    date_processed TEXT,
    strategic_score REAL,
    blog_qualification_status TEXT,
    blog_qualification_reason TEXT,
    keyword_info TEXT,
    keyword_properties TEXT,
    search_intent_info TEXT,
    serp_overview TEXT,
    score_breakdown TEXT,
    full_data TEXT NOT NULL,
    blueprint_data TEXT,
    ai_content_json TEXT,
    ai_content_model TEXT,
    featured_image_url TEXT,
    featured_image_local_path TEXT,
    in_article_images_data TEXT,
    social_media_posts_json TEXT,
    social_media_posts_status TEXT DEFAULT 'draft', -- NEW COLUMN
    last_workflow_step TEXT,
    error_message TEXT,
    wordpress_payload_json TEXT,
    final_package_json TEXT,
    slug TEXT UNIQUE,
    run_id INTEGER,
    keyword_info_normalized_with_bing TEXT,
    keyword_info_normalized_with_clickstream TEXT,
    monthly_searches TEXT,
    traffic_value REAL DEFAULT 0,
    check_url TEXT,
    related_keywords TEXT,
    keyword_categories TEXT,
     core_keyword TEXT,
    published_url TEXT,
     UNIQUE(keyword, client_id)
 );"""

CREATE_CLIENTS_TABLE = """
CREATE TABLE IF NOT EXISTS clients (
    client_id TEXT PRIMARY KEY,
    client_name TEXT NOT NULL,
    date_created TEXT NOT NULL
);
"""

CREATE_CLIENT_SETTINGS_TABLE = """
CREATE TABLE IF NOT EXISTS client_settings (
    client_id TEXT PRIMARY KEY,
    openai_api_key TEXT,
    pexels_api_key TEXT,
    location_code INTEGER,
    language_code TEXT,
    target_domain TEXT,
    device TEXT,
    os TEXT,
    informational_score REAL,
    commercial_score REAL,
    transactional_score REAL,
    navigational_score REAL,
    question_keyword_bonus REAL,
    ease_of_ranking_weight INTEGER,
    traffic_potential_weight INTEGER,
    commercial_intent_weight INTEGER,
    growth_trend_weight INTEGER,
    serp_features_weight INTEGER,
    serp_freshness_weight INTEGER,
    serp_volatility_weight INTEGER,
    competitor_weakness_weight INTEGER,
    max_cpc_for_scoring REAL,
    max_sv_for_scoring INTEGER,
    max_domain_rank_for_scoring INTEGER,
    max_referring_domains_for_scoring INTEGER,
    max_avg_referring_domains_filter INTEGER,
    featured_snippet_bonus REAL,
    ai_overview_bonus REAL,
    serp_freshness_bonus_max REAL,
    serp_freshness_old_threshold_days INTEGER,
    serp_volatility_stable_threshold_days INTEGER,
    enforce_intent_filter INTEGER,
    allowed_intents TEXT,
    require_question_keywords INTEGER,
    negative_keywords TEXT,
    min_monthly_trend_percentage REAL,
    min_competitor_word_count INTEGER,
    max_competitor_technical_warnings INTEGER,
    competitor_blacklist_domains TEXT,
    ugc_and_parasite_domains TEXT,
    num_competitors_to_analyze INTEGER,
    num_common_headings INTEGER,
    num_unique_angles INTEGER,
    max_initial_serp_urls_to_analyze INTEGER,
    calculate_rectangles INTEGER,
    people_also_ask_click_depth INTEGER,
    min_search_volume INTEGER,
    max_keyword_difficulty INTEGER,
    ai_content_model TEXT,
    num_in_article_images INTEGER,
    use_pexels_first INTEGER,
    cleanup_local_images INTEGER,
    onpage_enable_javascript INTEGER,
    onpage_load_resources INTEGER,
    onpage_disable_cookie_popup INTEGER,
    onpage_return_despite_timeout INTEGER,
    onpage_enable_browser_rendering INTEGER,
    onpage_store_raw_html INTEGER,
    onpage_validate_micromarkup INTEGER,
    onpage_check_spell INTEGER,
    onpage_accept_language TEXT,
    onpage_custom_user_agent TEXT,
    onpage_max_domains_per_request INTEGER,
    onpage_max_tasks_per_request INTEGER,
    onpage_enable_switch_pool INTEGER,
    onpage_browser_screen_resolution_ratio REAL,
    discovery_exact_match INTEGER,
    onpage_enable_custom_js INTEGER,
    onpage_custom_js TEXT,
    platforms TEXT,


    enable_automated_internal_linking INTEGER,
    db_type TEXT,
    max_words_for_ai_analysis INTEGER,
    ai_generation_temperature REAL,
    recommended_word_count_multiplier REAL,
    max_avg_lcp_time INTEGER,
    prohibited_intents TEXT,
    load_async_ai_overview INTEGER,
    onpage_custom_checks_thresholds TEXT,
    serp_remove_from_url_params TEXT,
     schema_author_type TEXT,
     client_knowledge_base TEXT,
    wordpress_url TEXT,
    wordpress_user TEXT,
    wordpress_app_password TEXT,
    wordpress_seo_plugin TEXT,
    default_wordpress_categories TEXT,
    default_wordpress_tags TEXT,
     last_updated TEXT NOT NULL,
     FOREIGN KEY (client_id) REFERENCES clients (client_id)
 );"""

INSERT_CLIENT_SETTINGS = """
INSERT INTO client_settings (client_id, brand_tone, target_audience, terms_to_avoid) VALUES (%s, %s, %s, %s)
ON CONFLICT (client_id) DO UPDATE SET brand_tone = %s, target_audience = %s, terms_to_avoid = %s
"""

SELECT_CLIENT_SETTINGS = """
SELECT brand_tone, target_audience, terms_to_avoid FROM client_settings WHERE client_id = %s;
"""

CREATE_DISCOVERY_RUNS_TABLE = """
CREATE TABLE IF NOT EXISTS discovery_runs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    client_id TEXT NOT NULL,
    start_time TEXT NOT NULL,
    end_time TEXT,
    status TEXT NOT NULL,
    parameters TEXT,
    results_summary TEXT,
    log_file_path TEXT,
    error_message TEXT,
    FOREIGN KEY (client_id) REFERENCES clients (client_id)
);
"""

CREATE_SCHEMA_VERSION_TABLE = """
CREATE TABLE IF NOT EXISTS schema_version (
    version INTEGER PRIMARY KEY,
    applied_at TEXT NOT NULL
);
"""

# NEW: Add a query to get current schema version
GET_SCHEMA_VERSION = """
SELECT version FROM schema_version ORDER BY version DESC LIMIT 1;
"""

# NEW: Add a query to insert schema version
INSERT_SCHEMA_VERSION = """
INSERT INTO schema_version (version, applied_at) VALUES (?, ?);
"""

CREATE_API_CACHE_TABLE = """
CREATE TABLE IF NOT EXISTS api_cache (
    key TEXT PRIMARY KEY,
    data TEXT NOT NULL,
    timestamp REAL NOT NULL,
    ttl_days INTEGER NOT NULL
);
"""

# --- Cache table queries (related to migration 002_add_slug_and_cache_table.sql)
INSERT_API_CACHE = """
INSERT OR REPLACE INTO api_cache (key, data, timestamp, ttl_days)
VALUES (?, ?, ?, ?);
"""

SELECT_API_CACHE = """
SELECT data, timestamp, ttl_days FROM api_cache WHERE key = ?;
"""

DELETE_EXPIRED_API_CACHE = """
DELETE FROM api_cache WHERE timestamp + (ttl_days * 86400) < ?;
"""

DELETE_API_CACHE_BY_KEY = """
DELETE FROM api_cache WHERE key = ?;
"""

TRUNCATE_API_CACHE = """
DELETE FROM api_cache;
"""

# --- Opportunity Queries ---
INSERT_OPPORTUNITY_OR_IGNORE = """
INSERT OR IGNORE INTO opportunities 
(keyword, client_id, date_added, full_data)
VALUES (?, ?, ?, ?);
"""

INSERT_OPPORTUNITY_WITH_BLUEPRINT = """
INSERT OR IGNORE INTO opportunities
(keyword, client_id, date_added, full_data, blueprint_data, slug)
VALUES (?, ?, ?, ?, ?, ?);
"""

SELECT_PENDING_OPPORTUNITIES = """
SELECT * FROM opportunities 
WHERE status = 'pending' AND client_id = ?;
"""

SELECT_ALL_OPPORTUNITIES_BY_CLIENT = """
SELECT * FROM opportunities 
WHERE client_id = ?
ORDER BY date_added DESC;
"""

SELECT_OPPORTUNITY_BY_ID = """
SELECT * FROM opportunities WHERE id = ?;
"""

SELECT_ALL_PROCESSED_KEYWORDS = """
SELECT keyword FROM opportunities 
WHERE client_id = ? AND status NOT IN ('rejected', 'failed');
"""

UPDATE_OPPORTUNITY_STATUS_WITH_DATE = """
UPDATE opportunities SET status = ?, date_processed = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_STATUS = """
UPDATE opportunities SET status = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_WORKFLOW_STATE = """
UPDATE opportunities SET last_workflow_step = ?, status = ?, error_message = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_BLUEPRINT = """
UPDATE opportunities
SET blueprint_data = ?
WHERE id = ?;
"""

UPDATE_OPPORTUNITY_BLUEPRINT_AND_SLUG = """
UPDATE opportunities
SET blueprint_data = ?, slug = ?
WHERE id = ?;
"""

UPDATE_OPPORTUNITY_AI_CONTENT = """
UPDATE opportunities
SET ai_content_json = ?, ai_content_model = ?, date_processed = ?
WHERE id = ?;
"""

UPDATE_OPPORTUNITY_IMAGES = """
UPDATE opportunities SET featured_image_url = ?, featured_image_local_path = ?, in_article_images_data = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_SOCIAL_POSTS = """
UPDATE opportunities SET social_media_posts_json = ? WHERE id = ?;
"""

SELECT_ALL_OPPORTUNITIES_FOR_EXPORT = """
SELECT * FROM opportunities ORDER BY client_id, date_added DESC;
"""

SELECT_PROCESSED_OPPORTUNITIES = """
SELECT * FROM opportunities
WHERE client_id = ? AND blueprint_data IS NOT NULL
ORDER BY date_processed DESC;
"""

UPDATE_OPPORTUNITY_WORDPRESS_PAYLOAD = (
    "UPDATE opportunities SET wordpress_payload_json = ? WHERE id = ?;"
)

UPDATE_OPPORTUNITY_FINAL_PACKAGE = (
    "UPDATE opportunities SET final_package_json = ? WHERE id = ?;"
)

# --- Dashboard Queries ---
COUNT_OPPORTUNITIES_BY_STATUS = """
SELECT status, COUNT(*) as count FROM opportunities WHERE client_id = ? GROUP BY status;
"""

SUM_DISCOVERY_COST_BY_CLIENT = """
SELECT SUM(CAST(JSON_EXTRACT(results_summary, '$.total_cost') AS REAL)) FROM discovery_runs WHERE client_id = ?;
"""

SUM_ANALYSIS_COST_BY_CLIENT = """
SELECT SUM(CAST(JSON_EXTRACT(blueprint_data, '$.metadata.total_api_cost') AS REAL)) FROM opportunities WHERE client_id = ?;
"""

SELECT_RECENTLY_GENERATED = """
SELECT id, keyword, status, date_processed FROM opportunities 
WHERE client_id = ? AND status = 'generated' 
ORDER BY date_processed DESC 
LIMIT 5;
"""


# --- Client Queries ---
INSERT_CLIENT = """
INSERT INTO clients (client_id, client_name, date_created) VALUES (?, ?, ?);
"""

SELECT_ALL_CLIENTS = """
SELECT client_id, client_name FROM clients ORDER BY date_created DESC;
"""

SELECT_CLIENT_SETTINGS = """
SELECT * FROM client_settings WHERE client_id = ?;
"""

# --- Discovery Run Queries ---
INSERT_DISCOVERY_RUN = """
INSERT INTO discovery_runs (client_id, start_time, status, parameters)
VALUES (?, ?, ?, ?);
"""

UPDATE_DISCOVERY_RUN_STATUS = """
UPDATE discovery_runs SET status = ? WHERE id = ?;
"""

UPDATE_DISCOVERY_RUN_COMPLETED = """
UPDATE discovery_runs SET end_time = ?, status = 'completed', results_summary = ?, total_api_cost = ? WHERE id = ?;
"""

UPDATE_DISCOVERY_RUN_FAILED = """
UPDATE discovery_runs SET end_time = ?, status = 'failed', error_message = ? WHERE id = ?;
"""

SELECT_ALL_DISCOVERY_RUNS_BY_CLIENT = """
SELECT * FROM discovery_runs WHERE client_id = ? ORDER BY start_time DESC;
"""

SELECT_ALL_DISCOVERY_RUNS_BY_CLIENT_PAGINATED = """
SELECT * FROM discovery_runs WHERE client_id = ? ORDER BY start_time DESC LIMIT ? OFFSET ?;
"""

SELECT_DISCOVERY_RUN_BY_ID = """
SELECT * FROM discovery_runs WHERE id = ?;
"""

SELECT_KEYWORDS_FOR_RUN_BY_REASON = """
SELECT * FROM opportunities WHERE run_id = ? AND blog_qualification_reason = ?;
"""

UPDATE_DISCOVERY_RUN_LOG_PATH = """
UPDATE discovery_runs SET log_file_path = ? WHERE id = ?;
"""

SELECT_OPPORTUNITY_BY_SLUG = """

SELECT * FROM opportunities WHERE slug = ?;

"""


CREATE_JOBS_TABLE = """

CREATE TABLE IF NOT EXISTS jobs (

    id TEXT PRIMARY KEY,

    status TEXT NOT NULL,

    progress INTEGER NOT NULL,

    result TEXT,

    error TEXT,

    started_at REAL NOT NULL,

    finished_at REAL

);

"""

GET_JOB = "SELECT * FROM jobs WHERE id = ?;"

UPDATE_JOB_STATUS_DIRECT = """
UPDATE jobs SET status = ?, progress = ?, finished_at = ? WHERE id = ?;
"""

UPDATE_JOB = """

INSERT OR REPLACE INTO jobs (id, status, progress, result, error, started_at, finished_at)

VALUES (?, ?, ?, ?, ?, ?, ?);

"""

GET_ALL_JOBS = "SELECT * FROM jobs ORDER BY started_at DESC LIMIT 100;"

CREATE_CONTENT_HISTORY_TABLE = """
CREATE TABLE IF NOT EXISTS content_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    timestamp TEXT NOT NULL,
    ai_content_json TEXT NOT NULL,
    restored_from_id INTEGER,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);
"""

# In data_access/queries.py, add history queries:
INSERT_CONTENT_HISTORY = """
INSERT INTO content_history
(opportunity_id, timestamp, ai_content_json)
VALUES (?, ?, ?);
"""

SELECT_CONTENT_HISTORY_BY_OPP_ID = """
SELECT id, opportunity_id, timestamp, ai_content_json FROM content_history
WHERE opportunity_id = ?
ORDER BY timestamp DESC;
"""

# Update/Add status update query:
UPDATE_OPPORTUNITY_AI_CONTENT_AND_STATUS = """
UPDATE opportunities
SET ai_content_json = ?, ai_content_model = ?, date_processed = ?, status = ?
WHERE id = ?;
"""

# In data_access/queries.py, define template queries:
COUNT_ALL_OPPORTUNITIES_BY_CLIENT = """
SELECT COUNT(*) FROM opportunities
WHERE client_id = ? 
"""

SELECT_ALL_OPPORTUNITIES_PAGINATED = """
SELECT {select_columns} FROM opportunities
WHERE client_id = ? {where_clause}
ORDER BY {order_by} {order_direction}
LIMIT ? OFFSET ?;
"""

SEARCH_OPPORTUNITIES_BY_KEYWORD = """
SELECT id, keyword, status FROM opportunities
WHERE client_id = ? AND keyword LIKE ?
ORDER BY date_added DESC
LIMIT 20;
"""

SELECT_HIGH_PRIORITY_OPPORTUNITIES = """
SELECT id, keyword, strategic_score, score_breakdown, keyword_info, keyword_properties, traffic_value FROM opportunities
WHERE client_id = ? AND status = 'validated'
ORDER BY strategic_score DESC
LIMIT ?;
"""

SELECT_ACTION_ITEMS = """
SELECT id, keyword, status, error_message, COALESCE(date_processed, date_added) as updated_at, strategic_score FROM opportunities
WHERE client_id = ? AND status IN ('paused_for_approval', 'failed')
ORDER BY updated_at DESC;
"""

CREATE_CONTENT_FEEDBACK_TABLE = """
CREATE TABLE IF NOT EXISTS content_feedback (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    rating INTEGER NOT NULL, -- 1-5 scale
    comments TEXT,
    timestamp TEXT NOT NULL,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);
"""

INSERT_CONTENT_FEEDBACK = """
INSERT INTO content_feedback (opportunity_id, rating, comments, timestamp)
VALUES (?, ?, ?, ?);
"""

UPDATE_OPPORTUNITY_SCORES = """
UPDATE opportunities
SET strategic_score = ?, score_breakdown = ?, blueprint_data = ?
WHERE id = ?;
"""

UPDATE_GENERATED_CONTENT_AND_STATUS = """
UPDATE opportunities
SET
    ai_content_json = ?,
    ai_content_model = ?,
    featured_image_url = ?,
    featured_image_local_path = ?,
    in_article_images_data = ?,
    social_media_posts_json = ?,
    final_package_json = ?,
    status = 'generated',
    last_workflow_step = 'generation_complete',
    date_processed = ?,
    total_api_cost = ?
WHERE id = ?;
"""

INSERT_DEFAULT_QUALIFICATION_SETTINGS = """
INSERT INTO qualification_settings (
    client_id, ease_of_ranking_weight, traffic_potential_weight, commercial_intent_weight,
    serp_features_weight, growth_trend_weight, serp_freshness_weight, serp_volatility_weight,
    competitor_weakness_weight, competitor_performance_weight, min_search_volume, max_keyword_difficulty,
    negative_keywords, prohibited_intents, max_y_pixel_threshold,
    max_forum_results_in_top_10, max_ecommerce_results_in_top_10,
    disallowed_page_types_in_top_3
) VALUES (?, 40, 15, 5, 5, 5, 5, 5, 20, 5, 100, 80, 'login,free,cheap', 'navigational', 800, 3, 2, 'E-commerce,Forum')
"""

FAIL_STALE_JOBS = """
UPDATE jobs
SET status = 'failed', error = ?, finished_at = ?
WHERE status = 'running';
"""
</file>

<file path="backend/data_mappers/dataforseo_mapper.py">
# FILE: data_mappers/dataforseo_mapper.py

from typing import Dict, Any
import logging
from backend.core.utils import parse_datetime_string  # ADDED IMPORT
import json

logger = logging.getLogger(__name__)


class DataForSEOMapper:
    """
    Provides static methods to sanitize and normalize raw DataForSEO API responses
    immediately after they are received, before they enter the main pipeline.
    Ensures consistent data types and handles common API quirks.
    """

    @staticmethod
    def _sanitize_serp_info(serp_info: Dict[str, Any]) -> Dict[str, Any]:
        """Sanitizes the 'serp_info' object, specifically handling 'se_results_count' and datetimes."""
        if isinstance(serp_info.get("se_results_count"), str):
            try:
                serp_info["se_results_count"] = int(serp_info["se_results_count"])
            except (ValueError, TypeError):
                logger.warning(
                    f"Failed to convert 'se_results_count' (was string) to int. Setting to 0. Raw: {serp_info.get('se_results_count')}"
                )
                serp_info["se_results_count"] = 0

        # Sanitize datetime fields
        serp_info["last_updated_time"] = parse_datetime_string(
            serp_info.get("last_updated_time")
        )
        serp_info["previous_updated_time"] = parse_datetime_string(
            serp_info.get("previous_updated_time")
        )

        return serp_info

    @staticmethod
    def sanitize_keyword_data_item(item: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sanitizes a single keyword data item (e.g., from keyword_ideas, keyword_suggestions, related_keywords).
        Applies cleaning to nested structures like 'keyword_info', 'keyword_properties', 'serp_info'.
        """
        if not isinstance(item, dict):
            logger.warning(
                f"Invalid item type received for sanitization: {type(item)}. Skipping."
            )
            return {}

        sanitized_item = item.copy()

        # Sanitize keyword_info
        if isinstance(sanitized_item.get("keyword_info"), dict):
            # Ensure CPC and Competition are floats, defaulting to 0.0 if missing or None
            sanitized_item["keyword_info"]["cpc"] = float(
                sanitized_item["keyword_info"].get("cpc") or 0.0
            )
            sanitized_item["keyword_info"]["competition"] = float(
                sanitized_item["keyword_info"].get("competition") or 0.0
            )

            # Ensure other numeric fields are handled
            sanitized_item["keyword_info"]["search_volume"] = int(
                sanitized_item["keyword_info"].get("search_volume") or 0
            )
            sanitized_item["keyword_info"]["low_top_of_page_bid"] = float(
                sanitized_item["keyword_info"].get("low_top_of_page_bid") or 0.0
            )
            sanitized_item["keyword_info"]["high_top_of_page_bid"] = float(
                sanitized_item["keyword_info"].get("high_top_of_page_bid") or 0.0
            )

            # Sanitize last_updated_time
            sanitized_item["keyword_info"]["last_updated_time"] = parse_datetime_string(
                sanitized_item["keyword_info"].get("last_updated_time")
            )

            # Ensure monthly_searches are properly parsed if they are raw strings
            if isinstance(sanitized_item["keyword_info"].get("monthly_searches"), str):
                try:
                    sanitized_item["keyword_info"]["monthly_searches"] = json.loads(
                        sanitized_item["keyword_info"]["monthly_searches"]
                    )
                except json.JSONDecodeError:
                    logger.warning(
                        f"Failed to parse monthly_searches JSON string for keyword '{sanitized_item.get('keyword')}'. Resetting."
                    )
                    sanitized_item["keyword_info"]["monthly_searches"] = []

            # Ensure individual monthly_searches items are sanitized for type consistency
            if isinstance(sanitized_item["keyword_info"].get("monthly_searches"), list):
                for month_data in sanitized_item["keyword_info"]["monthly_searches"]:
                    if isinstance(month_data, dict):
                        month_data["year"] = int(month_data.get("year") or 0)
                        month_data["month"] = int(month_data.get("month") or 0)
                        month_data["search_volume"] = int(
                            month_data.get("search_volume") or 0
                        )

        # Sanitize keyword_properties
        if isinstance(sanitized_item.get("keyword_properties"), dict):
            sanitized_item["keyword_properties"]["keyword_difficulty"] = int(
                sanitized_item["keyword_properties"].get("keyword_difficulty") or 0
            )

        # Sanitize search_intent_info
        if isinstance(sanitized_item.get("search_intent_info"), dict):
            sanitized_item["search_intent_info"]["foreign_intent"] = (
                sanitized_item["search_intent_info"].get("foreign_intent") or []
            )
            sanitized_item["search_intent_info"]["last_updated_time"] = (
                parse_datetime_string(
                    sanitized_item["search_intent_info"].get("last_updated_time")
                )
            )

        # Sanitize serp_info (crucial for se_results_count string/int issue)
        if isinstance(sanitized_item.get("serp_info"), dict):
            sanitized_item["serp_info"] = DataForSEOMapper._sanitize_serp_info(
                sanitized_item["serp_info"]
            )
            sanitized_item["serp_info"]["serp_item_types"] = (
                sanitized_item["serp_info"].get("serp_item_types") or []
            )

        # Sanitize avg_backlinks_info
        if isinstance(sanitized_item.get("avg_backlinks_info"), dict):
            for key in [
                "backlinks",
                "dofollow",
                "referring_pages",
                "referring_domains",
                "referring_main_domains",
                "rank",
                "main_domain_rank",
            ]:
                sanitized_item["avg_backlinks_info"][key] = float(
                    sanitized_item["avg_backlinks_info"].get(key) or 0.0
                )
            sanitized_item["avg_backlinks_info"]["last_updated_time"] = (
                parse_datetime_string(
                    sanitized_item["avg_backlinks_info"].get("last_updated_time")
                )
            )

        # Keyword Info Normalized with Bing
        for normalized_key in [
            "keyword_info_normalized_with_bing",
            "keyword_info_normalized_with_clickstream",
        ]:
            if isinstance(sanitized_item.get(normalized_key), dict):
                normalized_data = sanitized_item[normalized_key]
                normalized_data["search_volume"] = int(
                    normalized_data.get("search_volume") or 0
                )
                normalized_data["last_updated_time"] = parse_datetime_string(
                    normalized_data.get("last_updated_time")
                )
                if isinstance(normalized_data.get("monthly_searches"), list):
                    for month_data in normalized_data["monthly_searches"]:
                        if isinstance(month_data, dict):
                            month_data["year"] = int(month_data.get("year") or 0)
                            month_data["month"] = int(month_data.get("month") or 0)
                            month_data["search_volume"] = int(
                                month_data.get("search_volume") or 0
                            )

        return sanitized_item

    @staticmethod
    def sanitize_serp_overview_response(serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sanitizes a full SERP overview response from FullSerpAnalyzer.analyze_serp.
        Ensures consistent types for nested items, especially 'pixel_rank_data' and 'top_organic_results'.
        """
        sanitized_serp_data = serp_data.copy()

        # Sanitize top-level date/time fields
        sanitized_serp_data["datetime"] = parse_datetime_string(
            sanitized_serp_data.get("datetime")
        )
        sanitized_serp_data["last_updated_time"] = parse_datetime_string(
            sanitized_serp_data.get("last_updated_time")
        )
        sanitized_serp_data["previous_updated_time"] = parse_datetime_string(
            sanitized_serp_data.get("previous_updated_time")
        )

        # Sanitize pixel_rank_data
        if isinstance(sanitized_serp_data.get("raw_pixel_ranking_data"), list):
            for item in sanitized_serp_data["raw_pixel_ranking_data"]:
                if isinstance(item.get("rectangle"), dict):
                    item["rectangle"]["x"] = float(item["rectangle"].get("x") or 0.0)
                    item["rectangle"]["y"] = float(item["rectangle"].get("y") or 0.0)
                    item["rectangle"]["width"] = float(
                        item["rectangle"].get("width") or 0.0
                    )
                    item["rectangle"]["height"] = float(
                        item["rectangle"].get("height") or 0.0
                    )
                item["rank_absolute"] = int(item.get("rank_absolute") or 0)
                item["rank_group"] = int(item.get("rank_group") or 0)

        if sanitized_serp_data.get("first_organic_y_pixel") is not None:
            sanitized_serp_data["first_organic_y_pixel"] = float(
                sanitized_serp_data["first_organic_y_pixel"]
            )

        # Sanitize top_organic_results
        if isinstance(sanitized_serp_data.get("top_organic_results"), list):
            for result in sanitized_serp_data["top_organic_results"]:
                result["rank"] = int(result.get("rank") or 0)
                if isinstance(result.get("rating"), dict):
                    result["rating"]["value"] = float(
                        result["rating"].get("value") or 0.0
                    )
                    result["rating"]["votes_count"] = int(
                        result["rating"].get("votes_count") or 0
                    )
                    result["rating"]["rating_max"] = int(
                        result["rating"].get("rating_max") or 0
                    )

                # Ensure about_this_result_search_terms and related_terms are lists
                result["about_this_result_search_terms"] = (
                    result.get("about_this_result_search_terms") or []
                )
                result["about_this_result_related_terms"] = (
                    result.get("about_this_result_related_terms") or []
                )

        sanitized_serp_data["people_also_ask"] = (
            sanitized_serp_data.get("people_also_ask") or []
        )
        sanitized_serp_data["related_searches"] = (
            sanitized_serp_data.get("related_searches") or []
        )
        sanitized_serp_data["extracted_serp_features"] = (
            sanitized_serp_data.get("extracted_serp_features") or []
        )

        if isinstance(sanitized_serp_data.get("ai_overview_items"), list):
            for ai_item in sanitized_serp_data["ai_overview_items"]:
                ai_item["rank_group"] = int(ai_item.get("rank_group") or 0)
                ai_item["rank_absolute"] = int(ai_item.get("rank_absolute") or 0)
                if isinstance(ai_item.get("references"), list):
                    for ref in ai_item["references"]:
                        ref["date"] = parse_datetime_string(ref.get("date"))
                        ref["timestamp"] = parse_datetime_string(ref.get("timestamp"))
                if isinstance(ai_item.get("table"), dict):
                    if not isinstance(ai_item["table"].get("table_header"), list):
                        ai_item["table"]["table_header"] = []
                    if not isinstance(ai_item["table"].get("table_content"), list):
                        ai_item["table"]["table_content"] = []

        return sanitized_serp_data

    @staticmethod
    def sanitize_onpage_data_item(item: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sanitizes a single OnPage API response item from `get_onpage_data_for_urls`.
        Ensures consistent data types for numeric fields, especially in 'meta' and 'page_timing'.
        """
        sanitized_item = item.copy()

        # Sanitize meta fields
        meta = sanitized_item.get("meta", {})
        if isinstance(meta, dict):
            meta["charset"] = int(meta.get("charset") or 0)
            meta["internal_links_count"] = int(meta.get("internal_links_count") or 0)
            meta["external_links_count"] = int(meta.get("external_links_count") or 0)
            meta["inbound_links_count"] = int(meta.get("inbound_links_count") or 0)
            meta["images_count"] = int(meta.get("images_count") or 0)
            meta["images_size"] = int(meta.get("images_size") or 0)
            meta["scripts_count"] = int(meta.get("scripts_count") or 0)
            meta["scripts_size"] = int(meta.get("scripts_size") or 0)
            meta["stylesheets_count"] = int(meta.get("stylesheets_count") or 0)
            meta["stylesheets_size"] = int(meta.get("stylesheets_size") or 0)
            meta["title_length"] = int(meta.get("title_length") or 0)
            meta["description_length"] = int(meta.get("description_length") or 0)
            meta["render_blocking_scripts_count"] = int(
                meta.get("render_blocking_scripts_count") or 0
            )
            meta["render_blocking_stylesheets_count"] = int(
                meta.get("render_blocking_stylesheets_count") or 0
            )
            meta["cumulative_layout_shift"] = float(
                meta.get("cumulative_layout_shift") or 0.0
            )

            # Sanitize date/time fields
            meta["last_updated_time"] = parse_datetime_string(
                meta.get("last_updated_time")
            )

            content_meta = meta.get("content", {})
            if isinstance(content_meta, dict):
                content_meta["plain_text_size"] = int(
                    content_meta.get("plain_text_size") or 0
                )
                content_meta["plain_text_rate"] = float(
                    content_meta.get("plain_text_rate") or 0.0
                )
                content_meta["plain_text_word_count"] = int(
                    content_meta.get("plain_text_word_count") or 0
                )
                content_meta["automated_readability_index"] = float(
                    content_meta.get("automated_readability_index") or 0.0
                )
                content_meta["coleman_liau_readability_index"] = float(
                    content_meta.get("coleman_liau_readability_index") or 0.0
                )
                content_meta["dale_chall_readability_index"] = float(
                    content_meta.get("dale_chall_readability_index") or 0.0
                )
                content_meta["flesch_kincaid_readability_index"] = float(
                    content_meta.get("flesch_kincaid_readability_index") or 0.0
                )
                content_meta["smog_readability_index"] = float(
                    content_meta.get("smog_readability_index") or 0.0
                )
                content_meta["description_to_content_consistency"] = float(
                    content_meta.get("description_to_content_consistency") or 0.0
                )
                content_meta["title_to_content_consistency"] = float(
                    content_meta.get("title_to_content_consistency") or 0.0
                )
                content_meta["meta_keywords_to_content_consistency"] = float(
                    content_meta.get("meta_keywords_to_content_consistency") or 0.0
                )
            sanitized_item["meta"] = meta

        # Sanitize page_timing fields
        page_timing = sanitized_item.get("page_timing", {})
        if isinstance(page_timing, dict):
            page_timing["time_to_interactive"] = int(
                page_timing.get("time_to_interactive") or 0
            )
            page_timing["dom_complete"] = int(page_timing.get("dom_complete") or 0)
            page_timing["largest_contentful_paint"] = float(
                page_timing.get("largest_contentful_paint") or 0.0
            )
            page_timing["first_input_delay"] = float(
                page_timing.get("first_input_delay") or 0.0
            )
            page_timing["connection_time"] = int(
                page_timing.get("connection_time") or 0
            )
            page_timing["time_to_secure_connection"] = int(
                page_timing.get("time_to_secure_connection") or 0
            )
            page_timing["request_sent_time"] = int(
                page_timing.get("request_sent_time") or 0
            )
            page_timing["waiting_time"] = int(page_timing.get("waiting_time") or 0)
            page_timing["download_time"] = int(page_timing.get("download_time") or 0)
            page_timing["duration_time"] = int(page_timing.get("duration_time") or 0)
            page_timing["fetch_start"] = int(page_timing.get("fetch_start") or 0)
            page_timing["fetch_end"] = int(page_timing.get("fetch_end") or 0)
            sanitized_item["page_timing"] = page_timing

        # Sanitize top-level numeric fields
        sanitized_item["onpage_score"] = float(
            sanitized_item.get("onpage_score") or 0.0
        )
        sanitized_item["total_dom_size"] = int(
            sanitized_item.get("total_dom_size") or 0
        )
        sanitized_item["size"] = int(sanitized_item.get("size") or 0)
        sanitized_item["encoded_size"] = int(sanitized_item.get("encoded_size") or 0)
        sanitized_item["total_transfer_size"] = int(
            sanitized_item.get("total_transfer_size") or 0
        )
        sanitized_item["url_length"] = int(sanitized_item.get("url_length") or 0)
        sanitized_item["relative_url_length"] = int(
            sanitized_item.get("relative_url_length") or 0
        )

        # Sanitize fetch_time
        sanitized_item["fetch_time"] = parse_datetime_string(
            sanitized_item.get("fetch_time")
        )

        # Sanitize cache_control ttl
        if isinstance(sanitized_item.get("cache_control"), dict):
            sanitized_item["cache_control"]["ttl"] = int(
                sanitized_item["cache_control"].get("ttl") or 0
            )

        # Sanitize last_modified dates
        if isinstance(sanitized_item.get("last_modified"), dict):
            sanitized_item["last_modified"]["header"] = parse_datetime_string(
                sanitized_item["last_modified"].get("header")
            )
            sanitized_item["last_modified"]["sitemap"] = parse_datetime_string(
                sanitized_item["last_modified"].get("sitemap")
            )
            sanitized_item["last_modified"]["meta_tag"] = parse_datetime_string(
                sanitized_item["last_modified"].get("meta_tag")
            )

        return sanitized_item
</file>

<file path="backend/data_mappers/keyword_data_mapper.py">
# data_mappers/keyword_data_mapper.py

from typing import Dict, Any
from data_access.models import KeywordData


def map_keyword_data(raw_data: Dict[str, Any]) -> KeywordData:
    """Maps raw keyword data from the DataForSEO API to the KeywordData model."""
    keyword_info = raw_data.get("keyword_info", {})
    keyword_properties = raw_data.get("keyword_properties", {})
    search_intent_info = raw_data.get("search_intent_info", {})

    return KeywordData(
        keyword=raw_data.get("keyword"),
        search_volume=keyword_info.get("search_volume"),
        keyword_difficulty=keyword_properties.get("keyword_difficulty"),
        cpc=keyword_info.get("cpc"),
        main_intent=search_intent_info.get("main_intent"),
        search_volume_trend=keyword_info.get("search_volume_trend"),
        core_keyword=keyword_properties.get("core_keyword"),
    )
</file>

<file path="backend/data_mappers/serp_overview_mapper.py">
# data_mappers/serp_overview_mapper.py

from typing import Dict, Any
from data_access.models import SerpOverview


def map_serp_overview(raw_data: Dict[str, Any]) -> SerpOverview:
    """Maps raw SERP data from the DataForSEO API to the SerpOverview model."""
    serp_info = raw_data.get("serp_info", {})
    avg_backlinks_info = raw_data.get("avg_backlinks_info", {})

    return SerpOverview(
        serp_has_featured_snippet="featured_snippet"
        in serp_info.get("serp_item_types", []),
        serp_has_video_results="video" in serp_info.get("serp_item_types", []),
        serp_has_ai_overview="ai_overview" in serp_info.get("serp_item_types", []),
        people_also_ask=serp_info.get("people_also_ask", []),
        ai_overview_content=serp_info.get("ai_overview_content"),
        featured_snippet_content=serp_info.get("featured_snippet_content"),
        avg_referring_domains_top5_organic=avg_backlinks_info.get("referring_domains"),
        avg_main_domain_rank_top5_organic=avg_backlinks_info.get("main_domain_rank"),
        serp_last_updated_days_ago=serp_info.get("last_updated_time"),
        dominant_content_format=serp_info.get("dominant_content_format"),
    )
</file>

<file path="backend/external_apis/dataforseo_client_v2.py">
"""
This module provides a simplified and corrected client for the DataForSEO OnPage API,
focusing exclusively on the `instant_pages` endpoint.
"""

import base64
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import requests
from urllib.parse import urlparse
import hashlib
from backend.data_access.database_manager import DatabaseManager
from backend.data_mappers.dataforseo_mapper import DataForSEOMapper


class DataForSEOClientV2:
    """
    A simplified client for the DataForSEO API, using only the `instant_pages` endpoint.
    """

    # W17 FIX: Move hardcoded endpoints to constants
    LABS_KEYWORD_IDEAS = "dataforseo_labs/google/keyword_ideas/live"
    LABS_KEYWORD_SUGGESTIONS = "dataforseo_labs/google/keyword_suggestions/live"
    LABS_RELATED_KEYWORDS = "dataforseo_labs/google/related_keywords/live"
    LABS_RANKED_KEYWORDS = "dataforseo_labs/google/ranked_keywords/live"
    LABS_COMPETITORS_DOMAIN = "dataforseo_labs/google/competitors_domain/live"
    SERP_ADVANCED = "serp/google/organic/live/advanced"
    ONPAGE_INSTANT_PAGES = "on_page/instant_pages"
    ONPAGE_CONTENT_PARSING = "on_page/content_parsing/live"  # Add this line

    def __init__(
        self,
        login: str,
        password: str,
        db_manager: DatabaseManager,
        config: Dict[str, Any],
        enable_cache: bool = True,
    ):
        self.base_url = "https://api.dataforseo.com/v3"
        if not login or not password:
            raise ValueError("DataForSEO API login and password cannot be empty.")
        credentials = f"{login}:{password}"
        self.headers = {
            "Authorization": f"Basic {base64.b64encode(credentials.encode()).decode()}",
            "Content-Type": "application/json",
        }
        self.logger = logging.getLogger(self.__class__.__name__)
        self.db_manager = db_manager
        self.config = config  # Store the config object
        self.enable_cache = enable_cache

    def _enforce_api_filter_limit(
        self, filters: Optional[List[Any]], max_limit: int = 8
    ) -> Optional[List[Any]]:
        """
        Enforces the API filter limit (max 8 conditions) by truncating the filter list.
        This is a backend safeguard if frontend validation is bypassed.
        """
        if not filters:
            return None

        # Count actual filter conditions (which are lists, not "and"/"or" strings)
        condition_elements = [f for f in filters if isinstance(f, list)]
        if len(condition_elements) <= max_limit:
            return filters  # No need to truncate

        self.logger.warning(
            f"Backend safeguard: API filter list exceeds {max_limit} conditions ({len(condition_elements)} found). Truncating to the first {max_limit} conditions."
        )

        truncated_filters = []
        condition_count = 0
        for item in filters:
            if isinstance(item, list):  # This is a condition
                if condition_count < max_limit:
                    truncated_filters.append(item)
                    condition_count += 1
                else:
                    break  # Stop adding conditions
            elif truncated_filters and isinstance(truncated_filters[-1], list):
                # Add logical operator only if it follows a condition and we're still building
                truncated_filters.append(item)

        # Ensure the list doesn't end with a dangling logical operator
        if (
            truncated_filters
            and isinstance(truncated_filters[-1], str)
            and truncated_filters[-1].lower() in ["and", "or"]
        ):
            truncated_filters.pop()

        return truncated_filters

    def _post_request(
        self, endpoint: str, data: List[Dict[str, Any]], tag: Optional[str] = None
    ) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Handles the actual POST request to the API, with retries and exponential backoff for rate limits.
        """
        cache_key_string = json.dumps(
            {
                "endpoint": endpoint,
                "data": data,
                "filters": data[0].get("filters") if data else None,
            },
            sort_keys=True,
        )
        cache_key = hashlib.md5(cache_key_string.encode("utf-8")).hexdigest()

        if self.enable_cache:
            cached_response = self.db_manager.get_api_cache(cache_key)
            if cached_response:
                self.logger.info(f"Cache HIT for endpoint {endpoint} with tag '{tag}'.")
                return cached_response, 0.0

        self.logger.info(
            f"Cache MISS for endpoint {endpoint} with tag '{tag}'. Making live API call."
        )

        if tag:
            for task_item in data:
                if isinstance(task_item, dict):
                    task_item["tag"] = tag

        full_url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
        self.logger.info(
            f"Making POST request to {full_url} with data: {json.dumps(data)}"
        )
        retries = 3
        backoff_factor = 5

        for attempt in range(retries):
            try:
                response = requests.post(
                    full_url, headers=self.headers, data=json.dumps(data), timeout=120
                )

                # W20 FIX: Early exit for critical top-level HTTP errors
                if response.status_code >= 500:
                    self.logger.error(
                        f"DataForSEO API returned a server error ({response.status_code}). Aborting after {attempt + 1} attempts."
                    )
                    return None, 0.0  # Do not retry on server errors

                response.raise_for_status()  # Raise HTTPError for 4xx client errors

                response_json = response.json()

                # W20 FIX: Check top-level status_code from DataForSEO
                if response_json.get("status_code") != 20000:
                    self.logger.error(
                        f"DataForSEO API returned non-20000 status_code: {response_json.get('status_code')} - {response_json.get('status_message')}"
                    )
                    # No retry for auth errors, etc.
                    if response_json.get("status_code") in [40101, 40102, 40103]:
                        return None, 0.0

                # W20 FIX: Log critical task-level errors
                if response_json.get("tasks_error", 0) > 0:
                    for task in response_json.get("tasks", []):
                        if task.get("status_code") != 20000:
                            # Log specific, known critical errors
                            if task.get("status_code") == 40501:  # Duplicate crawl host
                                self.logger.critical(
                                    f"CRITICAL API ERROR (40501): Duplicate crawl host detected for URL {task.get('data', {}).get('url')}. This batch is invalid."
                                )
                            else:
                                self.logger.warning(
                                    f"Task-level error for {task.get('data', {}).get('url', 'N/A')}: {task.get('status_code')} - {task.get('status_message')}"
                                )

                cost = response_json.get("cost", 0.0)

                if self.enable_cache:
                    self.db_manager.set_api_cache(cache_key, response_json)

                return response_json, cost

            except requests.exceptions.HTTPError as e:
                # This will now primarily catch 4xx errors
                if (
                    response.status_code == 429 and attempt < retries - 1
                ):  # Specifically handle rate limits
                    wait_time = backoff_factor * (2**attempt)
                    self.logger.warning(
                        f"Rate limit exceeded (429). Retrying in {wait_time} seconds... (Attempt {attempt + 1}/{retries})"
                    )
                    time.sleep(wait_time)
                    continue
                else:
                    self.logger.error(
                        f"HTTP error during DataForSEO API request to {full_url}: {e}",
                        exc_info=True,
                    )
                    return None, 0.0
            except requests.exceptions.RequestException as e:
                self.logger.error(
                    f"Network error during DataForSEO API request to {full_url}: {e}",
                    exc_info=True,
                )
                if attempt < retries - 1:
                    time.sleep(backoff_factor * (2**attempt))
                    continue
                return None, 0.0

        return None, 0.0

    def _prioritize_and_limit_filters(self, filters: Optional[List[Any]]) -> List[Any]:
        """Enforces the 8-filter maximum rule by prioritizing essential filters."""
        if not filters:
            return []

        # Count actual filter conditions (excluding logical operators like "and", "or")
        condition_count = sum(1 for f in filters if isinstance(f, list))

        # If already within the limit, return as is.
        if condition_count <= 8:
            return filters

        self.logger.warning(
            f"Filter list exceeds 8 conditions ({condition_count} found). Prioritizing essential filters."
        )

        # Simple prioritization logic: Keep filters based on field name presence
        # Prioritized fields (essential for targeting): keyword_difficulty, search_volume, main_intent, competition_level, cpc, competition
        PRIORITIZED_FIELDS = [
            "keyword_difficulty",
            "search_volume",
            "main_intent",
            "competition_level",
            "cpc",
            "competition",
        ]

        prioritized_filters = []
        other_filters = []

        # Iterate through the filters to separate prioritized from others
        for element in filters:
            if isinstance(element, list) and len(element) >= 3:
                # Assuming element[0] is the field name, like "keyword_info.search_volume"
                field_name = element[0].lower()
                is_priority = any(
                    p_field in field_name for p_field in PRIORITIZED_FIELDS
                )

                if is_priority:
                    prioritized_filters.append(element)
                else:
                    other_filters.append(element)
            else:
                # Logical operators ('and', 'or') will be re-added later if needed
                pass

        # Combine prioritized filters (up to 8 slots)
        limited_filters_list = []  # Only actual conditions

        # 1. Add prioritized filters first
        for f in prioritized_filters:
            if len(limited_filters_list) < 8:
                limited_filters_list.append(f)
            else:
                break

        # 2. Fill remaining slots with non-prioritized filters if space permits
        for f in other_filters:
            if len(limited_filters_list) < 8:
                limited_filters_list.append(f)
            else:
                break

        # Reconstruct the filter list with "and" operators
        final_filters_structure = []
        for i, filt in enumerate(limited_filters_list):
            final_filters_structure.append(filt)
            if i < len(limited_filters_list) - 1:
                final_filters_structure.append("and")

        return final_filters_structure

    def get_technical_onpage_data(
        self, urls: List[str], client_cfg: Dict[str, Any]
    ) -> Tuple[Optional[List[Dict[str, Any]]], float]:
        """
        Performs a batch OnPage scan using the Instant Pages endpoint to get technical SEO data.
        """
        if not urls:
            return [], 0.0

        self.logger.info(
            f"Fetching OnPage data for {len(urls)} URLs with device preset '{client_cfg.get('device', 'desktop')}' using Instant Pages..."
        )

        endpoint = self.ONPAGE_INSTANT_PAGES
        # Group URLs into batches that comply with max_domains and max_tasks
        url_batches = self._group_urls_by_domain(
            urls,
            max_domains=client_cfg.get("onpage_max_domains_per_request", 5),
            batch_size=client_cfg.get("onpage_max_tasks_per_request", 20),
        )

        all_results = []
        total_cost = 0.0

        for i, batch in enumerate(url_batches):
            post_data = []
            for url in batch:
                task_data = {
                    "url": url,
                    # W8 FIX: Use configured value, not hardcoded False
                    "enable_browser_rendering": client_cfg.get(
                        "onpage_enable_browser_rendering", False
                    ),
                    # W5 FIX: Inject critical analysis parameters
                    "validate_micromarkup": client_cfg.get(
                        "onpage_validate_micromarkup", False
                    ),
                    "return_despite_timeout": client_cfg.get(
                        "onpage_return_despite_timeout", False
                    ),
                    "check_spell": client_cfg.get("onpage_check_spell", False),
                    # W7 FIX: Inject language header
                    "accept_language": client_cfg.get("onpage_accept_language"),
                    # Include configured user agent if available
                    "custom_user_agent": client_cfg.get("onpage_custom_user_agent"),
                    "switch_pool": client_cfg.get("onpage_enable_switch_pool", False),
                }

                # Add ip_pool_for_scan if it's in the client config
                if "ip_pool_for_scan" in client_cfg:
                    task_data["ip_pool_for_scan"] = client_cfg["ip_pool_for_scan"]

                # W7 FIX & W17 FIX: Include custom screen resolution if rendering is enabled and validate its range
                screen_ratio = client_cfg.get("onpage_browser_screen_resolution_ratio")
                if screen_ratio is not None and (
                    task_data["enable_browser_rendering"]
                    or client_cfg.get("onpage_enable_javascript", False)
                ):
                    # W17 FIX: Validate the range [0.5, 3.0]
                    if 0.5 <= screen_ratio <= 3.0:
                        task_data["browser_screen_resolution_ratio"] = screen_ratio
                    else:
                        self.logger.error(
                            f"Invalid screen ratio configured: {screen_ratio}. Must be between 0.5 and 3.0. Omitting parameter."
                        )
                        # Parameter will be omitted if outside valid range, relying on DataForSEO default.

                # W9 FIX: Include custom checks threshold if provided in config (continues from Task 4.4)
                thresholds_str = client_cfg.get("onpage_custom_checks_thresholds")
                if thresholds_str:
                    try:
                        task_data["checks_threshold"] = json.loads(thresholds_str)
                    except json.JSONDecodeError:
                        self.logger.error(
                            "Failed to parse onpage_custom_checks_thresholds JSON from config. Using default thresholds."
                        )

                # W12 FIX: Include custom JS if enabled (continues from Task 12.4)
                if client_cfg.get("onpage_enable_custom_js", False):
                    custom_js_script = client_cfg.get("onpage_custom_js")
                    if custom_js_script:
                        task_data["custom_js"] = custom_js_script

                # Remove keys if their value is None to maintain a clean API request
                task_data = {k: v for k, v in task_data.items() if v is not None}
                post_data.append(task_data)

            # --- Attempt 1 for the batch ---
            response, cost = self._post_request(
                endpoint, post_data, tag=f"onpage_instant_pages_content:batch{i + 1}"
            )
            total_cost += cost

            current_batch_results = []
            failed_urls_in_batch = []

            if response and response.get("tasks"):
                for task in response["tasks"]:
                    task_url = task.get("data", {}).get("url")

                    # Explicit Failure Criteria Check: Task failed OR result is malformed/empty
                    if (
                        task.get("status_code") != 20000
                        or not task.get("result")
                        or not task["result"][0].get("items")
                    ):
                        self.logger.warning(
                            f"Task for URL {task_url} failed/malformed response (Status: {task.get('status_code')}). Queuing for retry."
                        )
                        failed_urls_in_batch.append(task_url)
                    else:
                        for result_item in task["result"]:
                            if result_item and result_item.get("items"):
                                current_batch_results.extend(
                                    [
                                        DataForSEOMapper.sanitize_onpage_data_item(it)
                                        for it in result_item["items"]
                                    ]
                                )  # ADDED SANITIZATION
            else:
                self.logger.error(
                    f"Failed to get any response for instant_pages batch starting with URL: {batch[0]}"
                )
                failed_urls_in_batch.extend(batch)

            # --- Retry mechanism for failed URLs in this batch ---
            if failed_urls_in_batch:
                self.logger.info(
                    f"Retrying {len(failed_urls_in_batch)} failed URLs from batch {i + 1}..."
                )

                should_switch_pool = client_cfg.get("onpage_enable_switch_pool", False)

                # W15 FIX: Re-group failed URLs to enforce the max_domains limit (5 domains max per request)
                max_domains_per_retry = client_cfg.get(
                    "onpage_max_domains_per_request", 5
                )
                retry_batches = self._group_urls_by_domain(
                    failed_urls_in_batch,
                    max_domains=max_domains_per_retry,
                    batch_size=client_cfg.get("onpage_max_tasks_per_request", 20),
                )

                current_retry_cost = 0.0

                for retry_batch in retry_batches:
                    retry_post_data = []
                    for url in retry_batch:
                        # Reconstruct task_data using original structure but force `return_despite_timeout`
                        original_task_data = next(
                            (item for item in post_data if item.get("url") == url), {}
                        )
                        retry_task_data = {
                            **original_task_data,
                            "return_despite_timeout": True,
                        }

                        if should_switch_pool:
                            retry_task_data["switch_pool"] = True

                        retry_post_data.append(retry_task_data)

                    retry_response, retry_cost = self._post_request(
                        endpoint,
                        retry_post_data,
                        tag=f"onpage_instant_pages_content:retry_batch{i + 1}",
                    )
                    current_retry_cost += retry_cost

                    # Process retry_response (existing logic, moved and adapted)
                    if retry_response and retry_response.get("tasks"):
                        for task in retry_response["tasks"]:
                            task_url = task.get("data", {}).get("url")
                            if task.get("status_code") == 20000 and task.get("result"):
                                for result_item in task["result"]:
                                    if result_item and result_item.get("items"):
                                        current_batch_results.extend(
                                            result_item["items"]
                                        )
                            else:
                                self.logger.warning(
                                    f"Retry task for URL {task_url} failed again (Status: {task.get('status_code')})."
                                )

                total_cost += current_retry_cost

            all_results.extend(current_batch_results)
        return all_results, total_cost

    def get_content_onpage_data(
        self,
        urls: List[str],
        client_cfg: Dict[str, Any],
        enable_javascript: bool = False,
    ) -> Tuple[Optional[List[Dict[str, Any]]], float]:
        """
        Performs OnPage scans using the Content Parsing endpoint, with control over JS rendering.
        This function now sends requests for multiple URLs in parallel using a thread pool
        for improved performance, as the endpoint does not support batch processing.
        """
        if not urls:
            return [], 0.0

        self.logger.info(
            f"Fetching OnPage Content Parsing data for {len(urls)} URLs with enable_javascript={enable_javascript} in parallel..."
        )

        all_tasks = []
        total_cost = 0.0

        # Helper function to be executed in each thread
        def _fetch_single_url(url: str) -> Tuple[Optional[Dict[str, Any]], float]:
            post_data = [
                {
                    "url": url,
                    "enable_javascript": enable_javascript,
                    "store_raw_html": True,
                    "markdown_view": True,
                    "disable_cookie_popup": client_cfg.get(
                        "onpage_disable_cookie_popup", True
                    ),
                }
            ]
            tag = f"onpage_content_parsing_js_{str(enable_javascript).lower()}:{urlparse(url).netloc}"
            return self._post_request(self.ONPAGE_CONTENT_PARSING, post_data, tag=tag)

        # Use a ThreadPoolExecutor to send requests concurrently
        # The number of workers can be tuned, but 5 is a safe default to avoid overwhelming the API
        with ThreadPoolExecutor(max_workers=5) as executor:
            # map executes the function for each item in the urls list
            future_to_url = {
                executor.submit(_fetch_single_url, url): url for url in urls
            }
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    response, cost = future.result()
                    total_cost += cost
                    if response and response.get("tasks"):
                        all_tasks.extend(response["tasks"])
                    else:
                        self.logger.error(
                            f"Failed to get a valid response for content_parsing for URL: {url}"
                        )
                        all_tasks.append(
                            {
                                "status_code": 50000,
                                "status_message": "No response from API",
                                "data": {"url": url},
                            }
                        )
                except Exception as exc:
                    self.logger.error(f"{url} generated an exception: {exc}")
                    all_tasks.append(
                        {
                            "status_code": 50001,
                            "status_message": f"Request generated an exception: {exc}",
                            "data": {"url": url},
                        }
                    )

        if all_tasks:
            return all_tasks, total_cost

        self.logger.error(
            f"Failed to get any response for any of the {len(urls)} URLs."
        )
        return [
            {
                "status_code": 50000,
                "status_message": "No response from API",
                "data": {"url": url},
            }
            for url in urls
        ], 0.0

    def get_serp_results(
        self,
        keyword: str,
        location_code: int,
        language_code: str,
        client_cfg: Dict[str, Any],
        serp_call_params: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Fetches the advanced SERP data for a single keyword, with caching.
        """
        device = client_cfg.get("device", "desktop")
        self.logger.info(
            f"Fetching live SERP results for '{keyword}' on device '{device}'..."
        )
        endpoint = self.SERP_ADVANCED
        base_serp_params = {
            "keyword": keyword,
            "location_code": location_code,
            "language_code": language_code,
            "group_organic_results": False,  # NEW: Ensure no grouping for full analysis
        }
        if serp_call_params:
            base_serp_params.update(serp_call_params)

        if client_cfg.get("calculate_rectangles", False):
            base_serp_params["calculate_rectangles"] = True

        base_serp_params["depth"] = int(base_serp_params.get("depth", 10))

        # Add advanced features from client_cfg if they are enabled.
        if client_cfg.get("calculate_rectangles", False):
            base_serp_params["calculate_rectangles"] = True

        paa_depth = client_cfg.get("people_also_ask_click_depth", 0)
        if isinstance(paa_depth, int) and 1 <= paa_depth <= 4:
            base_serp_params["people_also_ask_click_depth"] = paa_depth

        # W3 FIX: Add support for loading AI overview asynchronously
        if client_cfg.get("load_async_ai_overview", False):
            base_serp_params["load_async_ai_overview"] = True

        # W11 FIX: Include URL removal parameters
        remove_params_str = client_cfg.get("serp_remove_from_url_params")
        if remove_params_str:
            # Assuming config value is a comma-separated string of parameters
            params_list = [p.strip() for p in remove_params_str.split(",") if p.strip()]

            # W14 FIX: Validate and clip URL removal parameters (max 10)
            if len(params_list) > 10:
                self.logger.warning(
                    f"Configuration defined {len(params_list)} parameters for removal, but DataForSEO limit is 10. Truncating."
                )

            base_serp_params["remove_from_url"] = params_list[:10]

        # Ensure device and OS are passed based on client config
        device = client_cfg.get("device", "desktop")
        os_name = client_cfg.get("os", "windows")

        # Adjust OS if device is mobile for compatibility
        if device == "mobile" and os_name not in ["android", "ios"]:
            os_name = "android"

        base_serp_params["device"] = device
        base_serp_params["os"] = os_name

        request_tag = f"serp_advanced:{keyword[:50]}"
        response, cost = self._post_request(
            endpoint, [base_serp_params], tag=request_tag
        )

        if response and response.get("tasks") and response["tasks"][0].get("result"):
            result_data = response["tasks"][0]["result"][0]
            sanitized_result_data = DataForSEOMapper.sanitize_serp_overview_response(
                result_data
            )  # ADDED SANITIZATION
            return sanitized_result_data, cost

        return None, cost

    def post_with_paging(
        self,
        endpoint: str,
        initial_task: Dict[str, Any],
        max_pages: int,
        paginated: bool = True,
        tag: Optional[str] = None,
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Executes a POST request and, if paginated=True, recursively retrieves all results using the correct pagination method.
        """
        all_items = []
        total_cost = 0.0
        current_task = initial_task.copy()

        if "filters" in current_task and (
            current_task["filters"] is None or len(current_task["filters"]) == 0
        ):
            current_task.pop("filters")

        page_count = 0
        previous_offset_token = None  # ADDED: For infinite loop prevention

        while True:
            if not paginated and page_count > 0:
                break

            if page_count >= max_pages:
                self.logger.info(
                    f"Reached max page limit ({max_pages}) for endpoint {endpoint}."
                )
                break

            page_count += 1
            self.logger.info(
                f"Submitting task to {endpoint} (Page {page_count}/{max_pages})..."
            )

            request_tag = (
                tag + f":p{page_count}"
                if tag
                else endpoint.split("/")[-1] + f":p{page_count}"
            )
            response, cost = self._post_request(
                endpoint, [current_task], tag=request_tag
            )
            total_cost += cost

            if (
                not response
                or response.get("status_code") != 20000
                or response.get("tasks_error", 0) > 0
            ):
                self.logger.error(
                    f"Paging for endpoint {endpoint} failed on page {page_count}. Response: {response}"
                )
                break

            tasks = response.get("tasks", [])
            if not tasks or "result" not in tasks[0]:
                self.logger.info(
                    f"No 'result' field in the first task for endpoint {endpoint} on page {page_count}. Stopping pagination."
                )
                break

            task_result = tasks[0].get("result")
            if not task_result:
                self.logger.info(
                    f"Task result is empty for endpoint {endpoint} on page {page_count}. Stopping pagination."
                )
                break

            items_count = 0
            offset_token = None
            if task_result and isinstance(task_result, list) and len(task_result) > 0:
                offset_token = task_result[0].get("offset_token")
                for result_item in task_result:
                    # Capture items from the main list
                    items = result_item.get("items")
                    if items:
                        items_count += len(items)
                        all_items.extend(items)

                    # Capture the valuable seed_keyword_data if it exists (from Keyword Suggestions)
                    # and if this is specifically from the Keyword Suggestions API.
                    # This avoids adding the same seed_keyword twice if it was also in the 'items' list
                    # or if the main search (e.g., Keyword Ideas) already returned it.
                    if endpoint == self.LABS_KEYWORD_SUGGESTIONS:
                        seed_data = result_item.get("seed_keyword_data")
                        if isinstance(seed_data, dict) and seed_data.get("keyword"):
                            seed_data["discovery_source"] = (
                                "keyword_suggestions_seed"  # Mark its source
                            )
                            all_items.append(
                                DataForSEOMapper.sanitize_keyword_data_item(seed_data)
                            )  # ADDED SANITIZATION

            if not paginated or page_count >= max_pages or items_count == 0:
                break

            if offset_token:
                # ADDED: Infinite loop prevention check
                if offset_token == previous_offset_token:
                    self.logger.warning(
                        f"API returned a duplicate offset_token. Halting pagination to prevent infinite loop for endpoint {endpoint}."
                    )
                    break
                previous_offset_token = offset_token  # Update the previous token

                current_task = {
                    "offset_token": offset_token,
                    "limit": initial_task.get("limit", 1000),
                }
                if "filters" in initial_task and initial_task["filters"] is not None:
                    current_task["filters"] = initial_task["filters"]
                if "order_by" in initial_task and initial_task["order_by"] is not None:
                    current_task["order_by"] = initial_task["order_by"]

                time.sleep(1)
            else:
                break

        return all_items, total_cost

    def _group_urls_by_domain(
        self, urls: List[str], max_domains: int = 5, batch_size: int = 20
    ) -> List[List[str]]:
        """
        Groups URLs into batches that comply with the identical-domain limit and batch size.
        """
        from collections import defaultdict, deque

        domain_cache = {}

        def get_domain(url):
            if url not in domain_cache:
                try:
                    domain_cache[url] = urlparse(url).netloc
                except Exception:
                    self.logger.warning(f"Could not parse domain for URL: {url}")
                    domain_cache[url] = url
            return domain_cache[url]

        # Use deques for efficient popping from the left
        domain_groups = defaultdict(deque)
        for url in urls:
            domain_groups[get_domain(url)].append(url)

        batches = []

        # Continue as long as there are URLs to process
        while sum(len(q) for q in domain_groups.values()) > 0:
            current_batch = []
            domain_counts = defaultdict(int)

            # A set of domains that have reached their limit for the current batch
            exhausted_domains = set()

            # Loop until the batch is full or no more URLs can be added
            while len(current_batch) < batch_size:
                url_added_in_this_pass = False

                # Iterate through domains that have URLs and are not exhausted for this batch
                for domain, url_queue in domain_groups.items():
                    if len(current_batch) >= batch_size:
                        break

                    if url_queue and domain not in exhausted_domains:
                        if domain_counts[domain] < max_domains:
                            current_batch.append(url_queue.popleft())
                            domain_counts[domain] += 1
                            url_added_in_this_pass = True
                        else:
                            exhausted_domains.add(domain)

                # If we went through all domains and couldn't add a single URL, stop filling this batch
                if not url_added_in_this_pass:
                    break

            if current_batch:
                batches.append(current_batch)
            # If we created an empty batch and there are still urls, something is wrong.
            # This should not happen with this logic, but as a safeguard:
            elif sum(len(q) for q in domain_groups.values()) > 0:
                self.logger.error(
                    "Could not form a valid batch. Breaking to prevent infinite loop."
                )
                break

        self.logger.info(f"Grouped {len(urls)} URLs into {len(batches)} batches.")
        return batches

    def _convert_filters_to_api_format(
        self, filters: Optional[List[Dict[str, Any]]]
    ) -> Optional[List[Any]]:
        if not filters:
            return None

        api_filters = []
        for i, f in enumerate(filters):
            api_filters.append([f["field"], f["operator"], f["value"]])
            if i < len(filters) - 1:
                api_filters.append("and")
        return api_filters

    def get_keyword_ideas(
        self,
        seed_keywords: List[str],
        location_code: int,
        language_code: str,
        client_cfg: Dict[str, Any],
        discovery_modes: List[str],
        filters: Dict[str, Any],
        order_by: Optional[Dict[str, List[str]]],
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms_override: Optional[bool] = None,
        include_clickstream_override: Optional[bool] = None,
        closely_variants_override: Optional[bool] = None,
        exact_match_override: Optional[bool] = None,
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Performs a comprehensive discovery burst using Keyword Ideas, Suggestions, and Related Keywords endpoints.
        """
        all_items = []
        total_cost = 0.0
        max_pages = client_cfg.get("discovery_max_pages", 1)

        # Dynamic parameters (fall back to client_cfg if override is None)
        ignore_synonyms = (
            ignore_synonyms_override
            if ignore_synonyms_override is not None
            else client_cfg.get("discovery_ignore_synonyms", False)
        )
        include_clickstream = (
            include_clickstream_override
            if include_clickstream_override is not None
            else client_cfg.get("include_clickstream_data", False)
        )
        closely_variants = (
            closely_variants_override
            if closely_variants_override is not None
            else client_cfg.get("closely_variants", False)
        )
        exact_match = (
            exact_match_override
            if exact_match_override is not None
            else client_cfg.get("exact_match", False)
        )

        if "keyword_ideas" in discovery_modes:
            self.logger.info(
                f"Fetching keyword ideas for {len(seed_keywords)} seeds..."
            )
            ideas_endpoint = self.LABS_KEYWORD_IDEAS

            sanitized_ideas_filters = self._prioritize_and_limit_filters(
                self._convert_filters_to_api_format(filters.get("ideas"))
            )

            ideas_task = {
                "keywords": seed_keywords,
                "location_code": location_code,
                "language_code": language_code,
                "limit": int(limit or 100),
                "include_serp_info": True,
                "ignore_synonyms": ignore_synonyms,
                "closely_variants": closely_variants,
                "filters": sanitized_ideas_filters,
                "order_by": order_by.get("ideas") if order_by else None,
                "include_clickstream_data": include_clickstream,
            }
            ideas_items, cost = self.post_with_paging(
                ideas_endpoint, ideas_task, max_pages=max_pages, tag="discovery_ideas"
            )
            total_cost += cost

            for item in ideas_items:
                item["discovery_source"] = "keyword_ideas"
                item["depth"] = 0
                all_items.append(DataForSEOMapper.sanitize_keyword_data_item(item))
            self.logger.info(f"Found {len(ideas_items)} ideas from Keyword Ideas API.")

        if "keyword_suggestions" in discovery_modes:
            self.logger.info("Fetching keyword suggestions...")
            suggestions_endpoint = self.LABS_KEYWORD_SUGGESTIONS
            for seed_keyword in seed_keywords:
                suggestions_task = {
                    "keyword": seed_keyword,
                    "location_code": location_code,
                    "language_code": language_code,
                    "limit": int(limit or 100),
                    "include_serp_info": True,
                    "exact_match": exact_match,
                    "ignore_synonyms": ignore_synonyms,
                    "include_seed_keyword": True,
                    "filters": self._prioritize_and_limit_filters(
                        self._convert_filters_to_api_format(filters.get("suggestions"))
                    ),
                    "order_by": order_by.get("suggestions") if order_by else None,
                    "include_clickstream_data": include_clickstream,
                }
                suggestions_items, cost = self.post_with_paging(
                    suggestions_endpoint,
                    suggestions_task,
                    max_pages=max_pages,
                    tag=f"discovery_suggestions:{seed_keyword[:20]}",
                )
                total_cost += cost
                for item in suggestions_items:
                    item["discovery_source"] = "keyword_suggestions"
                    item["depth"] = 0
                    all_items.append(DataForSEOMapper.sanitize_keyword_data_item(item))
                self.logger.info(
                    f"Found {len(suggestions_items)} suggestions for '{seed_keyword}'."
                )

        if "related_keywords" in discovery_modes:
            self.logger.info("Fetching related keywords...")
            related_endpoint = self.LABS_RELATED_KEYWORDS
            for seed in seed_keywords:
                related_task = {
                    "keyword": seed,
                    "location_code": location_code,
                    "language_code": language_code,
                    "depth": int(depth or client_cfg.get("discovery_related_depth", 3)),
                    "limit": int(limit or 100),
                    "include_serp_info": True,
                    "filters": self._prioritize_and_limit_filters(
                        self._convert_filters_to_api_format(filters.get("related"))
                    ),
                    "order_by": order_by.get("related") if order_by else None,
                    "include_clickstream_data": include_clickstream,
                    "replace_with_core_keyword": client_cfg.get(
                        "discovery_replace_with_core_keyword", False
                    ),
                }

                related_items, cost = self.post_with_paging(
                    related_endpoint,
                    related_task,
                    max_pages=max_pages,
                    tag=f"discovery_related:{seed[:20]}",
                )
                total_cost += cost
                for item in related_items:
                    keyword_data = item.get("keyword_data")
                    if keyword_data:
                        keyword_data["discovery_source"] = "related"
                        keyword_data["depth"] = item.get("depth")
                        all_items.append(
                            DataForSEOMapper.sanitize_keyword_data_item(keyword_data)
                        )
            self.logger.info(f"Total raw items from all sources: {len(all_items)}")

        return all_items, total_cost
</file>

<file path="backend/external_apis/on-page-api.py">
import os
import requests
import json
import sys
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# --- Configuration ---
DATAFORSEO_LOGIN = os.getenv("DATAFORSEO_LOGIN")
DATAFORSEO_PASSWORD = os.getenv("DATAFORSEO_PASSWORD")
API_URL = "https://api.dataforseo.com/v3/on_page/content_parsing/live"


def fetch_page_content_live(url: str):
    """
    Fetches the parsed content and raw HTML for a given URL using the synchronous
    'live' endpoint of the Dataforseo On-Page Content Parsing API.

    Args:
        url: The URL of the page to analyze.

    Returns:
        A dictionary containing the API response, or None if an error occurs.
    """
    if not DATAFORSEO_LOGIN or not DATAFORSEO_PASSWORD:
        print("Error: Dataforseo credentials not found in .env file.", file=sys.stderr)
        return None

    # The request body is a list containing a dictionary for the URL.
    post_data = [
        {
            "url": url,
            "store_raw_html": True,
            "enable_javascript": False,
            "convert_to_markdown": True,
        }
    ]

    headers = {"Content-Type": "application/json"}

    try:
        print(f"Sending request for URL: {url}", file=sys.stderr)
        response = requests.post(
            API_URL,
            auth=(DATAFORSEO_LOGIN, DATAFORSEO_PASSWORD),
            headers=headers,
            json=post_data,
        )
        # Raise an exception for bad status codes (4xx or 5xx)
        response.raise_for_status()

        response_json = response.json()

        # Check the response status from the API itself
        if response_json.get("status_code") == 20000:
            print(f"Successfully received data for: {url}", file=sys.stderr)
            return response_json
        else:
            status_msg = response_json.get("status_message", "No status message.")
            print(f"API returned an error for '{url}': {status_msg}", file=sys.stderr)
            return None

    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred for URL '{url}': {http_err}", file=sys.stderr)
        print(f"Response content: {response.text}", file=sys.stderr)
        return None
    except requests.exceptions.RequestException as req_err:
        print(f"A request error occurred for URL '{url}': {req_err}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"An unexpected error occurred for URL '{url}': {e}", file=sys.stderr)
        return None


if __name__ == "__main__":
    test_urls = [
        "https://www.theverge.com/2024/02/15/24074327/openai-sora-text-to-video-generator-ai",
        "https://www.wired.com/story/what-is-generative-ai/",
        "https://blog.google/technology/ai/google-gemini-ai/",
    ]

    all_results = []

    print("--- Starting On-Page Content Parsing (Live API) Tests ---", file=sys.stderr)
    for url in test_urls:
        result = fetch_page_content_live(url)
        if result:
            all_results.append(result)
        else:
            # Add a placeholder for failed requests to keep track
            all_results.append({"url": url, "error": "Failed to fetch content"})
        print("-" * 20, file=sys.stderr)

    print("\n--- All Test Results (JSON Output) ---", file=sys.stderr)
    # Use json.dumps to pretty-print the final list of results
    print(json.dumps(all_results, indent=2))
    print("--- Test Run Complete ---", file=sys.stderr)
</file>

<file path="backend/external_apis/openai_client.py">
from openai import OpenAI
import json
import logging
from typing import Dict, Any, List, Optional, Tuple
import time


class OpenAIClientWrapper:
    """
    Provides a robust wrapper for OpenAI API calls,
    handling authentication, retries, and structured outputs (JSON object format).
    """

    def __init__(self, api_key: str, client_cfg: Dict[str, Any]):
        if not api_key:
            raise ValueError("OpenAI API key is required.")
        self.client = OpenAI(api_key=api_key)
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = client_cfg
        self.latest_cost = 0.0

    def _calculate_cost(self, usage: Dict[str, Any], model: str) -> float:
        """Calculates the cost of a chat completion based on token usage."""
        # Pricing per 1M tokens
        pricing = {
            "gpt-4o": {"input": 5.00, "output": 15.00},
            "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
        }
        model_pricing = pricing.get(
            model, pricing["gpt-4o"]
        )  # Default to gpt-4o pricing

        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)

        input_cost = (prompt_tokens / 1_000_000) * model_pricing["input"]
        output_cost = (completion_tokens / 1_000_000) * model_pricing["output"]

        return input_cost + output_cost
        pass

    def call_chat_completion(
        self,
        messages: List[Dict[str, str]],
        schema: Optional[Dict[str, Any]] = None,
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_completion_tokens: int = 4096,
        retries: int = 3,
    ) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
        """
        Makes a robust OpenAI chat completion call, optionally enforcing JSON output
        and calculating the cost.
        """
        if model is None:
            model = self.client_cfg.get('default_model', 'gpt-5-nano')
        
        self.latest_cost = 0.0
        for attempt in range(retries):
            try:
                response_kwargs = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_completion_tokens": max_completion_tokens,
                }

                # gpt-5-nano and gpt-5-mini do not support temperature
                if model in ['gpt-5-nano', 'gpt-5-mini']:
                    del response_kwargs['temperature']

                if schema:
                    response_kwargs["response_format"] = {
                        "type": "json_schema",
                        "json_schema": {
                            "name": schema.get("name", "structured_output"),
                            "schema": schema,
                            "strict": True,
                        },
                    }

                response = self.client.chat.completions.create(**response_kwargs)

                if response.choices and response.choices[0].finish_reason == "length":
                    self.logger.warning(
                        f"OpenAI API response was truncated because the token limit was reached. "
                        f"Consider increasing 'max_completion_tokens_for_generation' in settings.ini. "
                        f"Current limit for this call: {max_completion_tokens}."
                    )

                if response.usage:
                    self.latest_cost = self._calculate_cost(
                        response.usage.dict(), model
                    )

                if response.choices and response.choices[0].message.content:
                    if schema:
                        try:
                            parsed_output = json.loads(
                                response.choices[0].message.content
                            )
                            self.logger.info(
                                f"Successfully parsed structured output from OpenAI (Attempt {attempt + 1}/{retries}). Cost: ${self.latest_cost:.4f}"
                            )
                            return parsed_output, None
                        except json.JSONDecodeError as e:
                            self.logger.warning(
                                f"Failed to decode JSON from OpenAI (Attempt {attempt + 1}/{retries}): {e}."
                            )
                            continue
                    else:
                        return response.choices[0].message.content, None

                self.logger.warning(
                    f"OpenAI returned no content (Attempt {attempt + 1}/{retries})."
                )
                continue

            except Exception as e:
                self.logger.error(
                    f"OpenAI API call failed (Attempt {attempt + 1}/{retries}): {e}"
                )
                if attempt < retries - 1:
                    time.sleep(5 * (attempt + 1))
                    continue
                return None, str(e)

        return None, "All OpenAI API call attempts failed."

    def call_image_generation(
        self,
        prompt: str,
        style_formula: str,
        quality: str,
        size: str,
        model: Optional[str] = None,
        retries: int = 3,
    ) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """
        Mocks OpenAI image generation. This function is present but *not used* in the final plan
        as Pexels is the exclusive image source. It's kept for potential future re-integration.
        """
        if model is None:
            model = self.client_cfg.get('default_image_model', 'dall-e-3')
        
        self.logger.info(
            "OpenAI image generation is configured but Pexels is prioritized. This function will not be called."
        )
        return (
            None,
            None,
            "OpenAI image generation bypassed; Pexels is the primary source.",
        )
</file>

<file path="backend/external_apis/pexels_client.py">
import requests
import logging
import os
from typing import List, Dict, Any, Optional, Tuple


class PexelsClient:
    """
    Manages communication with the Pexels API for free stock photos and videos.
    """

    def __init__(self, api_key: str):
        if not api_key:
            raise ValueError("Pexels API key is required.")
        self.base_url_photos = "https://api.pexels.com/v1/"
        self.base_url_videos = "https://api.pexels.com/videos/"  # Not used in this plan, but included for completeness
        self.headers = {"Authorization": api_key}
        self.logger = logging.getLogger(self.__class__.__name__)

    def search_photos(
        self,
        query: str,
        orientation: Optional[str] = None,
        size: Optional[str] = None,
        per_page: int = 1,
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Searches for photos on Pexels based on a query.
        Returns a list of photo dicts (simplified for direct use) and a dummy cost (Pexels is free).
        """
        endpoint = f"{self.base_url_photos}search"
        params = {
            "query": query,
            "per_page": per_page,
        }
        if orientation:
            params["orientation"] = orientation
        if size:
            params["size"] = size

        try:
            response = requests.get(
                endpoint, headers=self.headers, params=params, timeout=10
            )
            response.raise_for_status()
            data = response.json()

            photos = []
            for photo in data.get("photos", []):
                # Simplify the photo data to what's immediately useful
                photos.append(
                    {
                        "id": photo["id"],
                        "url": photo["url"],
                        "photographer": photo["photographer"],
                        "photographer_url": photo["photographer_url"],
                        "src": photo["src"],  # Contains different sizes
                        "alt": photo.get(
                            "alt", f"Photo by {photo['photographer']} on Pexels"
                        ),
                    }
                )

            self.logger.info(
                f"Found {len(photos)} photos on Pexels for query '{query}'."
            )
            return photos, 0.0  # Pexels is free, so cost is 0

        except requests.exceptions.RequestException as e:
            self.logger.error(
                f"Error searching Pexels photos for '{query}': {e}", exc_info=True
            )
            return [], 0.0
        except Exception as e:
            self.logger.error(
                f"Unexpected error in Pexels photo search for '{query}': {e}",
                exc_info=True,
            )
            return [], 0.0


def download_image_from_url(image_url: str, save_path: str) -> Optional[str]:
    """
    Downloads an image from a given URL and saves it locally.
    Returns the local file path on success, None on failure.
    """
    try:
        response = requests.get(image_url, stream=True, timeout=30)
        response.raise_for_status()

        # Ensure directory exists
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        with open(save_path, "wb") as out_file:
            for chunk in response.iter_content(chunk_size=8192):
                out_file.write(chunk)

        logging.getLogger(__name__).info(
            f"Downloaded image from {image_url} to {save_path}"
        )
        return save_path
    except requests.exceptions.RequestException as e:
        logging.getLogger(__name__).error(
            f"Failed to download image from {image_url}: {e}", exc_info=True
        )
        return None
    except Exception as e:
        logging.getLogger(__name__).error(
            f"An unexpected error occurred during image download: {e}", exc_info=True
        )
        return None
</file>

<file path="backend/pipeline/orchestrator/__init__.py">
# backend/pipeline/orchestrator/__init__.py
</file>

<file path="backend/pipeline/orchestrator/analysis_orchestrator.py">
# backend/pipeline/orchestrator/analysis_orchestrator.py
import logging
import traceback
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)


class AnalysisOrchestrator:
    def run_analysis_phase(
        self,
        opportunity_id: int,
        selected_competitor_urls: Optional[List[str]] = None,
        use_cached_serp: bool = False,
    ) -> Dict[str, Any]:
        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            return {
                "status": "failed",
                "message": f"Opportunity ID {opportunity_id} not found.",
            }

        keyword = opportunity.get("keyword")
        self.logger.info(
            f"--- Orchestrator: Starting Full Analysis for '{keyword}' ---"
        )
        self.db_manager.update_opportunity_workflow_state(
            opportunity_id, "analysis_started", "in_progress"
        )
        total_api_cost = 0.0

        try:
            # 1. Fetch Live SERP Data
            if use_cached_serp and opportunity.get("full_data", {}).get(
                "serp_overview"
            ):
                self.logger.info(f"Using cached SERP data for '{keyword}'...")
                live_serp_data = opportunity["full_data"]["serp_overview"]
                serp_api_cost = 0.0
            else:
                self.logger.info(f"Running live SERP data fetch for '{keyword}'...")
                from core.serp_analyzer import FullSerpAnalyzer

                serp_analyzer = FullSerpAnalyzer(
                    self.dataforseo_client, self.client_cfg
                )
                live_serp_data, serp_api_cost = serp_analyzer.analyze_serp(keyword)
                total_api_cost += serp_api_cost

            if not live_serp_data:
                raise ValueError("Failed to retrieve live SERP data for analysis.")

            # --- START MODIFICATION ---
            # 2. NEW: Pre-Analysis Validation Gate (Safeguard for AI Calls)
            # Count valid "blog/article" results in top 15
            top_results_for_validation = live_serp_data.get("top_organic_results", [])[
                :15
            ]
            min_relevant_results = self.client_cfg.get(
                "min_relevant_analysis_results", 3
            )
            article_type_results_count = sum(
                1
                for r in top_results_for_validation
                if r.get("page_type") in ["Blog/Article", "News"]
            )

            if article_type_results_count < min_relevant_results:
                reason = f"Analysis failed: SERP is dominated by non-article formats ({article_type_results_count} relevant results found in top 15), making it unsuitable for this workflow."
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id, "pre_analysis_validation_failed", "failed", reason
                )
                self.logger.warning(f"Analysis halted for '{keyword}': {reason}")
                return {
                    "status": "failed",
                    "message": reason,
                    "api_cost": total_api_cost,
                }

            self.logger.info(
                f"Pre-analysis validation passed for '{keyword}' ({article_type_results_count} relevant results in top 15). Proceeding with blueprint generation."
            )
            # --- END MODIFICATION ---

            # 3. Conditional Competitor OnPage Analysis
            competitor_analysis = []
            competitor_api_cost = 0.0

            if self.client_cfg.get("enable_deep_competitor_analysis", False):
                self.logger.info(
                    "Deep competitor analysis is ENABLED. Running OnPage competitor analysis."
                )
                from pipeline.step_04_analysis.competitor_analyzer import (
                    FullCompetitorAnalyzer,
                )

                competitor_analyzer = FullCompetitorAnalyzer(
                    self.dataforseo_client, self.client_cfg
                )
                top_organic_urls = [
                    result["url"]
                    for result in live_serp_data.get("top_organic_results", [])[
                        : self.client_cfg.get("num_competitors_to_analyze", 5)
                    ]
                ]
                competitor_analysis, competitor_api_cost = (
                    competitor_analyzer.analyze_competitors(
                        top_organic_urls, selected_competitor_urls
                    )
                )
                total_api_cost += competitor_api_cost
            else:
                self.logger.info(
                    "Deep competitor analysis is DISABLED. Skipping OnPage competitor analysis."
                )

            # 4. Content Intelligence Synthesis
            from pipeline.step_04_analysis.content_analyzer import ContentAnalyzer

            content_analyzer = ContentAnalyzer(self.openai_client, self.client_cfg)
            content_intelligence, content_api_cost = (
                content_analyzer.synthesize_content_intelligence(
                    keyword,
                    live_serp_data,
                    competitor_analysis,  # Pass this list; it will be empty for the fast workflow
                )
            )
            total_api_cost += content_api_cost

            # 5. Determine Strategy & Generate Outline
            from pipeline.step_05_strategy.decision_engine import (
                StrategicDecisionEngine,
            )

            strategy_engine = StrategicDecisionEngine(self.client_cfg)
            recommended_strategy = strategy_engine.determine_strategy(
                live_serp_data, competitor_analysis, content_intelligence
            )

            ai_outline, outline_api_cost = content_analyzer.generate_ai_outline(
                keyword, live_serp_data, content_intelligence
            )
            total_api_cost += outline_api_cost
            content_intelligence.update(ai_outline)

            if not content_intelligence.get("article_structure"):
                self.logger.critical(
                    "AI outline generation failed to produce an 'article_structure'."
                )
                raise ValueError("AI outline generation failed.")

            # 6. Assemble and Save Blueprint & Re-Score
            analysis_data = {
                "serp_overview": live_serp_data,
                "competitor_analysis": competitor_analysis,
                "content_intelligence": content_intelligence,
                "recommended_strategy": recommended_strategy,
            }

            blueprint = self.blueprint_factory.create_blueprint(
                seed_topic=keyword,
                winning_keyword_data=opportunity.get("full_data", {}).copy(),
                analysis_data=analysis_data,
                total_api_cost=total_api_cost,
                client_id=opportunity.get("client_id"),
            )

            opportunity["blueprint"] = blueprint

            final_score, final_score_breakdown = self.scoring_engine.calculate_score(
                opportunity
            )

            self.db_manager.update_opportunity_scores(
                opportunity_id, final_score, final_score_breakdown, blueprint
            )
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "analysis_completed", "paused_for_approval"
            )

            return {
                "status": "success",
                "message": "Analysis phase completed and opportunity re-scored.",
                "api_cost": total_api_cost,
            }

        except Exception as e:
            error_message = f"Analysis phase failed unexpectedly: {e}"
            self.logger.error(f"{error_message}\n{traceback.format_exc()}")
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "analysis_failed", "failed", error_message=str(e)
            )
            return {"status": "failed", "message": str(e), "api_cost": total_api_cost}

    def _run_analysis_background(
        self,
        job_id: str,
        opportunity_id: int,
        selected_competitor_urls: Optional[List[str]],
    ):
        try:
            self.run_analysis_phase(opportunity_id, selected_competitor_urls)
            self.job_manager.update_job_status(job_id, "completed", progress=100)
        except Exception as e:
            self.job_manager.update_job_status(job_id, "failed", error=str(e))
            raise

    def run_full_analysis(
        self, opportunity_id: int, selected_competitor_urls: Optional[List[str]] = None
    ) -> str:
        job_id = self.job_manager.create_job(
            target_function=self._run_analysis_background,
            args=(opportunity_id, selected_competitor_urls),
        )
        return job_id
</file>

<file path="backend/pipeline/orchestrator/content_orchestrator.py">
# backend/pipeline/orchestrator/content_orchestrator.py
import logging
import traceback
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)


class ContentOrchestrator:
    def _build_abstract_content_tree(
        self, opportunity: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Builds the Abstract Content Tree (ACT) from the blueprint's outline."""
        self.logger.info(
            f"Building Abstract Content Tree for opportunity ID: {opportunity['id']}"
        )

        blueprint = opportunity.get("blueprint", {})
        content_intelligence = blueprint.get("content_intelligence", {})
        outline_structure = content_intelligence.get("article_structure", [])

        if not outline_structure:
            raise ValueError(
                "Cannot build ACT: `article_structure` not found in blueprint."
            )

        act = []

        for i, section in enumerate(outline_structure):
            h2_title = section.get("h2")
            h3s = section.get("h3s", [])

            if not h2_title:
                continue

            node_type = "section_h2"
            if h2_title.lower().strip().startswith("introduction"):
                node_type = "introduction"
            elif h2_title.lower().strip().startswith("conclusion"):
                node_type = "conclusion"

            act.append(
                {
                    "id": f"section-{i}",
                    "type": node_type,
                    "title": h2_title,
                    "sub_points": h3s,
                    "status": "pending",
                    "content_html": "",
                }
            )

        self.logger.info(f"Successfully built ACT with {len(act)} nodes.")
        return act

    def _run_full_content_generation_background(
        self,
        job_id: str,
        opportunity_id: int,
        overrides: Optional[Dict[str, Any]] = None,
    ):
        """
        Internal method to execute the full agentic content generation and enrichment pipeline.
        This is the complete, final version of this function.
        """
        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            error_msg = f"Opportunity {opportunity_id} not found."
            self.logger.error(error_msg)
            self.job_manager.update_job_status(job_id, "failed", error=error_msg)
            return

        if opportunity.get("status") not in ["analyzed", "paused_for_approval"]:
            error_msg = "Opportunity not ready for content generation."
            self.job_manager.update_job_status(job_id, "failed", error=error_msg)
            return

        self.db_manager.update_opportunity_workflow_state(
            opportunity_id, "content_creation_started", "running"
        )
        self.job_manager.update_job_status(
            job_id, "running", progress=5, result={"step": "Initializing Generation"}
        )

        try:
            # --- START COST TRACKING MODIFICATION ---
            total_api_cost = opportunity.get("blueprint", {}).get("metadata", {}).get("total_api_cost", 0.0)
            self.logger.info(f"Initial cost from blueprint: ${total_api_cost:.4f}")
            # --- END COST TRACKING MODIFICATION ---

            self.job_manager.update_job_status(
                job_id, "running", progress=10, result={"step": "Building Content Tree"}
            )
            act = self._build_abstract_content_tree(opportunity)

            from agents.article_generator import SectionalArticleGenerator

            sectional_generator = SectionalArticleGenerator(
                self.openai_client, self.client_cfg, self.db_manager
            )

            full_article_context_for_conclusion = ""
            previous_content = ""
            for i, node in enumerate(act):
                progress = 15 + int((i / len(act)) * 40)
                self.job_manager.update_job_status(
                    job_id,
                    "running",
                    progress=progress,
                    result={"step": f"Generating: {node['title']}"},
                )

                content_html, cost = None, 0.0
                if node["type"] == "introduction":
                    content_html, cost = sectional_generator.generate_introduction(
                        opportunity
                    )
                elif node["type"] == "section_h2":
                    content_html, cost = sectional_generator.generate_section(
                        opportunity,
                        node["title"],
                        node.get("sub_points", []),
                        previous_content,
                    )
                elif node["type"] == "conclusion":
                    content_html, cost = sectional_generator.generate_conclusion(
                        opportunity, full_article_context_for_conclusion
                    )
                
                total_api_cost += cost # Aggregate cost

                if content_html:
                    node["content_html"] = content_html
                    full_article_context_for_conclusion += (
                        f"<h2>{node['title']}</h2>\n{content_html}\n"
                    )
                    previous_content = content_html
                else:
                    raise RuntimeError(
                        f"Failed to generate content for section '{node['title']}'."
                    )

            self.job_manager.update_job_status(
                job_id, "running", progress=60, result={"step": "Assembling Article"}
            )
            final_html_parts = [
                f"<h2>{node['title']}</h2>\n{node['content_html']}" for node in act
            ]
            final_article_html = "\n".join(final_html_parts)

            opportunity["ai_content"] = {"article_body_html": final_article_html}

            MAX_REFINEMENT_ATTEMPTS = 3
            current_html = opportunity["ai_content"]["article_body_html"]
            final_audit_results = {}

            for attempt in range(MAX_REFINEMENT_ATTEMPTS):
                self.job_manager.update_job_status(
                    job_id,
                    "running",
                    progress=65 + (attempt * 5),
                    result={"step": f"Auditing Content (Attempt {attempt + 1})"},
                )

                audit_results = self.content_auditor.audit_content(
                    article_html=current_html,
                    primary_keyword=opportunity.get("keyword", ""),
                    blueprint=opportunity.get("blueprint", {}),
                    client_cfg=self.client_cfg,
                )
                final_audit_results = audit_results

                structured_issues = audit_results.get("publish_readiness_issues", [])

                if not structured_issues:
                    self.logger.info(
                        f"Audit passed on attempt {attempt + 1}. No refinement needed."
                    )
                    break

                self.logger.warning(
                    f"Audit failed on attempt {attempt + 1}. Issues found: {len(structured_issues)}. Triggering self-healing."
                )
                self.job_manager.update_job_status(
                    job_id,
                    "running",
                    progress=70 + (attempt * 5),
                    result={"step": f"Self-Healing (Attempt {attempt + 1})"},
                )

                all_refinement_commands = []
                for issue in structured_issues:
                    if issue["issue"] == "unresolved_placeholder":
                        all_refinement_commands.append(
                            "- CRITICAL FIX: Remove all image placeholders like '[[IMAGE_ID:...]]' from the text."
                        )
                    elif issue["issue"] == "empty_heading":
                        all_refinement_commands.append(
                            f"- FIX: The following heading tag is empty: `{issue['context']}`. Based on the surrounding content, either remove this tag entirely or populate it with a relevant heading."
                        )
                    elif issue["issue"] == "short_paragraph":
                        all_refinement_commands.append(
                            f"- FIX: The paragraph `{issue['context']}` is too brief. Expand this paragraph to be at least 3 sentences long, providing more detail, or merge it with an adjacent paragraph if appropriate."
                        )
                    elif issue["issue"] == "word_count_deviation":
                        target_word_count = (
                            opportunity.get("blueprint", {})
                            .get("ai_content_brief", {})
                            .get("target_word_count", 1500)
                        )
                        all_refinement_commands.append(
                            f"- FIX: The article's word count is significantly off target. Review the entire article and expand or condense it to be approximately {target_word_count} words. {issue['context']}"
                        )

                if not all_refinement_commands:
                    break

                combined_command = (
                    "Please refine the entire HTML document by addressing the following issues:\n"
                    + "\n".join(all_refinement_commands)
                )

                refine_prompt_messages = [
                    {
                        "role": "system",
                        "content": "You are an expert content editor. You will receive a full HTML document and a list of specific issues to fix. Apply all fixes and return ONLY the complete, corrected HTML document. Preserve all original HTML tags and structure unless a fix requires changing them. Do not add any introductory text, just the refined HTML.",
                    },
                    {
                        "role": "user",
                        "content": f"COMMANDS:\n{combined_command}\n\nFULL HTML TO FIX:\n```html\n{current_html}\n```",
                    },
                ]

                refined_html, error = self.openai_client.call_chat_completion(
                    messages=refine_prompt_messages,
                    model=self.client_cfg.get("default_model", "gpt-5-nano"),
                    temperature=0.2,
                )
                total_api_cost += self.openai_client.latest_cost # Aggregate cost

                if error or not refined_html:
                    self.logger.error(
                        f"AI Refinement Agent failed on attempt {attempt + 1}: {error}"
                    )
                    break

                current_html = (
                    refined_html.strip()
                    .removeprefix("```html")
                    .removesuffix("```")
                    .strip()
                )

            opportunity["ai_content"]["article_body_html"] = current_html
            opportunity["ai_content"]["audit_results"] = final_audit_results

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=85,
                result={"step": "Generating Images & Social Posts"},
            )
            featured_image_data, image_cost = self.image_generator.generate_featured_image(
                opportunity
            )
            total_api_cost += image_cost
            social_posts, social_cost = self.social_crafter.craft_posts(opportunity)
            total_api_cost += social_cost

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=90,
                result={"step": "Formatting & Internal Linking"},
            )
            internal_link_suggestions, link_cost = (
                self.internal_linking_suggester.suggest_links(
                    opportunity["ai_content"]["article_body_html"],
                    opportunity.get("blueprint", {})
                    .get("ai_content_brief", {})
                    .get("key_entities_to_mention", []),
                    self.client_cfg.get("target_domain"),
                    self.client_id,
                )
            )
            total_api_cost += link_cost

            final_package = self.html_formatter.format_final_package(
                opportunity,
                internal_linking_suggestions=internal_link_suggestions,
                in_article_images_data=[],
            )

            self.job_manager.update_job_status(
                job_id, "running", progress=95, result={"step": "Saving to Database"}
            )
            self.db_manager.save_full_content_package(
                opportunity_id,
                opportunity["ai_content"],
                self.client_cfg.get("ai_content_model", "gpt-4o"),
                featured_image_data,
                [],
                social_posts,
                final_package,
                total_api_cost, # Pass total cost
            )

            self.job_manager.update_job_status(
                job_id,
                "completed",
                progress=100,
                result={
                    "status": "success",
                    "message": "Content generation completed.",
                },
            )

        except Exception as e:
            error_msg = (
                f"Agentic content generation failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "content_generation_failed", "failed", str(e)
            )
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def run_full_content_generation(
        self, opportunity_id: int, overrides: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Public method to initiate content generation asynchronously.
        Returns a job_id.
        """
        self.logger.info(
            f"--- Orchestrator: Initiating Full Content Generation for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_full_content_generation_background,
            args=(opportunity_id, overrides),
        )
        return job_id
</file>

<file path="backend/pipeline/orchestrator/cost_estimator.py">
# backend/pipeline/orchestrator/cost_estimator.py
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


class CostEstimator:
    def estimate_action_cost(
        self,
        action: str,
        opportunity_id: Optional[int] = None,
        discovery_params: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Estimates the API cost for a given workflow action without executing it.
        - For 'discovery', uses discovery_params.
        - For other actions, uses opportunity_id.
        """
        estimated_cost = 0.0
        explanation = []

        if action == "discovery":
            if not discovery_params:
                raise ValueError(
                    "discovery_params are required for 'discovery' action estimation."
                )

            KEYWORD_IDEAS_RATE = 0.005
            KEYWORD_SUGGESTIONS_RATE = 0.005
            RELATED_KEYWORDS_RATE = 0.005

            seed_keywords = discovery_params.get("seed_keywords", [])
            discovery_modes = discovery_params.get("discovery_modes", [])
            max_pages = self.client_cfg.get("discovery_max_pages", 1)
            num_seeds = len(seed_keywords)

            if "keyword_ideas" in discovery_modes:
                cost = KEYWORD_IDEAS_RATE * max_pages
                estimated_cost += cost
                explanation.append(
                    {
                        "service": "Keyword Ideas API",
                        "details": f"1 call x {max_pages} page(s) @ ${KEYWORD_IDEAS_RATE}/call",
                        "cost": cost,
                    }
                )

            if "keyword_suggestions" in discovery_modes:
                cost = KEYWORD_SUGGESTIONS_RATE * num_seeds * max_pages
                estimated_cost += cost
                explanation.append(
                    {
                        "service": "Keyword Suggestions API",
                        "details": f"{num_seeds} seed(s) x {max_pages} page(s) @ ${KEYWORD_SUGGESTIONS_RATE}/call",
                        "cost": cost,
                    }
                )

            if "related_keywords" in discovery_modes:
                cost = RELATED_KEYWORDS_RATE * num_seeds * max_pages
                estimated_cost += cost
                explanation.append(
                    {
                        "service": "Related Keywords API",
                        "details": f"{num_seeds} seed(s) x {max_pages} page(s) @ ${RELATED_KEYWORDS_RATE}/call",
                        "cost": cost,
                    }
                )

            return {"total_cost": estimated_cost, "breakdown": explanation}

        if not opportunity_id:
            raise ValueError(f"opportunity_id is required for action '{action}'.")

        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise ValueError("Opportunity not found.")

        if action == "analyze" or action == "validate":
            serp_base_cost = 0.005
            serp_cost_explanation = (
                f"1 x SERP Live Advanced call (~${serp_base_cost:.3f} base)"
            )

            if self.client_cfg.get("load_async_ai_overview", False):
                serp_base_cost += 0.002
                serp_cost_explanation += " + $0.002 (Async AI Overview)"

            if self.client_cfg.get("calculate_rectangles", False):
                serp_base_cost += 0.002
                serp_cost_explanation += " + $0.002 (Pixel Ranking)"

            paa_depth = self.client_cfg.get("people_also_ask_click_depth", 0)
            if isinstance(paa_depth, int) and paa_depth > 0:
                paa_cost = paa_depth * 0.00015
                serp_base_cost += paa_cost
                serp_cost_explanation += f" + ${paa_cost:.5f} (PAA Depth {paa_depth})"

            estimated_cost += serp_base_cost
            explanation.append(
                {
                    "service": "SERP Live Advanced Task",
                    "details": serp_cost_explanation,
                    "cost": serp_base_cost,
                }
            )

            if action == "analyze":
                num_competitors = self.client_cfg.get("num_competitors_to_analyze", 5)

                ONPAGE_BASIC_RATE = 0.000125
                ONPAGE_RENDER_RATE = 0.00425
                ONPAGE_CUSTOM_JS_RATE = 0.00025

                if self.client_cfg.get("onpage_enable_browser_rendering", False):
                    onpage_per_task_cost = ONPAGE_RENDER_RATE
                    onpage_cost_explanation = f"x OnPage Instant Pages (Browser Rendering ON @ ${onpage_per_task_cost:.5f} each)"
                else:
                    onpage_per_task_cost = ONPAGE_BASIC_RATE
                    onpage_cost_explanation = f"x OnPage Instant Pages (Basic Crawl @ ${onpage_per_task_cost:.5f} each)"

                if self.client_cfg.get("onpage_enable_custom_js", False):
                    onpage_per_task_cost += ONPAGE_CUSTOM_JS_RATE
                    onpage_cost_explanation += " + $0.00025 (Custom JavaScript)"

                onpage_cost = num_competitors * onpage_per_task_cost
                estimated_cost += onpage_cost

                explanation.append(
                    {
                        "service": f"{num_competitors} Competitor OnPage Tasks",
                        "details": onpage_cost_explanation,
                        "cost": onpage_cost,
                    }
                )

            ai_analysis_cost = 0.05
            estimated_cost += ai_analysis_cost
            explanation.append(
                {
                    "service": "OpenAI Analysis Buffer",
                    "details": "1 x OpenAI GPT-4o call for analysis",
                    "cost": ai_analysis_cost,
                }
            )

        elif action == "generate":
            model = self.client_cfg.get("ai_content_model", "gpt-4o")

            pricing = self.client_cfg.get("OPENAI_PRICING", {})
            input_rate = pricing.get(f"{model}_input", 5.00) / 1000000
            output_rate = pricing.get(f"{model}_output", 15.00) / 1000000

            article_input_tokens = 10000
            article_output_tokens = 5000
            article_cost = (article_input_tokens * input_rate) + (
                article_output_tokens * output_rate
            )

            social_cost = (2000 * input_rate) + (500 * output_rate)

            buffer_tokens = 5000
            buffer_cost = (
                (buffer_tokens * input_rate) + (buffer_tokens * output_rate)
            ) * 0.5

            estimated_cost += article_cost
            explanation.append(
                {
                    "service": "AI Article Generation",
                    "details": f"1 x OpenAI {model} call (10k in, 5k out)",
                    "cost": article_cost,
                }
            )

            estimated_cost += social_cost
            explanation.append(
                {
                    "service": "AI Social Posts",
                    "details": f"1 x OpenAI {model} call (2k in, 0.5k out)",
                    "cost": social_cost,
                }
            )

            estimated_cost += buffer_cost
            explanation.append(
                {
                    "service": "AI Refinement/Linking Buffer",
                    "details": "50% chance of refinement/linking tokens",
                    "cost": buffer_cost,
                }
            )

            if self.client_cfg.get("use_pexels_first", True):
                explanation.append(
                    {
                        "service": "Image Sourcing (Pexels)",
                        "details": "Cost: $0.00",
                        "cost": 0.00,
                    }
                )
            else:
                image_cost = self.client_cfg.get("num_in_article_images", 0) * 0.04
                estimated_cost += image_cost
                explanation.append(
                    {
                        "service": f"Image Generation ({self.client_cfg.get('default_image_model', 'dall-e-3')})",
                        "details": f"Estimated {self.client_cfg.get('num_in_article_images', 0)} images @ $0.04 each",
                        "cost": image_cost,
                    }
                )

        elif action == "validate":
            # Assuming SERP_LIVE_ADVANCED_RATE is 0.020 USD
            SERP_LIVE_ADVANCED_RATE = 0.020
            validation_cost = SERP_LIVE_ADVANCED_RATE
            details = f"1 x SERP Live Advanced call (~${SERP_LIVE_ADVANCED_RATE:.3f})"
            if self.client_cfg.get("load_async_ai_overview", False):
                validation_cost += 0.002
                details += " + $0.002 for asynchronous AI Overview retrieval."
            estimated_cost += validation_cost
            explanation.append(
                {
                    "service": "SERP Validation",
                    "details": details,
                    "cost": validation_cost,
                }
            )

        return {"estimated_cost": round(estimated_cost, 2), "explanation": explanation}
</file>

<file path="backend/pipeline/orchestrator/discovery_orchestrator.py">
# backend/pipeline/orchestrator/discovery_orchestrator.py
import logging
import traceback
import os
from typing import Dict, Any, List, Optional

from backend.services.serp_analysis_service import SerpAnalysisService

logger = logging.getLogger(__name__)


class DiscoveryOrchestrator:
    def _run_discovery_background(
        self,
        job_id: str,
        run_id: int,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        filters_override: Optional[Dict[str, Any]],
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = None,
        include_clickstream_data: Optional[bool] = None,
        closely_variants: Optional[bool] = None,
        exact_match: Optional[bool] = None,
    ):
        """Internal method to execute the consolidated discovery phase for a job."""
        log_dir = "discovery_logs"
        os.makedirs(log_dir, exist_ok=True)
        log_file_path = os.path.join(log_dir, f"run_{run_id}.log")

        run_logger = logging.getLogger(f"run_{run_id}")
        handler = logging.FileHandler(log_file_path)
        handler.setFormatter(
            logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        )
        run_logger.addHandler(handler)
        run_logger.setLevel(logging.INFO)

        self.db_manager.update_discovery_run_log_path(run_id, log_file_path)
        self.db_manager.update_discovery_run_status(run_id, "running")
        self.job_manager.update_job_status(job_id, "running", progress=0)

        run_config = self.global_cfg_manager.load_client_config(
            self.client_id, self.db_manager
        )
        if filters_override:
            run_logger.info(f"Applying filter overrides: {filters_override}")
            run_config.update(filters_override)

        run_logger.info(
            f"Starting discovery with modes: {discovery_modes}, filters: {filters}, order_by: {order_by}, limit: {limit}, depth: {depth}"
        )

        try:
            job_status = self.job_manager.get_job_status(job_id)
            if job_status and job_status.get("status") == "failed":
                run_logger.warning(
                    "Job found marked as 'failed' (cancelled). Exiting gracefully."
                )
                self.db_manager.update_discovery_run_status(run_id, "cancelled")
                return {"message": "Job cancelled by user request."}

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=10,
                result={"step": "Fetching & Scoring keywords"},
            )

            from pipeline.step_01_discovery.run_discovery import run_discovery_phase

            discovery_result = run_discovery_phase(
                seed_keywords=seed_keywords,
                dataforseo_client=self.dataforseo_client,
                db_manager=self.db_manager,
                client_id=self.client_id,
                client_cfg=run_config,
                discovery_modes=discovery_modes,
                filters=filters,
                order_by=order_by,
                limit=limit,
                depth=depth,
                ignore_synonyms=ignore_synonyms,
                include_clickstream_data=include_clickstream_data,
                closely_variants=closely_variants,
                run_logger=run_logger,
            )

            stats = discovery_result.get("stats", {})
            total_cost = discovery_result.get("total_cost", 0.0)
            processed_opportunities = discovery_result.get("opportunities", [])

            self.job_manager.update_job_status(
                job_id, "running", progress=75, result={"step": "Saving to Database"}
            )

            num_added = 0
            if processed_opportunities:
                run_logger.info(
                    f"Attempting to save {len(processed_opportunities)} processed opportunities..."
                )
                num_added = self.db_manager.add_opportunities(
                    processed_opportunities, self.client_id, run_id
                )
                run_logger.info(
                    f"Successfully saved {num_added} new keyword records. The database ignored {len(processed_opportunities) - num_added} duplicates."
                )

            results_summary = {
                "total_cost": total_cost,
                "source_counts": stats.get("raw_counts", {}),
                "total_raw_count": stats.get("total_raw_count", 0),
                "total_unique_count": stats.get("total_unique_count", 0),
                "disqualification_reasons": stats.get("disqualification_reasons", {}),
                "disqualified_count": stats.get("disqualified_count", 0),
                "final_qualified_count": stats.get("final_qualified_count", 0),
                "duplicates_removed": len(processed_opportunities) - num_added,
                "final_added_to_db": num_added,
            }

            self.db_manager.update_discovery_run_completed(run_id, results_summary)
            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=results_summary
            )
            run_logger.info("Discovery run completed successfully.")
            return results_summary
        except Exception as e:
            error_message = f"Discovery workflow failed: {e}\n{traceback.format_exc()}"
            run_logger.error(error_message)
            self.db_manager.update_discovery_run_failed(run_id, str(e))
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def run_discovery_and_save(
        self,
        run_id: int,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]] = None,
        order_by: Optional[List[str]] = None,
        filters_override: Optional[Dict[str, Any]] = None,
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = None,
        include_clickstream_data: Optional[bool] = None,
        closely_variants: Optional[bool] = None,
        exact_match: Optional[bool] = None,
    ) -> str:
        """
        Public method to initiate a discovery run asynchronously.
        Returns a job_id.
        """
        self.logger.info(
            f"--- Orchestrator: Initiating Full Discovery & Qualification for Run ID: {run_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_discovery_background,
            args=(
                run_id,
                seed_keywords,
                discovery_modes,
                filters,
                order_by,
                filters_override,
                limit,
                depth,
                ignore_synonyms,
                include_clickstream_data,
                closely_variants,
                exact_match,
            ),
        )
        return job_id
</file>

<file path="backend/pipeline/orchestrator/image_orchestrator.py">
# backend/pipeline/orchestrator/image_orchestrator.py
import logging
import traceback
import json

logger = logging.getLogger(__name__)


class ImageOrchestrator:
    def _run_single_image_generation_background(
        self, job_id: str, opportunity_id: int, original_prompt: str, new_prompt: str
    ):
        """Internal method to regenerate a single in-article image."""
        self.job_manager.update_job_status(job_id, "running", progress=0)

        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity or not opportunity.get("ai_content_json"):
                raise ValueError(
                    "Opportunity or content missing for single image regeneration."
                )

            opportunity["client_cfg"] = self.client_cfg

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=30,
                result={"step": "Generating single image"},
            )

            images_data, _ = self.image_generator.generate_images_from_prompts(
                [new_prompt]
            )

            if not images_data or not images_data[0]:
                raise RuntimeError("Image generation failed or returned no data.")

            new_image_data = images_data[0]

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=70,
                result={"step": "Updating content package"},
            )

            in_article_images = opportunity.get("in_article_images_data", [])
            if isinstance(in_article_images, str):
                in_article_images = (
                    json.loads(in_article_images) if in_article_images else []
                )

            updated_images = []
            found_and_updated = False
            for img in in_article_images:
                if img.get("original_prompt") == original_prompt:
                    img.update(new_image_data)
                    found_and_updated = True
                updated_images.append(img)

            if not found_and_updated:
                new_image_data["original_prompt"] = new_prompt
                updated_images.append(new_image_data)

            self.db_manager.update_opportunity_images(
                opportunity_id,
                opportunity.get("featured_image_url"),
                opportunity.get("featured_image_local_path"),
                updated_images,
            )

            final_package = self.html_formatter.format_final_package(opportunity)
            self.db_manager.update_opportunity_final_package(
                opportunity_id, final_package
            )

            result_message = {
                "status": "success",
                "message": f"Single image regenerated for prompt: {original_prompt}",
            }

            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=result_message
            )
            return result_message

        except Exception as e:
            error_msg = (
                f"Single image regeneration failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def regenerate_single_image(
        self, opportunity_id: int, original_prompt: str, new_prompt: str
    ) -> str:
        """Public method to initiate single image regeneration asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Single Image Regeneration for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_single_image_generation_background,
            args=(opportunity_id, original_prompt, new_prompt),
        )
        return job_id

    def _run_featured_image_regeneration_background(
        self, job_id: str, opportunity_id: int, prompt: str
    ):
        """Internal method to regenerate a featured image."""
        self.job_manager.update_job_status(
            job_id, "running", progress=10, result={"step": "Generating Featured Image"}
        )
        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                raise ValueError(
                    "Opportunity not found for featured image regeneration."
                )

            opportunity["keyword"] = prompt
            opportunity["ai_content"] = {"meta_title": prompt}

            featured_image_data, cost = self.image_generator.generate_featured_image(
                opportunity
            )

            if featured_image_data:
                self.db_manager.update_opportunity_images(
                    opportunity_id,
                    featured_image_data.get("remote_url"),
                    featured_image_data.get("local_path"),
                    json.loads(opportunity.get("in_article_images_data", "[]")),
                )

            result_message = {
                "status": "success",
                "message": "Featured image regenerated successfully.",
                "api_cost": cost,
            }
            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=result_message
            )
            return result_message

        except Exception as e:
            error_msg = (
                f"Featured image regeneration failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def regenerate_featured_image(self, opportunity_id: int, prompt: str) -> str:
        """Public method to initiate featured image regeneration asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Featured Image Regeneration for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_featured_image_regeneration_background,
            args=(opportunity_id, prompt),
        )
        return job_id
</file>

<file path="backend/pipeline/orchestrator/main.py">
# backend/pipeline/orchestrator/main.py
import logging

from backend.app_config.manager import ConfigManager
from backend.data_access.database_manager import DatabaseManager
from backend.external_apis.dataforseo_client_v2 import DataForSEOClientV2
from backend.external_apis.openai_client import OpenAIClientWrapper
from backend.agents.image_generator import ImageGenerator
from backend.agents.social_media_crafter import SocialMediaCrafter
from backend.agents.internal_linking_suggester import InternalLinkingSuggester
from backend.agents.html_formatter import HtmlFormatter
from backend.core.blueprint_factory import BlueprintFactory
from backend.agents.content_auditor import ContentAuditor
from backend.agents.prompt_assembler import DynamicPromptAssembler
from backend.services.serp_analysis_service import SerpAnalysisService
from backend.pipeline.step_03_prioritization.scoring_engine import ScoringEngine
from backend.pipeline.step_01_discovery.cannibalization_checker import (
    CannibalizationChecker,
)
from backend.jobs import JobManager

from .discovery_orchestrator import DiscoveryOrchestrator
from .analysis_orchestrator import AnalysisOrchestrator
from .content_orchestrator import ContentOrchestrator
from .image_orchestrator import ImageOrchestrator
from .social_orchestrator import SocialOrchestrator
from .validation_orchestrator import ValidationOrchestrator
from .workflow_orchestrator import WorkflowOrchestrator
from .cost_estimator import CostEstimator

logger = logging.getLogger(__name__)


class WorkflowOrchestrator(
    DiscoveryOrchestrator,
    AnalysisOrchestrator,
    ContentOrchestrator,
    ImageOrchestrator,
    SocialOrchestrator,
    ValidationOrchestrator,
    WorkflowOrchestrator,
    CostEstimator,
):
    def __init__(
        self,
        global_cfg_manager: ConfigManager,
        db_manager: DatabaseManager,
        client_id: str,
        job_manager: "JobManager",
    ):
        self.global_cfg_manager = global_cfg_manager
        self.db_manager = db_manager
        self.client_id = client_id
        self.job_manager = job_manager
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = self.global_cfg_manager.load_client_config(
            self.client_id, self.db_manager
        )

        self.openai_client = OpenAIClientWrapper(
            self.client_cfg.get("openai_api_key"), self.client_cfg
        )
        self.dataforseo_client = DataForSEOClientV2(
            login=self.client_cfg["dataforseo_login"],
            password=self.client_cfg["dataforseo_password"],
            config=self.client_cfg,
            db_manager=self.db_manager,
            enable_cache=self.client_cfg.get("enable_cache", True),
        )

        self.image_generator = ImageGenerator(self.client_cfg)
        self.social_crafter = SocialMediaCrafter(self.openai_client, self.client_cfg)
        self.internal_linking_suggester = InternalLinkingSuggester(
            self.openai_client, self.client_cfg, self.db_manager
        )
        self.html_formatter = HtmlFormatter()
        self.blueprint_factory = BlueprintFactory(
            self.openai_client, self.client_cfg, self.dataforseo_client, self.db_manager
        )
        self.scoring_engine = ScoringEngine(self.client_cfg)
        self.cannibalization_checker = CannibalizationChecker(
            self.client_cfg.get("target_domain"),
            self.dataforseo_client,
            self.client_cfg,
            self.db_manager,
        )
        self.content_auditor = ContentAuditor()
        self.prompt_assembler = DynamicPromptAssembler(self.db_manager)
        self.serp_analysis_service = SerpAnalysisService(self.dataforseo_client, self.client_cfg)
</file>

<file path="backend/pipeline/orchestrator/social_orchestrator.py">
# backend/pipeline/orchestrator/social_orchestrator.py
import logging
import traceback

logger = logging.getLogger(__name__)


class SocialOrchestrator:
    def _run_social_posts_regeneration_background(
        self, job_id: str, opportunity_id: int
    ):
        """Internal method to regenerate social media posts."""
        self.job_manager.update_job_status(
            job_id, "running", progress=10, result={"step": "Crafting Social Posts"}
        )
        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                raise ValueError("Opportunity not found for social media regeneration.")

            opportunity["client_cfg"] = self.client_cfg

            social_posts, cost = self.social_crafter.craft_posts(opportunity)
            if social_posts:
                self.db_manager.update_opportunity_social_posts(
                    opportunity_id, social_posts
                )

            result_message = {
                "status": "success",
                "message": "Social media posts regenerated successfully.",
                "api_cost": cost,
            }
            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=result_message
            )
            return result_message

        except Exception as e:
            error_msg = (
                f"Social media post regeneration failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def regenerate_social_posts(self, opportunity_id: int) -> str:
        """Public method to initiate social media post regeneration asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Social Media Post Regeneration for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_social_posts_regeneration_background,
            args=(opportunity_id,),
        )
        return job_id
</file>

<file path="backend/pipeline/orchestrator/validation_orchestrator.py">
# backend/pipeline/orchestrator/validation_orchestrator.py
import logging
import traceback

logger = logging.getLogger(__name__)


class ValidationOrchestrator:
    def run_validation_phase(self, opportunity_id: int):
        """
        Runs a cost-effective final validation gate before committing to a full analysis.
        Makes one live SERP call and a deep cannibalization check.
        """
        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            return {
                "status": "failed",
                "message": f"Opportunity ID {opportunity_id} not found.",
            }

        self.logger.info(
            f"--- Orchestrator: Starting Live SERP Validation for '{opportunity.get('keyword')}' ---"
        )
        self.db_manager.update_opportunity_workflow_state(
            opportunity_id, "validation_started", "in_progress"
        )
        total_cost = 0.0

        try:
            from core.serp_analyzer import FullSerpAnalyzer

            serp_analyzer = FullSerpAnalyzer(self.dataforseo_client, self.client_cfg)
            serp_overview, serp_api_cost = serp_analyzer.analyze_serp(
                opportunity.get("keyword")
            )
            total_cost += serp_api_cost
            if not serp_overview:
                raise ValueError("Failed to retrieve live SERP data for validation.")

            from pipeline.step_04_analysis.run_analysis import run_final_validation

            is_valid, reason = run_final_validation(
                serp_overview, opportunity, self.client_cfg, self.dataforseo_client
            )

            if is_valid:
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id, "validation_passed", "validated"
                )
                self.logger.info(
                    f"Validation PASSED for '{opportunity.get('keyword')}'."
                )
                return {
                    "status": "success",
                    "message": "Validation passed. Ready for full analysis.",
                    "api_cost": total_cost,
                }
            else:
                self.db_manager.update_opportunity_status(opportunity_id, "rejected")
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id,
                    "validation_failed",
                    "rejected",
                    error_message=reason,
                )
                self.logger.warning(
                    f"Validation FAILED for '{opportunity.get('keyword')}': {reason}"
                )
                return {"status": "failed", "message": reason, "api_cost": total_cost}
        except Exception as e:
            error_msg = f"Validation phase failed unexpectedly: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "validation_failed", "failed", error_message=str(e)
            )
            return {"status": "failed", "message": str(e), "api_cost": total_cost}

    def _run_validation_background(self, job_id: str, opportunity_id: int):
        """Internal method to execute the validation phase for a job."""
        self.job_manager.update_job_status(job_id, "running", progress=0)
        try:
            result = self.run_validation_phase(opportunity_id)

            if result["status"] == "success":
                self.job_manager.update_job_status(
                    job_id, "completed", progress=100, result=result
                )
            else:
                self.job_manager.update_job_status(
                    job_id, "failed", progress=100, error=result["message"]
                )
            return result
        except Exception as e:
            error_msg = f"Validation background failed: {e}\n{traceback.format_exc()}"
            self.logger.error(error_msg)
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "validation_failed", "failed", str(e)
            )
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def run_validation(self, opportunity_id: int) -> str:
        """Public method to initiate the validation phase asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Validation for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_validation_background, args=(opportunity_id,)
        )
        return job_id
</file>

<file path="backend/pipeline/orchestrator/workflow_orchestrator.py">
# backend/pipeline/orchestrator/workflow_orchestrator.py
import logging
import traceback
import threading

logger = logging.getLogger(__name__)


class WorkflowOrchestrator:
    def _run_full_auto_workflow_background(
        self, job_id: str, opportunity_id: int, override_validation: bool
    ):
        """Internal method to execute the full workflow from validation to generation."""
        try:
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "in_progress", "running"
            )
            self.job_manager.update_job_progress(job_id, "Workflow Started", "Starting full automation workflow.")

            if not override_validation:
                self.job_manager.update_job_progress(job_id, "Validation", "Running validation checks.")
                validation_result = self.run_validation_phase(opportunity_id)
                if validation_result.get("status") == "failed":
                    self.logger.warning(
                        f"Workflow for opportunity {opportunity_id} stopped due to validation failure: {validation_result.get('message')}"
                    )
                    raise RuntimeError(f"Validation failed: {validation_result.get('message')}")
            else:
                self.logger.info(
                    f"Validation step skipped for opportunity {opportunity_id} due to override."
                )
                self.job_manager.update_job_progress(job_id, "Validation", "Validation skipped by user override.")
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id,
                    "validation_overridden",
                    "validated",
                    error_message="Validation manually overridden.",
                )

            self.job_manager.update_job_progress(job_id, "Analysis", "Starting in-depth analysis.")
            analysis_result = self.run_analysis_phase(
                opportunity_id, selected_competitor_urls=None
            )
            if analysis_result.get("status") == "failed":
                reason = f"Disqualified during analysis: {analysis_result.get('message')}"
                self.logger.warning(
                    f"Full auto workflow for {opportunity_id} stopped: {reason}"
                )
                raise RuntimeError(reason)

            self.db_manager.update_opportunity_workflow_state(
                opportunity_id,
                "analysis_completed",
                "paused_for_approval",
                error_message="Awaiting user approval to proceed to content generation.",
            )
            self.job_manager.update_job_progress(job_id, "Paused", "Analysis complete. Awaiting user approval.", status="paused")
            self.logger.info(
                f"Full auto workflow for {opportunity_id} paused after analysis, awaiting user approval."
            )
            # This background job ends here. A new one is started by the 'approve_analysis' endpoint.
            # So, we set the job to a 'paused' but technically 'completed' state from the runner's perspective.
            self.job_manager.update_job_status(job_id, "paused", progress=50, result={"status": "paused", "message": "Awaiting approval."})

        except Exception as e:
            error_msg = f"Full auto workflow for {opportunity_id} failed: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            # The job's _run_job wrapper will catch this and handle the final 'failed' state.
            raise

    def run_full_auto_workflow(
        self, opportunity_id: int, override_validation: bool = False
    ) -> str:
        """Public method to initiate the full auto workflow asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Full Auto Workflow for Opportunity ID: {opportunity_id} (Async) with override: {override_validation} ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_full_auto_workflow_background,
            args=(opportunity_id, override_validation),
        )
        return job_id

    def _run_full_automation_workflow_background(
        self, job_id: str, opportunity_id: int, override_validation: bool
    ):
        """Internal method for the true 'fire and forget' full automation workflow."""
        try:
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "in_progress", "running"
            )
            self.job_manager.update_job_progress(job_id, "Workflow Started", "Starting full automation workflow.")

            if not override_validation:
                self.job_manager.update_job_progress(job_id, "Validation", "Running validation checks.")
                validation_result = self.run_validation_phase(opportunity_id)
                if validation_result.get("status") == "failed":
                    self.logger.warning(
                        f"Automation for opportunity {opportunity_id} stopped due to validation failure: {validation_result.get('message')}"
                    )
                    raise RuntimeError(f"Validation failed: {validation_result.get('message')}")
            else:
                self.logger.info(
                    f"Validation step skipped for opportunity {opportunity_id} due to override."
                )
                self.job_manager.update_job_progress(job_id, "Validation", "Validation skipped by user override.")
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id,
                    "validation_overridden",
                    "validated",
                    error_message="Validation manually overridden.",
                )

            self.job_manager.update_job_progress(job_id, "Analysis", "Starting in-depth analysis.")
            analysis_result = self.run_analysis_phase(
                opportunity_id, selected_competitor_urls=None
            )
            if analysis_result.get("status") == "failed":
                reason = f"Disqualified during analysis: {analysis_result.get('message')}"
                self.logger.warning(
                    f"Full automation for {opportunity_id} stopped: {reason}"
                )
                raise RuntimeError(reason)

            self.logger.info(
                f"Analysis complete for {opportunity_id}, proceeding directly to content generation."
            )
            self.job_manager.update_job_progress(job_id, "Content Generation", "Analysis complete, starting content generation.")

            # This now calls the content generation logic which also uses update_job_progress
            self._run_full_content_generation_background(job_id, opportunity_id, overrides=None)
            
            # The _run_job wrapper will mark the job as completed.
            # We add the redirect_url to the result.
            final_result = {
                "status": "success",
                "message": "Full automation workflow completed successfully.",
                "redirect_url": f"/opportunities/{opportunity_id}"
            }
            self.job_manager.update_job_status(job_id, "completed", progress=100, result=final_result)

        except Exception as e:
            error_msg = f"Full automation workflow for {opportunity_id} failed: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            # The job's _run_job wrapper will catch this and handle the final 'failed' state.
            raise
    
    # ... (keep existing public methods like run_full_automation_workflow) ...

    def continue_workflow_after_approval(self, job_id: str, opportunity_id: int, overrides: dict = None) -> str:
        """
        Continues a paused workflow job after user approval, proceeding to content generation.
        This now creates a NEW job for the content generation phase.
        """
        self.logger.info(
            f"Attempting to resume workflow from job {job_id} on opportunity {opportunity_id}."
        )

        job_info = self.job_manager.get_job_status(job_id)
        if not job_info or job_info.get("status") != "paused":
            error_msg = f"Cannot resume job {job_id}: Job not found or not in 'paused' state. Current status: {job_info.get('status') if job_info else 'Not Found'}."
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        self.db_manager.update_opportunity_status(opportunity_id, "in_progress")

        # Create a new job for the content generation part of the workflow
        new_job_id = self.job_manager.create_job(
            target_function=self._run_full_content_generation_background,
            args=(opportunity_id, overrides),
        )
        
        # Link the old job to the new one for traceability if needed
        self.job_manager.update_job_status(job_id, "completed", progress=100, result={"status": "resumed", "next_job_id": new_job_id})

        return new_job_id

    def _run_content_refresh_workflow_background(
        self, job_id: str, opportunity_id: int
    ):
        """Internal method to execute content refresh for a job."""
        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                raise ValueError("Opportunity not found for content refresh.")

            self.logger.info(
                f"--- Orchestrator: Starting Content Refresh Workflow for '{opportunity.get('keyword')}' ---"
            )
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "refresh_started", "running"
            )
            self.job_manager.update_job_progress(job_id, "Refresh Started", "Re-analyzing content and SERP data.")

            analysis_result = self.run_analysis_phase(
                opportunity_id, selected_competitor_urls=None
            )
            if analysis_result.get("status") == "failed":
                raise RuntimeError(f"Refresh failed during analysis: {analysis_result.get('message')}")

            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity or opportunity.get("status") != "analyzed":
                raise RuntimeError("Opportunity not in 'analyzed' state after refresh analysis.")

            self.job_manager.update_job_progress(job_id, "Content Generation", "Analysis complete, re-generating content.")
            
            self._run_full_content_generation_background(job_id, opportunity_id, overrides=None)

            self.db_manager.update_opportunity_workflow_state(
                opportunity_id,
                "refresh_completed",
                "generated",
                error_message="Content refreshed.",
            )
            
            final_result = {
                "status": "success",
                "message": "Content refresh completed successfully.",
                "redirect_url": f"/opportunities/{opportunity_id}"
            }
            self.job_manager.update_job_status(job_id, "completed", progress=100, result=final_result)
            self.logger.info(f"Content refresh for opportunity {opportunity_id} completed successfully.")

        except Exception as e:
            error_msg = f"Content refresh workflow failed for opportunity {opportunity_id}: {e}\n{traceback.format_exc()}"
            self.logger.error(error_msg)
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "refresh_failed", "failed", str(e)
            )
            # The job's _run_job wrapper will catch this and handle the final 'failed' state.
            raise

    def run_content_refresh_workflow(self, opportunity_id: int) -> str:
        """Public method to initiate an asynchronous content refresh workflow."""
        self.logger.info(
            f"--- Orchestrator: Initiating Content Refresh Workflow for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_content_refresh_workflow_background,
            args=(opportunity_id,),
        )
        return job_id
</file>

<file path="backend/pipeline/step_01_discovery/keyword_discovery/expander.py">
# pipeline/step_01_discovery/keyword_discovery/expander.py
import logging
from typing import List, Dict, Any, Optional

from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class NewKeywordExpander:
    def __init__(
        self,
        client: DataForSEOClientV2,
        config: Dict[str, Any],
        logger: Optional[logging.Logger] = None,
    ):
        self.client = client
        self.config = config
        self.logger = logger or logging.getLogger(self.__class__.__name__)

    def expand(
        self,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        existing_keywords: set,
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = False,
    ) -> Dict[str, Any]:
        if not discovery_modes:
            raise ValueError("At least one discovery mode must be selected.")

        # Filter out seed keywords that already exist
        original_seed_count = len(seed_keywords)
        seed_keywords = [
            kw for kw in seed_keywords if kw.lower() not in existing_keywords
        ]
        if not seed_keywords:
            self.logger.info(
                "All seed keywords already exist in the database. Skipping expansion."
            )
            return {
                "total_cost": 0.0,
                "raw_counts": {},
                "total_raw_count": 0,
                "total_unique_count": 0,
                "final_keywords": [],
            }
        self.logger.info(
            f"Filtered seed keywords from {original_seed_count} to {len(seed_keywords)}."
        )

        location_code = self.config.get("location_code")
        language_code = self.config.get("language_code")
        if not location_code or not language_code:
            raise ValueError("Location and language codes must be set.")

        # The frontend provides filters with 'keyword_data.' prefix, suitable for 'related_keywords'.
        # We need to create versions of these filters without the prefix for other modes.

        related_filters = filters
        ideas_filters = []
        if filters:
            for f in filters:
                new_filter = f.copy()
                if "field" in new_filter and new_filter["field"].startswith(
                    "keyword_data."
                ):
                    new_filter["field"] = new_filter["field"][len("keyword_data.") :]
                ideas_filters.append(new_filter)

        # Suggestions filters are the same as ideas filters (no prefix)
        suggestions_filters = ideas_filters

        structured_filters = {
            "ideas": ideas_filters,
            "suggestions": suggestions_filters,
            "related": related_filters,
        }

        # W21 FIX: Set default order_by if not provided, now structured as a dict
        if not order_by:
            ideas_suggestions_orderby = [
                "keyword_info.search_volume,desc",
            ]
            related_orderby = [
                "keyword_data.keyword_info.search_volume,desc",
            ]
            structured_orderby = {
                "ideas": ideas_suggestions_orderby,
                "suggestions": ideas_suggestions_orderby,
                "related": related_orderby,
            }
        else:
            # If order_by is provided from frontend, structure it for each mode
            related_orderby = [f"keyword_data.{rule}" for rule in order_by]
            structured_orderby = {
                "ideas": order_by,
                "suggestions": order_by,
                "related": related_orderby,
            }

        # Make a single burst call to the DataForSEOClientV2
        all_ideas, total_cost = self.client.get_keyword_ideas(
            seed_keywords=seed_keywords,
            location_code=location_code,
            language_code=language_code,
            client_cfg=self.config,
            discovery_modes=discovery_modes,
            filters=structured_filters,  # Use the structured filters directly
            order_by=structured_orderby,
            limit=limit,
            depth=depth,
            ignore_synonyms_override=ignore_synonyms,
        )
        self.logger.info(
            f"Burst discovery completed. Found {len(all_ideas)} raw keyword ideas. Cost: ${total_cost:.4f}"
        )

        # Filter out any duplicates and existing keywords from the burst results
        final_keywords_deduplicated = []
        seen_keywords = set(
            existing_keywords
        )  # Start with already existing to prevent re-adding

        # Recalculate raw counts per source based on `discovery_source` field added by get_keyword_ideas
        raw_counts = {"keyword_ideas": 0, "suggestions": 0, "related": 0}
        for item in all_ideas:
            kw_text = item.get("keyword", "").lower()
            if kw_text and kw_text not in seen_keywords:
                final_keywords_deduplicated.append(item)
                seen_keywords.add(kw_text)
                source = item.get("discovery_source")
                if source in raw_counts:
                    raw_counts[source] += 1
            elif kw_text:
                self.logger.debug(
                    f"Skipping duplicate or existing keyword: {item.get('keyword')}"
                )

        self.logger.info(
            f"Total unique new keywords after deduplication: {len(final_keywords_deduplicated)}"
        )

        return {
            "total_cost": total_cost,
            "raw_counts": raw_counts,
            "total_raw_count": len(all_ideas),  # Total raw from API before processing
            "total_unique_count": len(final_keywords_deduplicated),
            "final_keywords": final_keywords_deduplicated,
        }
</file>

<file path="backend/pipeline/step_01_discovery/keyword_discovery/filters.py">
# pipeline/step_01_discovery/keyword_discovery/filters.py
import json
import logging
from typing import List, Any, Tuple, Dict

logger = logging.getLogger(__name__)

FORBIDDEN_API_FILTER_FIELDS = [
    "relevance",
    "sv_bing",
    "sv_clickstream",
]  # Define forbidden fields


def sanitize_filters_for_api(filters: List[Any]) -> List[Any]:
    """
    Removes any filters attempting to use forbidden internal metrics or data sources.
    """
    sanitized = []
    for item in filters:
        if isinstance(item, list) and len(item) >= 1 and isinstance(item[0], str):
            field_path = item[0].lower()
            if any(
                forbidden in field_path for forbidden in FORBIDDEN_API_FILTER_FIELDS
            ):
                logger.warning(
                    f"Forbidden field '{field_path}' detected in API filter. Removing it."
                )
                continue
        sanitized.append(item)
    return sanitized


def build_discovery_filters(config: Dict[str, Any]) -> Tuple[List[Any], List[Any]]:
    """
    Builds filter lists for API-side filtering for KD, SV, Competition, and Intent.
    """
    std_api_filters = []
    rel_api_filters = []

    min_sv = config.get("min_search_volume")
    if min_sv is not None:
        std_api_filters.extend([["keyword_info.search_volume", ">=", min_sv], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword_info.search_volume", ">=", min_sv], "and"]
        )

    max_kd = config.get("max_keyword_difficulty")
    if max_kd is not None:
        std_api_filters.extend(
            [["keyword_properties.keyword_difficulty", "<=", max_kd], "and"]
        )
        rel_api_filters.extend(
            [
                ["keyword_data.keyword_properties.keyword_difficulty", "<=", max_kd],
                "and",
            ]
        )

    allowed_comp_levels = config.get("allowed_competition_levels")
    if allowed_comp_levels:
        std_api_filters.extend(
            [["keyword_info.competition_level", "in", allowed_comp_levels], "and"]
        )
        rel_api_filters.extend(
            [
                [
                    "keyword_data.keyword_info.competition_level",
                    "in",
                    allowed_comp_levels,
                ],
                "and",
            ]
        )

    allowed_intents = config.get("allowed_intents")
    if config.get("enforce_intent_filter", False) and allowed_intents:
        std_api_filters.extend(
            [["search_intent_info.main_intent", "in", allowed_intents], "and"]
        )
        rel_api_filters.extend(
            [
                ["keyword_data.search_intent_info.main_intent", "in", allowed_intents],
                "and",
            ]
        )

    # NEW: Closely Variants
    closely_variants = config.get("closely_variants")
    if closely_variants is not None:
        std_api_filters.extend(
            [["closely_variants", "=", closely_variants], "and"]
        )  # This param is at top level
        # Related keywords endpoint does not have closely_variants

    # NEW: CPC Range Filters
    min_cpc_filter = config.get("min_cpc_filter")
    max_cpc_filter = config.get("max_cpc_filter")
    if min_cpc_filter is not None:
        std_api_filters.extend([["keyword_info.cpc", ">=", min_cpc_filter], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword_info.cpc", ">=", min_cpc_filter], "and"]
        )
    if max_cpc_filter is not None:
        std_api_filters.extend([["keyword_info.cpc", "<=", max_cpc_filter], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword_info.cpc", "<=", max_cpc_filter], "and"]
        )

    # NEW: Competition Range Filters
    min_competition = config.get("min_competition")
    max_competition = config.get("max_competition")
    if min_competition is not None:
        std_api_filters.extend(
            [["keyword_info.competition", ">=", min_competition], "and"]
        )
        rel_api_filters.extend(
            [["keyword_data.keyword_info.competition", ">=", min_competition], "and"]
        )
    if max_competition is not None:
        std_api_filters.extend(
            [["keyword_info.competition", "<=", max_competition], "and"]
        )
        rel_api_filters.extend(
            [["keyword_data.keyword_info.competition", "<=", max_competition], "and"]
        )

    # NEW: Max Competition Level Filter
    max_competition_level = config.get("max_competition_level")
    if max_competition_level:
        levels = ["LOW", "MEDIUM", "HIGH"]
        allowed_levels = levels[: levels.index(max_competition_level) + 1]
        std_api_filters.extend(
            [["keyword_info.competition_level", "in", allowed_levels], "and"]
        )
        rel_api_filters.extend(
            [
                ["keyword_data.keyword_info.competition_level", "in", allowed_levels],
                "and",
            ]
        )

    # NEW: Regex Filter (from Task 34)
    search_phrase_regex = config.get("search_phrase_regex")
    if search_phrase_regex and search_phrase_regex.strip():
        std_api_filters.extend([["keyword", "regex", search_phrase_regex], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword", "regex", search_phrase_regex], "and"]
        )

    if std_api_filters:
        std_api_filters.pop()
    if rel_api_filters:
        rel_api_filters.pop()

    # Apply sanitation (Weakness 3.7 Fix)
    std_api_filters = sanitize_filters_for_api(std_api_filters)
    rel_api_filters = sanitize_filters_for_api(rel_api_filters)

    logger.info(f"Built standard API filters: {json.dumps(std_api_filters)}")
    logger.info(f"Built related API filters: {json.dumps(rel_api_filters)}")

    return std_api_filters, rel_api_filters
</file>

<file path="backend/pipeline/step_01_discovery/__init__.py">
# pipeline/step_01_discovery/__init__.py
</file>

<file path="backend/pipeline/step_01_discovery/blog_content_qualifier.py">
# pipeline/step_01_discovery/blog_content_qualifier.py
from typing import Dict, Any, Tuple
from .disqualification_rules import apply_disqualification_rules


def assign_status_from_score(
    opportunity: Dict[str, Any], score: float, client_cfg: Dict[str, Any]
) -> Tuple[str, str]:
    """
    Assigns a final status to a keyword based on its score and hard disqualification rules.
    """
    # First, check for hard-stop, non-negotiable disqualification rules.
    is_disqualified, reason, is_hard_stop = apply_disqualification_rules(
        opportunity, client_cfg, cannibalization_checker=None
    )

    if is_disqualified and is_hard_stop:
        return "rejected", reason

    # If not hard-stopped, categorize based on the strategic score.
    if score >= client_cfg.get("qualified_threshold", 70):
        return "qualified", "Qualified: High strategic score."
    elif score >= client_cfg.get("review_threshold", 50):
        return "review", "Review: Moderate strategic score."
    else:
        return "rejected", f"Rejected: Low strategic score ({score:.1f})."
</file>

<file path="backend/pipeline/step_01_discovery/cannibalization_checker.py">
import logging
from typing import List, Dict, Any
from urllib.parse import urlparse

from backend.data_access.database_manager import DatabaseManager


class CannibalizationChecker:
    def __init__(
        self,
        target_domain: str,
        dataforseo_client: Any,
        client_cfg: Dict[str, Any],
        db_manager: DatabaseManager,
    ):
        self.target_domain = (
            target_domain.lower().replace("www.", "") if target_domain else None
        )
        self.logger = logging.getLogger(self.__class__.__name__)
        self.dataforseo_client = dataforseo_client
        self.client_cfg = client_cfg
        self.db_manager = db_manager

    def is_url_in_serp(
        self, serp_results: List[Dict[str, Any]], keyword: str, client_id: str
    ) -> bool:
        """
        Returns True if the target domain is found in the list of SERP results
        OR if the keyword already exists in the opportunities database for the client.
        """
        if self.db_manager.check_existing_keywords(client_id, [keyword]):
            self.logger.warning(
                f"Cannibalization detected: Keyword '{keyword}' already exists in the database for client '{client_id}'."
            )
            return True

        if not self.target_domain:
            return False

        for result in serp_results:
            try:
                url = result.get("url")
                if not url:
                    continue
                url_domain = urlparse(url).netloc.lower().replace("www.", "")
                if url_domain == self.target_domain or url_domain.endswith(
                    f".{self.target_domain}"
                ):
                    self.logger.warning(
                        f"Cannibalization detected: Found '{url}' in SERP for '{keyword}'."
                    )
                    return True
            except Exception:
                continue
        return False
</file>

<file path="backend/pipeline/step_01_discovery/disqualification_rules.py">
# pipeline/step_01_discovery/disqualification_rules.py
import logging
import re
from typing import Dict, Any, Tuple, Optional
from datetime import datetime
import numpy as np
from core import utils

from .cannibalization_checker import CannibalizationChecker


def apply_disqualification_rules(
    opportunity: Dict[str, Any],
    client_cfg: Dict[str, Any],
    cannibalization_checker: CannibalizationChecker,
) -> Tuple[bool, Optional[str], bool]:
    """
    Applies the comprehensive 20-rule set to disqualify a keyword based on data from the discovery phase.
    Reads all thresholds from client_cfg.
    Returns (is_disqualified, reason, is_hard_stop).
    """
    keyword = opportunity.get("keyword", "Unknown Keyword")

    # --- Failsafe Validation ---
    required_keys = [
        "keyword_info",
        "keyword_properties",
        "serp_info",
        "search_intent_info",
    ]
    for key in required_keys:
        if key not in opportunity or opportunity[key] is None:
            logging.getLogger(__name__).warning(
                f"Disqualifying '{keyword}' due to missing or null '{key}' data."
            )
            return True, f"Rule 1: Missing critical data structure ({key}).", True

    serp_info = opportunity.get("serp_info", {})
    if not serp_info:
        logging.getLogger(__name__).warning(
            f"Disqualifying '{keyword}' due to empty 'serp_info' data."
        )
        return True, "Rule 1: Missing SERP info data.", True

    keyword_info = opportunity.get("keyword_info") or {}
    keyword_props = opportunity.get("keyword_properties") or {}
    avg_backlinks = opportunity.get("avg_backlinks_info") or {}
    intent_info = opportunity.get("search_intent_info") or {}

    # Tier 1: Foundational Checks
    if not all([keyword_info, keyword_props, intent_info]):
        return (
            True,
            "Rule 1: Missing critical data structures (keyword_info, keyword_properties, or search_intent_info).",
            True,
        )

    # Rule 2: Check primary intent
    allowed_intents = client_cfg.get("allowed_intents", ["informational"])
    main_intent = intent_info.get("main_intent")
    foreign_intents = intent_info.get("foreign_intent", [])

    if main_intent not in allowed_intents:
        return True, f"Rule 2: Non-target main intent ('{main_intent}').", True

    # Rule 2b (NEW): Check secondary intents for prohibitive types
    prohibited_intents = set(client_cfg.get("prohibited_intents", ["navigational"]))
    foreign_intents = intent_info.get("foreign_intent", []) or []
    if not prohibited_intents.isdisjoint(set(foreign_intents)):
        offending_intents = prohibited_intents.intersection(set(foreign_intents))
        return (
            True,
            f"Rule 2b: Contains a prohibited secondary intent ({', '.join(offending_intents)}).",
            True,
        )

    if keyword_props.get("is_another_language"):
        return True, "Rule 3: Language mismatch.", True

    negative_keywords = set(
        kw.lower() for kw in client_cfg.get("negative_keywords", [])
    )
    core_keyword = keyword_props.get("core_keyword")
    if any(neg_kw in keyword.lower() for neg_kw in negative_keywords) or (
        core_keyword
        and any(neg_kw in core_keyword.lower() for neg_kw in negative_keywords)
    ):
        return True, "Rule 4: Contains a negative keyword.", True

    # Tier 2: Volume & Trend Analysis
    if utils.safe_compare(
        keyword_info.get("search_volume"), client_cfg.get("min_search_volume"), "lt"
    ):
        return (
            True,
            f"Rule 5: Below search volume floor (minimum: {client_cfg.get('min_search_volume', 100)} SV). Current: {keyword_info.get('search_volume', 0)} SV.",
            False,
        )

    trends = keyword_info.get("search_volume_trend", {})
    try:
        yearly_trend = trends.get("yearly")
        quarterly_trend = trends.get("quarterly")

        yearly_threshold = client_cfg.get("yearly_trend_decline_threshold", -25)
        quarterly_threshold = client_cfg.get("quarterly_trend_decline_threshold", 0)

        yearly_check = utils.safe_compare(yearly_trend, yearly_threshold, "lt")
        quarterly_check = utils.safe_compare(quarterly_trend, quarterly_threshold, "lt")

        if yearly_check and quarterly_check:
            return (
                True,
                f"Rule 6: Consistently declining trend. Yearly trend: {yearly_trend}% (below {yearly_threshold}% threshold), Quarterly trend: {quarterly_trend}% (below {quarterly_threshold}% threshold). Consider manual review for seasonality.",
                False,
            )
    except TypeError:
        logging.getLogger(__name__).error(
            f"TypeError during trend analysis for keyword '{keyword}'. "
            f"trends.get('yearly') value: {trends.get('yearly')}, type: {type(trends.get('yearly'))}. "
            f"trends.get('quarterly') value: {trends.get('quarterly')}, type: {type(trends.get('quarterly'))}."
        )
        return (
            True,
            "Rule 6: Failed to process trend data due to invalid format.",
            False,
        )

    monthly_searches = keyword_info.get("monthly_searches", [])
    if monthly_searches and len(monthly_searches) > 1:
        volumes = [
            ms["search_volume"]
            for ms in monthly_searches
            if ms.get("search_volume") is not None and ms["search_volume"] > 0
        ]
        if len(volumes) > 1 and np.mean(volumes) > 0:
            volatility_threshold = client_cfg.get(
                "search_volume_volatility_threshold", 1.5
            )
            std_dev_to_mean_ratio = np.std(volumes) / np.mean(volumes)
            if std_dev_to_mean_ratio > volatility_threshold:
                return (
                    True,
                    f"Rule 7: Extreme search volume volatility. Std Dev / Mean ratio: {std_dev_to_mean_ratio:.2f} (above {volatility_threshold} threshold). Could indicate a fleeting trend or strong seasonality. Manual review recommended.",
                    False,
                )

    # Rule 7b: Check for recent sharp decline using raw monthly searches
    monthly_searches = opportunity.get(
        "monthly_searches", []
    )  # Get from opportunity object, which is deserialized
    if monthly_searches and len(monthly_searches) >= 4:
        # Sort by year and month to ensure correctness (most recent first for trend)
        try:
            sorted_searches = sorted(
                monthly_searches, key=lambda x: (x["year"], x["month"]), reverse=True
            )
            if len(sorted_searches) >= 4:
                # Compare latest month with 3 months prior (index 0 vs index 3)
                latest_vol = sorted_searches[0].get("search_volume")
                past_vol = sorted_searches[3].get("search_volume")

                if latest_vol is not None and past_vol is not None and past_vol > 0:
                    if (
                        latest_vol / past_vol
                    ) < 0.5:  # If volume has dropped by more than 50% in 3 months
                        return (
                            True,
                            "Rule 7b: Recent sharp decline in search volume (>50% drop in last 3 months).",
                            False,
                        )
        except (TypeError, KeyError):
            logging.getLogger(__name__).warning(
                f"Could not parse monthly_searches for recent trend analysis on keyword '{keyword}'."
            )

    # Tier 3: Commercial & Competitive Analysis
    if utils.safe_compare(
        keyword_info.get("competition"),
        client_cfg.get("max_paid_competition_score", 0.8),
        "gt",
    ) and (keyword_info.get("competition_level") == "HIGH"):
        return True, "Rule 8: Excessive paid competition.", False

    if utils.safe_compare(
        keyword_info.get("high_top_of_page_bid"),
        client_cfg.get("max_high_top_of_page_bid", 15.0),
        "gt",
    ):
        return (
            True,
            f"Rule 9: Prohibitively high CPC bids (${client_cfg.get('max_high_top_of_page_bid', 15.00)}).",
            False,
        )

    if utils.safe_compare(
        keyword_props.get("keyword_difficulty"),
        client_cfg.get("max_kd_hard_limit", 70),
        "gt",
    ):
        return (
            True,
            f"Rule 10: Extreme keyword difficulty (>{client_cfg.get('max_kd_hard_limit', 70)}).",
            False,
        )

    if utils.safe_compare(
        avg_backlinks.get("referring_main_domains"),
        client_cfg.get("max_referring_main_domains_limit", 100),
        "gt",
    ):
        return (
            True,
            f"Rule 11: Overly authoritative competitor domains (>{client_cfg.get('max_referring_main_domains_limit', 100)} referring main domains).",
            False,
        )

    if utils.safe_compare(
        avg_backlinks.get("main_domain_rank"),
        client_cfg.get("max_avg_domain_rank_threshold", 500),
        "lt",
    ):
        return (
            True,
            f"Rule 12: SERP dominated by high-authority domains (avg rank < {client_cfg.get('max_avg_domain_rank_threshold', 500)}).",
            False,
        )

    if (avg_backlinks.get("referring_domains") or 0) > 0:
        pages_to_domain_ratio = (avg_backlinks.get("referring_pages") or 0) / (
            avg_backlinks.get("referring_domains") or 1
        )
        if pages_to_domain_ratio > client_cfg.get("max_pages_to_domain_ratio", 15):
            return (
                True,
                "Rule 13: Potential spammy competitor profile (high page/domain ratio).",
                False,
            )

    # Tier 4: Content, SERP & Keyword Structure

    # Rule: Check for hostile SERP environment
    is_hostile, hostile_reason = _check_hostile_serp_environment(opportunity)
    if is_hostile:
        return True, hostile_reason, True

    non_evergreen_pattern = _get_non_evergreen_year_pattern()
    if non_evergreen_pattern and re.search(non_evergreen_pattern, keyword):
        return (
            True,
            "Rule 14: Non-evergreen temporal keyword (matches pattern for past/current years).",
            False,
        )

    word_count = len(keyword.split())
    is_question = utils.is_question_keyword(keyword)  # This now exists

    min_wc = client_cfg.get("min_keyword_word_count", 2)
    max_wc = client_cfg.get("max_keyword_word_count", 8)

    is_outside_range = word_count < min_wc or word_count > max_wc

    # Rule 15 (Refined with override): Check word count and potentially override for high-value keywords
    if is_outside_range and not is_question:
        sv = keyword_info.get("search_volume", 0)
        cpc = keyword_info.get("cpc")  # Get the value, which could be None
        if cpc is None:
            cpc = 0.0  # Default to 0.0 if it's None

        high_sv_override = client_cfg.get("high_value_sv_override_threshold", 10000)
        high_cpc_override = client_cfg.get("high_value_cpc_override_threshold", 5.0)

        if sv >= high_sv_override or cpc >= high_cpc_override:
            logging.getLogger(__name__).info(
                f"Override: High value SV/CPC bypasses word count rule for '{keyword}'."
            )
            pass  # Allow the keyword to proceed
        else:
            return (
                True,
                f"Rule 15: Non-question keyword word count ({word_count}) is outside the acceptable range ({min_wc}-{max_wc} words).",
                False,
            )

    serp_info = opportunity.get("serp_info", {})
    serp_types = set(serp_info.get("serp_item_types", []))

    crowded_features = {
        "video",
        "images",
        "people_also_ask",
        "carousel",
        "featured_snippet",
        "short_videos",
    }
    if len(serp_types.intersection(crowded_features)) > client_cfg.get(
        "crowded_serp_features_threshold", 4
    ):
        return (
            True,
            f"Rule 17: SERP is overly crowded (>{client_cfg.get('crowded_serp_features_threshold', 4)} attention-grabbing features).",
            False,
        )

    # Rule 18: Check for navigational intent safely
    is_navigational = False
    if intent_info:
        if intent_info.get("main_intent") == "navigational":
            is_navigational = True
        else:
            foreign_intent = intent_info.get("foreign_intent")
            if foreign_intent and "navigational" in foreign_intent:
                is_navigational = True
    if is_navigational:
        return True, "Rule 18: Strong navigational intent.", True

    if serp_info.get("last_updated_time") and serp_info.get("previous_updated_time"):
        try:
            last_update = datetime.fromisoformat(
                serp_info["last_updated_time"].replace(" +00:00", "")
            )
            prev_update = datetime.fromisoformat(
                serp_info["previous_updated_time"].replace(" +00:00", "")
            )
            days_between_updates = (last_update - prev_update).days
            if days_between_updates < client_cfg.get("min_serp_stability_days", 14):
                return (
                    True,
                    f"Rule 19: Unstable SERP (updated every {days_between_updates} days).",
                    False,
                )
        except ValueError:
            logging.getLogger(__name__).warning(
                f"Could not parse SERP update times for '{keyword}': {serp_info.get('last_updated_time')}, {serp_info.get('previous_updated_time')}"
            )

    cpc_value = keyword_info.get("cpc")
    if cpc_value is None:
        cpc_value = 0.0
    if (
        intent_info.get("main_intent") in ["commercial", "transactional"]
        and cpc_value == 0
    ):
        return True, "Rule 20: Low-value commercial intent (zero CPC).", False

    return False, None, False


def _check_hostile_serp_environment(
    opportunity: Dict[str, Any],
) -> Tuple[bool, Optional[str]]:
    """
    Rule 16: Disqualifies keywords where the SERP is dominated by features hostile to blog content.
    """
    serp_info = opportunity.get("serp_info", {})
    if not serp_info:
        return False, None  # Cannot analyze if SERP info is missing

    serp_types = set(serp_info.get("serp_item_types", []))

    # Define hostile features based on detailed SERP analysis
    HOSTILE_FEATURES = {
        # Strong transactional/e-commerce intent
        "shopping",
        "popular_products",
        "refine_products",
        "explore_brands",
        # Strong local intent
        "local_pack",
        "map",
        "local_services",
        # Purely transactional/utility intent (Google-owned tools)
        "google_flights",
        "google_hotels",
        "hotels_pack",
        # App-related intent
        "app",
        # Job-seeking intent
        "jobs",
        # Direct utility/tool intent
        "math_solver",
        "currency_box",
        "stocks_box",
    }

    found_hostile_features = serp_types.intersection(HOSTILE_FEATURES)

    if found_hostile_features:
        return (
            True,
            f"Rule 16: SERP is hostile to blog content. Contains dominant non-article features: {', '.join(found_hostile_features)}.",
        )

    return False, None


def _get_non_evergreen_year_pattern() -> str:
    """
    Generates a regex pattern to find past years up to the current year,
    dynamically adjusting to avoid disqualifying valid keywords in the future.
    Example for current year 2024: \b(201\d|202[0-4])\b
    """
    current_year = datetime.now().year

    patterns = []
    # Handle decades before the current one (e.g., 2010s)
    for decade_start in range(2010, (current_year // 10) * 10, 10):
        patterns.append(
            f"{decade_start}|{decade_start + 1}|{decade_start + 2}|{decade_start + 3}|{decade_start + 4}|{decade_start + 5}|{decade_start + 6}|{decade_start + 7}|{decade_start + 8}|{decade_start + 9}"
        )

    # Handle years in the current decade up to the current year
    current_decade_start_year = (current_year // 10) * 10
    current_decade_years = [
        str(year) for year in range(current_decade_start_year, current_year + 1)
    ]
    if current_decade_years:
        patterns.append("|".join(current_decade_years))

    if not patterns:
        return ""  # Should not happen unless current_year is before 2010

    return r"\b(" + "|".join(patterns) + r")\b"
</file>

<file path="backend/pipeline/step_01_discovery/keyword_expander.py">
# pipeline/step_01_discovery/keyword_expander.py
import logging
from typing import List, Dict, Any, Optional
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from .keyword_discovery.expander import NewKeywordExpander


class KeywordExpander:
    """
    A wrapper class that uses the new modular keyword expansion system.
    """

    def __init__(
        self,
        client: DataForSEOClientV2,
        config: Dict[str, Any],
        run_logger: Optional[logging.Logger] = None,
    ):
        self.client = client
        self.config = config
        self.logger = run_logger or logging.getLogger(self.__class__.__name__)
        self.expander = NewKeywordExpander(client, config, self.logger)

    def expand_seed_keyword(
        self,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        existing_keywords: set,
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = False,
    ) -> Dict[str, Any]:
        """
        Delegates the keyword expansion to the new NewKeywordExpander.
        """
        self.logger.info(
            f"Starting keyword expansion with {len(seed_keywords)} seeds and modes: {discovery_modes}"
        )

        results = self.expander.expand(
            seed_keywords,
            discovery_modes,
            filters,
            order_by,
            existing_keywords,
            limit,
            depth,
            ignore_synonyms,
        )

        self.logger.info(
            f"Keyword expansion complete. Found {results['total_unique_count']} unique keywords."
        )

        return results
</file>

<file path="backend/pipeline/step_01_discovery/run_discovery.py">
import logging
from typing import List, Dict, Any, Optional

from data_access.database_manager import DatabaseManager
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from pipeline.step_01_discovery.keyword_expander import KeywordExpander
from pipeline.step_01_discovery.disqualification_rules import (
    apply_disqualification_rules,
)
from pipeline.step_01_discovery.cannibalization_checker import CannibalizationChecker
from pipeline.step_03_prioritization.scoring_engine import ScoringEngine
from pipeline.step_01_discovery.blog_content_qualifier import assign_status_from_score
from backend.services.serp_analysis_service import SerpAnalysisService


def run_discovery_phase(
    seed_keywords: List[str],
    dataforseo_client: DataForSEOClientV2,
    db_manager: "DatabaseManager",
    client_id: str,
    client_cfg: Dict[str, Any],
    discovery_modes: List[str],
    filters: Optional[List[Any]],
    order_by: Optional[List[str]],
    limit: Optional[int] = None,
    depth: Optional[int] = None,
    ignore_synonyms: Optional[bool] = False,
    include_clickstream_data: Optional[bool] = None,
    closely_variants: Optional[bool] = None,
    run_logger: Optional[logging.Logger] = None,
) -> Dict[str, Any]:
    logger = run_logger or logging.getLogger(__name__)
    logger.info("--- Starting Consolidated Keyword Discovery & Scoring Phase ---")

    expander = KeywordExpander(dataforseo_client, client_cfg, logger)
    cannibalization_checker = CannibalizationChecker(
        client_cfg.get("target_domain"), dataforseo_client, client_cfg, db_manager
    )
    scoring_engine = ScoringEngine(client_cfg)

    # 1. Get keywords that already exist for this client to avoid API calls for them.
    existing_keywords = set(db_manager.get_all_processed_keywords_for_client(client_id))
    logger.info(
        f"Found {len(existing_keywords)} existing keywords to exclude from API request."
    )

    # 2. Expand seed keywords into a large list of opportunities.
    expansion_result = expander.expand_seed_keyword(
        seed_keywords,
        discovery_modes,
        filters,
        order_by,
        existing_keywords,
        limit,
        depth,
        ignore_synonyms,
    )

    all_expanded_keywords = expansion_result.get("final_keywords", [])
    total_cost = expansion_result.get("total_cost", 0.0)

    # --- Scoring and Disqualification Loop (Consolidated Logic) ---
    processed_opportunities = []
    disqualification_reasons = {}
    status_counts = {"qualified": 0, "review": 0, "rejected": 0}
    required_keys = [
        "keyword_info",
        "keyword_properties",
        "serp_info",
        "search_intent_info",
    ]

    for opp in all_expanded_keywords:
        # Pre-validation of opportunity structure
        missing_keys = [
            key for key in required_keys if key not in opp or opp[key] is None
        ]
        if missing_keys:
            logger.warning(
                f"Skipping opportunity '{opp.get('keyword')}' due to missing required data: {', '.join(missing_keys)}"
            )
            continue

        # 3. Apply Hard Disqualification Rules (Cannibalization, Negative Keywords, etc.)
        is_disqualified, reason, is_hard_stop = apply_disqualification_rules(
            opp, client_cfg, cannibalization_checker
        )

        if is_disqualified and is_hard_stop:
            opp["status"] = "rejected"
            opp["blog_qualification_status"] = "rejected"
            opp["blog_qualification_reason"] = reason
            status_counts["rejected"] += 1
            disqualification_reasons[reason] = (
                disqualification_reasons.get(reason, 0) + 1
            )
        else:
            # 4. Score the remaining keywords
            score, breakdown = scoring_engine.calculate_score(opp)
            opp["strategic_score"] = score
            opp["score_breakdown"] = breakdown

            # 5. Assign Status based on Strategic Score
            status, reason = assign_status_from_score(opp, score, client_cfg)
            opp["status"] = status
            opp["blog_qualification_status"] = status
            opp["blog_qualification_reason"] = reason
            status_counts[status.split("_")[0]] = (
                status_counts.get(status.split("_")[0], 0) + 1
            )  # count qualified/review/rejected

        processed_opportunities.append(opp)

    disqualified_count = status_counts.get("rejected", 0)
    passed_count = status_counts.get("qualified", 0) + status_counts.get("review", 0)

    logger.info(
        f"Scoring and Qualification complete. Passed: {passed_count}, Rejected: {disqualified_count}."
    )

    stats = {
        **expansion_result,
        "disqualification_reasons": disqualification_reasons,
        "disqualified_count": disqualified_count,
        "final_qualified_count": passed_count,
    }

    return {
        "stats": stats,
        "total_cost": total_cost,
        "opportunities": processed_opportunities,
    }
</file>

<file path="backend/pipeline/step_02_qualification/__init__.py">
# pipeline/step_02_qualification/__init__.py
</file>

<file path="backend/pipeline/step_02_qualification/competitor_analyzer.py">
import logging
from typing import List, Dict, Any, Tuple

from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class CompetitorAnalyzer:
    """
    Analyzes top organic competitors from SERP data.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.min_word_count = self.config.get("min_competitor_word_count", 300)

    def analyze_competitors(
        self, top_results: List[Dict[str, Any]]
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Fetches OnPage data for top competitors and performs a basic analysis.
        """
        if not top_results:
            return [], 0.0

        # Limit the number of competitors to analyze based on config
        num_to_analyze = self.config.get("num_competitors_to_analyze", 5)
        competitor_urls = [
            res["url"] for res in top_results[:num_to_analyze] if res.get("url")
        ]

        onpage_results, cost = self.client.get_onpage_data_for_urls(competitor_urls)

        if not onpage_results:
            return [], cost

        analyzed_competitors = []
        for result in onpage_results:
            if "error" in result:
                self.logger.warning(
                    f"Could not analyze competitor {result.get('url')}: {result.get('error')}"
                )
                continue

            content_meta = result.get("meta", {}).get("content", {})
            word_count = content_meta.get("plain_text_word_count")
            if word_count and word_count >= self.min_word_count:
                analyzed_competitors.append(
                    {
                        "url": result.get("url"),
                        "word_count": word_count,
                        "readability_score": content_meta.get(
                            "flesch_kincaid_readability_index"
                        ),
                        "onpage_score": result.get("onpage_score"),
                        "internal_links": result.get("meta", {}).get(
                            "internal_links_count"
                        ),
                        "external_links": result.get("meta", {}).get(
                            "external_links_count"
                        ),
                        "headings": result.get("meta", {}).get("htags"),
                    }
                )

        return analyzed_competitors, cost
</file>

<file path="backend/pipeline/step_02_qualification/serp_analyzer.py">
import logging
from typing import Dict, Any, Tuple, Optional

from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from datetime import datetime


class SerpAnalyzer:
    """
    Analyzes the SERP for a given keyword to extract key insights.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def analyze_serp(self, keyword: str) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Fetches SERP data and extracts insights like featured snippets, PAA, etc.
        """
        location_code = self.config.get("location_code")
        language_code = self.config.get("language_code")

        serp_results, cost = self.client.get_serp_results(
            keyword, location_code, language_code
        )

        if not serp_results:
            return None, cost

        analysis = {
            "serp_has_featured_snippet": False,
            "serp_has_video_results": False,
            "serp_has_ai_overview": False,
            "people_also_ask": [],
            "top_organic_results": [],
            "serp_last_updated_days_ago": None,
        }

        item_types = serp_results.get("item_types", [])
        if "featured_snippet" in item_types:
            analysis["serp_has_featured_snippet"] = True
        if "video" in item_types:
            analysis["serp_has_video_results"] = True
        if "ai_overview" in item_types:
            analysis["serp_has_ai_overview"] = True

        # Extract PAA and top organic results
        for item in serp_results.get("items", []):
            if item.get("type") == "people_also_ask":
                analysis["people_also_ask"] = [
                    q.get("title") for q in item.get("items", []) if q.get("title")
                ]
            elif item.get("type") == "organic":
                analysis["top_organic_results"].append(
                    {
                        "rank": item.get("rank_absolute"),
                        "url": item.get("url"),
                        "title": item.get("title"),
                        "domain": item.get("domain"),
                        "main_domain_rank": item.get(
                            "main_domain_rank", 1000
                        ),  # Default to low rank
                    }
                )

        # Calculate SERP freshness
        datetime_str = serp_results.get("datetime")
        if datetime_str:
            try:
                serp_date = datetime.strptime(datetime_str, "%Y-%m-%d %H:%M:%S +00:00")
                analysis["serp_last_updated_days_ago"] = (
                    datetime.utcnow() - serp_date
                ).days
            except ValueError:
                self.logger.warning(f"Could not parse SERP datetime: {datetime_str}")

        return analysis, cost
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/__init__.py">
# pipeline/step_03_prioritization/scoring_components/__init__.py
from .ease_of_ranking import calculate_ease_of_ranking_score
from .traffic_potential import calculate_traffic_potential_score
from .commercial_intent import calculate_commercial_intent_score
from .growth_trend import calculate_growth_trend_score
from .serp_features import calculate_serp_features_score
from .serp_volatility import calculate_serp_volatility_score
from .competitor_weakness import calculate_competitor_weakness_score
from .serp_crowding import calculate_serp_crowding_score
from .keyword_structure import calculate_keyword_structure_score
from .serp_threat import calculate_serp_threat_score
from .volume_volatility import calculate_volume_volatility_score
from .serp_freshness import calculate_serp_freshness_score
from .competitor_performance import calculate_competitor_performance_score

__all__ = [
    "calculate_ease_of_ranking_score",
    "calculate_traffic_potential_score",
    "calculate_commercial_intent_score",
    "calculate_growth_trend_score",
    "calculate_serp_features_score",
    "calculate_serp_volatility_score",
    "calculate_competitor_weakness_score",
    "calculate_serp_crowding_score",
    "calculate_keyword_structure_score",
    "calculate_serp_threat_score",
    "calculate_volume_volatility_score",
    "calculate_serp_freshness_score",
    "calculate_competitor_performance_score",  # ADDED THIS LINE
]
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/commercial_intent.py">
# pipeline/step_03_prioritization/scoring_components/commercial_intent.py
from typing import Dict, Any, Tuple
from backend.core import utils  # NEW: Import the utils module


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_commercial_intent_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the keyword's strategic value for blog content,
    balancing commercial indicators with the type of user intent.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    intent_info = (
        data.get("search_intent_info")
        if isinstance(data.get("search_intent_info"), dict)
        else {}
    )
    keyword = data.get("keyword", "")

    cpc = keyword_info.get("cpc", 0.0)
    if cpc is None:
        cpc = 0.0
    max_cpc = config.get("max_cpc_for_scoring", 10.0)
    cpc_score = _normalize_value(cpc, max_cpc)

    # Add bonus for wide CPC bid spread, indicating market inefficiency
    low_bid = keyword_info.get("low_top_of_page_bid", 0.0) or 0.0
    high_bid = keyword_info.get("high_top_of_page_bid", 0.0) or 0.0
    if low_bid > 0 and high_bid > low_bid:
        bid_spread_ratio = high_bid / low_bid
        if bid_spread_ratio > 5:  # e.g., low is $1, high is >$5
            cpc_score = min(100, cpc_score + 15)

    main_intent = intent_info.get("main_intent", "informational")
    foreign_intents = intent_info.get("foreign_intent", []) or []

    intent_scores = {
        "informational": 75,
        "commercial": 60,
        "transactional": 10,
        "navigational": 0,
    }
    intent_score = intent_scores.get(main_intent, 75)
    explanation = f"Base score for '{main_intent}' intent is {intent_score}."

    if main_intent == "informational" and (
        "commercial" in foreign_intents or "transactional" in foreign_intents
    ):
        intent_score = min(100, intent_score + 25)
        explanation += " Bonus for commercial secondary intent."

    # REPLACED: Use the centralized utility function
    if utils.is_question_keyword(keyword):
        intent_score = min(100, intent_score + 15)
        explanation += " Bonus for being a question keyword."

    competition_level = keyword_info.get("competition_level")
    if competition_level == "LOW":
        cpc_score = min(100, cpc_score + 20)

    final_score = (cpc_score * 0.5) + (intent_score * 0.5)
    breakdown = {
        "CPC & Competition": {
            "value": f"${cpc:.2f} ({competition_level})",
            "score": round(cpc_score),
            "explanation": "Normalized CPC, with bonuses for low competition and wide bid spread.",
        },
        "Strategic Intent": {
            "value": main_intent.title(),
            "score": round(intent_score),
            "explanation": explanation,
        },
    }
    return round(final_score), breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/competitor_performance.py">
from typing import Dict, Any, Tuple
import logging

logger = logging.getLogger(__name__)


def _normalize_value(
    value: float, target_value: float, is_lower_better: bool = True
) -> float:
    """Helper to normalize a value to a 0-100 scale, with target_value being ideal."""
    if value is None or target_value is None or target_value == 0:
        return 50.0  # Neutral score if data is missing or target is zero

    if is_lower_better:
        # Example: LCP target 2500ms.
        # If value is 1250, score = 100. If value is 5000, score = 0.
        # This formula provides 100 at 0, 50 at target, 0 at 2*target
        score = max(0.0, min(100.0, 100 * (1 - (value / (2 * target_value)))))
    else:
        # Example: High metric, higher is better. e.g. High security score
        score = max(
            0.0, min(100.0, 100 * (value / (2 * target_value)))
        )  # Max out at 2*target for 100, linear

    return score


def calculate_competitor_performance_score(
    opportunity: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the technical performance (e.g., Core Web Vitals)
    of top organic competitors. Weak competitor performance indicates a higher opportunity.
    """
    if not isinstance(opportunity, dict) or not opportunity.get("blueprint"):
        return 50.0, {"message": "Invalid opportunity data for scoring."}

    competitor_analysis = opportunity["blueprint"].get("competitor_analysis", [])
    if not competitor_analysis:
        return 50.0, {
            "message": "No competitor analysis available for performance scoring."
        }

    lcp_times = []
    for comp in competitor_analysis:
        if (
            comp.get("page_timing")
            and comp["page_timing"].get("largest_contentful_paint") is not None
        ):
            lcp_times.append(comp["page_timing"]["largest_contentful_paint"])

    if not lcp_times:
        return 50.0, {"message": "No LCP data available from competitors."}

    avg_lcp_ms = sum(lcp_times) / len(lcp_times)

    # Get the target LCP from client config (lower is better for performance)
    # This target defines what "good" performance is. Competitors worse than this are an opportunity.
    target_good_lcp_ms = config.get(
        "max_avg_lcp_time", 2500
    )  # Default to 2.5s as a good target

    # Score: higher if competitors' LCP is high (poor performance)
    # We want to invert the normalization: a higher LCP (worse performance) means higher score (better opportunity)
    # If avg_lcp_ms is 2*target_good_lcp_ms, score is 100. If it's target_good_lcp_ms, score is 50.
    score = _normalize_value(avg_lcp_ms, target_good_lcp_ms, is_lower_better=False)

    explanation = f"Average competitor LCP is {avg_lcp_ms:.0f}ms. Higher value indicates worse competitor performance, which is a better opportunity. Target LCP for good performance is {target_good_lcp_ms}ms."
    breakdown = {
        "Avg. Competitor LCP": {
            "value": f"{avg_lcp_ms:.0f}ms",
            "score": round(score),
            "explanation": explanation,
        }
    }

    return round(score), breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/competitor_weakness.py">
# pipeline/step_03_prioritization/scoring_components/competitor_weakness.py
from typing import Dict, Any, Tuple


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_competitor_weakness_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the authority of ranking competitors using
    data available at the discovery phase (avg_backlinks_info).
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    breakdown = {}
    avg_backlinks = (
        data.get("avg_backlinks_info")
        if isinstance(data.get("avg_backlinks_info"), dict)
        else {}
    )

    # 1. Average Main Domain Rank of competitors
    domain_rank = avg_backlinks.get("main_domain_rank", 500)
    max_rank = config.get("max_domain_rank_for_scoring", 1000)
    domain_rank_score = _normalize_value(domain_rank, max_rank, invert=True)
    breakdown["Avg. Domain Rank"] = {
        "value": f"{domain_rank:.0f}",
        "score": round(domain_rank_score),
        "explanation": f"Normalized against a max of {max_rank}. Lower is better.",
    }

    # 2. Average Referring Main Domains
    ref_domains = avg_backlinks.get("referring_main_domains", 50)
    max_ref_domains = config.get("max_referring_domains_for_scoring", 100)
    ref_domains_score = _normalize_value(ref_domains, max_ref_domains, invert=True)
    breakdown["Avg. Referring Domains"] = {
        "value": f"{ref_domains:.1f}",
        "score": round(ref_domains_score),
        "explanation": f"Normalized against a max of {max_ref_domains}. Lower is better.",
    }

    # Weighted average for final score
    final_score = (domain_rank_score * 0.6) + (ref_domains_score * 0.4)
    return round(final_score), breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/ease_of_ranking.py">
# pipeline/step_03_prioritization/scoring_components/ease_of_ranking.py
import math
from typing import Dict, Any, Tuple


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_ease_of_ranking_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """Calculates a score based on how easy it is to rank for the keyword."""
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    breakdown = {}
    keyword_props = (
        data.get("keyword_properties")
        if isinstance(data.get("keyword_properties"), dict)
        else {}
    )
    avg_backlinks = (
        data.get("avg_backlinks_info")
        if isinstance(data.get("avg_backlinks_info"), dict)
        else {}
    )
    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}

    # 1. Keyword Difficulty (KD)
    kd = keyword_props.get("keyword_difficulty", 50)
    kd_score = _normalize_value(kd, 100, invert=True)
    breakdown["Keyword Difficulty"] = {
        "value": kd,
        "score": round(kd_score),
        "explanation": "Lower is better.",
    }

    # 2. Average Main Domain Rank of competitors
    domain_rank = avg_backlinks.get("main_domain_rank", 500)
    max_rank = config.get("max_domain_rank_for_scoring", 1000)
    domain_rank_score = _normalize_value(domain_rank, max_rank, invert=True)
    breakdown["Avg. Domain Rank"] = {
        "value": f"{domain_rank:.0f}",
        "score": round(domain_rank_score),
        "explanation": f"Normalized against a max of {max_rank}. Lower is better.",
    }

    # 3. Average Page Rank of top competing pages
    page_rank = avg_backlinks.get("rank", 50)
    page_rank_score = _normalize_value(page_rank, 100, invert=True)
    breakdown["Avg. Page Rank"] = {
        "value": f"{page_rank:.0f}",
        "score": round(page_rank_score),
        "explanation": "Represents page-level authority. Lower is better.",
    }

    # 4. Dofollow Ratio
    total_backlinks = avg_backlinks.get("backlinks", 0)
    dofollow_backlinks = avg_backlinks.get("dofollow", 0)
    dofollow_ratio = dofollow_backlinks / total_backlinks if total_backlinks > 0 else 0
    dofollow_score = _normalize_value(
        dofollow_ratio, 1, invert=True
    )  # Lower ratio is better
    breakdown["Dofollow Ratio"] = {
        "value": f"{dofollow_ratio:.1%}",
        "score": round(dofollow_score),
        "explanation": "Ratio of dofollow links. A lower ratio indicates weaker competitor backlink profiles.",
    }

    # 5. Search Engine Results Count
    results_count = serp_info.get("se_results_count", 1_000_000)
    if results_count > 0:
        log_score = _normalize_value(
            math.log(results_count + 1), math.log(1_000_000_000 + 1), invert=True
        )
    else:
        log_score = 100.0
    breakdown["Total Results"] = {
        "value": f"{results_count:,}",
        "score": round(log_score),
        "explanation": "Log-normalized. Fewer competing pages is better.",
    }

    # Weighted average for final score
    final_score = (
        (kd_score * 0.40)
        + (domain_rank_score * 0.25)
        + (page_rank_score * 0.20)
        + (dofollow_score * 0.10)
        + (log_score * 0.05)
    )
    return round(final_score), breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/growth_trend.py">
# pipeline/step_03_prioritization/scoring_components/growth_trend.py
from typing import Dict, Any, Tuple
import math


def calculate_growth_trend_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a volume-weighted score based on the keyword's search volume trend.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    trends = (
        keyword_info.get("search_volume_trend")
        if isinstance(keyword_info.get("search_volume_trend"), dict)
        else {}
    )
    sv = keyword_info.get("search_volume", 0) or 0

    yearly = trends.get("yearly", 0)
    quarterly = trends.get("quarterly", 0)
    monthly = trends.get("monthly", 0)

    def score_trend(value):
        if value is None:
            return 50  # Neutral score for missing data
        if value > 25:
            return 100
        if value > 10:
            return 75
        if value < -25:
            return 0
        if value < -10:
            return 25
        return 50

    yearly_score = score_trend(yearly)
    quarterly_score = score_trend(quarterly)
    monthly_score = score_trend(monthly)

    base_trend_score = (
        (yearly_score * 0.3) + (quarterly_score * 0.4) + (monthly_score * 0.3)
    )

    # Weight the trend score by search volume magnitude
    # A log scale helps moderate the effect of massive search volumes
    sv_weight = min(
        math.log(sv + 1) / math.log(100000), 1.0
    )  # Normalize against 100k SV

    # Final score is a blend: 70% trend, 30% volume weight. This prevents tiny keywords with huge trends from dominating.
    final_score = (base_trend_score * 0.7) + (sv_weight * 100 * 0.3)

    explanation = f"Weighted score from trends (Y:{yearly}%, Q:{quarterly}%, M:{monthly}%) and search volume."
    breakdown = {
        "Growth Trend": {
            "value": f"{yearly}% YoY",
            "score": round(final_score),
            "explanation": explanation,
        }
    }
    return round(final_score), breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/keyword_structure.py">
# pipeline/step_03_prioritization/scoring_components/keyword_structure.py
from typing import Dict, Any, Tuple


def calculate_keyword_structure_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Scores the keyword based on its structure, rewarding the "long-tail sweet spot"
    and adding a bonus for search depth.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword = data.get("keyword", "")
    word_count = len(keyword.split())
    depth = data.get("depth", 0)  # From related_keywords endpoint

    # Score is based on word count, with a peak at the 4-6 word sweet spot
    if word_count >= 4 and word_count <= 6:
        score = 100.0
    elif word_count == 3 or word_count == 7:
        score = 75.0
    elif word_count == 2 or word_count == 8:
        score = 50.0
    else:  # 1 word or 9+ words
        score = 25.0

    # Add a bonus for depth, rewarding more specific queries
    if depth > 0:
        score = min(100, score + (depth * 5))  # +5 points per depth level

    explanation = f"Keyword has {word_count} words and search depth of {depth}. The 4-6 word range is the sweet spot."
    breakdown = {
        "Keyword Structure": {
            "value": f"{word_count} words (Depth: {depth})",
            "score": score,
            "explanation": explanation,
        }
    }

    return score, breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/serp_crowding.py">
# pipeline/step_03_prioritization/scoring_components/serp_crowding.py
from typing import Dict, Any, Tuple


def calculate_serp_crowding_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on how crowded the SERP is with attention-grabbing features.
    A less crowded SERP is a better opportunity.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    serp_types = set(serp_info.get("serp_item_types", []))

    # Define features that compete for user attention
    CROWDING_FEATURES = {
        "video",
        "short_videos",
        "images",
        "people_also_ask",
        "carousel",
        "multi_carousel",
        "featured_snippet",
        "ai_overview",
    }

    crowding_feature_count = len(serp_types.intersection(CROWDING_FEATURES))

    # The score is inverted: more features = lower score
    if crowding_feature_count >= 5:
        score = 0.0
    elif crowding_feature_count == 4:
        score = 25.0
    elif crowding_feature_count == 3:
        score = 50.0
    elif crowding_feature_count == 2:
        score = 75.0
    elif crowding_feature_count == 1:
        score = 90.0
    else:
        score = 100.0

    explanation = f"{crowding_feature_count} attention-grabbing features found. A lower count is better."
    breakdown = {
        "SERP Crowding": {
            "value": crowding_feature_count,
            "score": score,
            "explanation": explanation,
        }
    }

    return score, breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/serp_features.py">
# pipeline/step_03_prioritization/scoring_components/serp_features.py
from typing import Dict, Any, Tuple


def calculate_serp_features_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the SERP environment, rewarding opportunities
    and penalizing attention-grabbing distractions.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    serp_types = set(serp_info.get("serp_item_types", []))

    score = 50.0  # Start with a neutral base score
    notes = []

    # Positive modifiers for high-value features
    if "featured_snippet" in serp_types:
        score += config.get("featured_snippet_bonus", 40)
        notes.append("Featured Snippet (+40)")
    if "people_also_ask" in serp_types:
        score += 25
        notes.append("People Also Ask (+25)")

    # Negative modifiers for threats and attention-grabbing features
    if "ai_overview" in serp_types:
        score -= config.get("ai_overview_penalty", 20)
        notes.append("AI Overview (-20)")
    if "video" in serp_types or "short_videos" in serp_types:
        score -= 15
        notes.append("Video Results (-15)")
    if "images" in serp_types:
        score -= 10
        notes.append("Image Carousel (-10)")

    final_score = max(0, min(100.0, score))
    explanation = (
        "Score reflects SERP opportunities. " + ", ".join(notes)
        if notes
        else "Neutral SERP environment."
    )

    breakdown = {
        "SERP Opportunity": {
            "value": len(notes),
            "score": final_score,
            "explanation": explanation.strip(),
        }
    }
    return final_score, breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/serp_freshness.py">
# pipeline/step_03_prioritization/scoring_components/serp_freshness.py
from typing import Dict, Any, Tuple
from datetime import datetime


def calculate_serp_freshness_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on SERP freshness. An older SERP is a better opportunity.
    """
    if not isinstance(data, dict):
        return 50.0, {
            "Freshness": {
                "value": "N/A",
                "score": 50,
                "explanation": "Invalid data format.",
            }
        }

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    last_update_str = serp_info.get("last_updated_time")

    if not last_update_str:
        return 50.0, {
            "Freshness": {
                "value": "N/A",
                "score": 50,
                "explanation": "No freshness data.",
            }
        }

    try:
        last_update = datetime.fromisoformat(last_update_str.replace(" +00:00", ""))
        days_since_update = (datetime.now() - last_update).days

        # Score increases as the SERP gets older
        if days_since_update > 90:
            score = 100.0
        elif days_since_update > 60:
            score = 80.0
        elif days_since_update > 30:
            score = 60.0
        elif days_since_update > 14:
            score = 40.0
        else:
            score = 20.0

        explanation = f"SERP last updated {days_since_update} days ago. Older SERPs are better opportunities."
        breakdown = {
            "Freshness": {
                "value": f"{days_since_update} days",
                "score": score,
                "explanation": explanation,
            }
        }
        return score, breakdown

    except (ValueError, TypeError):
        return 50.0, {
            "Freshness": {
                "value": "Error",
                "score": 50,
                "explanation": "Could not parse update timestamp.",
            }
        }
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/serp_threat.py">
# pipeline/step_03_prioritization/scoring_components/serp_threat.py
from typing import Dict, Any, Tuple


def calculate_serp_threat_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a unified "threat" score for the SERP. A lower score is better.
    This score is inverted for the final calculation (higher threat = lower opportunity).
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    serp_types = set(serp_info.get("serp_item_types", []))

    threat_level = 0
    notes = []

    # Threat 1: Hostile, non-blog features
    HOSTILE_FEATURES = {
        "shopping",
        "popular_products",
        "local_pack",
        "google_flights",
        "google_hotels",
        "app",
        "jobs",
        "math_solver",
        "currency_box",
    }
    found_hostile = serp_types.intersection(HOSTILE_FEATURES)
    if found_hostile:
        threat_level += 50
        notes.append(f"Hostile features found ({', '.join(found_hostile)})")

    # Threat 2: AI Overview
    if "ai_overview" in serp_types:
        threat_level += config.get("ai_overview_penalty", 25)
        notes.append("AI Overview is present")

    # Threat 3: Paid Ads (implicit threat)
    if "paid" in serp_types:
        threat_level += 10
        notes.append("Paid ads are present")

    # Normalize the threat level to a 0-100 score
    final_threat_score = min(100, threat_level)

    # The final score is inverted: 100 is low threat, 0 is high threat.
    opportunity_score = 100 - final_threat_score

    explanation = (
        "Score reflects threats to organic CTR. " + "; ".join(notes)
        if notes
        else "No major threats found."
    )
    breakdown = {
        "SERP Threat": {
            "value": f"{final_threat_score}%",
            "score": opportunity_score,
            "explanation": explanation,
        }
    }

    return opportunity_score, breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/serp_volatility.py">
# pipeline/step_03_prioritization/scoring_components/serp_volatility.py
from typing import Dict, Any, Tuple
from datetime import datetime


def calculate_serp_volatility_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on SERP stability. A more volatile SERP can be an opportunity.
    """
    if not isinstance(data, dict):
        return 50.0, {
            "SERP Stability": {
                "value": "N/A",
                "score": 50,
                "explanation": "Invalid data format.",
            }
        }

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    breakdown = {}

    last_update_str = serp_info.get("last_updated_time")
    prev_update_str = serp_info.get("previous_updated_time")

    if not last_update_str or not prev_update_str:
        return 50.0, {
            "SERP Stability": {
                "value": "N/A",
                "score": 50,
                "explanation": "Insufficient data to calculate SERP volatility.",
            }
        }

    try:
        last_update = datetime.fromisoformat(last_update_str.replace(" +00:00", ""))
        prev_update = datetime.fromisoformat(prev_update_str.replace(" +00:00", ""))
        days_between_updates = (last_update - prev_update).days

        stable_threshold = config.get("serp_volatility_stable_threshold_days", 30)

        score = 0.0
        if days_between_updates < 7:  # Highly volatile
            score = 100.0
        elif days_between_updates < 21:  # Moderately volatile
            score = 75.0
        elif days_between_updates < stable_threshold:  # Relatively stable
            score = 50.0
        else:  # Very stable
            score = 25.0

        explanation = f"SERP updated every {days_between_updates} days. More frequent updates can signal an opportunity."
        breakdown["SERP Stability"] = {
            "value": f"{days_between_updates} days",
            "score": score,
            "explanation": explanation,
        }
        return score, breakdown

    except (ValueError, TypeError):
        return 50.0, {
            "SERP Stability": {
                "value": "Error",
                "score": 50,
                "explanation": "Could not parse SERP update timestamps.",
            }
        }
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/traffic_potential.py">
# pipeline/step_03_prioritization/scoring_components/traffic_potential.py
from typing import Dict, Any, Tuple


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_traffic_potential_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a blended score based on both commercial traffic value and raw audience size.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    sv = keyword_info.get("search_volume", 0) or 0
    cpc = keyword_info.get("cpc", 0.0) or 0.0

    # 1. Calculate Traffic Value Score
    traffic_value = sv * cpc
    max_traffic_value = config.get("max_traffic_value_for_scoring", 50000)
    traffic_value_score = _normalize_value(traffic_value, max_traffic_value)

    # 2. Calculate Raw Search Volume Score
    max_sv = config.get("max_sv_for_scoring", 100000)
    raw_sv_score = _normalize_value(sv, max_sv)

    # 3. Blend the scores to balance commercial value and audience size
    final_score = (traffic_value_score * 0.7) + (raw_sv_score * 0.3)

    explanation = f"Blended score: 70% from Est. Traffic Value (${traffic_value:,.0f}) and 30% from Raw SV ({sv})."
    breakdown = {
        "Traffic Potential": {
            "value": f"{sv} SV | ${cpc:.2f} CPC",
            "score": round(final_score),
            "explanation": explanation,
        }
    }

    return round(final_score), breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_components/volume_volatility.py">
# pipeline/step_03_prioritization/scoring_components/volume_volatility.py
import numpy as np
from typing import Dict, Any, Tuple


def calculate_volume_volatility_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the stability of monthly search volume.
    Lower volatility is generally better for long-term planning.
    """
    if not isinstance(data, dict):
        return 50.0, {
            "Volatility": {
                "value": "N/A",
                "score": 50,
                "explanation": "Invalid data format.",
            }
        }

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    monthly_searches = keyword_info.get("monthly_searches", [])

    if not monthly_searches or len(monthly_searches) < 3:
        return 50.0, {
            "Volatility": {
                "value": "N/A",
                "score": 50,
                "explanation": "Insufficient data.",
            }
        }

    volumes = [
        ms["search_volume"]
        for ms in monthly_searches
        if isinstance(ms, dict)
        and ms.get("search_volume") is not None
        and ms["search_volume"] > 0
    ]
    if len(volumes) < 3:
        return 50.0, {
            "Volatility": {
                "value": "N/A",
                "score": 50,
                "explanation": "Insufficient data.",
            }
        }

    mean_volume = np.mean(volumes)
    std_dev = np.std(volumes)

    if mean_volume == 0:
        return 0.0, {
            "Volatility": {"value": "0", "score": 0, "explanation": "No search volume."}
        }

    coeff_of_variation = std_dev / mean_volume

    # Score is inverted: higher volatility = lower score
    # A CoV of 0.5 (50%) is considered moderately high.
    score = max(0, 100 - (coeff_of_variation * 150))  # Scale the penalty

    explanation = (
        f"Coefficient of Variation: {coeff_of_variation:.2%}. Lower is more stable."
    )
    breakdown = {
        "Volatility": {
            "value": f"{coeff_of_variation:.2%}",
            "score": round(score),
            "explanation": explanation,
        }
    }

    return round(score), breakdown
</file>

<file path="backend/pipeline/step_03_prioritization/__init__.py">
# pipeline/step_03_prioritization/__init__.py
</file>

<file path="backend/pipeline/step_03_prioritization/run_prioritization.py">
import logging
from typing import List, Dict, Any

from .scoring_engine import ScoringEngine


def run_prioritization_phase(
    opportunities: List[Dict[str, Any]], client_cfg: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Orchestrates the prioritization phase.

    1. Scores each opportunity based on a weighted formula.
    2. Sorts the opportunities by their calculated score.

    Returns a sorted list of opportunities with scoring data.
    """
    logger = logging.getLogger(__name__)
    logger.info("--- Starting Prioritization Phase ---")

    scoring_engine = ScoringEngine(client_cfg)

    # 1. Score Opportunities
    scored_opportunities = []
    for opp in opportunities:
        score, score_breakdown = scoring_engine.calculate_score(opp)
        opp["strategic_score"] = score
        # Add the focused competition score directly into the breakdown for persistence
        if "low_competition_score" in score_breakdown:
            opp["low_competition_score"] = score_breakdown["low_competition_score"][
                "score"
            ]
        opp["score_breakdown"] = score_breakdown
        scored_opportunities.append(opp)

    # 2. Sort Opportunities
    sorted_opportunities = sorted(
        scored_opportunities, key=lambda x: x["strategic_score"], reverse=True
    )

    logger.info(f"  -> Scored and sorted {len(sorted_opportunities)} opportunities.")
    logger.info("--- Prioritization Phase Complete ---")

    return sorted_opportunities
</file>

<file path="backend/pipeline/step_03_prioritization/scoring_engine.py">
import logging
from typing import Dict, Any, Tuple
from .scoring_components import (
    calculate_ease_of_ranking_score,
    calculate_traffic_potential_score,
    calculate_commercial_intent_score,
    calculate_growth_trend_score,
    calculate_serp_features_score,
    calculate_serp_volatility_score,
    calculate_competitor_weakness_score,
    calculate_serp_crowding_score,
    calculate_keyword_structure_score,
    calculate_serp_threat_score,
    calculate_volume_volatility_score,
    calculate_serp_freshness_score,
    calculate_competitor_performance_score,  # ADDED THIS IMPORT
)


class ScoringEngine:
    """
    Calculates a strategic score for each keyword opportunity by orchestrating
    a suite of modular scoring components.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def calculate_score(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[float, Dict[str, Any]]:
        """
        Calculates the final opportunity score by combining weighted scores
        from all registered scoring components.
        """
        if not isinstance(opportunity, dict):
            self.logger.warning(
                "Invalid data format passed to calculate_score. Expected a dictionary."
            )
            return 0.0, {"error": "Invalid data format."}

        breakdown = {}
        data_source = opportunity.get("full_data", opportunity)

        # --- Execute all scoring components ---
        ease_score, ease_breakdown = calculate_ease_of_ranking_score(
            data_source, self.config
        )
        breakdown["ease_of_ranking"] = {
            "name": "Ease of Ranking",
            "score": ease_score,
            "breakdown": ease_breakdown,
        }

        traffic_score, traffic_breakdown = calculate_traffic_potential_score(
            data_source, self.config
        )
        breakdown["traffic_potential"] = {
            "name": "Traffic Potential",
            "score": traffic_score,
            "breakdown": traffic_breakdown,
        }

        intent_score, intent_breakdown = calculate_commercial_intent_score(
            data_source, self.config
        )
        breakdown["commercial_intent"] = {
            "name": "Commercial Intent",
            "score": intent_score,
            "breakdown": intent_breakdown,
        }

        trend_score, trend_breakdown = calculate_growth_trend_score(
            data_source, self.config
        )
        breakdown["growth_trend"] = {
            "name": "Growth Trend",
            "score": trend_score,
            "breakdown": trend_breakdown,
        }

        features_score, features_breakdown = calculate_serp_features_score(
            data_source, self.config
        )
        breakdown["serp_features"] = {
            "name": "SERP Opportunity",
            "score": features_score,
            "breakdown": features_breakdown,
        }

        volatility_score, volatility_breakdown = calculate_serp_volatility_score(
            data_source, self.config
        )
        breakdown["serp_volatility"] = {
            "name": "SERP Volatility",
            "score": volatility_score,
            "breakdown": volatility_breakdown,
        }

        weakness_score, weakness_breakdown = calculate_competitor_weakness_score(
            data_source, self.config
        )
        breakdown["competitor_weakness"] = {
            "name": "Competitor Weakness",
            "score": weakness_score,
            "breakdown": weakness_breakdown,
        }

        crowding_score, crowding_breakdown = calculate_serp_crowding_score(
            data_source, self.config
        )
        breakdown["serp_crowding"] = {
            "name": "SERP Crowding",
            "score": crowding_score,
            "breakdown": crowding_breakdown,
        }

        structure_score, structure_breakdown = calculate_keyword_structure_score(
            data_source, self.config
        )
        breakdown["keyword_structure"] = {
            "name": "Keyword Structure",
            "score": structure_score,
            "breakdown": structure_breakdown,
        }

        threat_score, threat_breakdown = calculate_serp_threat_score(
            data_source, self.config
        )
        breakdown["serp_threat"] = {
            "name": "SERP Threat",
            "score": threat_score,
            "breakdown": threat_breakdown,
        }

        volume_volatility_score, volume_volatility_breakdown = (
            calculate_volume_volatility_score(data_source, self.config)
        )
        breakdown["volume_volatility"] = {
            "name": "Volume Volatility",
            "score": volume_volatility_score,
            "breakdown": volume_volatility_breakdown,
        }

        freshness_score, freshness_breakdown = calculate_serp_freshness_score(
            data_source, self.config
        )
        breakdown["serp_freshness"] = {
            "name": "SERP Freshness",
            "score": freshness_score,
            "breakdown": freshness_breakdown,
        }

        performance_score, performance_breakdown = (
            calculate_competitor_performance_score(opportunity, self.config)
        )
        breakdown["competitor_performance"] = {
            "name": "Competitor Tech Performance",
            "score": performance_score,
            "breakdown": performance_breakdown,
        }
        # --- Apply weights from config and calculate final score ---
        weights = {
            "ease": self.config.get("ease_of_ranking_weight", 25),
            "traffic": self.config.get("traffic_potential_weight", 20),
            "intent": self.config.get("commercial_intent_weight", 15),
            "weakness": self.config.get("competitor_weakness_weight", 10),
            "structure": self.config.get("keyword_structure_weight", 5),
            "trend": self.config.get("growth_trend_weight", 5),
            "features": self.config.get("serp_features_weight", 5),
            "crowding": self.config.get("serp_crowding_weight", 5),
            "volatility": self.config.get("serp_volatility_weight", 5),
            "threat": self.config.get("serp_threat_weight", 5),
            "freshness": self.config.get("serp_freshness_weight", 0),
            "competitor_performance": self.config.get(
                "competitor_performance_weight", 5
            ),  # ADDED THIS LINE
            "volume_volatility": self.config.get("volume_volatility_weight", 0),
        }

        total_weight = sum(weights.values())
        if total_weight == 0:
            return 0.0, breakdown  # Avoid division by zero

        final_score = (
            (ease_score * weights["ease"])
            + (traffic_score * weights["traffic"])
            + (intent_score * weights["intent"])
            + (weakness_score * weights["weakness"])
            + (structure_score * weights["structure"])
            + (trend_score * weights["trend"])
            + (features_score * weights["features"])
            + (crowding_score * weights["crowding"])
            + (volatility_score * weights["volatility"])
            + (threat_score * weights["threat"])
            + (freshness_score * weights["freshness"])
            + (volume_volatility_score * weights["volume_volatility"])
            + (performance_score * weights["competitor_performance"])  # ADDED THIS LINE
        ) / total_weight

        for key, breakdown_data in breakdown.items():
            # Map breakdown key to weight key
            weight_key_map = {
                "ease_of_ranking": "ease",
                "traffic_potential": "traffic",
                "commercial_intent": "intent",
                "competitor_weakness": "weakness",
                "keyword_structure": "structure",
                "growth_trend": "trend",
                "serp_features": "features",
                "serp_crowding": "crowding",
                "serp_volatility": "volatility",
                "serp_threat": "threat",
                "volume_volatility": "volume_volatility",
                "serp_freshness": "freshness",
                "competitor_performance": "competitor_performance",  # ADDED THIS LINE
            }
            weight_key = weight_key_map.get(key, "")
            breakdown_data["weight"] = weights.get(weight_key, 0)

        return round(final_score, 2), breakdown
</file>

<file path="backend/pipeline/step_04_analysis/content_analysis_modules/ai_intelligence_caller.py">
from typing import List, Dict, Any, Tuple
from external_apis.openai_client import OpenAIClientWrapper


def get_ai_content_analysis(
    openai_client: OpenAIClientWrapper,
    messages: List[Dict[str, str]],
    model: str,
    max_completion_tokens: int,
) -> Tuple[Dict[str, Any], str]:
    """
    Calls the OpenAI API to get content analysis and returns the response and any error.
    """
    schema = {
        "name": "extract_deep_content_insights",
        "type": "object",
        "properties": {
            "unique_angles_to_include": {"type": "array", "items": {"type": "string"}},
            "key_entities_from_competitors": {
                "type": "array",
                "items": {"type": "string"},
            },
            "core_questions_answered_by_competitors": {
                "type": "array",
                "items": {"type": "string"},
            },
            "identified_content_gaps": {"type": "array", "items": {"type": "string"}},
        },
        "required": [
            "unique_angles_to_include",
            "key_entities_from_competitors",
            "core_questions_answered_by_competitors",
            "identified_content_gaps",
        ],
        "additionalProperties": False,
    }

    response, error = openai_client.call_chat_completion(
        messages=messages,
        schema=schema,
        model=model,
        max_completion_tokens=max_completion_tokens,
    )

    return response, error
</file>

<file path="backend/pipeline/step_04_analysis/content_analysis_modules/heading_analyzer.py">
from typing import List, Dict, Any
from collections import Counter


def extract_common_headings(
    competitor_analysis: List[Dict[str, Any]], num_headings: int
) -> List[str]:
    """Extracts the most common H2 and H3 headings from competitor data."""
    all_headings = Counter(
        h
        for c in competitor_analysis
        if c.get("headings")
        for h_type in ["h2", "h3"]
        for h in c["headings"].get(h_type, [])
    )
    return [h for h, count in all_headings.most_common(num_headings)]
</file>

<file path="backend/pipeline/step_04_analysis/content_analysis_modules/metric_analyzer.py">
from typing import List, Dict, Any, Optional


def calculate_average_word_count(competitor_analysis: List[Dict[str, Any]]) -> int:
    """Calculates the average word count from a list of competitor data."""
    word_counts = [
        c.get("word_count") for c in competitor_analysis if c and c.get("word_count")
    ]
    return int(sum(word_counts) / len(word_counts)) if word_counts else 1500


def calculate_average_readability(
    competitor_analysis: List[Dict[str, Any]],
) -> Optional[float]:
    """Calculates the average readability score from a list of competitor data."""
    readability_scores = [
        c.get("readability_score")
        for c in competitor_analysis
        if c.get("readability_score") is not None
    ]
    return (
        sum(readability_scores) / len(readability_scores)
        if readability_scores
        else None
    )
</file>

<file path="backend/pipeline/step_04_analysis/__init__.py">
# pipeline/step_04_analysis/__init__.py
</file>

<file path="backend/pipeline/step_04_analysis/competitor_analyzer.py">
import logging
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import urlparse
import textstat

from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class FullCompetitorAnalyzer:
    """
    Performs a deep-dive analysis of top organic competitors using the OnPage Instant Pages API.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.min_word_count = self.config.get("min_competitor_word_count", 300)

        # Combine all blacklists dynamically

        excluded_domains_config = self.config.get(
            "competitor_analysis_excluded_domains", []
        )

        if isinstance(excluded_domains_config, str):
            excluded_domains = set(
                d.strip() for d in excluded_domains_config.split(",")
            )

        else:
            excluded_domains = set(excluded_domains_config)

        self.blacklist_domains = excluded_domains.union(
            set(self.config.get("ugc_and_parasite_domains", []))
        )

    def analyze_competitors(
        self, competitor_urls: List[str], selected_urls: Optional[List[str]] = None
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Fetches and analyzes competitor data using a two-tier, adaptive fetching strategy.
        First attempts a cheap scan without JS, then retries failures with JS enabled.
        """
        urls_to_scan = selected_urls or competitor_urls
        if not urls_to_scan:
            return [], 0.0

        total_api_cost = 0.0
        successful_results = []
        urls_that_need_js_retry = []

        # --- Tier 1: Fast, cheap scan with JavaScript DISABLED ---
        self.logger.info(
            f"Starting Tier 1 analysis for {len(urls_to_scan)} URLs (JS disabled)."
        )
        try:
            initial_tasks, initial_cost = self.client.get_content_onpage_data(
                urls_to_scan, self.config, enable_javascript=False
            )
            total_api_cost += initial_cost

            for task in initial_tasks:
                task_url = task.get("data", {}).get("url")

                if task.get("result") is None:
                    self.logger.warning(
                        f"Tier 1 scan for {task_url} returned a null result. Queuing for JS-enabled retry."
                    )
                    urls_that_need_js_retry.append(task_url)
                    continue

                result = task.get("result", [{}])[0]

                if (
                    task.get("status_code") == 20000
                    and result.get("crawl_status") != "Page content is empty"
                    and result.get("items_count", 0) > 0
                ):
                    successful_results.extend(result.get("items", []))
                else:
                    self.logger.warning(
                        f"Tier 1 scan failed for {task_url}. Reason: {result.get('crawl_status', task.get('status_message'))}. Queuing for JS-enabled retry."
                    )
                    urls_that_need_js_retry.append(task_url)
        except Exception as e:
            self.logger.error(
                f"Error during Tier 1 competitor analysis: {e}", exc_info=True
            )

        # --- Tier 2: Slower, more expensive scan with JavaScript ENABLED for failures ---
        if urls_that_need_js_retry:
            self.logger.info(
                f"Starting Tier 2 analysis for {len(urls_that_need_js_retry)} failed URLs (JS enabled)."
            )
            try:
                retry_tasks, retry_cost = self.client.get_content_onpage_data(
                    urls_that_need_js_retry, self.config, enable_javascript=True
                )
                total_api_cost += retry_cost

                for task in retry_tasks:
                    task_url = task.get("data", {}).get("url")

                    if task.get("result") is None:
                        self.logger.error(
                            f"Tier 2 retry FAILED for {task_url}. Reason: API returned a null result. This URL will be excluded from analysis."
                        )
                        continue

                    result = task.get("result", [{}])[0]

                    if (
                        task.get("status_code") == 20000
                        and result.get("items_count", 0) > 0
                    ):
                        self.logger.info(
                            f"Tier 2 JS-enabled retry SUCCEEDED for {task_url}."
                        )
                        successful_results.extend(result.get("items", []))
                    else:
                        self.logger.error(
                            f"Tier 2 retry FAILED for {task_url}. Reason: {result.get('crawl_status', task.get('status_message'))}. This URL will be excluded from analysis."
                        )
            except Exception as e:
                self.logger.error(
                    f"Error during Tier 2 competitor analysis: {e}", exc_info=True
                )

        # --- Final Processing ---
        final_competitor_list = self._process_content_parsing_results(
            successful_results
        )

        return final_competitor_list, total_api_cost

    def _process_content_parsing_results(
        self, results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Processes the successful results from the Content Parsing API call into a standardized competitor object.
        """
        final_competitors = []
        for result in results:
            url = result.get("url")  # URL is at the top level in the new API response
            if not url or result.get("status_code") != 200:
                continue

            domain = urlparse(url).netloc
            if domain in self.blacklist_domains:
                self.logger.info(f"Skipping blacklisted competitor: {domain}")
                continue

            page_content = result.get("page_content", {})
            main_topic_content = ""
            headings = {"h1": [], "h2": [], "h3": [], "h4": [], "h5": [], "h6": []}

            # Extract main content and headings from the structured 'main_topic' array
            if page_content and page_content.get("main_topic"):
                for topic in page_content["main_topic"]:
                    h_level = topic.get("level")
                    h_title = topic.get("h_title")
                    if h_level and h_title:
                        tag = f"h{h_level}"
                        if tag in headings:
                            headings[tag].append(h_title)

                    if topic.get("primary_content"):
                        for pc in topic["primary_content"]:
                            if pc and pc.get("text"):
                                main_topic_content += pc["text"] + " "

            main_topic_content = main_topic_content.strip()

            # Manually calculate word count and readability
            word_count = len(main_topic_content.split())
            readability_score = None
            if (
                word_count > 100
            ):  # textstat needs a reasonable amount of text to be accurate
                try:
                    readability_score = textstat.flesch_kincaid_grade(
                        main_topic_content
                    )
                except Exception as e:
                    self.logger.warning(
                        f"Could not calculate readability for {url}: {e}"
                    )

            if word_count >= self.min_word_count:
                processed_competitor = {
                    "url": url,
                    "title": headings["h1"][0] if headings.get("h1") else None,
                    "word_count": word_count,
                    "readability_score": readability_score,
                    "headings": headings,
                    "main_content_text": main_topic_content,  # Clean text for readability calculation
                    "full_content_markdown": result.get(
                        "page_as_markdown"
                    ),  # Clean markdown for AI analysis
                    # Set technical fields to defaults as they are not available from this endpoint
                    "technical_warnings": [],
                    "page_timing": {},
                    "onpage_score": None,
                }
                final_competitors.append(processed_competitor)
            else:
                self.logger.info(
                    f"Skipping competitor {url} due to low parsed word count: {word_count}"
                )

        return final_competitors
</file>

<file path="backend/pipeline/step_04_analysis/content_analyzer.py">
import logging
from typing import List, Dict, Any, Tuple

from external_apis.openai_client import OpenAIClientWrapper

# Keep these imports if you want to reuse them for the deep-dive path
from .content_analysis_modules.ai_intelligence_caller import get_ai_content_analysis


class ContentAnalyzer:
    """
    Orchestrates the analysis of competitor content (or SERP data) to synthesize intelligence.
    """

    def __init__(self, openai_client: OpenAIClientWrapper, config: Dict[str, Any]):
        self.openai_client = openai_client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.num_common_headings = self.config.get("num_common_headings", 8)
        self.num_unique_angles = self.config.get("num_unique_angles", 5)
        self.max_words_for_ai_analysis = self.config.get(
            "max_words_for_ai_analysis", 2000
        )
        self.num_competitors_for_ai_analysis = self.config.get(
            "num_competitors_for_ai_analysis", 3
        )

    # --- START MODIFICATION ---
    def synthesize_content_intelligence(
        self,
        keyword: str,
        serp_overview: Dict[str, Any],
        competitor_analysis: List[
            Dict[str, Any]
        ],  # This will be empty if deep analysis is skipped
    ) -> Tuple[Dict[str, Any], float]:
        """
        Synthesizes content intelligence by orchestrating data preparation and AI analysis.
        Conditionally uses deep competitor content or rich SERP data.
        """
        if competitor_analysis:
            from .content_analysis_modules.ai_content_preparer import (
                prepare_competitor_content_for_ai,
            )
            from .content_analysis_modules.ai_prompt_builder import (
                get_ai_prompt_messages,
            )

            self.logger.info(
                "Synthesizing intelligence from deep competitor content analysis (legacy path)."
            )

            # 1. Prepare Competitor Content for AI
            content_for_ai, using_markdown = prepare_competitor_content_for_ai(
                competitor_analysis,
                self.num_competitors_for_ai_analysis,
                self.max_words_for_ai_analysis,
            )

            # 2. Build AI Prompt (legacy)
            ai_prompt_messages = get_ai_prompt_messages(
                keyword, content_for_ai, using_markdown
            )

        else:
            self.logger.info(
                "Synthesizing intelligence from rich SERP data (new path)."
            )
            # 1. Prepare SERP Data for AI (already extracted by FullSerpAnalyzer)
            # We just need to ensure it's structured for the prompt.
            # All the new fields are already in serp_overview.

            # 2. Build AI Prompt (new, SERP-only)
            ai_prompt_messages = self._build_synthesis_prompt_from_serp(
                keyword, serp_overview
            )

        # 3. Call AI for Analysis (common to both paths)
        ai_analysis_response, error = get_ai_content_analysis(
            openai_client=self.openai_client,
            messages=ai_prompt_messages,
            model=self.config.get("default_model", "gpt-5-nano"),
            max_completion_tokens=self.config.get(
                "max_completion_tokens_for_generation", 4096
            ),
        )
        total_ai_cost = self.openai_client.latest_cost

        if error or not ai_analysis_response:
            self.logger.error(f"Failed to get deep content analysis from AI: {error}")
            return {
                "analysis_error": f"AI-powered content intelligence failed. Reason: {error}"
            }, total_ai_cost

        # 4. Assemble Final Intelligence Object
        ai_analysis_response["unique_angles_to_include"] = list(
            set(ai_analysis_response.get("unique_angles_to_include", []))
        )[: self.num_unique_angles]

        # --- NEW: Incorporate AI Overview Sources into AI Content Brief
        if (
            serp_overview.get("ai_overview_sources") and not competitor_analysis
        ):  # Only for SERP-only mode
            if "source_and_inspiration_content" not in ai_analysis_response:
                ai_analysis_response["source_and_inspiration_content"] = {}
            ai_analysis_response["source_and_inspiration_content"][
                "ai_overview_sources"
            ] = serp_overview["ai_overview_sources"]

        return ai_analysis_response, total_ai_cost

    # New private method for SERP-only prompt building
    def _build_synthesis_prompt_from_serp(
        self, keyword: str, serp_data: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """
        Builds a comprehensive prompt for AI content intelligence synthesis
        based purely on rich SERP data.
        """
        system_prompt = "You are a world-class SEO content strategist. Your task is to analyze structured SERP data to reverse-engineer a winning content strategy. Your insights must be actionable and highly specific."

        prompt_sections = [f'**Primary Keyword:** "{keyword}"\n']

        if serp_data.get("knowledge_graph_facts"):
            facts_list = '\n- '.join(serp_data['knowledge_graph_facts'])
            prompt_sections.append(
                f"**Verified Facts from Knowledge Graph (Incorporate these as core facts):**\n- {facts_list}\n"
            )

        if serp_data.get("paid_ad_copy"):
            paid_titles = [ad["title"] for ad in serp_data["paid_ad_copy"]]
            paid_descriptions = [ad["description"] for ad in serp_data["paid_ad_copy"]]
            prompt_sections.append(
                f"**High-Conversion Language from Top Paid Ads (Analyze for compelling headlines/intro/CTAs):**\n- Titles: {paid_titles}\n- Descriptions: {paid_descriptions}\n"
            )

        if serp_data.get("top_organic_sitelinks"):
            sitelinks_list = '\n- '.join(serp_data['top_organic_sitelinks'])
            prompt_sections.append(
                f"**High-Priority Subtopics from Competitor Sitelinks (Must include these as H2/H3s):**\n- {sitelinks_list}\n"
            )

        if serp_data.get("top_organic_faqs"):
            faqs_list = '\n- '.join(serp_data['top_organic_faqs'])
            prompt_sections.append(
                f"**High-Priority Questions from Competitor FAQ Snippets (Must include these in a dedicated FAQ section):**\n- {faqs_list}\n"
            )

        if serp_data.get("ai_overview_sources"):
            sources_list = '\n- '.join(serp_data['ai_overview_sources'])
            prompt_sections.append(
                f"**Authoritative Sources Used by Google's AI Overview (Give analytical priority to concepts from these sources):**\n- {sources_list}\n"
            )

        if serp_data.get("discussion_snippets"):
            snippets_list = '\n- '.join(serp_data['discussion_snippets'])
            prompt_sections.append(
                f"**Voice of the Customer from Discussions/Forums (Analyze for tone, pain points, and authentic perspective):**\n- {snippets_list}\n"
            )

        # Add basic organic results for general context
        if serp_data.get("top_organic_results"):
            org_titles_desc_list = '\n- '.join([
                f"Title: {r['title']}\nDescription: {r['description']}"
                for r in serp_data["top_organic_results"]
            ])
            prompt_sections.append(
                f"**Top Organic Result Snippets (for general content analysis):**\n- {org_titles_desc_list}\n"
            )

        if serp_data.get("people_also_ask"):
            paa_list = '\n- '.join(serp_data['people_also_ask'])
            prompt_sections.append(
                f"**People Also Ask Questions:**\n- {paa_list}\n"
            )
        if serp_data.get("related_searches"):
            related_list = '\n- '.join(serp_data['related_searches'])
            prompt_sections.append(
                f"**Related Searches:**\n- {related_list}\n"
            )
        if serp_data.get("ai_overview_content"):
            prompt_sections.append(
                f"**Google's AI Overview Content:**\n{serp_data['ai_overview_content']}\n"
            )
        if serp_data.get("featured_snippet_content"):
            prompt_sections.append(
                f"**Featured Snippet Content:**\n{serp_data['featured_snippet_content']}\n"
            )

        user_prompt_content = f"""
        Analyze the following comprehensive SERP intelligence report to generate a content strategy blueprint.

        {"".join(prompt_sections)}

        **Your Analysis Task:**
        1.  **Unique Angles & Insights:** Based on ALL the provided SERP data (Knowledge Graph, Paid Ads, FAQs, Sitelinks, AI Overview sources, discussions, organic snippets), identify 2-3 truly unique value propositions or content differentiation angles. Where are the gaps and opportunities for our content to stand out as superior?
        2.  **Key Entities:** List the 5-10 most critical entities (people, products, brands, concepts) from the entire SERP. These must be central to the topic.
        3.  **Core Questions Answered:** Synthesize the 5-7 most fundamental user questions that this keyword intends to answer, drawing from PAA, FAQ snippets, and top organic descriptions. These should form the backbone of the article's problem-solving narrative.
        4.  **Identified Content Gaps:** What specific sub-topics are implied or partially covered in the SERP, but could be expanded into full, authoritative sections in our article? What related long-tail questions (from PAA or Related Searches) are not adequately addressed by top results?

        Provide your analysis in the required structured JSON format.
        """
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt_content.strip()},
        ]

    # --- END MODIFICATION ---

    def generate_ai_outline(
        self,
        keyword: str,
        serp_overview: Dict[str, Any],
        content_intelligence: Dict[str, Any],
    ) -> Tuple[Dict[str, List[str]], float]:
        """
        Uses OpenAI to generate a structured content outline with H2s and corresponding H3s.
        (This can also be refactored into a separate module if desired)
        """
        prompt_messages = self._build_outline_prompt(
            keyword, serp_overview, content_intelligence
        )

        schema = {
            "name": "generate_structured_content_outline",
            "type": "object",
            "properties": {
                "article_structure": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "h2": {
                                "type": "string",
                                "description": "The H2 heading of the section.",
                            },
                            "h3s": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "A list of H3 subheadings for this H2 section.",
                            },
                        },
                        "required": ["h2", "h3s"],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["article_structure"],
            "additionalProperties": False,
        }

        response, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
            max_completion_tokens=self.config.get(
                "max_completion_tokens_for_generation", 4096
            ),
        )
        total_ai_cost = self.openai_client.latest_cost

        if error or not response or not response.get("article_structure"):
            self.logger.error(f"Failed to generate structured AI outline: {error}")
            return {"article_structure": []}, total_ai_cost

        return response, total_ai_cost

    def _build_outline_prompt(
        self,
        keyword: str,
        serp_overview: Dict[str, Any],
        content_intelligence: Dict[str, Any],
    ) -> List[Dict[str, str]]:
        """Builds the prompt for the AI structured outline generation."""
        prompt = f"""
        You are an expert SEO content strategist. Create a logical and comprehensive content outline for an article about "{keyword}". The output must be a structured list of sections, each with an H2 and a list of corresponding H3 subheadings.

        **Analysis Data:**
        - **Common Competitor Headings to Incorporate:** {", ".join(content_intelligence.get("common_headings_to_cover", []))}
        - **Unique Angles & Gaps to Address:** {", ".join(content_intelligence.get("unique_angles_to_include", []))}
        - **Key Entities to Mention:** {", ".join(content_intelligence.get("key_entities_from_competitors", []))}
        - **People Also Ask Questions to Answer:** {", ".join(serp_overview.get("paa_questions", []))}

        **Instructions:**
        1. Create a logical flow for the article.
        2. The first section must be titled 'Introduction'.
        3. The last section must be titled 'Conclusion'.
        4. If there are 'People Also Ask' questions, create a dedicated H2 section titled 'Frequently Asked Questions' and use the questions as H3s.
        5. Structure the entire output as a JSON object matching the requested schema.
        """
        return [{"role": "user", "content": prompt}]
</file>

<file path="backend/pipeline/step_04_analysis/run_analysis.py">
import logging
from typing import Dict, Any, Optional, List, Tuple

from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from external_apis.openai_client import OpenAIClientWrapper
from core.blueprint_factory import BlueprintFactory
from core.serp_analyzer import FullSerpAnalyzer
from .competitor_analyzer import FullCompetitorAnalyzer
from .content_analyzer import ContentAnalyzer
from pipeline.step_05_strategy.decision_engine import StrategicDecisionEngine
from pipeline.step_03_prioritization.scoring_engine import ScoringEngine

# --- NEW FUNCTION: run_final_validation ---
from urllib.parse import urlparse

# Then, replace the entire `run_final_validation` function with this new version.


def run_final_validation(
    live_serp_data: Dict[str, Any],
    opportunity: Dict[str, Any],
    client_cfg: Dict[str, Any],
    dataforseo_client: Any,
) -> Tuple[bool, Optional[str]]:
    full_data = opportunity.get("full_data", {})
    if not isinstance(full_data, dict):
        full_data = {}
    cached_serp_info = full_data.get("serp_info", {})
    cached_features = set(cached_serp_info.get("serp_item_types", []))
    live_features = set(live_serp_data.get("item_types", []))

    # NEW: Definitive Cannibalization Check (FIXED LOGIC)
    target_domain = client_cfg.get("target_domain", "").lower().replace("www.", "")
    if target_domain:
        for result in live_serp_data.get("top_organic_results", []):
            try:
                url_domain = (
                    urlparse(result.get("url", "")).netloc.lower().replace("www.", "")
                )
                # CRITICAL FIX: Check for exact match or subdomain suffix to avoid 'pet.com' matching 'competitor.com'
                if url_domain == target_domain or url_domain.endswith(
                    f".{target_domain}"
                ):
                    return (
                        False,
                        f"Final Validation Failed (Cannibalization): Target domain '{target_domain}' found in live SERP at URL '{result.get('url')}'.",
                    )
            except Exception:
                continue

    # Hostile features (configurable)
    hostile_features = set(
        client_cfg.get(
            "hostile_serp_features",
            [
                "shopping",
                "local_pack",
                "google_flights",
                "google_hotels",
                "popular_products",
            ],
        )
    )
    newly_added_hostile = live_features.intersection(
        hostile_features
    ) - cached_features.intersection(hostile_features)
    if newly_added_hostile:
        return (
            False,
            f"Final Validation Failed: Live SERP contains new hostile features: {', '.join(newly_added_hostile)}.",
        )

    # Non-blog content check (configurable domains & threshold)
    top_5_organic = live_serp_data.get("top_organic_results", [])[:5]
    non_blog_domains_cfg = set(client_cfg.get("final_validation_non_blog_domains", []))
    ugc_domains_cfg = set(
        client_cfg.get("ugc_and_parasite_domains", [])
    )  # Get from config

    hostile_domains = non_blog_domains_cfg.union(ugc_domains_cfg).union(
        client_cfg.get("competitor_blacklist_domains", [])
    )  # Combine all relevant hostile domains

    non_blog_count = sum(
        1
        for item in top_5_organic
        if any(domain in item.get("url", "") for domain in hostile_domains)
    )
    if non_blog_count >= client_cfg.get("max_non_blog_results", 4):
        return (
            False,
            "Final Validation Failed: Live SERP is dominated by non-blog/UGC/blacklisted content.",
        )

    # AI Overview comprehensiveness check (configurable threshold)
    disable_ai_overview_check = client_cfg.get("disable_ai_overview_check", False)
    if not disable_ai_overview_check:
        ai_overview_content = live_serp_data.get("ai_overview_content", "")
        if ai_overview_content and len(ai_overview_content.split()) > client_cfg.get(
            "max_ai_overview_words", 250
        ):
            return (
                False,
                "Final Validation Failed: AI Overview is too comprehensive, making a blog post redundant.",
            )

    # Organic visibility check (pixel ranking, configurable threshold)
    max_pixel_y = client_cfg.get("max_first_organic_y_pixel")
    if max_pixel_y is not None:
        first_organic_y = live_serp_data.get("first_organic_y_pixel")
        if first_organic_y is None:
            # Cannot check visibility without pixel data, proceed if other checks pass
            pass
        elif first_organic_y > max_pixel_y:
            return (
                False,
                f"Final Validation Failed: First organic result is too far down ({first_organic_y}px > {max_pixel_y}px).",
            )

    # NEW: LCP Check
    avg_lcp = live_serp_data.get("avg_page_timing", {}).get("largest_contentful_paint")
    if avg_lcp is not None and avg_lcp > client_cfg.get("max_avg_lcp_time", 4000):
        return (
            False,
            f"Final Validation Failed: Live SERP indicates poor page speed (Avg LCP: {avg_lcp}ms).",
        )

    return True, "Final validation passed."


# --- END NEW FUNCTION ---


def run_analysis_phase(
    opportunity: Dict[str, Any],
    openai_client: OpenAIClientWrapper,
    dataforseo_client: DataForSEOClientV2,
    client_cfg: Dict[str, Any],
    blueprint_factory: BlueprintFactory,
    scoring_engine: ScoringEngine,
    selected_competitor_urls: Optional[List[str]] = None,
) -> Tuple[Dict[str, Any], float]:
    logger = logging.getLogger(__name__)
    keyword = opportunity.get("keyword")
    logger.info(f"--- Starting Deep-Dive Analysis Phase for '{keyword}' ---")

    total_api_cost = 0.0

    serp_analyzer = FullSerpAnalyzer(dataforseo_client, client_cfg)
    competitor_analyzer = FullCompetitorAnalyzer(dataforseo_client, client_cfg)
    content_analyzer = ContentAnalyzer(openai_client, client_cfg)
    strategy_engine = StrategicDecisionEngine(client_cfg)
    # Blueprint factory is passed in, no need to re-initialize

    # 1. Make the single expensive, live SERP call for analysis
    logger.info(f"Making live SERP call for analysis of '{keyword}'...")
    serp_overview, serp_api_cost = serp_analyzer.analyze_serp(keyword)
    total_api_cost += serp_api_cost
    if not serp_overview:
        raise ValueError("Failed to retrieve live SERP data for analysis.")

    # VALIDATION GATE IS NOW REMOVED FROM THIS FUNCTION
    logger.info(f"Proceeding with full analysis for '{keyword}'.")

    # 2. On-Page competitor metadata and content analysis
    top_organic_urls = [
        result["url"]
        for result in serp_overview.get("top_organic_results", [])[
            : client_cfg.get("num_competitors_to_analyze", 5)
        ]
    ]
    competitor_analysis, competitor_api_cost = competitor_analyzer.analyze_competitors(
        top_organic_urls, selected_competitor_urls
    )
    total_api_cost += competitor_api_cost

    # 3. Content Intelligence Synthesis using the full content
    content_intelligence, content_api_cost = (
        content_analyzer.synthesize_content_intelligence(
            competitor_analysis,
            keyword,
            serp_overview.get("dominant_content_format", "Comprehensive Article"),
        )
    )
    total_api_cost += content_api_cost

    # 4. Determine Strategy
    recommended_strategy = strategy_engine.determine_strategy(
        serp_overview, competitor_analysis, content_intelligence
    )

    # 5. AI Content Outline Generation
    ai_outline, outline_api_cost = content_analyzer.generate_ai_outline(
        keyword, serp_overview, content_intelligence
    )
    total_api_cost += outline_api_cost
    content_intelligence.update(ai_outline)

    # 6. Assemble the final Blueprint
    analysis_data = {
        "serp_overview": serp_overview,
        "competitor_analysis": competitor_analysis,
        "content_intelligence": content_intelligence,
        "recommended_strategy": recommended_strategy,
    }

    blueprint = blueprint_factory.create_blueprint(
        seed_topic=keyword,
        winning_keyword_data=opportunity.get("full_data", {}).copy(),
        analysis_data=analysis_data,
        total_api_cost=total_api_cost,
        client_id=opportunity.get("client_id"),
    )

    opportunity["blueprint"] = blueprint

    # --- RE-SCORING ---
    # Re-calculate the strategic score with the new, rich data from the live SERP call
    # This ensures the score is based on the most accurate, up-to-date information
    final_score, final_score_breakdown = scoring_engine.calculate_score(opportunity)
    opportunity["strategic_score"] = final_score
    opportunity["score_breakdown"] = final_score_breakdown
    opportunity["full_data"]["strategic_score"] = final_score
    opportunity["full_data"]["score_breakdown"] = final_score_breakdown

    logger.info(f"  -> Final, updated strategic score: {final_score}")
    logger.info(f"  -> Total API Cost for Blueprint Generation: ${total_api_cost:.4f}")
    logger.info("--- Deep-Dive Analysis Phase Complete ---")

    return opportunity, total_api_cost
</file>

<file path="backend/pipeline/step_05_strategy/decision_engine.py">
import logging
from typing import Dict, Any, List
import json


class StrategicDecisionEngine:
    """
    Analyzes SERP and competitor data to recommend a specific content strategy.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def determine_strategy(
        self,
        serp_overview: Dict[str, Any],
        competitor_analysis: List[Dict[str, Any]],
        content_intelligence: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Determines the optimal content format and strategic goal based on the analysis data.
        """
        content_format = serp_overview.get(
            "dominant_content_format", "Comprehensive Article"
        )
        strategic_goal = "Create a definitive guide that outranks competitors through superior depth and quality."

        top_results = serp_overview.get("top_organic_results", [])

        # --- START MODIFICATION ---
        # Check if deep analysis was performed to adjust logic
        deep_analysis_enabled = self.config.get(
            "enable_deep_competitor_analysis", False
        )

        # NEW: Repurpose focus_competitors for SERP-only mode
        focus_competitors_info = []
        if deep_analysis_enabled and competitor_analysis:
            focus_competitors_info = [
                {"url": c.get("url"), "onpage_score": c.get("onpage_score")}
                for c in competitor_analysis
                if c and c.get("url")
            ][:3]
        else:
            # In SERP-only mode, we can show top organic results as "focus competitors"
            focus_competitors_info = [
                {"url": r.get("url"), "title": r.get("title")} for r in top_results[:3]
            ]

        # NEW: Detect rating-heavy SERPs based on serp_overview data
        rating_count = sum(
            1
            for r in top_results
            if r.get("rating") and r["rating"].get("value") is not None
        )
        avg_rating_value = (
            sum(
                r["rating"]["value"]
                for r in top_results
                if r.get("rating") and r["rating"].get("value")
            )
            / rating_count
            if rating_count > 0
            else 0
        )

        if rating_count >= 3 and avg_rating_value >= 4.0:
            content_format = "Review Article"
            strategic_goal = "Produce an authoritative review or comparison that leverages strong social proof and clearly outlines pros/cons, aiming for rich snippets."
            # Prioritize this strategy by returning early after setting it
            return {
                "content_format": content_format,
                "strategic_goal": strategic_goal,
                "focus_competitors": focus_competitors_info,
                "final_qualification_assessment": {
                    "scorecard": self.generate_qualification_scorecard(
                        {
                            "serp_overview": serp_overview,
                            "competitor_analysis": competitor_analysis,
                            "content_intelligence": content_intelligence,
                        }
                    ),
                    **self._determine_final_recommendation(
                        self.generate_qualification_scorecard(
                            {
                                "serp_overview": serp_overview,
                                "competitor_analysis": competitor_analysis,
                                "content_intelligence": content_intelligence,
                            }
                        )
                    ),
                },
            }

        # ... (existing dynamic content format recommendations, e.g., Recipe, Scholarly, etc. - no change) ...

        # Rule: Weak competition (applies only if deep analysis was performed)
        if (
            content_format == "Comprehensive Article"
            and deep_analysis_enabled
            and competitor_analysis
        ):
            onpage_scores = [
                c.get("onpage_score")
                for c in competitor_analysis
                if c and c.get("onpage_score")
            ]
            if onpage_scores and (sum(onpage_scores) / len(onpage_scores)) < 60:
                strategic_goal = "Exploit the technical weaknesses of competitors by creating a fast, well-structured, and technically superior article."

        # FINAL QUALIFICATION GATE
        scorecard = self.generate_qualification_scorecard(
            {
                "serp_overview": serp_overview,
                "competitor_analysis": competitor_analysis,
                "content_intelligence": content_intelligence,
            }
        )
        recommendation = self._determine_final_recommendation(scorecard)
        # --- END MODIFICATION ---

        return {
            "content_format": content_format,
            "strategic_goal": strategic_goal,
            "focus_competitors": focus_competitors_info,  # Use the conditionally populated info
            "final_qualification_assessment": {
                "scorecard": scorecard,
                **recommendation,
            },
        }

    def generate_qualification_scorecard(self, analysis_data: dict) -> dict:
        """Generates a scorecard of qualification factors, adapted for SERP-only mode."""
        serp_overview = analysis_data.get("serp_overview", {})
        competitor_analysis = analysis_data.get("competitor_analysis", [])

        # --- START MODIFICATION ---
        deep_analysis_enabled = self.config.get(
            "enable_deep_competitor_analysis", False
        )

        hostility_score = 0
        for item in serp_overview.get("items", []):  # Iterate through raw SERP items
            if item.get("rank_absolute", 99) <= 10:
                # Count known hostile/attention-grabbing features
                if item.get("type") in [
                    "video",
                    "local_pack",
                    "carousel",
                    "twitter",
                    "shopping",
                    "app",
                    "short_videos",
                    "images",
                ]:
                    hostility_score += 1
                # Knowledge Graph with AI Overview is a stronger signal
                elif item.get("type") == "knowledge_graph" and (
                    "ai_overview_item" in json.dumps(item)
                ):  # Check for AI overview within KG items
                    hostility_score += 2
                elif item.get("type") == "ai_overview":  # Direct AI overview item
                    hostility_score += 2

        is_hostile_serp_environment = hostility_score > 5
        has_ai_overview = serp_overview.get("serp_has_ai_overview", False) or (
            "ai_overview_content" in serp_overview
            and serp_overview["ai_overview_content"] is not None
        )

        # Average competitor weaknesses calculation
        average_competitor_weaknesses = 0
        if deep_analysis_enabled and competitor_analysis:
            technical_warnings = [
                w
                for comp in competitor_analysis
                for w in comp.get("technical_warnings", [])
            ]
            average_competitor_weaknesses = (
                (len(technical_warnings) / len(competitor_analysis))
                if competitor_analysis
                else 0
            )
        else:
            # If deep analysis is disabled, we cannot assess technical weaknesses directly,
            # so we could default to a neutral or slightly positive value to avoid premature disqualification.
            average_competitor_weaknesses = 2  # Assume a moderate level if unknown

        # Has clear content angle (now based purely on content_intelligence from SERP)
        content_intelligence = analysis_data.get("content_intelligence", {})
        has_clear_content_angle = bool(
            content_intelligence.get("unique_angles_to_include")
            or content_intelligence.get("core_questions_answered_by_competitors")
        )

        # Is intent well-defined (now based on all SERP features)
        is_intent_well_defined = bool(
            serp_overview.get("paa_questions")
            or serp_overview.get("extracted_serp_features")
            or serp_overview.get("top_organic_faqs")  # NEW
            or serp_overview.get("top_organic_sitelinks")  # NEW
        )

        return {
            "hostility_score": hostility_score,
            "is_hostile_serp_environment": is_hostile_serp_environment,
            "has_ai_overview": has_ai_overview,
            "average_competitor_weaknesses": average_competitor_weaknesses,
            "has_clear_content_angle": has_clear_content_angle,
            "is_intent_well_defined": is_intent_well_defined,
        }
        # --- END MODIFICATION ---

    def _determine_final_recommendation(self, scorecard: dict) -> dict:
        """Determines the final go/no-go recommendation."""
        confidence_score = 100
        positive_factors = []
        negative_factors = []

        if scorecard["is_hostile_serp_environment"]:
            confidence_score -= 30
            negative_factors.append("SERP is dominated by non-article formats.")
        if scorecard["has_ai_overview"]:
            confidence_score -= 15
            negative_factors.append(
                "Google AI Overview is present, increasing ranking difficulty."
            )
        if scorecard["average_competitor_weaknesses"] < 2:
            confidence_score -= 20
            negative_factors.append("Competitors are technically strong.")
        if scorecard["average_competitor_weaknesses"] > 4:
            confidence_score += 10
            positive_factors.append(
                "Competitors show significant technical weaknesses."
            )
        if not scorecard["has_clear_content_angle"]:
            confidence_score -= 40
            negative_factors.append("No clear content differentiation angle was found.")
        if scorecard["has_clear_content_angle"]:
            positive_factors.append("A unique content angle has been identified.")
        if scorecard["is_intent_well_defined"]:
            positive_factors.append("User intent is well-defined by SERP features.")

        if confidence_score >= 80:
            recommendation = "Proceed"
        elif 50 <= confidence_score < 80:
            recommendation = "Proceed with Caution"
        else:
            recommendation = "Reject"

        return {
            "recommendation": recommendation,
            "confidence_score": confidence_score,
            "positive_factors": positive_factors,
            "negative_factors": negative_factors,
        }
</file>

<file path="backend/pipeline/step_06_content_creation/__init__.py">
# This file marks the directory as a Python package.
</file>

<file path="backend/pipeline/__init__.py">
# backend/pipeline/__init__.py
from .orchestrator.main import WorkflowOrchestrator as WorkflowOrchestrator
</file>

<file path="backend/pipeline/orchestrator.py">
# backend/pipeline/orchestrator.py
import logging

from app_config.manager import ConfigManager
from data_access.database_manager import DatabaseManager
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from external_apis.openai_client import OpenAIClientWrapper
from agents.image_generator import ImageGenerator
from agents.social_media_crafter import SocialMediaCrafter
from agents.internal_linking_suggester import InternalLinkingSuggester
from agents.html_formatter import HtmlFormatter
from core.blueprint_factory import BlueprintFactory
from agents.content_auditor import ContentAuditor
from agents.prompt_assembler import DynamicPromptAssembler
from pipeline.step_03_prioritization.scoring_engine import ScoringEngine
from pipeline.step_01_discovery.cannibalization_checker import (
    CannibalizationChecker,
)
from jobs import JobManager

from .orchestrator.discovery_orchestrator import DiscoveryOrchestrator
from .orchestrator.analysis_orchestrator import AnalysisOrchestrator
from .orchestrator.content_orchestrator import ContentOrchestrator
from .orchestrator.image_orchestrator import ImageOrchestrator
from .orchestrator.social_orchestrator import SocialOrchestrator
from .orchestrator.validation_orchestrator import ValidationOrchestrator
from .orchestrator.workflow_orchestrator import WorkflowOrchestrator
from .orchestrator.cost_estimator import CostEstimator

logger = logging.getLogger(__name__)


class WorkflowOrchestrator(
    DiscoveryOrchestrator,
    AnalysisOrchestrator,
    ContentOrchestrator,
    ImageOrchestrator,
    SocialOrchestrator,
    ValidationOrchestrator,
    WorkflowOrchestrator,
    CostEstimator,
):
    def __init__(
        self,
        global_cfg_manager: ConfigManager,
        db_manager: DatabaseManager,
        client_id: str,
        job_manager: "JobManager",
    ):
        self.global_cfg_manager = global_cfg_manager
        self.db_manager = db_manager
        self.client_id = client_id
        self.job_manager = job_manager
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = self.global_cfg_manager.load_client_config(
            self.client_id, self.db_manager
        )

        self.openai_client = OpenAIClientWrapper(
            api_key=self.client_cfg.get("openai_api_key"), client_cfg=self.client_cfg
        )
        self.dataforseo_client = DataForSEOClientV2(
            login=self.client_cfg["dataforseo_login"],
            password=self.client_cfg["dataforseo_password"],
            config=self.client_cfg,
            db_manager=self.db_manager,
            enable_cache=self.client_cfg.get("enable_cache", True),
        )

        self.image_generator = ImageGenerator(self.client_cfg)
        self.social_crafter = SocialMediaCrafter(self.openai_client, self.client_cfg)
        self.internal_linking_suggester = InternalLinkingSuggester(
            self.openai_client, self.client_cfg, self.db_manager
        )
        self.html_formatter = HtmlFormatter()
        self.blueprint_factory = BlueprintFactory(
            self.openai_client, self.client_cfg, self.dataforseo_client, self.db_manager
        )
        self.scoring_engine = ScoringEngine(self.client_cfg)
        self.cannibalization_checker = CannibalizationChecker(
            self.client_cfg.get("target_domain"),
            self.dataforseo_client,
            self.client_cfg,
            self.db_manager,
        )
        self.content_auditor = ContentAuditor()
        self.prompt_assembler = DynamicPromptAssembler(self.db_manager)
</file>

<file path="backend/services/discovery_service.py">
# services/discovery_service.py

from typing import Dict, Any
from data_access.database_manager import DatabaseManager


class DiscoveryService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def create_discovery_run(self, client_id: str, parameters: Dict[str, Any]) -> int:
        """Creates a new discovery run record and returns its ID."""
        return self.db_manager.create_discovery_run(client_id, parameters)

    def get_disqualification_reasons(self, run_id: int) -> Dict[str, int]:
        """
        Retrieves a summary of disqualification reasons for a specific discovery run.
        """
        keywords = self.db_manager.get_keywords_for_run(run_id)

        disqualification_reasons = {}
        for keyword in keywords:
            if keyword.get("blog_qualification_status") == "rejected":
                reason = keyword.get("blog_qualification_reason")
                if reason:
                    disqualification_reasons[reason] = (
                        disqualification_reasons.get(reason, 0) + 1
                    )

        return disqualification_reasons
</file>

<file path="backend/services/disqualification_service.py">
# services/disqualification_service.py
import json
from typing import List, Dict, Any
from data_access.database_manager import DatabaseManager


class DisqualificationService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def disqualify(
        self, client_id: str, keywords: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Applies disqualification rules to a list of keywords.
        """
        qualification_settings = self.db_manager.get_qualification_settings(client_id)
        disqualification_rules = json.loads(
            qualification_settings.get("disqualification_rules", "[]")
        )

        brand_keywords = qualification_settings.get("brand_keywords", [])
        competitor_brand_keywords = qualification_settings.get(
            "competitor_brand_keywords", []
        )

        qualified_keywords = []
        for keyword in keywords:
            disqualified = False
            keyword_text = keyword.get("keyword", "").lower()

            if any(brand_kw in keyword_text for brand_kw in brand_keywords):
                continue

            if any(brand_kw in keyword_text for brand_kw in competitor_brand_keywords):
                continue

            for rule in disqualification_rules:
                field = rule.get("field")
                operator = rule.get("operator")
                value = rule.get("value")

                field_value = keyword
                for key in field.split("."):
                    field_value = field_value.get(key, {})

                if operator == "=" and field_value == value:
                    disqualified = True
                    break
                elif operator == ">" and field_value > value:
                    disqualified = True
                    break
                elif operator == "<" and field_value < value:
                    disqualified = True
                    break

            if not disqualified:
                qualified_keywords.append(keyword)

        return qualified_keywords
</file>

<file path="backend/services/keyword_data_aggregator.py">
# services/keyword_data_aggregator.py

from typing import List, Dict, Any, Optional
from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class KeywordDataAggregator:
    def __init__(
        self, dataforseo_client: DataForSEOClientV2, client_cfg: Dict[str, Any]
    ):
        self.dataforseo_client = dataforseo_client
        self.client_cfg = client_cfg

    def get_keyword_data(
        self,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        limit: Optional[int],
        depth: Optional[int],
        ignore_synonyms: Optional[bool],
    ) -> List[Dict[str, Any]]:
        """
        Calls the keyword discovery endpoints, deduplicates the results, and returns a unified list of keyword data objects.
        """
        # W21 FIX: Set default order_by if not provided, now structured as a dict
        if not order_by:
            ideas_suggestions_orderby = [
                "keyword_properties.keyword_difficulty,asc",
                "keyword_info.search_volume,desc",
            ]
            related_orderby = [
                "keyword_data.keyword_properties.keyword_difficulty,asc",
                "keyword_data.keyword_info.search_volume,desc",
            ]
            structured_orderby = {
                "ideas": ideas_suggestions_orderby,
                "suggestions": ideas_suggestions_orderby,
                "related": related_orderby,
            }
        else:
            # If order_by is provided from frontend, structure it for each mode
            related_orderby = [f"keyword_data.{rule}" for rule in order_by]
            structured_orderby = {
                "ideas": order_by,
                "suggestions": order_by,
                "related": related_orderby,
            }

        # Structure filters for the client
        ideas_suggestions_filters = []
        if filters:
            for f in filters:
                new_filter = f.copy()
                if "field" in new_filter and new_filter["field"].startswith(
                    "keyword_data."
                ):
                    new_filter["field"] = new_filter["field"][len("keyword_data.") :]
                ideas_suggestions_filters.append(new_filter)

        structured_filters = {
            "ideas": ideas_suggestions_filters,
            "suggestions": ideas_suggestions_filters,
            "related": filters,  # Related keeps the prefix
        }

        all_ideas, _ = self.dataforseo_client.get_keyword_ideas(
            seed_keywords=seed_keywords,
            location_code=self.client_cfg.get("location_code"),
            language_code=self.client_cfg.get("language_code"),
            client_cfg=self.client_cfg,
            discovery_modes=discovery_modes,
            filters=structured_filters,
            order_by=structured_orderby,
            limit=limit,
            depth=depth,
            ignore_synonyms=ignore_synonyms,
        )

        final_keywords_deduplicated = []
        seen_keywords = set()
        for item in all_ideas:
            kw_text = item.get("keyword", "").lower()
            if kw_text and kw_text not in seen_keywords:
                final_keywords_deduplicated.append(item)
                seen_keywords.add(kw_text)

        return final_keywords_deduplicated
</file>

<file path="backend/services/opportunities_service.py">
# services/opportunities_service.py

from typing import List, Dict, Any, Tuple
from data_access.database_manager import DatabaseManager


class OpportunitiesService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def get_all_opportunities(
        self, client_id: str, params: Dict[str, Any]
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        Retrieves keyword opportunities for a client, supporting filtering, sorting, and pagination.
        Returns (opportunities_list, total_count).
        """
        return self.db_manager.get_all_opportunities(client_id, params)

    def get_all_opportunities_summary(
        self, client_id: str, params: Dict[str, Any], select_columns: str = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        Retrieves a lightweight summary of keyword opportunities for a client.
        Returns (opportunities_list, total_count).
        """
        return self.db_manager.get_all_opportunities(
            client_id, params, summary=True, select_columns=select_columns
        )

    def get_opportunities_by_category(
        self, client_id: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Retrieves all opportunities for a client, grouped by category.
        """
        opportunities, _ = self.db_manager.get_all_opportunities(client_id, {})

        opportunities_by_category = {}
        for opportunity in opportunities:
            categories = opportunity.get("keyword_info", {}).get("categories", [])
            for category in categories:
                if category not in opportunities_by_category:
                    opportunities_by_category[category] = []
                opportunities_by_category[category].append(opportunity)

        return opportunities_by_category

    def get_opportunities_by_cluster(
        self, client_id: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Retrieves all opportunities for a client, grouped by cluster.
        """
        opportunities, _ = self.db_manager.get_all_opportunities(client_id, {})

        opportunities_by_cluster = {}
        for opportunity in opportunities:
            cluster_name = opportunity.get("cluster_name")
            if cluster_name:
                if cluster_name not in opportunities_by_cluster:
                    opportunities_by_cluster[cluster_name] = []
                opportunities_by_cluster[cluster_name].append(opportunity)

        return opportunities_by_cluster
</file>

<file path="backend/services/qualification_service.py">
# services/qualification_service.py

from typing import List, Dict, Any, Optional
from .keyword_data_aggregator import KeywordDataAggregator
from .disqualification_service import DisqualificationService
from .scoring_service import ScoringService
from .serp_analysis_service import SerpAnalysisService


class QualificationService:
    def __init__(
        self,
        keyword_data_aggregator: KeywordDataAggregator,
        disqualification_service: DisqualificationService,
        scoring_service: ScoringService,
        serp_analysis_service: SerpAnalysisService,
    ):
        self.keyword_data_aggregator = keyword_data_aggregator
        self.disqualification_service = disqualification_service
        self.scoring_service = scoring_service
        self.serp_analysis_service = serp_analysis_service

    def qualify_keywords(
        self,
        client_id: str,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        limit: Optional[int],
        depth: Optional[int],
        ignore_synonyms: Optional[bool],
    ) -> List[Dict[str, Any]]:
        """
        Orchestrates the entire qualification flow.
        """
        keyword_data = self.keyword_data_aggregator.get_keyword_data(
            seed_keywords,
            discovery_modes,
            filters,
            order_by,
            limit,
            depth,
            ignore_synonyms,
        )

        analyzed_keywords = self.serp_analysis_service.analyze_keywords_serp(
            keyword_data
        )

        qualified_keywords = self.disqualification_service.disqualify(
            client_id, analyzed_keywords
        )

        scored_keywords = []
        for keyword in qualified_keywords:
            score, breakdown = self.scoring_service.calculate_score(client_id, keyword)
            keyword["strategic_score"] = score
            keyword["score_breakdown"] = breakdown
            scored_keywords.append(keyword)

        return scored_keywords
</file>

<file path="backend/services/scoring_service.py">
# services/scoring_service.py

from typing import Dict, Any, Tuple
from data_access.database_manager import DatabaseManager


class ScoringService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def calculate_score(
        self, client_id: str, keyword_data: Dict[str, Any]
    ) -> Tuple[float, Dict[str, Any]]:
        """
        Calculates a strategic score for a keyword based on the client's qualification settings.
        """
        qualification_settings = self.db_manager.get_qualification_settings(client_id)

        traffic_potential_weight = qualification_settings.get(
            "traffic_potential_weight", 0
        )
        cpc_weight = qualification_settings.get("cpc_weight", 0)
        search_intent_weight = qualification_settings.get("search_intent_weight", 0)
        competitor_strength_weight = qualification_settings.get(
            "competitor_strength_weight", 0
        )
        serp_features_weight = qualification_settings.get("serp_features_weight", 0)
        trend_weight = qualification_settings.get("trend_weight", 0)
        seasonality_weight = qualification_settings.get("seasonality_weight", 0)
        serp_volatility_weight = qualification_settings.get("serp_volatility_weight", 0)

        search_volume = keyword_data.get(
            "keyword_info_normalized_with_clickstream", {}
        ).get("search_volume", 0)
        keyword_difficulty = keyword_data.get("keyword_properties", {}).get(
            "keyword_difficulty", 0
        )
        cpc = keyword_data.get("keyword_info", {}).get("cpc", 0)
        main_intent = keyword_data.get("search_intent_info", {}).get("main_intent")
        avg_referring_domains = keyword_data.get("avg_backlinks_info", {}).get(
            "referring_domains", 0
        )
        serp_item_types = keyword_data.get("serp_info", {}).get("serp_item_types", [])
        monthly_searches = keyword_data.get("keyword_info", {}).get(
            "monthly_searches", []
        )
        serp_last_updated_days_ago = keyword_data.get("serp_overview", {}).get(
            "serp_last_updated_days_ago"
        )
        serp_update_interval_days = keyword_data.get("serp_overview", {}).get(
            "serp_update_interval_days"
        )

        traffic_potential_score = search_volume * (1 - (keyword_difficulty / 100))
        cpc_score = cpc * 100
        competitor_strength_score = 100 - (avg_referring_domains / 10)
        serp_features_score = 0
        if "featured_snippet" in serp_item_types:
            serp_features_score += 20
        if "video" in serp_item_types:
            serp_features_score += 10
        if "ai_overview" in serp_item_types:
            serp_features_score -= 10

        trend_score = 0
        if len(monthly_searches) > 1:
            latest_search_volume = monthly_searches[0]["search_volume"]
            oldest_search_volume = monthly_searches[-1]["search_volume"]
            if oldest_search_volume > 0:
                trend_score = (
                    (latest_search_volume - oldest_search_volume) / oldest_search_volume
                ) * 100

        seasonality_score = 0
        if len(monthly_searches) > 11:
            # Calculate the average search volume for each month
            monthly_averages = [0] * 12
            for i in range(12):
                monthly_averages[i] = monthly_searches[i]["search_volume"]

            # Calculate the standard deviation of the monthly averages
            mean = sum(monthly_averages) / 12
            variance = sum([((x - mean) ** 2) for x in monthly_averages]) / 12
            std_dev = variance**0.5

            # Normalize the standard deviation to a score between 0 and 100
            if mean > 0:
                seasonality_score = 100 - (std_dev / mean) * 100

        serp_volatility_score = 0
        if (
            serp_last_updated_days_ago is not None
            and serp_update_interval_days is not None
        ):
            if serp_update_interval_days > 0:
                serp_volatility_score = (
                    100 - (serp_last_updated_days_ago / serp_update_interval_days) * 100
                )

        search_intent_score = 0
        if main_intent == "informational":
            search_intent_score = 100 * qualification_settings.get(
                "informational_intent_weight", 0
            )
        elif main_intent == "navigational":
            search_intent_score = 50 * qualification_settings.get(
                "navigational_intent_weight", 0
            )
        elif main_intent == "commercial":
            search_intent_score = 75 * qualification_settings.get(
                "commercial_intent_weight", 0
            )
        elif main_intent == "transactional":
            search_intent_score = 90 * qualification_settings.get(
                "transactional_intent_weight", 0
            )

        score = (
            (traffic_potential_score * traffic_potential_weight)
            + (cpc_score * cpc_weight)
            + (search_intent_score * search_intent_weight)
            + (competitor_strength_score * competitor_strength_weight)
            + (serp_features_score * serp_features_weight)
            + (trend_score * trend_weight)
            + (seasonality_score * seasonality_weight)
            + (serp_volatility_score * serp_volatility_weight)
        )

        breakdown = {
            "traffic_potential_score": traffic_potential_score,
            "cpc_score": cpc_score,
            "search_intent_score": search_intent_score,
            "competitor_strength_score": competitor_strength_score,
            "serp_features_score": serp_features_score,
            "trend_score": trend_score,
            "seasonality_score": seasonality_score,
            "serp_volatility_score": serp_volatility_score,
        }

        return score, breakdown
</file>

<file path="backend/services/serp_analysis_service.py">
# backend/services/serp_analysis_service.py

from typing import Dict, Any, List
from backend.core.serp_analyzer import FullSerpAnalyzer
from backend.external_apis.dataforseo_client_v2 import DataForSEOClientV2


class SerpAnalysisService:
    def __init__(self, dataforseo_client: DataForSEOClientV2, config: Dict[str, Any]):
        self.serp_analyzer = FullSerpAnalyzer(dataforseo_client, config)
        self.dataforseo_client = dataforseo_client
        self.config = config

    def analyze_serp_for_blog_opportunity(
        self, serp_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Analyzes the SERP data to determine if there is a good opportunity for a blog article.
        """
        if not serp_results or not serp_results.get("top_organic_results"):
            return {
                "blog_opportunity": False,
                "opportunity_score": 0,
                "competitor_urls": [],
            }

        top_results = serp_results.get("top_organic_results", [])
        blog_count = 0
        competitor_urls = []

        for result in top_results[:10]:
            page_type = result.get("page_type")
            if page_type == "blog" or page_type == "news":
                blog_count += 1
                competitor_urls.append(result.get("url"))

        # Simple logic: if there are at least 3 blog/news articles in the top 10,
        # it's a good opportunity.
        blog_opportunity = blog_count >= 3
        opportunity_score = blog_count / 10.0

        return {
            "blog_opportunity": blog_opportunity,
            "opportunity_score": opportunity_score,
            "competitor_urls": competitor_urls,
        }

    def analyze_keywords_serp(
        self, keywords_data: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Analyzes the SERP data for a list of keywords.
        """
        for keyword_data in keywords_data:
            keyword = keyword_data.get("keyword")
            if keyword:
                serp_results, _ = self.serp_analyzer.analyze_serp(keyword)
                serp_analysis = self.analyze_serp_for_blog_opportunity(serp_results)

                # Add competitor content if it's a blog opportunity
                if serp_analysis["blog_opportunity"]:
                    competitor_content, _ = (
                        self.dataforseo_client.get_content_onpage_data(
                            serp_analysis["competitor_urls"], self.config
                        )
                    )
                    serp_analysis["competitor_content"] = competitor_content
                else:
                    serp_analysis["competitor_content"] = []

                keyword_data["serp_analysis"] = serp_analysis

        return keywords_data
</file>

<file path="backend/tests/test_content_generation.py">
import pytest
from unittest.mock import MagicMock, patch
from backend.external_apis.openai_client import OpenAIClientWrapper
from backend.pipeline.step_04_analysis.content_analyzer import ContentAnalyzer

@pytest.fixture
def mock_openai_client():
    """Mocks the OpenAIClientWrapper to avoid actual API calls."""
    client = MagicMock(spec=OpenAIClientWrapper)
    client.latest_cost = 0.1
    return client

@pytest.fixture
def content_analyzer(mock_openai_client):
    """Provides a ContentAnalyzer instance with a mocked OpenAI client."""
    config = {
        "default_model": "gpt-5-nano",
        "max_completion_tokens_for_generation": 8192,
    }
    return ContentAnalyzer(openai_client=mock_openai_client, config=config)

def test_openai_client_enforces_json_schema_mode():
    """
    Verifies that the OpenAI client wrapper correctly uses 'json_schema' mode
    for gpt-5-nano and gpt-5-mini when a schema is provided.
    """
    with patch('openai.OpenAI') as mock_openai:
        # Arrange
        mock_create = MagicMock()
        mock_openai.return_value.chat.completions.create = mock_create

        client_wrapper = OpenAIClientWrapper(api_key="fake_key", client_cfg={})
        messages = [{"role": "user", "content": "Test prompt"}]
        schema = {"type": "object", "properties": {"key": {"type": "string"}}}

        # Act
        client_wrapper.call_chat_completion(
            messages=messages,
            schema=schema,
            model='gpt-5-nano' # Test with one of the target models
        )

        # Assert
        mock_create.assert_called_once()
        call_args = mock_create.call_args.kwargs
        assert "response_format" in call_args
        assert call_args["response_format"]["type"] == "json_schema"
        assert "json_schema" in call_args["response_format"]

def test_full_content_analysis_and_outline_workflow(content_analyzer, mock_openai_client):
    """
    Tests the full content analysis and outline generation workflow,
    ensuring it handles mocked AI responses correctly and produces a valid output.
    """
    # Arrange: Mock the return values for the two AI calls
    mock_synthesis_response = {
        "unique_angles_to_include": ["Angle 1", "Angle 2"],
        "key_entities_from_competitors": ["Entity A", "Entity B"],
        "core_questions_answered_by_serp": ["Question 1?", "Question 2?"],
        "identified_content_gaps": ["Gap A", "Gap B"],
    }
    mock_outline_response = {
        "article_structure": [
            {"h2": "Introduction", "h3s": []},
            {"h2": "Main Topic", "h3s": ["Sub-topic 1", "Sub-topic 2"]},
            {"h2": "Conclusion", "h3s": []},
        ]
    }
    # The client will return these values in order for the two calls
    mock_openai_client.call_chat_completion.side_effect = [
        (mock_synthesis_response, None),
        (mock_outline_response, None)
    ]

    keyword = "test keyword"
    serp_overview = {"paa_questions": ["PAA Question 1?"]}

    # Act: Run the synthesis and outline generation
    content_intelligence, total_cost_synthesis = content_analyzer.synthesize_content_intelligence(
        keyword=keyword,
        serp_overview=serp_overview,
        competitor_analysis=[] # Use the SERP-only path
    )

    outline, total_cost_outline = content_analyzer.generate_ai_outline(
        keyword=keyword,
        serp_overview=serp_overview,
        content_intelligence=content_intelligence
    )

    # Assert
    assert total_cost_synthesis == 0.1
    assert total_cost_outline == 0.1
    assert "unique_angles_to_include" in content_intelligence
    assert "article_structure" in outline
    assert len(outline["article_structure"]) == 3
    assert outline["article_structure"][1]["h2"] == "Main Topic"
    assert "Sub-topic 1" in outline["article_structure"][1]["h3s"]
    assert mock_openai_client.call_chat_completion.call_count == 2

print("Test script created at backend/tests/test_content_generation.py")
print("You can run this test using pytest:")
print("pytest backend/tests/test_content_generation.py")
</file>

<file path="backend/tests/test_filter_transformation.py">
# tests/test_filter_transformation.py
from pipeline.step_01_discovery.keyword_discovery.expander import (
    _transform_filters_for_api,
)


def test_no_filters():
    """Test that None is returned when no filters are provided."""
    assert _transform_filters_for_api(None) is None


def test_single_filter():
    """Test that a single filter is transformed into a flat list."""
    filters = [{"field": "keyword_info.search_volume", "operator": ">", "value": 100}]
    expected = ["keyword_info.search_volume", ">", 100]
    assert _transform_filters_for_api(filters) == expected


def test_multiple_filters():
    """Test that multiple filters are correctly interleaved with the 'and' operator."""
    filters = [
        {"field": "keyword_info.search_volume", "operator": ">", "value": 100},
        {
            "field": "keyword_properties.keyword_difficulty",
            "operator": "<",
            "value": 50,
        },
    ]
    expected = [
        ["keyword_info.search_volume", ">", 100],
        "and",
        ["keyword_properties.keyword_difficulty", "<", 50],
    ]
    assert _transform_filters_for_api(filters) == expected


def test_three_filters():
    """Test that three filters are correctly interleaved with the 'and' operator."""
    filters = [
        {"field": "keyword_info.search_volume", "operator": ">", "value": 100},
        {
            "field": "keyword_properties.keyword_difficulty",
            "operator": "<",
            "value": 50,
        },
        {"field": "keyword_info.cpc", "operator": ">", "value": 0.5},
    ]
    expected = [
        ["keyword_info.search_volume", ">", 100],
        "and",
        ["keyword_properties.keyword_difficulty", "<", 50],
        "and",
        ["keyword_info.cpc", ">", 0.5],
    ]
    assert _transform_filters_for_api(filters) == expected


def test_empty_filter_list():
    """Test that an empty list of filters returns None."""
    assert _transform_filters_for_api([]) is None
</file>

<file path="backend/tests/test_onpage_instant_pages.py">
# tests/test_onpage_instant_pages.py
import os
import json
import logging
import base64
import requests
from dotenv import load_dotenv

TEST_URL = "https://www.wikipedia.org/"
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def test_instant_pages_workflow():
    load_dotenv()
    api_login = os.getenv("DATAFORSEO_LOGIN")
    api_password = os.getenv("DATAFORSEO_PASSWORD")
    assert api_login and api_password, "DATAFORSEO credentials not found in .env file."

    credentials = f"{api_login}:{api_password}"
    headers = {
        "Authorization": f"Basic {base64.b64encode(credentials.encode()).decode()}",
        "Content-Type": "application/json",
    }

    post_data = [{"url": TEST_URL, "enable_browser_rendering": True}]

    response = requests.post(
        "https://api.dataforseo.com/v3/on_page/instant_pages",
        headers=headers,
        data=json.dumps(post_data),
        timeout=120,
    )
    response.raise_for_status()
    response_json = response.json()

    assert response_json["status_code"] == 20000, "API call was not successful."
    task_result = response_json["tasks"][0]["result"][0]
    item = task_result["items"][0]

    assert "meta" in item, "Response missing 'meta' object."
    assert "content" in item["meta"], "Response missing 'meta.content' object."
    assert "plain_text_word_count" in item["meta"]["content"], (
        "Content parsing failed: word count is missing."
    )

    word_count = item["meta"]["content"]["plain_text_word_count"]
    assert isinstance(word_count, int) and word_count > 50, (
        f"Expected a valid word count, got {word_count}."
    )

    logging.info(
        f"SUCCESS: 'instant_pages' test passed. Found word count: {word_count}."
    )
    print(json.dumps(item["meta"]["content"], indent=2))


if __name__ == "__main__":
    test_instant_pages_workflow()
</file>

<file path="backend/Dockerfile">
# Use an official Python runtime as a parent image
FROM python:3.9-slim

# Set the working directory in the container
WORKDIR /app

# Copy the dependencies file to the working directory
COPY requirements.txt .

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copy the content of the local src directory to the working directory
COPY . ./backend/

# Make port 8000 available to the world outside this container
EXPOSE 8000

# Command to run the application
CMD ["uvicorn", "backend.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
</file>

<file path="backend/jobs.py">
# jobs.py
import threading
import time
import uuid
import logging
from typing import Dict, Any, Callable, Optional
from datetime import datetime
from backend.data_access import queries

# Import DatabaseManager
from backend.data_access.database_manager import DatabaseManager

logger = logging.getLogger(__name__)


class JobManager:
    """Manages asynchronous jobs, their status, and results, backed by a database."""

    # MODIFIED: __init__ now requires a db_manager
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.db_manager.fail_stale_jobs()
        # The in-memory job store and lock are no longer needed.
        # self.jobs: Dict[str, Dict[str, Any]] = {}
        # self.lock = threading.Lock()

    def create_job(
        self, target_function: Callable, args: tuple = (), kwargs: dict = {}
    ) -> str:
        """
        Creates a new job, saves its initial state to the DB, starts it in a
        separate thread, and returns its ID.
        """
        job_id = str(uuid.uuid4())
        job_info = {
            "id": job_id,
            "status": "pending",
            "progress": 0,
            "result": None,
            "error": None,
            "started_at": time.time(),
            "finished_at": None,
            "function_name": target_function.__name__,
        }

        # MODIFIED: Save job to DB instead of in-memory dict
        self.db_manager.update_job(job_info)

        logger.info(f"Job {job_id} created for function {target_function.__name__}")
        thread = threading.Thread(
            target=self._run_job, args=(job_id, target_function, args, kwargs)
        )
        thread.daemon = True
        thread.start()
        return job_id

    def update_job_progress(self, job_id: str, step: str, message: str, status: Optional[str] = None):
        """Appends a progress log to the job record in the database."""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "message": message,
        }
        
        # This operation needs to be atomic to prevent race conditions.
        # We'll fetch the current job, update the log, and save it back.
        # A more advanced setup might use a database transaction or a JSON_APPEND function.
        job_info = self.get_job_status(job_id)
        if job_info:
            progress_log = job_info.get("progress_log", [])
            if isinstance(progress_log, str): # Handle case where it might be a JSON string
                try:
                    progress_log = json.loads(progress_log)
                except json.JSONDecodeError:
                    progress_log = []
            
            progress_log.append(log_entry)
            job_info["progress_log"] = progress_log

            # Optionally update the overall job status at the same time
            if status:
                job_info["status"] = status

            self.db_manager.update_job(job_info)

    def _run_job(
        self, job_id: str, target_function: Callable, args: tuple, kwargs: dict
    ):
        """Internal method to execute the target function and update job status in the DB."""
        logger.info(f"Job {job_id} starting. DB manager: {self.db_manager}")
        try:
            # Initialize the progress log
            self.update_job_status(job_id, "running", progress=5)
            self.update_job_progress(job_id, "Job Started", "The workflow is initializing.")
            
            result = target_function(job_id, *args, **kwargs)
            
            self.update_job_progress(job_id, "Job Finished", "The workflow completed successfully.")
            self.update_job_status(job_id, "completed", progress=100, result=result)
            logger.info(f"Job {job_id} completed successfully.")
        except Exception as e:
            error_message = f"Job {job_id} failed: {e}"
            logger.error(error_message, exc_info=True)
            self.update_job_progress(job_id, "Job Failed", str(e))
            self.update_job_status(job_id, "failed", progress=100, error=str(e))


    def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """Retrieves the current status of a job from the database."""
        # MODIFIED: Fetch from DB
        return self.db_manager.get_job(job_id)

    def update_job_status(
        self,
        job_id: str,
        status: str,
        progress: int,
        result: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None,
    ):
        """
        Updates job status using a direct UPDATE query (W10 FIX).
        """
        conn = self.db_manager._get_conn()
        finished_at = (
            datetime.now().timestamp() if status in ["completed", "failed"] else None
        )

        if result or error:
            # If result/error is present, use the original UPDATE_JOB (INSERT OR REPLACE)
            # that handles all fields.
            job_info = self.db_manager.get_job(job_id)
            if job_info:
                job_info["status"] = status
                job_info["progress"] = progress
                job_info["result"] = result
                job_info["error"] = error
                job_info["finished_at"] = finished_at
                self.db_manager.update_job(job_info)
        else:
            # Execute the direct, optimized status/progress update
            # This avoids fetching the entire job record first. (W10 FIX)
            with conn:
                conn.execute(
                    queries.UPDATE_JOB_STATUS_DIRECT,
                    (status, progress, finished_at, job_id),
                )

    # Global job manager instance is no longer initialized here.
    # It will be initialized in api/main.py where it has access to the db_manager.
    # job_manager = JobManager()

    def cancel_job(self, job_id: str) -> bool:
        """Marks a job as 'failed' with a 'cancelled by user' message."""
        job_info = self.get_job_status(job_id)
        if job_info and job_info["status"] in ["pending", "running", "paused"]:
            # The crucial part: mark as failed in the DB so the running thread sees it
            self.update_job_status(
                job_id,
                "failed",
                job_info.get("progress", 0),
                error="Cancelled by user.",
            )
            logger.info(f"Job {job_id} was marked as 'failed' (cancelled by user).")
            return True
        return False
</file>

<file path="backend/requirements.txt">
fastapi
uvicorn
python-dotenv
scikit-learn
sentence-transformers
requests
textstat
bleach
openai
beautifulsoup4
markdown
</file>

<file path="client/my-content-app/src/components/layout/AppSidebar.jsx">
import React from 'react';
import { Layout, Menu, Typography } from 'antd';
import { RocketOutlined, BulbOutlined } from '@ant-design/icons';
import { NavLink, useLocation } from 'react-router-dom';

const { Sider } = Layout;
const { Title } = Typography;

// Menu items for the sidebar
const menuItems = [
  {
    key: '/',
    icon: <RocketOutlined />,
    label: <NavLink to="/">Discovery</NavLink>,
  },
  {
    key: '/opportunities',
    icon: <BulbOutlined />,
    label: <NavLink to="/opportunities">Opportunities</NavLink>,
  },
  // Add other navigation links here as you build new pages
  // {
  //   key: '/pipeline',
  //   icon: <BranchesOutlined />,
  //   label: <NavLink to="/pipeline">Content Pipeline</NavLink>,
  // },
];

const AppSidebar = () => {
  const location = useLocation(); // To highlight the active menu item

  return (
    <Sider collapsible breakpoint="lg" collapsedWidth="0">
      <div style={{ padding: '16px', textAlign: 'center' }}>
        <Title level={4} style={{ color: 'white', margin: 0 }}>Content AI</Title>
      </div>
      <Menu
        theme="dark"
        mode="inline"
        selectedKeys={[location.pathname]} // Highlight the current path
        items={menuItems}
      />
    </Sider>
  );
};

export default AppSidebar;
</file>

<file path="client/my-content-app/src/components/layout/MainLayout.jsx">
import { Menu, Layout, Typography, Button, Space, Select, Spin, Input, Card, Tag } from 'antd'; // Add Button and Typography
import { DashboardOutlined, SettingOutlined, LogoutOutlined, BulbOutlined, RocketOutlined, ClockCircleOutlined } from '@ant-design/icons'; // Add new icons
import { NavLink, useLocation, useNavigate, Outlet } from 'react-router-dom'; // Add useNavigate
import { useAuth } from '../../context/AuthContext'; // NEW
import { useClient } from '../../context/ClientContext'; // NEW
import { getClients, searchAllAssets } from '../../services/clientService'; // NEW
import { useQuery } from 'react-query'; // NEW
import useDebounce from '../../hooks/useDebounce'; // Assuming you have a useDebounce hook
import React, { useState, useEffect } from 'react';

const { Sider, Header, Content } = Layout;
const { Title, Text } = Typography;

// REPLACE the existing `menuItems` definition with this:
const menuItems = [
  {
    key: '/dashboard',
    icon: <DashboardOutlined />,
    label: <NavLink to="/dashboard">Dashboard</NavLink>,
  },
  {
    key: '/opportunities',
    icon: <BulbOutlined />,
    label: <NavLink to="/opportunities">Opportunities</NavLink>,
  },
  {
    key: '/discovery',
    icon: <RocketOutlined />,
    label: <NavLink to="/discovery">Discovery</NavLink>,
  },
  {
    key: '/activity-log',
    icon: <ClockCircleOutlined />,
    label: <NavLink to="/activity-log">Activity Log</NavLink>,
  },
  {
    key: '/settings',
    icon: <SettingOutlined />,
    label: <NavLink to="/settings">Settings</NavLink>,
  },
];

// REPLACE the existing `MainLayout` component with this:
const MainLayout = () => {
  const location = useLocation();
  const navigate = useNavigate();
  const { logout } = useAuth();
  const { clientId, setClientId } = useClient();
  const { data: clients = [], isLoading: isLoadingClients } = useQuery('clients', getClients);

  const [globalSearchResults, setGlobalSearchResults] = useState([]);
  const [isSearchLoading, setIsSearchLoading] = useState(false);
  const [searchTerm, setSearchTerm] = useState('');
  const debouncedSearchTerm = useDebounce(searchTerm, 500);

  useEffect(() => {
    const handleGlobalSearch = async () => {
      if (!debouncedSearchTerm || debouncedSearchTerm.length < 3 || !clientId) {
        setGlobalSearchResults([]);
        return;
      }
      setIsSearchLoading(true);
      try {
        const results = await searchAllAssets(clientId, debouncedSearchTerm);
        setGlobalSearchResults(results);
      } catch (error) {
        console.error("Global search failed:", error);
        setGlobalSearchResults([]);
      } finally {
        setIsSearchLoading(false);
      }
    };

    handleGlobalSearch();
  }, [debouncedSearchTerm, clientId]);

  const handleSearchResultClick = (e) => {
    const [type, id] = e.key.split('-');
    setGlobalSearchResults([]); // Clear search results after selection
    if (type === 'opportunity') navigate(`/opportunities/${id}`);
    if (type === 'discovery_run') navigate(`/discovery/run/${id}`);
    // Add more navigation logic for other types if needed
  };

  const handleLogout = async () => {
    await logout();
    navigate('/login');
  };

  const handleSelectClientFromDropdown = (value) => {
    setClientId(value);
    localStorage.setItem('clientId', value);
    navigate('/dashboard'); // CRITICAL FIX: Redirect to a safe page (dashboard) after switching client
  };

  return (
    <Layout style={{ minHeight: '100vh' }}>
      <Sider collapsible breakpoint="lg" collapsedWidth="0">
        <div style={{ padding: '16px', textAlign: 'center' }}>
          <Title level={4} style={{ color: 'white', margin: 0 }}>Content AI</Title>
          <Text style={{ color: 'rgba(255,255,255,0.6)', fontSize: '0.8em' }}>
            {isLoadingClients ? 'Loading Clients...' : `Client: ${clients.find(c => c.client_id === clientId)?.client_name || clientId}`}
          </Text>
        </div>
        <Menu
          theme="dark"
          mode="inline"
          selectedKeys={[location.pathname]}
          items={menuItems}
        />
      </Sider>
      <Layout>
        <Header style={{ background: '#fff', padding: '0 16px', display: 'flex', justifyContent: 'space-between', alignItems: 'center', borderBottom: '1px solid #f0f0f0' }}>
<div style={{ padding: '0 16px', color: 'rgba(255, 255, 255, 0.65)' }}>
  <Space>
    {isLoadingClients ? <Spin size="small" /> : `Client: ${clients.find(c => c.client_id === clientId)?.client_name || 'N/A'}`}
  </Space>
</div>
          <div style={{ flexGrow: 1, margin: '0 20px', maxWidth: '400px' }}>
            <Input.Search
              placeholder="Search keywords, opportunities, runs..."
              onChange={(e) => setSearchTerm(e.target.value)}
              style={{ width: '100%' }}
              allowClear
              loading={isSearchLoading}
            />
            {globalSearchResults.length > 0 && (
              <Card
                size="small"
                style={{ position: 'absolute', zIndex: 100, width: 'inherit', marginTop: '5px' }}
                bodyStyle={{ padding: '0px' }}
              >
                <Menu
  onClick={handleSearchResultClick}
  style={{ width: '100%', borderRight: 0 }}
>
  {globalSearchResults.map(item => {
    const key = `${item.type}-${item.id}`;
    let labelText = item.keyword || item.name || 'N/A';
    let typeTag = item.type.replace(/_/g, ' ').toUpperCase();

    return (
      <Menu.Item key={key} icon={item.type === 'opportunity' ? <BulbOutlined /> : <RocketOutlined />}>
        <Space size="small">
          <Text ellipsis={true} style={{ maxWidth: '200px' }}>{labelText}</Text>
          <Tag color={item.type === 'opportunity' ? 'blue' : 'purple'}>{typeTag}</Tag>
        </Space>
      </Menu.Item>
    );
  })}
</Menu>
              </Card>
            )}
          </div>
          <Space>
            <Select
              value={clientId}
              style={{ width: 180 }}
              onChange={handleSelectClientFromDropdown}
              loading={isLoadingClients}
              disabled={isLoadingClients || clients.length === 0}
              options={clients.map(c => ({ value: c.client_id, label: c.client_name }))}
            />
            <Button icon={<LogoutOutlined />} onClick={handleLogout}>Logout</Button>
          </Space>
        </Header>
        <Content style={{ margin: '16px' }}>
          <Outlet />
        </Content>
      </Layout>
    </Layout>
  );
};

export default MainLayout;
</file>

<file path="client/my-content-app/src/components/ContentDiffViewer.jsx">
// This is a new file. Create it with the following content:
// You'll need to install react-diff-viewer: npm install react-diff-viewer
import React from 'react';
import DiffViewer from 'react-diff-viewer';
import { Typography } from 'antd';

const { Text } = Typography;

const ContentDiffViewer = ({ oldValue, newValue, oldTitle = "Previous Version", newTitle = "Current Version" }) => {
  if (!oldValue && !newValue) {
    return <Text type="secondary">No content to compare.</Text>;
  }
  if (oldValue === newValue) {
    return <Text type="success">Content is identical.</Text>;
  }

  return (
    <DiffViewer
      oldValue={oldValue || ''}
      newValue={newValue || ''}
      splitView={true}
      leftTitle={oldTitle}
      rightTitle={newTitle}
      showDiffOnly={false} // Show entire file with diffs highlighted
      use={true} // Enable styling
      styles={{
        variables: {
          dark: {
            diffViewerBackground: '#262626',
            diffViewerColor: '#f0f2f5',
            addedBackground: '#003a29',
            removedBackground: '#320a0b',
            wordAddedBackground: '#006d3d',
            wordRemovedBackground: '#9e2b2f',
          },
        },
        diffContainer: {
          fontSize: '12px',
          fontFamily: 'monospace',
        },
        line: {
          wordBreak: 'break-word',
        }
      }}
    />
  );
};

export default ContentDiffViewer;
</file>

<file path="client/my-content-app/src/components/CostConfirmationModal.jsx">
// This is a new file. Create it with the following content:
import React from 'react';
import { Modal, Typography, Spin, Alert, List } from 'antd';
import { useQuery } from 'react-query';
import { estimateActionCost } from '../services/orchestratorService'; // NEW

const { Title, Text, Paragraph } = Typography;

const CostConfirmationModal = ({ open, onCancel, onConfirm, opportunityId, actionType }) => {
  const { data: costEstimate, isLoading, isError, error } = useQuery(
    ['costEstimate', opportunityId, actionType],
    () => estimateActionCost(opportunityId, actionType),
    {
      enabled: open && !!opportunityId && !!actionType, // Only fetch when modal is open
      staleTime: 5 * 60 * 1000, // Cost estimates don't change frequently
    }
  );

  const getActionTitle = (type) => {
    switch (type) {
      case 'analyze': return 'Run Deep-Dive Analysis';
      case 'generate': return 'Generate Full Content Package';
      case 'refresh': return 'Refresh Content (Analysis & Generation)';
      case 'validate': return 'Run Live SERP Validation';
      default: return 'Perform Action';
    }
  };

  return (
    <Modal
      title={getActionTitle(actionType)}
      open={open}
      onCancel={onCancel}
      onOk={onConfirm}
      confirmLoading={isLoading}
      okText="Confirm & Proceed"
      cancelText="Cancel"
    >
      {isLoading ? (
        <Spin tip="Estimating API costs..." />
      ) : isError ? (
        <Alert
          message="Error Estimating Cost"
          description={error?.message || 'Could not fetch cost estimation. Proceed with caution.'}
          type="error"
          showIcon
        />
      ) : (
        <>
          <Paragraph>
            This action will incur API costs. Please review the estimate below before proceeding.
          </Paragraph>
          <List
            size="small"
            bordered
            dataSource={costEstimate?.breakdown || []}
            renderItem={item => (
    <List.Item
        actions={[
            <Text key="cost" strong>
                {item.cost ? `$${item.cost.toFixed(4)}` : item.cost === 0 ? '$0.0000' : 'N/A'}
            </Text>
        ]}
    >
        <List.Item.Meta
            title={item.service}
            description={item.details}
        />
    </List.Item>
)}
footer={
    <Paragraph strong style={{ fontSize: '1.2em' }}>Estimated Total: <Text code>${costEstimate?.total_cost?.toFixed(4) || '0.0000'}</Text> USD</Paragraph>
}
/>
<Alert
            message="This is an estimate. Actual costs may vary."
            type="info"
            showIcon
            style={{ marginTop: '16px' }}
          />
        </>
      )}
    </Modal>
  );
};

export default CostConfirmationModal;
</file>

<file path="client/my-content-app/src/components/GlobalJobTracker.jsx">
import React from 'react';
import { Alert, Spin } from 'antd';
import { useJobs } from '../context/JobContext';
import { CheckCircleOutlined, CloseCircleOutlined } from '@ant-design/icons';

const GlobalJobTracker = () => {
  const { activeJobs } = useJobs();

  if (Object.keys(activeJobs).length === 0) {
    return null;
  }

  return (
    <div style={{ position: 'fixed', bottom: 24, right: 24, zIndex: 1000, display: 'flex', flexDirection: 'column', gap: '16px' }}>
      {Object.entries(activeJobs).map(([jobId, job]) => {
        let icon, type;
        switch (job.status) {
          case 'running':
            icon = <Spin />;
            type = 'info';
            break;
          case 'completed':
            icon = <CheckCircleOutlined />;
            type = 'success';
            break;
          case 'failed':
            icon = <CloseCircleOutlined />;
            type = 'error';
            break;
          default:
            icon = <Spin />;
            type = 'info';
        }

        return (
          <Alert
            key={jobId}
            message={`Job Status: ${job.status.charAt(0).toUpperCase() + job.status.slice(1)}`}
            description={job.message}
            type={type}
            showIcon
            icon={icon}
            style={{ boxShadow: '0 2px 8px rgba(0,0,0,0.15)' }}
          />
        );
      })}
    </div>
  );
};

export default GlobalJobTracker;
</file>

<file path="client/my-content-app/src/components/JobStatusIndicator.jsx">
import React from 'react';
import { useQuery } from 'react-query';
import { getJobStatus } from '../services/orchestratorService'; // This service function needs to be created
import { Progress, Tag, Tooltip } from 'antd';
import { LoadingOutlined, CheckCircleOutlined, CloseCircleOutlined, ClockCircleOutlined, PauseCircleOutlined } from '@ant-design/icons'; // ADD PauseCircleOutlined

const JobStatusIndicator = ({ jobId }) => {
  const { data: job, isLoading } = useQuery(
    ['jobStatus', jobId],
    () => getJobStatus(jobId),
    {
      refetchInterval: (data) => (data?.status === 'running' || data?.status === 'pending' || data?.status === 'paused' ? 3000 : false), // Refetch for paused jobs too
      enabled: !!jobId,
    }
  );

  if (isLoading && !job) return <Tag icon={<LoadingOutlined spin />}>Loading...</Tag>;
  if (!job) return <Tag>Unknown</Tag>;

  const statusConfig = {
    pending: { icon: <LoadingOutlined />, color: 'default', text: 'Pending' },
    running: { icon: <LoadingOutlined spin />, color: 'processing', text: job.result?.step || 'Running...' },
    completed: { icon: <CheckCircleOutlined />, color: 'success', text: 'Completed' },
    failed: { icon: <CloseCircleOutlined />, color: 'error', text: 'Failed' },
    paused: { icon: <PauseCircleOutlined />, color: 'warning', text: 'Paused (Awaiting Approval)' }, // NEW text
  };
  
  const config = statusConfig[job.status] || { icon: <ClockCircleOutlined />, color: 'default', text: 'Queued' };

  return (
    <Tooltip title={job.error || job.result?.message || job.status_message}> {/* Include job.status_message */}
      <div style={{ display: 'flex', alignItems: 'center', gap: 4 }}>
        <Tag icon={config.icon} color={config.color}>{config.text}</Tag>
        {(job.status === 'running' || job.status === 'paused') && ( // Show progress for paused as well
          <Progress percent={job.progress} size="small" status="active" showInfo={false} style={{ width: '100px', margin: 0 }} />
        )}
      </div>
    </Tooltip>
  );
};
export default JobStatusIndicator;
</file>

<file path="client/my-content-app/src/components/PromptTemplateEditor.jsx">
// This is a new file. Create it with the following content:
import React, { useRef, useState, useEffect } from 'react';
import { Input, Button, Space, Tooltip, Typography, List, Divider, Row, Col, Card } from 'antd';
import { BulbOutlined, CopyOutlined } from '@ant-design/icons';

const { TextArea } = Input;
const { Text } = Typography;

const PLACEHOLDERS = [
  { name: "[TOPIC]", desc: "The main keyword or topic of the article." },
  { name: "[PRIMARY KEYWORD]", desc: "The exact target keyword for SEO." },
  { name: "[LSI/secondary keywords]", desc: "List of related keywords/entities to include." },
  { name: "[WORD_COUNT]", desc: "The target word count for the article." },
  { name: "[CTA_URL]", desc: "The call-to-action URL to promote." },
  { name: "[[IMAGE: <prompt>]]", desc: "Placeholder for in-article images (AI will fill <prompt>)." },
  // Add other placeholders as they become relevant
];

const PromptTemplateEditor = ({ value, onChange, disabled }) => {
  const textAreaRef = useRef(null);
  const [copied, setCopied] = useState(false);

  useEffect(() => {
    if (copied) {
      const timer = setTimeout(() => setCopied(false), 2000);
      return () => clearTimeout(timer);
    }
  }, [copied]);

  const handleInsertPlaceholder = (placeholder) => {
    const textarea = textAreaRef.current?.resizableTextArea?.textArea;
    if (textarea) {
      const start = textarea.selectionStart;
      const end = textarea.selectionEnd;
      const newValue = value.substring(0, start) + placeholder + value.substring(end, value.length);
      onChange(newValue);
      // Move cursor after inserted placeholder
      setTimeout(() => {
        textarea.selectionStart = textarea.selectionEnd = start + placeholder.length;
        textarea.focus();
      }, 0);
    }
  };

  return (
    <Row gutter={16}>
      <Col span={18}>
        <TextArea
          ref={textAreaRef}
          value={value}
          onChange={(e) => onChange(e.target.value)}
          rows={20}
          placeholder="Enter your custom AI prompt template here..."
          disabled={disabled}
          autoSize={{ minRows: 15, maxRows: 30 }}
        />
        <Space style={{ marginTop: '10px' }}>
          <Tooltip title="Copy entire prompt to clipboard">
            <Button icon={<CopyOutlined />} onClick={() => { handleInsertPlaceholder(value); setCopied(true); }}>
              {copied ? 'Copied!' : 'Copy Full Prompt'}
            </Button>
          </Tooltip>
        </Space>
      </Col>
      <Col span={6}>
        <Card title={<Space><BulbOutlined /> Available Placeholders</Space>} size="small">
          <List
            size="small"
            dataSource={PLACEHOLDERS}
            renderItem={item => (
              <List.Item
                actions={[
                  <Button
                    key="insert"
                    type="link"
                    size="small"
                    onClick={() => handleInsertPlaceholder(item.name)}
                    disabled={disabled}
                  >
                    Insert
                  </Button>
                ]}
              >
                <List.Item.Meta
                  title={<Text code>{item.name}</Text>}
                  description={<Text type="secondary" ellipsis={{tooltip: item.desc}}>{item.desc}</Text>}
                />
              </List.Item>
            )}
          />
          <Divider style={{ margin: '16px 0' }} />
          <Text type="secondary" style={{fontSize: '0.8em'}}>
            These placeholders will be dynamically replaced with data from your opportunity blueprint.
          </Text>
        </Card>
      </Col>
    </Row>
  );
};

export default PromptTemplateEditor;
</file>

<file path="client/my-content-app/src/context/AuthContext.jsx">
// This is a new file. Create it with the following content:
import React, { createContext, useContext, useState, useEffect } from 'react';
import { login as apiLogin, logout as apiLogout } from '../services/authService';

export const AuthContext = createContext();

export const useAuth = () => useContext(AuthContext);

export const AuthProvider = ({ children }) => {
  const [isAuthenticated, setIsAuthenticated] = useState(false);
  const [authToken, setAuthToken] = useState(null);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    try {
      const storedToken = localStorage.getItem('authToken');
      if (storedToken) {
        setAuthToken(storedToken);
        setIsAuthenticated(true);
      }
    } catch (error) {
      console.error("Failed to read authToken from localStorage:", error);
    } finally {
      setLoading(false);
    }
  }, []);

  const login = async (password) => {
    const response = await apiLogin(password);
    if (response && response.token) {
      try {
        localStorage.setItem('authToken', response.token);
        setAuthToken(response.token);
        setIsAuthenticated(true);
      } catch (error) {
        console.error("Failed to store authToken in localStorage:", error);
        // Even if localStorage fails, keep session in memory for current tab
        setAuthToken(response.token);
        setIsAuthenticated(true);
      }
    } else {
      throw new Error('No token received');
    }
  };

  const logout = async () => {
    try {
      await apiLogout();
      localStorage.removeItem('authToken');
      setAuthToken(null);
      setIsAuthenticated(false);
    } catch (error) {
      console.error("Logout failed:", error);
      // Even if API logout fails, clear local state
      localStorage.removeItem('authToken');
      setAuthToken(null);
      setIsAuthenticated(false);
      throw error; // Re-throw to allow component to show error
    }
  };

  return (
    <AuthContext.Provider value={{ isAuthenticated, authToken, login, logout, loading }}>
      {children}
    </AuthContext.Provider>
  );
};
</file>

<file path="client/my-content-app/src/context/ClientContext.jsx">
import React, { createContext, useContext, useState, useEffect } from 'react';

export const ClientContext = createContext();

export const useClient = () => useContext(ClientContext);

export const ClientProvider = ({ children }) => {
    const [clientId, setClientIdState] = useState(() => { // Renamed local state setter
        try {
            const storedClientId = localStorage.getItem('clientId'); // Changed key from 'currentClientId'
            return storedClientId ? storedClientId : 'Lark_Main_Site'; // Use a consistent default if not found
        } catch (error) {
            console.error("Could not access localStorage for clientId:", error);
            return 'Lark_Main_Site';
        }
    });

    useEffect(() => {
        try {
            localStorage.setItem('clientId', clientId); // Changed key from 'currentClientId'
        } catch (error) {
            console.error("Could not write to localStorage for clientId:", error);
        }
    }, [clientId]);
      
    // New function to update clientId and persist to localStorage
    const updateClientId = (newClientId) => {
        setClientIdState(newClientId);
        try {
            localStorage.setItem('clientId', newClientId); // Changed key from 'currentClientId'
        } catch (error) {
            console.error("Could not write to localStorage for clientId:", error);
        }
    };

    const value = { clientId, setClientId: updateClientId }; // Provide the wrapped setter

    return (
        <ClientContext.Provider value={value}>
            {children}
        </ClientContext.Provider>
    );
};
</file>

<file path="client/my-content-app/src/context/JobContext.jsx">
import React, { createContext, useState, useContext } from 'react';

const JobContext = createContext();

export const useJobs = () => useContext(JobContext);

export const JobProvider = ({ children }) => {
  const [activeJobs, setActiveJobs] = useState({});

  const startJob = (jobId, message) => {
    setActiveJobs(prev => ({ ...prev, [jobId]: { status: 'running', message } }));
  };

  const updateJob = (jobId, status, message) => {
    setActiveJobs(prev => {
      if (!prev[jobId]) return prev;
      return { ...prev, [jobId]: { ...prev[jobId], status, message } };
    });
  };

  const completeJob = (jobId) => {
    setTimeout(() => {
      setActiveJobs(prev => {
        const newJobs = { ...prev };
        delete newJobs[jobId];
        return newJobs;
      });
    }, 5000); // Remove after 5 seconds
  };

  return (
    <JobContext.Provider value={{ activeJobs, startJob, updateJob, completeJob }}>
      {children}
    </JobContext.Provider>
  );
};
</file>

<file path="client/my-content-app/src/context/NotificationContext.jsx">
import React, { useContext } from 'react';
import { notification } from 'antd';

// Create a context to hold the notification logic
const NotificationContext = React.createContext();

// Custom hook to access the notification context
export const useNotifications = () => useContext(NotificationContext);

// Provider component that wraps your app
export const NotificationProvider = ({ children }) => {
  const showNotification = (type, message, description) => {
    notification[type]({
      message,
      description,
    });
  };

  return (
    <NotificationContext.Provider value={{ showNotification }}>
      {children}
    </NotificationContext.Provider>
  );
};
</file>

<file path="client/my-content-app/src/hooks/useClient.js">
import { useContext } from 'react';
import { ClientContext } from '../context/ClientContext';

// Simple wrapper hook to ensure useClient is used within its Provider
export const useClient = () => {
    const context = useContext(ClientContext);
    if (context === undefined) {
        throw new Error('useClient must be used within a ClientProvider');
    }
    return context;
};
</file>

<file path="client/my-content-app/src/hooks/useDebounce.js">
import { useState, useEffect } from 'react';

function useDebounce(value, delay) {
  const [debouncedValue, setDebouncedValue] = useState(value);

  useEffect(() => {
    const handler = setTimeout(() => {
      setDebouncedValue(value);
    }, delay);

    return () => {
      clearTimeout(handler);
    };
  }, [value, delay]);

  return debouncedValue;
}

export default useDebounce;
</file>

<file path="client/my-content-app/src/pages/ActivityLog/ActivityLogPage.jsx">
// This is a new file. Create it with the following content:
import React from 'react';
import { Layout, Typography, Table, Space, Alert, Spin, Button, Tooltip } from 'antd';
import { useQuery, useMutation, useQueryClient } from 'react-query';
import { getAllJobs, cancelJob } from '../../services/orchestratorService';
import { formatDistanceToNow, format } from 'date-fns';
import { CloseCircleOutlined } from '@ant-design/icons';
import { useNotifications } from '../../context/NotificationContext';
import JobStatusIndicator from '../../components/JobStatusIndicator'; // NEW

const { Content } = Layout;
const { Title, Text } = Typography;



const ActivityLogPage = () => {
  const queryClient = useQueryClient();
  const { showNotification } = useNotifications();

  const { data: jobs = [], isLoading, isError, error } = useQuery(
    'allJobs',
    getAllJobs,
    {
      refetchInterval: (data) =>
        data?.some((job) => job.status === 'running' || job.status === 'pending') ? 3000 : false, // Refetch every 3s if any job is running
    }
  );

  const { mutate: cancelJobMutation, isLoading: isCancelling } = useMutation(
    (jobId) => cancelJob(jobId),
    {
      onSuccess: () => {
        showNotification('info', 'Job Cancellation', 'Job cancellation request sent.');
        queryClient.invalidateQueries('allJobs'); // Refetch to show updated status
      },
      onError: (err) => {
        showNotification('error', 'Cancellation Failed', err.message || 'An error occurred during cancellation.');
      },
    }
  );

  const columns = [
    {
      title: 'Job ID',
      dataIndex: 'id',
      key: 'id',
      render: (text) => <Text code>{text.substring(0, 8)}...</Text>,
    },
    {
      title: 'Type',
      dataIndex: 'function_name',
      key: 'function_name',
      render: (text) => text ? text.replace(/_background$/, '').replace(/_/g, ' ').replace('run ', '').replace('run', '').trim().toUpperCase() : 'N/A',
    },
    {
      title: 'Status',
      dataIndex: 'status',
      key: 'status',
      render: (_, record) => <JobStatusIndicator jobId={record.id} />, // Use the new component
    },
    {
      title: 'Current Step',
      dataIndex: 'result',
      key: 'step',
      render: (result) => result?.step || 'N/A',
    },
    {
      title: 'Started',
      dataIndex: 'started_at',
      key: 'started_at',
      render: (timestamp) => timestamp ? formatDistanceToNow(new Date(timestamp * 1000), { addSuffix: true }) : 'N/A',
    },
    {
      title: 'Finished',
      dataIndex: 'finished_at',
      key: 'finished_at',
      render: (timestamp) => timestamp ? format(new Date(timestamp * 1000), 'MMM d, hh:mm a') : 'N/A',
    },
    {
      title: 'Error',
      dataIndex: 'error',
      key: 'error',
      render: (errorMsg) => errorMsg ? (
        <Tooltip title={errorMsg}>
          <Text type="danger" ellipsis>{errorMsg}</Text>
        </Tooltip>
      ) : 'N/A',
    },
    {
      title: 'Actions',
      key: 'actions',
      render: (_, record) => (
        <Space size="small">
          {record.status === 'running' && (
            <Tooltip title="Cancel Job">
              <Button 
                danger 
                icon={<CloseCircleOutlined />} 
                size="small" 
                onClick={() => cancelJobMutation(record.id)} 
                loading={isCancelling} 
              />
            </Tooltip>
          )}
        </Space>
      ),
    },
  ];

  if (isLoading) {
    return <Spin tip="Loading activity log..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (isError) {
    return <Alert message="Error" description={error?.message || "Failed to load activity log. Please try again."} type="error" showIcon />;
  }

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Title level={2}>Activity Log</Title>
        <Table
          columns={columns}
          dataSource={jobs}
          rowKey="id"
          pagination={{ pageSize: 10 }}
          scroll={{ x: 1000 }}
        />
      </Content>
    </Layout>
  );
};

export default ActivityLogPage;
</file>

<file path="client/my-content-app/src/pages/Auth/LoginPage.jsx">
// This is a new file. Create it with the following content:
import React, { useState } from 'react';
import { Form, Input, Button, Card, Typography, Alert, Space } from 'antd';
import { LockOutlined } from '@ant-design/icons';
import { useAuth } from '../../context/AuthContext';
import { useNavigate } from 'react-router-dom';

const { Title } = Typography;

const LoginPage = () => {
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);
  const { login } = useAuth();
  const navigate = useNavigate();

  const onFinish = async (values) => {
    setLoading(true);
    setError(null);
    try {
      await login(values.password); // Only password is required for dummy login
      navigate('/dashboard'); // Redirect to the main dashboard on success
    } catch (err) {
      setError(err.message || 'Login failed. Please check your password.');
    } finally {
      setLoading(false);
    }
  };

  return (
    <Space
      direction="vertical"
      align="center"
      style={{
        width: '100%',
        minHeight: '100vh',
        justifyContent: 'center',
        background: '#f0f2f5',
      }}
    >
      <Card style={{ width: 350, textAlign: 'center' }}>
        <Title level={3}>Content AI Login</Title>
        {error && (
          <Alert message="Authentication Failed" description={error} type="error" showIcon style={{ marginBottom: 20 }} />
        )}
        <Form
          name="login"
          initialValues={{ remember: true }}
          onFinish={onFinish}
        >
          <Form.Item
            name="password"
            rules={[{ required: true, message: 'Please enter your password!' }]}
          >
            <Input.Password prefix={<LockOutlined />} placeholder="Password" />
          </Form.Item>

          <Form.Item>
            <Button type="primary" htmlType="submit" loading={loading} block>
              Log in
            </Button>
          </Form.Item>
        </Form>
      </Card>
    </Space>
  );
};

export default LoginPage;
</file>

<file path="client/my-content-app/src/pages/BlogPage/BlogPage.css">
.blog-layout {
    background: #fff;
    padding: 40px 0;
}

.blog-content {
    max-width: 800px;
    margin: 0 auto;
    padding: 0 24px;
}

.blog-post-container {
    border-radius: 8px;
    overflow: hidden;
}

.blog-title {
    font-size: 3em;
    font-weight: 700;
    line-height: 1.2;
    margin-bottom: 24px;
    color: #1a1a1a;
}

.author-info {
    display: flex;
    align-items: center;
    gap: 12px;
    margin-bottom: 32px;
}

.featured-image {
    width: 100%;
    height: auto;
    border-radius: 8px;
    margin-bottom: 32px;
    object-fit: cover;
}

.blog-body {
    font-size: 1.1em;
    line-height: 1.8;
    color: #333;
}

.blog-body h1, .blog-body h2, .blog-body h3 {
    font-weight: 600;
    margin-top: 2em;
    margin-bottom: 1em;
    color: #1a1a1a;
}

.blog-body p {
    margin-bottom: 1.5em;
}

.blog-body a {
    color: #1890ff;
    text-decoration: none;
}

.blog-body a:hover {
    text-decoration: underline;
}

.blog-body ul, .blog-body ol {
    padding-left: 2em;
    margin-bottom: 1.5em;
}

.blog-body blockquote {
    border-left: 4px solid #e8e8e8;
    padding-left: 1em;
    margin: 2em 0;
    color: #666;
    font-style: italic;
}

.loading-container {
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100vh;
}

/* Styles to make the Quill Editor body match the blog body for a WYSIWYG experience */
.blog-body .ql-editor {
    font-size: 1.1em;
    line-height: 1.8;
    color: #333;
    min-height: 600px;
}

.blog-body .ql-editor h1, .blog-body .ql-editor h2, .blog-body .ql-editor h3 {
    font-weight: 600;
    margin-top: 2em;
    margin-bottom: 1em;
    color: #1a1a1a;
}

.blog-body .ql-editor p {
    margin-bottom: 1.5em;
}

.blog-body .ql-editor blockquote {
    border-left: 4px solid #e8e8e8;
    padding-left: 1em;
    margin: 2em 0;
    color: #666;
    font-style: italic;
}
</file>

<file path="client/my-content-app/src/pages/BlogPage/BlogPage.jsx">
import React from 'react';
import { useParams } from 'react-router-dom';
import { useQuery } from 'react-query';
import { getOpportunityById } from '../../services/opportunitiesService';
import { Layout, Typography, Spin, Alert, Result, Button, Avatar, Space } from 'antd';
import { UserOutlined } from '@ant-design/icons';
import { Link } from 'react-router-dom';
import './BlogPage.css';

const { Content } = Layout;
const { Title, Text } = Typography;

const BlogPage = () => {
  const { opportunityId } = useParams();
  const { data: opportunity, isLoading, isError, error } = useQuery(
    ['opportunity', opportunityId],
    () => getOpportunityById(opportunityId)
  );

  if (isLoading) {
    return (
      <div className="loading-container">
        <Spin size="large" tip="Loading article..." />
      </div>
    );
  }

  if (isError) {
    return <Alert message="Error" description={error.message} type="error" showIcon />;
  }

  const {
    keyword,
    final_package_json,
    author_name = 'AI Assistant',
    publication_date = new Date().toLocaleDateString(),
  } = opportunity || {};

  if (!final_package_json || !final_package_json.article_html_final) {
    return (
      <Result
        status="404"
        title="Content Not Found"
        subTitle="The final content package for this opportunity has not been generated yet."
        extra={<Button type="primary"><Link to={`/opportunities/${opportunityId}`}>Go Back</Link></Button>}
      />
    );
  }

  return (
    <Layout className="blog-layout">
      <Content className="blog-content">
        <div className="blog-post-container">
          <Title level={1} className="blog-title">{final_package_json.meta_title || keyword}</Title>
          <div className="author-info">
            <Avatar icon={<UserOutlined />} />
            <Space direction="vertical" size={0}>
              <Text strong>{author_name}</Text>
              <Text type="secondary">Published on {publication_date}</Text>
            </Space>
          </div>

          {final_package_json.featured_image_relative_path && (
            <img src={final_package_json.featured_image_relative_path} alt={keyword} className="featured-image" />
          )}

          <div
            className="blog-body"
            dangerouslySetInnerHTML={{ __html: final_package_json.article_html_final }}
          />
        </div>
      </Content>
    </Layout>
  );
};

export default BlogPage;
</file>

<file path="client/my-content-app/src/pages/ClientDashboard/AddNewClientModal.jsx">
// This is a new file. Create it with the following content:
import React from 'react';
import { Modal, Form, Input } from 'antd';

const AddNewClientModal = ({ open, onCancel, onAddClient, loading }) => {
  const [form] = Form.useForm();

  const handleOk = () => {
    form.validateFields()
      .then(values => {
        onAddClient(values);
        form.resetFields();
      })
      .catch(info => {
        console.log('Validate Failed:', info);
      });
  };

  return (
    <Modal
      title="Add New Client"
      open={open}
      onOk={handleOk}
      onCancel={onCancel}
      confirmLoading={loading}
      okText="Add Client"
    >
      <Form
        form={form}
        layout="vertical"
        name="add_client_form"
      >
        <Form.Item
          name="client_name"
          label="Client Name"
          rules={[{ required: true, message: 'Please enter the client\'s name!' }]}
        >
          <Input autoFocus />
        </Form.Item>
        <Form.Item
          name="client_id"
          label="Client ID"
          rules={[{ required: true, message: 'Please enter a unique client ID!' }]}
          extra="This should be a unique identifier for the client (e.g., my_company_name)."
        >
          <Input />
        </Form.Item>
      </Form>
    </Modal>
  );
};

export default AddNewClientModal;
</file>

<file path="client/my-content-app/src/pages/ClientDashboard/ClientDashboardPage.jsx">
// This is a new file. Create it with the following content:
import React, { useState } from 'react';
import { Layout, Typography, Spin, Alert, Card, Row, Col, Statistic, Button, Space } from 'antd';
import { useQuery, useMutation, useQueryClient } from 'react-query';
import { getClients, getDashboardStats, addClient } from '../../services/clientService';
import { useClient } from '../../hooks/useClient';
import { useNotifications } from '../../context/NotificationContext';
import AddNewClientModal from './AddNewClientModal';
import { useNavigate } from 'react-router-dom'; // NEW

const { Content } = Layout;
const { Title } = Typography;

const ClientCard = ({ client, onSelectClient, isActive }) => {
  const { data: stats, isLoading: isLoadingStats, isError: isErrorStats } = useQuery(
    ['dashboardStats', client.client_id],
    () => getDashboardStats(client.client_id),
    {
      enabled: !!client.client_id,
      staleTime: 5 * 60 * 1000, // 5 minutes
      cacheTime: 10 * 60 * 1000, // 10 minutes
    }
  );

  return (
    <Card
      title={<Title level={5} style={{ margin: 0 }}>{client.client_name}</Title>}
      extra={
        <Button
          type={isActive ? "primary" : "default"}
          onClick={() => onSelectClient(client.client_id)}
          disabled={isActive}
        >
          {isActive ? 'Current Client' : 'Select'}
        </Button>
      }
      style={{ marginBottom: '16px' }}
      loading={isLoadingStats}
    >
      <Row gutter={16}>
        <Col span={8}>
          <Statistic title="Opportunities" value={stats?.status_counts?.all || 0} />
        </Col>
        <Col span={8}>
          <Statistic title="Qualified" value={stats?.status_counts?.validated || 0} />
        </Col>
        <Col span={8}>
          <Statistic title="Generated" value={stats?.status_counts?.generated || 0} />
        </Col>
      </Row>
      {isErrorStats && (
        <Alert
          message="Error loading stats"
          description={isErrorStats?.message || 'Failed to load dashboard statistics.'}
          type="error"
          style={{ marginTop: '16px' }}
        />
      )}
    </Card>
  );
};

const ClientDashboardPage = () => {
  const { clientId, setClientId } = useClient();
  const { showNotification } = useNotifications();
  const queryClient = useQueryClient();
  const navigate = useNavigate(); // NEW
  const [isModalVisible, setIsModalVisible] = useState(false);

  const { data: clients = [], isLoading, isError, error } = useQuery(
    'clients',
    getClients,
  );

  const { mutate: addClientMutation, isLoading: isAddingClient } = useMutation(
    (newClient) => addClient(newClient),
    {
      onSuccess: () => {
        showNotification('success', 'Client Added', 'New client has been successfully added.');
        queryClient.invalidateQueries('clients'); // Invalidate to refetch client list
        setIsModalVisible(false);
      },
      onError: (err) => {
        showNotification('error', 'Failed to Add Client', err.message || 'An error occurred while adding the client.');
      },
    }
  );

  const handleSelectClient = (selectedClientId) => {
    setClientId(selectedClientId);
    navigate('/dashboard'); // Navigate to the new dashboard page after selecting
  };

  if (isLoading) {
    return <Spin tip="Loading clients..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (isError) {
    return <Alert message="Error" description={error?.message || "Failed to load clients. Please try again."} type="error" showIcon />;
  }

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Title level={2}>Client Dashboard</Title>
        <Space style={{ marginBottom: '16px' }}>
          <Button type="primary" onClick={() => setIsModalVisible(true)}>Add New Client</Button>
        </Space>

        <Row gutter={[16, 16]}>
          {clients.map(client => (
            <Col xs={24} sm={12} md={8} lg={6} key={client.client_id}>
              <ClientCard 
                client={client} 
                onSelectClient={handleSelectClient} 
                isActive={client.client_id === clientId} 
              />
            </Col>
          ))}
        </Row>
      </Content>
      <AddNewClientModal
        open={isModalVisible}
        onCancel={() => setIsModalVisible(false)}
        onAddClient={addClientMutation}
        loading={isAddingClient}
      />
    </Layout>
  );
};

export default ClientDashboardPage;
</file>

<file path="client/my-content-app/src/pages/ClientSettings/ClientSettingsPage.jsx">
// my-content-app/src/pages/ClientSettings/ClientSettingsPage.jsx
// NEW FILE
import React from 'react';
import { Layout, Typography, Form, Input, Button, Spin, Alert } from 'antd';
import { useQuery, useMutation, useQueryClient } from 'react-query';
import { getClientSettings, updateClientSettings } from '../../services/clientSettingsService';
import { useNotifications } from '../../context/NotificationContext';

const { Content } = Layout;
const { Title } = Typography;
const { TextArea } = Input;

const ClientSettingsPage = ({ clientId = 'default' }) => {
  const [form] = Form.useForm();
  const queryClient = useQueryClient();
  const { showNotification } = useNotifications();

  const { data: settings, isLoading, isError, error } = useQuery(
    ['clientSettings', clientId],
    () => getClientSettings(clientId),
    {
      enabled: !!clientId,
      onSuccess: (data) => {
        form.setFieldsValue(data);
      }
    }
  );

  const { mutate: updateSettingsMutation, isLoading: isUpdating } = useMutation(
    (newSettings) => updateClientSettings(clientId, newSettings),
    {
      onSuccess: () => {
        showNotification('success', 'Settings Updated', 'Client settings have been saved successfully.');
        queryClient.invalidateQueries(['clientSettings', clientId]);
      },
      onError: (err) => {
        showNotification('error', 'Update Failed', err.message || 'An error occurred while saving settings.');
      }
    }
  );

  const onFinish = (values) => {
    updateSettingsMutation(values);
  };

  if (isLoading) return <Spin tip="Loading settings..." />;
  if (isError) return <Alert message="Error" description={error.message} type="error" showIcon />;

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Title level={2}>Client AI & Content Settings</Title>
        <Form
          form={form}
          layout="vertical"
          onFinish={onFinish}
          initialValues={settings}
        >
          <Form.Item
            name="brand_tone"
            label="Brand Tone & Voice"
            tooltip="Define the desired tone for the AI-generated content (e.g., professional, witty, conversational)."
          >
            <TextArea rows={4} placeholder="e.g., Professional and authoritative, but accessible to a general audience." />
          </Form.Item>

          <Form.Item
            name="target_audience"
            label="Target Audience"
            tooltip="Describe the primary audience for the content."
          >
            <TextArea rows={4} placeholder="e.g., Marketing managers at mid-sized tech companies." />
          </Form.Item>

          <Form.Item
            name="terms_to_avoid"
            label="Terms to Avoid"
            tooltip="List any specific words or phrases the AI should not use. Separate with commas."
          >
            <Input placeholder="e.g., synergy, disruptive, unicorn" />
          </Form.Item>

          <Form.Item
            name="client_knowledge_base"
            label="Client Knowledge Base"
            tooltip="Provide key facts, product names, and unique selling propositions for your brand. This will be injected into every AI content generation prompt."
          >
            <TextArea rows={6} placeholder="e.g., Our flagship product is 'ProfitPilot', an AI-powered SEO tool designed for small businesses to automate keyword research and content generation. We focus on ROI and actionable insights, not just vanity metrics." />
          </Form.Item>

          <Form.Item>
            <Button type="primary" htmlType="submit" loading={isUpdating}>
              Save Settings
            </Button>
          </Form.Item>
        </Form>
      </Content>
    </Layout>
  );
};

export default ClientSettingsPage;
</file>

<file path="client/my-content-app/src/pages/Dashboard/DashboardPage.jsx">
import React from 'react';
import { useQuery } from 'react-query';
import { Layout, Typography, Spin, Alert, Row, Col, Card, Statistic, Table, Tag } from 'antd';
import { 
  FileTextOutlined, 
  CheckCircleOutlined, 
  DollarCircleOutlined, 
  ExperimentOutlined,
  ClockCircleOutlined,
  WarningOutlined,
  ArrowRightOutlined,
  ReadOutlined
} from '@ant-design/icons';
import { useClient } from '../../hooks/useClient';
import { getDashboardData } from '../../services/clientService';
import { format } from 'date-fns';
import { useNavigate } from 'react-router-dom';
import FunnelChart from '../DiscoveryPage/components/FunnelChart';

const { Content } = Layout;
const { Title, Text, Link } = Typography;

const KpiCard = ({ icon, title, value, prefix, precision = 0 }) => (
  <Card>
    <Statistic 
      title={title} 
      value={value} 
      precision={precision} 
      prefix={icon ? React.createElement(icon, { style: { marginRight: 8 } }) : prefix} 
    />
  </Card>
);

const DashboardPage = () => {
  const { clientId } = useClient();
  const navigate = useNavigate();

  const { data, isLoading, isError, error } = useQuery(
    ['dashboardData', clientId],
    () => getDashboardData(clientId),
    {
      enabled: !!clientId,
    }
  );

  if (isLoading) {
    return <Spin tip="Loading dashboard..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (isError) {
    return <Alert message="Error" description={error.message} type="error" showIcon />;
  }

  const { kpis, funnelData, actionItems, recent_items } = data;

  const actionColumns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
      render: (text, record) => <Link onClick={() => navigate(`/opportunities/${record.id}`)}>{text}</Link>,
    },
    {
      title: 'Score',
      dataIndex: 'strategic_score',
      key: 'strategic_score',
      render: (score) => (
        <Tag color="blue">
          {typeof score === 'number' ? score.toFixed(1) : 'N/A'}
        </Tag>
      ),
    },
    {
      title: 'Updated',
      dataIndex: 'updated_at',
      key: 'updated_at',
      render: (ts) => ts ? format(new Date(ts), 'yyyy-MM-dd') : 'N/A',
    },
    {
      title: 'Action',
      key: 'action',
      render: (_, record) => <Link onClick={() => navigate(`/opportunities/${record.id}`)}>Review <ArrowRightOutlined /></Link>,
    },
  ];

  const failedColumns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
      render: (text, record) => <Link onClick={() => navigate(`/opportunities/${record.id}`)}>{text}</Link>,
    },
    {
      title: 'Error',
      dataIndex: 'error_message',
      key: 'error_message',
      render: (msg) => <Text type="danger" ellipsis={{ tooltip: msg }}>{msg || 'No details'}</Text>,
    },
    {
      title: 'Action',
      key: 'action',
      render: (_, record) => <Link onClick={() => navigate(`/opportunities/${record.id}`)}>Details <ArrowRightOutlined /></Link>,
    },
  ];
  
  const recentActivityColumns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
      render: (text, record) => <Link onClick={() => navigate(`/opportunities/${record.id}`)}>{text}</Link>,
    },
    {
      title: 'Generated On',
      dataIndex: 'date_processed',
      key: 'date_processed',
      render: (ts) => ts ? format(new Date(ts), 'yyyy-MM-dd HH:mm') : 'N/A',
    },
    {
        title: 'Action',
        key: 'action',
        render: (_, record) => <Link onClick={() => navigate(`/opportunities/${record.id}?tab=Article`)}>View Article <ReadOutlined /></Link>,
    },
  ];

  // Safely extract funnel data
  const getFunnelCount = (stage) => funnelData?.find(d => d.stage === stage)?.count || 0;

  return (
    <Layout>
      <Content style={{ padding: '24px' }}>
        <Title level={2}>Dashboard for {clientId}</Title>
        
        <Row gutter={[24, 24]} style={{ marginBottom: '24px' }}>
          <Col xs={24} sm={12} md={6}><KpiCard icon={FileTextOutlined} title="Total Opportunities" value={kpis.totalOpportunities} /></Col>
          <Col xs={24} sm={12} md={6}><KpiCard icon={CheckCircleOutlined} title="Content Generated" value={kpis.contentGenerated} /></Col>
          <Col xs={24} sm={12} md={6}><KpiCard icon={DollarCircleOutlined} title="Est. Monthly Traffic Value" value={kpis.totalTrafficValue} prefix="$" precision={2} /></Col>
          <Col xs={24} sm={12} md={6}><KpiCard icon={ExperimentOutlined} title="Total API Cost" value={kpis.totalApiCost} prefix="$" precision={2} /></Col>
        </Row>

        <Row gutter={[24, 24]}>
          {/* Left Column */}
          <Col xs={24} lg={16}>
            <div style={{ display: 'flex', flexDirection: 'column', gap: '24px' }}>
              {actionItems?.awaitingApproval?.length > 0 && (
                <Card title={<><ClockCircleOutlined style={{marginRight: 8}} /> Awaiting Your Approval</>}>
                  <Table
                    dataSource={actionItems.awaitingApproval}
                    columns={actionColumns}
                    rowKey="id"
                    pagination={{ pageSize: 5 }}
                    size="small"
                  />
                </Card>
              )}
              {actionItems?.failed?.length > 0 && (
                <Card title={<><WarningOutlined style={{marginRight: 8}} /> Failed Workflows</>}>
                  <Table
                    dataSource={actionItems.failed}
                    columns={failedColumns}
                    rowKey="id"
                    pagination={{ pageSize: 5 }}
                    size="small"
                  />
                </Card>
              )}
            </div>
          </Col>

          {/* Right Column */}
          <Col xs={24} lg={8}>
             <div style={{ display: 'flex', flexDirection: 'column', gap: '24px' }}>
                <Card title="Recent Activity">
                    <Table
                        dataSource={recent_items}
                        columns={recentActivityColumns}
                        rowKey="id"
                        pagination={{ pageSize: 10 }}
                        size="small"
                        locale={{ emptyText: 'No recent activity' }}
                    />
                </Card>
                <Card title="Content Pipeline Funnel">
                  <FunnelChart 
                    totalRaw={getFunnelCount('Total')}
                    unique={getFunnelCount('Validated')}
                    qualified={getFunnelCount('Analyzed')}
                    disqualified={getFunnelCount('Disqualified')}
                    addedToDB={getFunnelCount('Generated')}
                  />
                </Card>
             </div>
          </Col>
        </Row>
      </Content>
    </Layout>
  );
};

export default DashboardPage;
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/components/DiscoveryForm.jsx">
import React from 'react';
import { Input, Button, Typography, Form, Row, Col, InputNumber, Select, Switch, Modal } from 'antd';
import { RocketOutlined } from '@ant-design/icons';
import { useDiscoveryFilters } from '../hooks/useDiscoveryFilters';

const { Title } = Typography;
const { Option } = Select;

const DiscoveryForm = ({ isSubmitting, onSubmit }) => {
  const [form] = Form.useForm();
  useDiscoveryFilters();

      const onFinish = (values) => {
        const currentFilters = form.getFieldValue('filters'); // Assuming 'filters' is the form item holding the filter array
        if (currentFilters && currentFilters.length > 8) {
            Modal.error({
                title: 'Too Many Filters',
                content: 'You can specify a maximum of 8 filter conditions. Please remove some filters.',
            });
            return;
        }
        const { keyword, limit, search_volume_value, difficulty_value, competition_level, search_intent, include_clickstream_data, closely_variants, ignore_synonyms, exact_match } = values;

// ADDITION: Read discovery mode and max pages from form values
const discovery_modes = form.getFieldValue('discovery_modes') || ['ideas'];
const discovery_max_pages = form.getFieldValue('discovery_max_pages') || 1;

        // ... (rest of filtering logic) ...
        const filters = [];
        const filterPathPrefix = 'keyword_data.'; // Defaulting to a common prefix, backend will handle specifics
        if (search_volume_value !== undefined && search_volume_value !== null) {
          filters.push({ field: `${filterPathPrefix}keyword_info.search_volume`, operator: '>', value: search_volume_value });
        }
        if (difficulty_value !== undefined && difficulty_value !== null) {
          filters.push({ field: `${filterPathPrefix}keyword_properties.keyword_difficulty`, operator: '<', value: difficulty_value });
        }
        if (competition_level && competition_level.length > 0) {
          filters.push({ field: `${filterPathPrefix}keyword_info.competition_level`, operator: 'in', value: competition_level });
        }
        if (search_intent && search_intent.length > 0) {
          filters.push({ field: `${filterPathPrefix}search_intent_info.main_intent`, operator: 'in', value: search_intent });
        }

        const runData = {
          seed_keywords: [keyword],
          limit: limit,
          filters: filters.length > 0 ? filters : null,
          include_clickstream_data,
          closely_variants,
          ignore_synonyms,
          exact_match,
// ADD these fields to runData:
discovery_modes,
discovery_max_pages,
        };
        
        onSubmit({ runData });
      };

  

      return (

  

        <Form form={form} layout="vertical" onFinish={onFinish} initialValues={{

  

          limit: 1000,

  

          search_volume_value: 500,

  

          difficulty_value: 20,

  

          competition_level: ['LOW'],

  

          search_intent: ['informational'],

  

        }}>

  

          <Title level={4}>1. Enter a Seed Keyword</Title>

  

          <Form.Item name="keyword" rules={[{ required: true, message: 'Please enter a seed keyword.' }]}>

  

            <Input placeholder="e.g., content marketing" />

  

          </Form.Item>

  

          

  

          <Title level={4}>2. Add Filters (Optional)</Title>

  

          <Row gutter={16}>

  

            <Col span={12}>

  

              <Form.Item name="limit" label="Limit (Number of keywords to find)">

  

                <InputNumber style={{ width: '100%' }} placeholder="e.g., 1000" />

  

              </Form.Item>

  

            </Col>

  

            <Col span={12}>

  

              <Form.Item name="search_volume_value" label="Monthly Search Volume (Greater than)">

  

                <InputNumber style={{ width: '100%' }} placeholder="e.g., 500" />

  

              </Form.Item>

  

            </Col>

  

            <Col span={12}>

  

              <Form.Item name="difficulty_value" label="SEO Difficulty (Less than)">

  

                <InputNumber style={{ width: '100%' }} placeholder="e.g., 20" min={0} max={100} />

  

              </Form.Item>

  

            </Col>

  

            <Col span={12}>

  

              <Form.Item name="competition_level" label="Competition Level">

  

                <Select mode="multiple" placeholder="Any" allowClear>

  

                  <Option value="LOW">Low</Option>

  

                  <Option value="MEDIUM">Medium</Option>

  

                  <Option value="HIGH">High</Option>

  

                </Select>

  

              </Form.Item>

  

            </Col>

  

            <Col span={12}>

  

              <Form.Item name="search_intent" label="Search Intent">

  

                <Select mode="multiple" placeholder="Any" allowClear>

  

                  <Option value="informational">Informational</Option>

  

                  <Option value="commercial">Commercial</Option>

  

                  <Option value="transactional">Transactional</Option>

  

                  <Option value="navigational">Navigational</Option>

  

                </Select>

  

              </Form.Item>

  

            </Col>

  

          </Row>

          <Row gutter={16}>
            <Col span={24}>
              <Form.Item name="include_clickstream_data" label="Include Clickstream Demographics" valuePropName="checked" tooltip="Provides audience demographic data but doubles the API cost of the discovery run.">
                <Switch />
              </Form.Item>
            </Col>
            <Col span={12}>
              <Form.Item name="closely_variants" label="Use Phrase Match" valuePropName="checked" tooltip="Limits results to keywords that are close variants of the seed keyword (more targeted).">
                <Switch />
              </Form.Item>
            </Col>
            <Col span={12}>
              <Form.Item name="exact_match" label="Use Exact Match" valuePropName="checked" tooltip="Returns only keywords that exactly match the seed keyword's phrasing.">
                <Switch />
              </Form.Item>
            </Col>
            <Col span={12}>
              <Form.Item name="ignore_synonyms" label="Ignore Synonyms" valuePropName="checked" tooltip="Returns only core keywords, excluding highly similar variations.">
                <Switch />
              </Form.Item>
            </Col>
          </Row>

      <Row justify="end" align="middle" style={{ marginTop: '24px' }}>
        <Col>
          <Button type="primary" htmlType="submit" icon={<RocketOutlined />} loading={isSubmitting} size="large">
            Find Opportunities
          </Button>
        </Col>
      </Row>
    </Form>
  );
};

export default DiscoveryForm;
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/components/DiscoveryHistory.jsx">
import React, { useState, useMemo } from 'react';
import { Table, Tag, Tooltip, Button, Modal, Progress, Empty, Typography, Input, Row, Col, DatePicker, Alert } from 'antd';
import { ReloadOutlined, ProfileOutlined, ClockCircleOutlined, CheckCircleOutlined, CloseCircleOutlined, LoadingOutlined, EyeOutlined } from '@ant-design/icons';
import { useNavigate } from 'react-router-dom';
import { formatDistanceToNow } from 'date-fns';
import RunDetailsModal from './RunDetailsModal';
import DiscoveryStatsBreakdown from './DiscoveryStatsBreakdown'; // NEW: For expandable row

const { Title, Text } = Typography;
const { Search } = Input;
const { RangePicker } = DatePicker;

const STATUS_CONFIG = {
  running: { color: 'processing', text: 'Running', icon: <LoadingOutlined spin /> },
  completed: { color: 'success', text: 'Completed', icon: <CheckCircleOutlined /> },
  failed: { color: 'error', text: 'Failed', icon: <CloseCircleOutlined /> },
  pending: { color: 'default', text: 'Pending', icon: <ClockCircleOutlined /> },
};

const DiscoveryHistory = ({ runs, totalRuns, page, setPage, isLoading, onRerun, isRerunning }) => {
  const navigate = useNavigate();
  const [filterText, setFilterText] = useState('');
  const [dateRange, setDateRange] = useState(null);
  const [errorModal, setErrorModal] = useState({ open: false, content: '' });
  const [detailsModal, setDetailsModal] = useState({ open: false, run: null });

  const handleShowError = (errorMsg) => {
    setErrorModal({ open: true, content: errorMsg || 'No error details provided.' });
  };

  const handleShowDetails = (run) => {
    setDetailsModal({ open: true, run: run });
  };

  const filteredRuns = useMemo(() => {
    let filtered = runs;

    if (filterText) {
      const lowerCaseFilter = filterText.toLowerCase();
      filtered = filtered.filter(run => 
        run.parameters?.seed_keywords?.some(kw => kw.toLowerCase().includes(lowerCaseFilter)) || 
        run.status.toLowerCase().includes(lowerCaseFilter)
      );
    }

    if (dateRange) {
      const [start, end] = dateRange;
      filtered = filtered.filter(run => {
        const runDate = new Date(run.start_time);
        return runDate >= start && runDate <= end;
      });
    }

    return filtered;
  }, [runs, filterText, dateRange]);

  const expandedRowRender = (record) => {
    if (!record.results_summary) {
      return <Text type="secondary">No detailed summary available for this run.</Text>;
    }
    return <DiscoveryStatsBreakdown summary={record.results_summary} runId={record.id} />;
  };

  const columns = [
    {
      title: 'Start Time',
      dataIndex: 'start_time',
      key: 'start_time',
      render: (text) => text ? `${formatDistanceToNow(new Date(text))} ago` : 'N/A',
      sorter: (a, b) => new Date(a.start_time) - new Date(b.start_time),
      defaultSortOrder: 'descend',
    },
    {
      title: 'Status',
      dataIndex: 'status',
      key: 'status',
      render: (status, record) => {
        const config = STATUS_CONFIG[status] || STATUS_CONFIG.pending;
        const progress = record.results_summary?.progress || (status === 'running' ? record.progress || 0 : 0);
        return (
          <div style={{ display: 'flex', alignItems: 'center', flexDirection: 'column' }}>
            <Tag icon={config.icon} color={config.color} onClick={() => status === 'failed' && handleShowError(record.error_message)} style={{ cursor: status === 'failed' ? 'pointer' : 'default', marginBottom: 4 }}>
              {config.text}
            </Tag>
            {status === 'running' && <Progress percent={progress} size="small" status="active" showInfo={false} style={{ width: 100 }} />}
          </div>
        );
      },
      sorter: (a, b) => a.status.localeCompare(b.status),
    },
    {
      title: 'Seed Keywords',
      dataIndex: 'parameters',
      key: 'seed_keywords',
      responsive: ['md'],
      render: (params) => {
        const keywords = params?.seed_keywords || [];
        if (keywords.length === 0) return <Text type="secondary">N/A</Text>;
        const displayedKeywords = keywords.slice(0, 3);
        const remainingCount = keywords.length - displayedKeywords.length;
        return (
          <>
            {displayedKeywords.map(kw => <Tag key={kw}>{kw}</Tag>)}
            {remainingCount > 0 && <Tooltip title={keywords.slice(3).join(', ')}><Tag>+{remainingCount} more</Tag></Tooltip>}
          </>
        );
      },
    },
    {
      title: 'Discovery Mode',
      dataIndex: 'parameters',
      key: 'discovery_mode',
      responsive: ['lg'],
      render: (params) => {
        const modes = params?.discovery_modes || ['ideas'];
        const filters = params?.filters;
        const orderBy = params?.order_by;
        const tooltipContent = (
          <pre style={{ maxWidth: 500, whiteSpace: 'pre-wrap' }}>
            {JSON.stringify({ filters, order_by: orderBy }, null, 2)}
          </pre>
        );
        return (
          <Tooltip title={tooltipContent}>
            {modes.map(mode => <Tag key={mode}>{mode.replace('_', ' ').toUpperCase()}</Tag>)}
          </Tooltip>
        );
      },
    },
    {
      title: 'Actions',
      key: 'actions',
      align: 'right',
      fixed: 'right',
      render: (_, record) => (
        <div style={{ display: 'flex', gap: '8px', justifyContent: 'flex-end' }}>
          <Tooltip title="Re-run this discovery with the same settings">
            <Button icon={<ReloadOutlined />} onClick={() => onRerun(record.id)} disabled={isRerunning} />
          </Tooltip>
          {record.status === 'completed' && (
            <>
              <Tooltip title="View Run Details">
                <Button icon={<EyeOutlined />} onClick={() => handleShowDetails(record)} />
              </Tooltip>
              <Tooltip title="View Keywords">
                <Button type="primary" icon={<ProfileOutlined />} onClick={() => navigate(`/discovery-run/${record.id}`)} />
              </Tooltip>
            </>
          )}
        </div>
      ),
    },
  ];

  return (
    <div style={{ marginTop: '32px' }}>
        <Row justify="space-between" align="middle" style={{ marginBottom: '16px' }} gutter={[16, 16]}>
            <Col><Title level={3} style={{margin: 0}}>Discovery History</Title></Col>
            <Col flex="auto" style={{textAlign: 'right'}}>
              <Search 
                placeholder="Filter by keyword or status..." 
                allowClear 
                value={filterText} 
                onChange={e => setFilterText(e.target.value)} 
                style={{ width: 250, marginRight: '8px' }} 
              />
              <RangePicker onChange={(dates) => setDateRange(dates)} />
            </Col>
        </Row>
      <Table
        loading={isLoading}
        columns={columns}
        dataSource={filteredRuns}
        rowKey="id"
        scroll={{ x: 800 }}
        locale={{ emptyText: <Empty description="No discovery runs found. Start one above to see your history." /> }}
        pagination={{
          current: page,
          pageSize: 10,
          total: totalRuns,
          onChange: setPage,
        }}
        expandable={{ expandedRowRender }} // NEW: Enable expandable rows
      />
      <Modal title="Run Failed" open={errorModal.open} onOk={() => setErrorModal({ open: false, content: '' })} onCancel={() => setErrorModal({ open: false, content: '' })} footer={[<Button key="back" onClick={() => setErrorModal({ open: false, content: '' })}>Close</Button>]}>
<Text strong>Error Message:</Text>
<pre style={{ marginTop: '8px', background: '#f5f5f5', padding: '12px', borderRadius: '4px', overflowX: 'auto', whiteSpace: 'pre-wrap', wordBreak: 'break-word' }}>{errorModal.content}</pre>
</Modal>
      
      <RunDetailsModal 
        run={detailsModal.run}
        open={detailsModal.open}
        onCancel={() => setDetailsModal({ open: false, run: null })}
      />
    </div>
  );
};

export default DiscoveryHistory;
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/components/DiscoveryStatsBreakdown.jsx">
import React, { useState } from 'react';
import { Row, Col, Typography, Empty, Modal, Table } from 'antd';
import PieChartCard from './PieChartCard';
import FunnelChart from './FunnelChart'; // NEW: From Task 35
import { getDisqualifiedKeywords } from '../../../services/discoveryService';

const { Title, Text } = Typography;

const DiscoveryStatsBreakdown = ({ summary, runId }) => {
  const [modalVisible, setModalVisible] = useState(false);
  const [modalTitle, setModalTitle] = useState('');
  const [modalData, setModalData] = useState([]);
  const [modalLoading, setModalLoading] = useState(false);

  if (!summary) {
    return <Empty description="No summary data available for this run." />;
  }

  const handleReasonClick = async (reason) => {
    setModalTitle(`Disqualified Keywords: ${reason}`);
    setModalVisible(true);
    setModalLoading(true);
    try {
      const data = await getDisqualifiedKeywords(runId, reason);
      setModalData(data);
    } catch (error) {
      console.error('Failed to fetch disqualified keywords:', error);
    } finally {
      setModalLoading(false);
    }
  };

  // Destructure with defaults to prevent errors if fields are missing
  const {
    source_counts = {},
    disqualification_reasons = {},
    total_raw_count = 0,
    total_unique_count = 0,
    disqualified_count = 0,
    final_added_to_db = 0,
    final_qualified_count = 0
  } = summary;

  const modalColumns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
    },
    {
      title: 'Search Volume',
      dataIndex: ['keyword_info', 'search_volume'],
      key: 'search_volume',
    },
  ];

  return (
    <div style={{ padding: '16px', backgroundColor: '#fafafa' }}>
      <Row gutter={[32, 32]}>
        {/* Section 1: Keyword Sources */}
        <Col xs={24} md={12} lg={8}>
          <PieChartCard title="Keyword Sources" data={source_counts} />
        </Col>

        {/* Section 2: Processing Funnel (Visualized) */}
        <Col xs={24} lg={16}>
            <FunnelChart 
                totalRaw={total_raw_count}
                unique={total_unique_count}
                qualified={final_qualified_count}
                disqualified={disqualified_count}
                addedToDB={final_added_to_db}
            />
        </Col>

        {/* Section 3: Disqualification Reasons */}
        <Col xs={24} md={12} lg={8}>
          <PieChartCard title="Disqualification Reasons" data={disqualification_reasons} onSliceClick={handleReasonClick} />
        </Col>
      </Row>
      <Modal
        title={modalTitle}
        open={modalVisible}
        onCancel={() => setModalVisible(false)}
        footer={null}
        width={800}
      >
        <Table
          loading={modalLoading}
          dataSource={modalData}
          columns={modalColumns}
          rowKey="id"
          pagination={{ pageSize: 10 }}
        />
      </Modal>
    </div>
  );
};

export default DiscoveryStatsBreakdown;
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/components/FilterBuilder.jsx">
import React, { useState } from 'react';
import { Select, Button, InputNumber, Row, Col } from 'antd';
import { PlusOutlined, DeleteOutlined } from '@ant-design/icons';

const { Option } = Select;

const FilterBuilder = ({ availableFilters, onChange }) => {
  const [filters, setFilters] = useState([{ field: null, operator: null, value: null }]);

  const handleFilterChange = (index, field, value) => {
    const newFilters = [...filters];
    newFilters[index][field] = value;

    // Reset operator and value if field changes
    if (field === 'field') {
      newFilters[index]['operator'] = null;
      newFilters[index]['value'] = null;
    }

    setFilters(newFilters);
    onChange(newFilters);
  };

  const addFilter = () => {
    const newFilters = [...filters, { field: null, operator: null, value: null }];
    setFilters(newFilters);
    onChange(newFilters);
  };

  const removeFilter = (index) => {
    const newFilters = filters.filter((_, i) => i !== index);
    setFilters(newFilters);
    onChange(newFilters);
  };

  const getOperatorsForField = (fieldName) => {
    const modeFilters = availableFilters?.filtersData?.modes.find(m => m.id === 'keyword_ideas')?.filters;
    const field = modeFilters?.find(f => f.name === fieldName);
    return field ? field.operators : [];
  };

  const getInputForField = (fieldName, index) => {
    const modeFilters = availableFilters?.filtersData?.modes.find(m => m.id === 'keyword_ideas')?.filters;
    const field = modeFilters?.find(f => f.name === fieldName);
    if (!field) return null;

    switch (field.type) {
      case 'number':
        return <InputNumber value={filters[index].value} onChange={(val) => handleFilterChange(index, 'value', val)} />;
      case 'select':
        return (
          <Select
            style={{ width: 120 }}
            value={filters[index].value}
            onChange={(val) => handleFilterChange(index, 'value', val)}
          >
            {field.options.map(opt => <Option key={opt} value={opt}>{opt}</Option>)}
          </Select>
        );
      default:
        return null;
    }
  };

  return (
    <div>
      {filters.map((filter, index) => (
        <Row key={index} gutter={8} style={{ marginBottom: 8 }}>
          <Col>
            <Select
              style={{ width: 200 }}
              placeholder="Select field"
              value={filter.field}
              onChange={(val) => handleFilterChange(index, 'field', val)}
            >
              {availableFilters?.filters.map(f => <Option key={f.name} value={f.name}>{f.label}</Option>)}
            </Select>
          </Col>
          <Col>
            <Select
              style={{ width: 80 }}
              placeholder="Op"
              value={filter.operator}
              onChange={(val) => handleFilterChange(index, 'operator', val)}
              disabled={!filter.field}
            >
              {getOperatorsForField(filter.field).map(op => <Option key={op} value={op}>{op}</Option>)}
            </Select>
          </Col>
          <Col>
            {getInputForField(filter.field, index)}
          </Col>
          <Col>
            <Button icon={<DeleteOutlined />} onClick={() => removeFilter(index)} danger />
          </Col>
        </Row>
      ))}
      <Button type="dashed" onClick={addFilter} icon={<PlusOutlined />} disabled={filters.length >= 8}>
        Add Filter ({filters.length}/8)
      </Button>
    </div>
  );
};

export default FilterBuilder;
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/components/FunnelChart.css">
/* Styling for the FunnelChart component */
.funnel-chart-container {
  background-color: #f9f9f9; /* Light background for the expanded row */
  padding: 24px;
  border-radius: 8px;
  border: 1px solid #e8e8e8;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.09);
  margin-bottom: 24px;
}

.funnel-steps {
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 100%;
  position: relative;
  min-height: 150px; /* Ensure some height for visualization */
}

.funnel-step {
  height: 30px; /* Fixed height for each step */
  margin-bottom: 5px; /* Spacing between steps */
  border-radius: 4px;
  display: flex;
  justify-content: center;
  align-items: center;
  color: white;
  font-size: 14px;
  font-weight: bold;
  transition: all 0.3s ease-in-out;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
}

.funnel-step:hover {
  transform: scale(1.02);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
}

.funnel-value {
  color: white !important;
}
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/components/FunnelChart.jsx">
import React from 'react';
import { Typography, Row, Col, Statistic, Tooltip } from 'antd';
import './FunnelChart.css'; // Specific styles for the funnel chart

const { Text, Title } = Typography;

const FunnelChart = ({ totalRaw = 0, unique = 0, qualified = 0, disqualified = 0, addedToDB = 0 }) => {
  const data = [
    { label: "Total Found", value: totalRaw },
    { label: "Unique Keywords", value: unique },
    { label: "Qualified Keywords", value: qualified },
    { label: "Disqualified", value: disqualified },
    { label: "Added to Pipeline", value: addedToDB },
  ];

  const getColor = (label) => {
    switch(label) {
      case "Total Found": return "#1890ff";
      case "Unique Keywords": return "#2db7f5";
      case "Qualified Keywords": return "#52c41a";
      case "Disqualified": return "#f5222d";
      case "Added to Pipeline": return "#722ed1";
      default: return "#d9d9d9";
    }
  };

  return (
    <div className="funnel-chart-container">
      <Title level={5} style={{ marginBottom: '24px', textAlign: 'center' }}>Discovery Funnel</Title>
      <div className="funnel-steps">
        {data.map((item, index) => (
          <Tooltip title={item.label} key={index}>
            <div className="funnel-step" style={{ 
                backgroundColor: getColor(item.label),
                width: `${Math.max(20, (item.value / totalRaw) * 100)}%`, // Scale width
                zIndex: data.length - index // Ensure larger bars are behind
            }}>
              <Text strong className="funnel-value">{item.value.toLocaleString()}</Text>
            </div>
          </Tooltip>
        ))}
      </div>
      <Row gutter={[16, 16]} justify="space-around" style={{ marginTop: '24px' }}>
        {data.map((item, index) => (
          <Col key={index} span={Math.floor(24 / data.length)}>
            <Statistic 
              title={item.label} 
              value={item.value.toLocaleString()} 
              valueStyle={{ color: getColor(item.label) }} 
            />
          </Col>
        ))}
      </Row>
    </div>
  );
};

export default FunnelChart;
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/components/PieChartCard.jsx">
import React from 'react';
import { Card, Typography, Empty, Tooltip } from 'antd';
import { PieChart, Pie, Cell, Tooltip as RechartsTooltip, Legend, ResponsiveContainer } from 'recharts';

const { Title } = Typography;

const COLORS = ['#0088FE', '#00C49F', '#FFBB28', '#FF8042', '#AF19FF', '#FF19AF'];

const PieChartCard = ({ title, data, onSliceClick }) => {
  const chartData = Object.entries(data).map(([name, value]) => ({ name, value }));

  if (chartData.length === 0) {
    return (
      <Card>
        <Title level={5} style={{ marginBottom: '16px' }}>{title}</Title>
        <Empty description={`No ${title.toLowerCase()} data`} />
      </Card>
    );
  }

  const renderLegend = (props) => {
    const { payload } = props;
    return (
      <ul style={{ listStyle: 'none', padding: '0', margin: '0', maxHeight: '300px', overflowY: 'auto' }}>
        {payload.map((entry, index) => {
          const { value, color } = entry;
          const maxLength = 40;
          const isTruncated = value.length > maxLength;
          const truncatedValue = isTruncated ? `${value.substring(0, maxLength)}...` : value;

          return (
            <li key={`item-${index}`} style={{ marginBottom: '4px', display: 'flex', alignItems: 'center' }}>
              <span style={{ width: '10px', height: '10px', backgroundColor: color, marginRight: '10px', display: 'inline-block' }}></span>
              {isTruncated ? (
                <Tooltip title={value}>
                  <span style={{ cursor: 'default' }}>{truncatedValue}</span>
                </Tooltip>
              ) : (
                <span>{value}</span>
              )}
            </li>
          );
        })}
      </ul>
    );
  };


  return (
    <Card>
      <Title level={5} style={{ marginBottom: '16px' }}>{title}</Title>
      <ResponsiveContainer width="100%" height={300}>
        <PieChart>
          <Pie
            data={chartData}
            cx="50%"
            cy="50%"
            labelLine={false}
            outerRadius={80}
            fill="#8884d8"
            dataKey="value"
            nameKey="name"
            onClick={(data) => onSliceClick && onSliceClick(data.name)}
          >
            {chartData.map((entry, index) => (
              <Cell key={`cell-${index}`} fill={COLORS[index % COLORS.length]} />
            ))}
          </Pie>
          <RechartsTooltip />
          <Legend content={renderLegend} />
        </PieChart>
      </ResponsiveContainer>
    </Card>
  );
};

export default PieChartCard;
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/components/RunDetailsModal.jsx">
import React from 'react';
import { Modal, Tag, Row, Col, Descriptions, Statistic, Steps, Card, Typography } from 'antd';
import { formatDistanceStrict } from 'date-fns';
import PieChartCard from './PieChartCard';

const { Title, Text } = Typography;
const { Step } = Steps;

const STATUS_CONFIG = {
  completed: { color: 'success', text: 'Completed' },
  failed: { color: 'error', text: 'Failed' },
  running: { color: 'processing', text: 'Running' },
  pending: { color: 'default', text: 'Pending' },
};

const RunDetailsModal = ({ run, open, onCancel }) => {
  if (!run) return null;

  const {
    id,
    start_time,
    end_time,
    status,
    parameters = {},
    results_summary = {},
  } = run;

  const {
    total_cost = 0,
    source_counts = {},
    total_raw_count = 0,
    total_unique_count = 0,
    final_qualified_count = 0,
    duplicates_removed = 0,
    final_added_to_db = 0,
    disqualification_reasons = {},
  } = results_summary;

  const statusInfo = STATUS_CONFIG[status] || STATUS_CONFIG.pending;

  return (
    <Modal
      title={
        <div style={{ display: 'flex', alignItems: 'center' }}>
          <Title level={4} style={{ margin: 0 }}>Discovery Run #{id}</Title>
          <Tag color={statusInfo.color} style={{ marginLeft: '12px' }}>{statusInfo.text}</Tag>
        </div>
      }
      open={open}
      onCancel={onCancel}
      footer={null}
      width="80vw"
      style={{ top: 20 }}
    >
      {/* Key Metrics */}
      <Row gutter={[32, 16]} style={{ marginBottom: '24px' }}>
        <Col><Statistic title="Total Cost" prefix="$" value={total_cost.toFixed(2)} /></Col>
        <Col><Statistic title="Run Duration" value={end_time ? formatDistanceStrict(new Date(end_time), new Date(start_time)) : 'N/A'} /></Col>
        <Col><Statistic title="Keywords Found" value={total_unique_count} /></Col>
        <Col><Statistic title="Added to Pipeline" value={final_added_to_db} valueStyle={{ color: '#3f8600' }} /></Col>
      </Row>

      {/* Processing Funnel */}
      <Card title="Processing Funnel" style={{ marginBottom: '24px' }}>
        <Steps current={5} size="small">
          <Step title="Total Found" description={`${total_raw_count.toLocaleString()}`} />
          <Step title="Unique" description={`${total_unique_count.toLocaleString()}`} />
          <Step title="Qualified" description={`${final_qualified_count.toLocaleString()}`} />
          <Step title="Duplicates Removed" description={`${duplicates_removed.toLocaleString()}`} />
          <Step title="Added to DB" description={<Text strong style={{color: '#3f8600'}}>{final_added_to_db.toLocaleString()}</Text>} />
        </Steps>
      </Card>

      <Row gutter={[24, 24]}>
        {/* Parameters */}
        <Col xs={24} lg={8}>
          <Card title="Run Parameters">
            <Descriptions bordered column={1} size="small">
              <Descriptions.Item label="Seed Keywords">
                {(parameters.seed_keywords || []).map(kw => <Tag key={kw}>{kw}</Tag>)}
              </Descriptions.Item>
              {Object.entries(parameters.filters_override || {}).map(([key, value]) => (
                <Descriptions.Item key={key} label={key.replace(/_/g, ' ')}>{String(value)}</Descriptions.Item>
              ))}
            </Descriptions>
          </Card>
        </Col>

        {/* Visualizations */}
        <Col xs={24} lg={16}>
          <Row gutter={[24, 24]}>
            <Col xs={24} md={12}>
              <PieChartCard title="Keyword Sources" data={source_counts} />
            </Col>
            <Col xs={24} md={12}>
              <PieChartCard title="Disqualification Reasons" data={disqualification_reasons} />
            </Col>
          </Row>
        </Col>
      </Row>
    </Modal>
  );
};

export default RunDetailsModal;
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/components/RunDetailsPage.jsx">
import React, { useState, useEffect } from 'react';
import { useParams, useNavigate } from 'react-router-dom';
import { Table, Tag, Spin, Alert, Typography, Button, Row, Col } from 'antd';
import { ArrowLeftOutlined, CheckCircleOutlined } from '@ant-design/icons';
import { getKeywordsForRun } from '../../../services/discoveryService';
import { overrideDisqualification } from '../../../services/opportunitiesService';
import { useNotifications } from '../../../hooks/useNotifications';

const { Title, Text } = Typography;

const RunDetailsPage = () => {
  const { runId } = useParams();
  const navigate = useNavigate();
  const [keywords, setKeywords] = useState([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);
  const [updatingIds, setUpdatingIds] = useState(new Set());
  const { showNotification } = useNotifications();

  useEffect(() => {
    const fetchKeywords = async () => {
      try {
        setLoading(true);
        const response = await getKeywordsForRun(runId);
        setKeywords(response || []);
      } catch (err) {
        setError('Failed to fetch keyword details for this run.');
        console.error(err);
      } finally {
        setLoading(false);
      }
    };

    fetchKeywords();
  }, [runId]);

  const handleOverride = async (opportunityId) => {
    setUpdatingIds(prev => new Set(prev).add(opportunityId));
    try {
      await overrideDisqualification(opportunityId);
      showNotification('success', 'Keyword Re-qualified', 'The keyword has been moved to the pending queue.');
      // Optimistically update the UI
      setKeywords(prevKeywords => 
        prevKeywords.map(kw => 
          kw.id === opportunityId 
            ? { ...kw, blog_qualification_status: 'passed_manual_override', blog_qualification_reason: 'Manually overridden by user.' }
            : kw
        )
      );
    } catch (err) {
      showNotification('error', 'Override Failed', err.message || 'Could not re-qualify the keyword.');
      console.error(err);
    } finally {
      setUpdatingIds(prev => {
        const newSet = new Set(prev);
        newSet.delete(opportunityId);
        return newSet;
      });
    }
  };

  const columns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
      sorter: (a, b) => a.keyword.localeCompare(b.keyword),
    },
    {
      title: 'Qualification Status',
      dataIndex: 'blog_qualification_status',
      key: 'blog_qualification_status',
      render: (status) => {
        let color = 'default';
        if (status === 'passed') color = 'success';
        else if (status === 'failed' || status === 'rejected') color = 'error';
        else if (status === 'passed_manual_override') color = 'processing';
        return (
          <Tag color={color}>
            {status ? status.replace(/_/g, ' ').toUpperCase() : 'N/A'}
          </Tag>
        );
      },
      filters: [
        { text: 'Passed', value: 'passed' },
        { text: 'Failed', value: 'failed' },
        { text: 'Override', value: 'passed_manual_override' },
      ],
      onFilter: (value, record) => record.blog_qualification_status === value,
    },
    {
      title: 'Reason',
      dataIndex: 'blog_qualification_reason',
      key: 'blog_qualification_reason',
      render: (reason) => reason || <Text type="secondary">N/A</Text>,
    },
    {
        title: 'Search Volume',
        dataIndex: ['keyword_info', 'search_volume'],
        key: 'search_volume',
        sorter: (a, b) => (a.keyword_info?.search_volume || 0) - (b.keyword_info?.search_volume || 0),
        render: (sv) => sv ? sv.toLocaleString() : 'N/A',
    },
    {
        title: 'Keyword Difficulty',
        dataIndex: ['keyword_properties', 'keyword_difficulty'],
        key: 'keyword_difficulty',
        sorter: (a, b) => (a.keyword_properties?.keyword_difficulty || 0) - (b.keyword_properties?.keyword_difficulty || 0),
        render: (kd) => kd || 'N/A',
    },
    {
      title: 'Actions',
      key: 'actions',
      render: (_, record) => {
        const isFailed = record.blog_qualification_status === 'failed' || record.blog_qualification_status === 'rejected';
        if (isFailed) {
          return (
            <Button 
              type="primary" 
              ghost 
              size="small"
              icon={<CheckCircleOutlined />}
              onClick={() => handleOverride(record.id)}
              loading={updatingIds.has(record.id)}
            >
              Re-qualify
            </Button>
          );
        }
        return null;
      },
    },
  ];

  if (loading) {
    return <Spin tip="Loading keywords..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (error) {
    return <Alert message="Error" description={error} type="error" showIcon />;
  }

  return (
    <>
      <div style={{ padding: '16px 24px', backgroundColor: '#fff', borderBottom: '1px solid #f0f0f0' }}>
        <Row align="middle" gutter={16}>
          <Col>
            <Button icon={<ArrowLeftOutlined />} onClick={() => navigate(-1)} />
          </Col>
          <Col>
            <Title level={4} style={{ margin: 0 }}>
              Keywords for Discovery Run #{runId}
            </Title>
            <Text type="secondary">{keywords.length.toLocaleString()} keywords found</Text>
          </Col>
        </Row>
      </div>
      <div style={{ padding: '24px' }}>
        <Table
          columns={columns}
          dataSource={keywords}
          rowKey="id"
          loading={loading}
          pagination={{ pageSize: 50 }}
        />
      </div>
    </>
  );
};

export default RunDetailsPage;
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/hooks/useDiscoveryFilters.js">
import { useQuery } from 'react-query';
import apiClient from '../../../services/apiClient';

const fetchDiscoveryFilters = async () => {
  const data = await apiClient.get('/api/discovery/available-filters');
  return data;
};

export const useDiscoveryFilters = () => {
  const { data, isLoading, isError } = useQuery('discoveryFilters', fetchDiscoveryFilters, {
    staleTime: Infinity, // This data is static, so we can cache it indefinitely
    cacheTime: Infinity,
  });

  return {
    filtersData: data,
    isLoading,
    isError,
  };
};
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/hooks/useDiscoveryRuns.js">
import { useState, useEffect } from 'react';
import { useQuery, useMutation, useQueryClient } from 'react-query';
import {
  getDiscoveryRuns,
  startDiscoveryRun,
  rerunDiscoveryRun,
  getJobStatus,
} from '../../../services/discoveryService';
import { useClient } from '../../../hooks/useClient';
import { useNotifications } from '../../../context/NotificationContext';

export const useDiscoveryRuns = () => {
  const queryClient = useQueryClient();
  const { clientId } = useClient();
  const { showNotification } = useNotifications();
  const [page, setPage] = useState(1);

  // Query to fetch discovery run history
  const {
    data,
    isLoading, // True on initial fetch
    isError, // True if query failed
    error, // Error object
  } = useQuery(
    ['discoveryRuns', clientId, page], // Unique query key, depends on clientId and page
    () => getDiscoveryRuns(clientId, page), // Function to fetch data
    {
      enabled: !!clientId, // Only run query if clientId is available
      keepPreviousData: true, // Keep previous data while fetching new page
    }
  );

  // Poll for updates on running jobs
  useEffect(() => {
    const runningJobs = data?.items?.filter(run => run.status === 'running' && run.job_id);
    if (runningJobs && runningJobs.length > 0) {
      const interval = setInterval(() => {
        runningJobs.forEach(run => {
          getJobStatus(run.job_id).then(job => {
            if (job.status === 'completed' || job.status === 'failed') {
              queryClient.invalidateQueries(['discoveryRuns', clientId]);
            }
          });
        });
      }, 5000); // Poll every 5s

      return () => clearInterval(interval);
    }
  }, [data, clientId, queryClient]);

  // Mutation for starting a new discovery run
  const startRunMutation = useMutation(startDiscoveryRun, {
    // Optimistic update logic
    onMutate: async (newRunRequest) => {
      await queryClient.cancelQueries(['discoveryRuns', clientId]); // Cancel any outgoing refetches

      const previousRuns = queryClient.getQueryData(['discoveryRuns', clientId, page]); // Snapshot previous state

      // Optimistically add a temporary 'running' job to the cache
      queryClient.setQueryData(['discoveryRuns', clientId, 1], (old) => ({
        ...old,
        items: [
          {
            id: `temp-${Date.now()}`, // Temporary ID for optimistic update
            start_time: new Date().toISOString(),
            status: 'running',
            parameters: { seed_keywords: newRunRequest.runData.seed_keywords },
            results_summary: { progress: 0 }, // Initial progress
            error_message: null,
          },
          ...(old?.items || []),
        ]
      }));
      setPage(1); // Go to the first page to see the new run

      return { previousRuns }; // Return context for rollback
    },
    onError: (err, newRun, context) => {
      queryClient.setQueryData(['discoveryRuns', clientId, page], context.previousRuns); // Rollback on error
      showNotification('error', 'Failed to start discovery run', err.message);
    },
    onSuccess: (data) => {
       showNotification('success', 'Discovery run started successfully!', `Job ID: ${data.job_id}`);
    },
    onSettled: () => {
      queryClient.invalidateQueries(['discoveryRuns', clientId]); // Refetch to get actual server state
    },
  });
  
  // Mutation for re-running an existing discovery job
  const rerunMutation = useMutation(rerunDiscoveryRun, {
    onSuccess: (data) => {
      showNotification('success', 'Re-run started successfully!', `Job ID: ${data.job_id}`);
      queryClient.invalidateQueries(['discoveryRuns', clientId]); // Refetch history to show new job
    },
    onError: (err) => {
      showNotification('error', 'Failed to start re-run', err.message);
    },
  });

  return {
    runs: data?.items || [],
    totalRuns: data?.total_items || 0,
    page,
    setPage,
    isLoading,
    isError,
    error,
    startRunMutation,
    rerunMutation,
  };
};
</file>

<file path="client/my-content-app/src/pages/DiscoveryPage/DiscoveryPage.jsx">
import React, { useState } from 'react';
import { Layout, Typography, Spin, Alert, Card, Divider, message } from 'antd';
import { useNavigate } from 'react-router-dom'; // Import useNavigate
import { useDiscoveryRuns } from './hooks/useDiscoveryRuns';
import DiscoveryForm from './components/DiscoveryForm';
import DiscoveryHistory from './components/DiscoveryHistory';
import { useClient } from '../../hooks/useClient';
import CostConfirmationModal from '../../components/CostConfirmationModal';
import { estimateActionCost } from '../../services/orchestratorService';

const { Content } = Layout;
const { Title } = Typography;

const DiscoveryPage = () => {
  const { runs, isLoading, isError, error, startRunMutation, rerunMutation } = useDiscoveryRuns();
  const { clientId } = useClient();
  const navigate = useNavigate(); // Initialize navigate

  const handleRerun = (runId) => {
      rerunMutation.mutate(runId);
  }

  if (isLoading) {
    return (
      <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: '100vh' }}>
        <Spin size="large" tip="Loading Discovery Hub..." />
      </div>
    );
  }

  if (isError) {
    return (
        <Alert
            message="Error"
            description={error.message || "Failed to load discovery run history. Please try again."}
            type="error"
            showIcon
            style={{ margin: '16px' }}
        />
    );
  }

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Title level={2}>Keyword Discovery</Title>
        <Card>
          <DiscoveryForm
            isSubmitting={startRunMutation.isLoading}
            onSubmit={({ runData }) => {
              startRunMutation.mutate({ clientId, runData }, {
                onSuccess: (data) => {
                  const newRun = data.run_summary;
                  message.success(`Discovery run #${newRun.id} started successfully!`);
                  navigate(`/discovery/run/${newRun.id}`);
                },
                onError: (err) => {
                  message.error(`Failed to start discovery run: ${err.message}`);
                }
              });
            }}
          />
        </Card>

        <Divider />

        <DiscoveryHistory
            runs={runs}
            isLoading={startRunMutation.isLoading || rerunMutation.isLoading} 
            onRerun={handleRerun}
            isRerunning={rerunMutation.isLoading}
        />
      </Content>
    </Layout>
  );
};

export default DiscoveryPage;
</file>

<file path="client/my-content-app/src/pages/NotFoundPage/NotFoundPage.jsx">
import React from 'react';
import { Result, Button } from 'antd';
import { Link } from 'react-router-dom';

const NotFoundPage = () => (
  <Result
    status="404"
    title="404"
    subTitle="Sorry, the page you visited does not exist."
    extra={<Button type="primary"><Link to="/">Back Home</Link></Button>}
  />
);

export default NotFoundPage;
</file>

<file path="client/my-content-app/src/pages/OpportunitiesPage/components/ScoreBreakdownModal.jsx">
import React, { useState, useEffect } from 'react';
import { Modal, Typography, Descriptions, Tag, Row, Col, Alert, Space, Spin, Popover, Progress } from 'antd';
import {
  BarChartOutlined, FireOutlined, ThunderboltOutlined, CalendarOutlined, GlobalOutlined, SmileOutlined,
  BuildOutlined, StarOutlined, UsergroupAddOutlined, ApartmentOutlined, WarningOutlined, RiseOutlined,
  DashboardOutlined, BulbOutlined, InfoCircleOutlined
} from '@ant-design/icons';
import { getScoreNarrative } from '../../../services/orchestratorService';

const { Title, Text, Paragraph } = Typography;

const factorExplanations = {
  ease_of_ranking: "How hard it is to rank on the first page of Google for this keyword. It looks at competitor strength and keyword difficulty.",
  traffic_potential: "An estimate of the traffic you could get if you rank for this keyword, based on search volume and click-through rates.",
  commercial_intent: "How likely a user searching for this keyword is to make a purchase or take a commercial action.",
  growth_trend: "Whether this keyword is becoming more or less popular over time.",
  serp_features: "Measures the presence of special search results like Featured Snippets or People Also Ask boxes, which can affect click-through rates.",
  serp_volatility: "How often the search results for this keyword change. High volatility can mean it's easier to break in, but also harder to hold a position.",
  competitor_weakness: "Analyzes the weaknesses of the top-ranking competitors, such as their backlink profiles and domain authority.",
  serp_crowding: "How many non-traditional results (like ads, images, videos) are on the page, which can push organic results down.",
  keyword_structure: "Analyzes the keyword itself, like its length. Longer keywords are often more specific and easier to rank for.",
  serp_threat: "Identifies major, authoritative domains (like Wikipedia, government sites) that are very difficult to outrank.",
  volume_volatility: "How much the search volume fluctuates month-to-month. High volatility can indicate seasonality.",
  serp_freshness: "How recently the search results have been updated. Older results can be easier to displace.",
  competitor_performance: "A technical analysis of competitor websites, looking at things like page load speed and mobile-friendliness.",
};


const ScoreBreakdownModal = ({ open, onCancel, opportunity }) => {
  const [narrative, setNarrative] = useState('');
  const [isLoadingNarrative, setIsLoadingNarrative] = useState(false);

  useEffect(() => {
    if (open && opportunity?.id) {
      setIsLoadingNarrative(true);
      getScoreNarrative(opportunity.id)
        .then(response => {
          setNarrative(response.data.narrative);
        })
        .catch(err => {
          console.error("Failed to fetch score narrative:", err);
          setNarrative("Could not load the strategic summary.");
        })
        .finally(() => {
          setIsLoadingNarrative(false);
        });
    } else {
      setNarrative('');
    }
  }, [open, opportunity]);

  const keyword = opportunity?.keyword;
  const scoreBreakdown = opportunity?.score_breakdown || opportunity?.full_data?.score_breakdown;

  if (!scoreBreakdown) {
    return <Modal title="Score Breakdown" open={open} onCancel={onCancel} footer={null}><Alert message="No score breakdown available for this opportunity." type="info" showIcon /></Modal>;
  }

  const getScoreColor = (score) => {
    if (score >= 80) return 'success';
    if (score >= 60) return 'processing';
    if (score >= 40) return 'warning';
    return 'error';
  };

  const renderFactor = (factorKey, icon) => {
    const factor = scoreBreakdown[factorKey];
    if (!factor) return null;

    const mainExplanation = factorExplanations[factorKey] || "No explanation available.";

    return (
      <Col span={24} style={{ marginBottom: 16 }}>
        <div style={{ background: '#fafafa', padding: '12px 16px', borderRadius: '8px' }}>
          <Row align="middle" justify="space-between">
            <Col>
              <Space align="center">
                {icon}
                <Title level={5} style={{ margin: 0 }}>{factor.name}</Title>
                <Popover content={<Paragraph style={{ maxWidth: 300 }}>{mainExplanation}</Paragraph>} trigger="hover">
                  <InfoCircleOutlined style={{ color: 'rgba(0, 0, 0, 0.45)', cursor: 'pointer' }} />
                </Popover>
              </Space>
            </Col>
            <Col>
              <Space>
                <Text type="secondary" style={{ fontSize: '0.9em' }}>Weight: {factor.weight || 0}%</Text>
                <Progress type="circle" percent={factor.score} width={40} format={percent => `${percent}`} status={getScoreColor(factor.score)} />
              </Space>
            </Col>
          </Row>
          <Descriptions column={1} size="small" style={{ marginTop: 12 }}>
            {Object.entries(factor.breakdown || {}).map(([subFactorKey, subFactor]) => (
              <Descriptions.Item key={subFactorKey} label={<Text strong>{subFactorKey}</Text>}>
                <Row justify="space-between" align="top">
                  <Col span={16}>
                    <Space direction="vertical" size={0}>
                      <Text>{subFactor.value}</Text>
                      {subFactor.explanation && <Paragraph type="secondary" style={{ margin: 0, fontSize: '0.85em' }}>{subFactor.explanation}</Paragraph>}
                    </Space>
                  </Col>
                  <Col span={4} style={{ textAlign: 'right' }}>
                    {subFactor.score !== undefined && <Tag color={getScoreColor(subFactor.score)}>{subFactor.score?.toFixed(0)}</Tag>}
                  </Col>
                </Row>
              </Descriptions.Item>
            ))}
             {factor.breakdown?.message && <Alert message={factor.breakdown.message} type="warning" showIcon style={{width: '100%'}}/>}
          </Descriptions>
        </div>
      </Col>
    );
  };

  return (
    <Modal
      title={<Title level={4} style={{ margin: 0 }}>Strategic Score Breakdown: &quot;{keyword}&quot;</Title>}
      open={open}
      onCancel={onCancel}
      footer={null}
      width={800}
    >
      {isLoadingNarrative ? (
        <Spin tip="Loading AI-powered summary..." />
      ) : (
        narrative && <Alert message="Strategic Summary" description={<Paragraph style={{ whiteSpace: 'pre-wrap' }}>{narrative}</Paragraph>} type="info" showIcon icon={<BulbOutlined />} style={{ marginBottom: '24px' }} />
      )}
      <Row gutter={[16, 16]}>
        {renderFactor('ease_of_ranking', <BarChartOutlined />)}
        {renderFactor('traffic_potential', <FireOutlined />)}
        {renderFactor('commercial_intent', <ThunderboltOutlined />)}
        {renderFactor('growth_trend', <RiseOutlined />)}
        {renderFactor('serp_features', <StarOutlined />)}
        {renderFactor('competitor_weakness', <BuildOutlined />)}
        {renderFactor('serp_volatility', <SmileOutlined />)}
        {renderFactor('serp_crowding', <UsergroupAddOutlined />)}
        {renderFactor('keyword_structure', <ApartmentOutlined />)}
        {renderFactor('serp_threat', <WarningOutlined />)}
        {renderFactor('volume_volatility', <CalendarOutlined />)}
        {renderFactor('serp_freshness', <GlobalOutlined />)}
        {renderFactor('competitor_performance', <DashboardOutlined />)}
      </Row>
    </Modal>
  );
};

export default ScoreBreakdownModal;
</file>

<file path="client/my-content-app/src/pages/OpportunitiesPage/hooks/useOpportunities.js">
import { useQuery } from 'react-query';
import { useState, useMemo, useEffect } from 'react';
import { getOpportunities, getDashboardStats } from '../../../services/opportunitiesService';
import { useClient } from '../../../hooks/useClient';

export const useOpportunities = () => {
  const { clientId } = useClient();
  const [pagination, setPagination] = useState({ current: 1, pageSize: 20, total: 0 });
  const [activeStatus, setActiveStatus] = useState('review');
  const [sorter, setSorter] = useState({ field: 'strategic_score', order: 'descend' });
  const [statusCounts, setStatusCounts] = useState({});

  const { data: statsData, isLoading: isLoadingStats } = useQuery(
    ['dashboardStats', clientId],
    () => getDashboardStats(clientId),
    {
      enabled: !!clientId,
      onSuccess: (data) => {
        if (data.status_counts) {
          setStatusCounts(data.status_counts);
          if (!activeStatus && Object.keys(data.status_counts).length > 0) {
            setActiveStatus(Object.keys(data.status_counts)[0]);
          }
        }
      },
    }
  );
  
  const { data: opportunitiesData, isLoading, isError, error, refetch } = useQuery(
    ['opportunities', clientId, pagination.current, pagination.pageSize, sorter, activeStatus],
    () => getOpportunities(clientId, { 
      page: pagination.current, 
      limit: pagination.pageSize, 
      sort_by: sorter.field, 
      sort_direction: sorter.order === 'ascend' ? 'asc' : 'desc',
      status: activeStatus 
    }),
    {
      enabled: !!clientId,
      staleTime: 60 * 1000,
      onSuccess: (data) => {
        setPagination(prev => ({ ...prev, total: data.total_items || 0 }));
      }
    }
  );

  const opportunities = useMemo(() => opportunitiesData?.items || [], [opportunitiesData]);

  const handleTableChange = (newPagination, newFilters, newSorter) => {
    setPagination(prev => ({ ...prev, current: newPagination.current, pageSize: newPagination.pageSize }));
    
    const effectiveSorter = Array.isArray(newSorter) ? newSorter[0] : newSorter;
    if (effectiveSorter?.field) {
        setSorter({ field: effectiveSorter.field, order: effectiveSorter.order });
    } else {
        setSorter({ field: 'strategic_score', order: 'descend' });
    }
    refetch();
  };

  return {
    opportunities,
    isLoading: isLoading || isLoadingStats,
    isError, error,
    pagination,
    handleTableChange,
    activeStatus, setActiveStatus,
    statusCounts,
    refetchOpportunities: refetch
  };
};
</file>

<file path="client/my-content-app/src/pages/OpportunitiesPage/hooks/useOpportunities.refactored.js">
import { useQuery } from 'react-query';
import { useState } from 'react';
import { getOpportunities } from '../../../services/opportunitiesService';
import { useClient } from '../../../hooks/useClient';

export const useOpportunities = () => {
  const { clientId } = useClient();
  const [pagination, setPagination] = useState({ current: 1, pageSize: 20, total: 0 });
  const [activeStatus, setActiveStatus] = useState('review');
  const [sorter, setSorter] = useState({ field: 'strategic_score', order: 'descend' });
  const [keyword, setKeyword] = useState('');

  const { data, isLoading, isError, error, refetch } = useQuery(
    ['opportunities', clientId, pagination.current, pagination.pageSize, activeStatus, sorter, keyword],
    () => getOpportunities(clientId, {
      page: pagination.current,
      limit: pagination.pageSize,
      status: activeStatus,
      sort_by: sorter.field,
      sort_direction: sorter.order === 'ascend' ? 'asc' : 'desc',
      keyword: keyword,
    }),
    {
      enabled: !!clientId,
      staleTime: 60 * 1000, // Keep data fresh for 1 minute
      onSuccess: (response) => {
        setPagination(prev => ({ ...prev, total: response.total_items || 0 }));
      }
    }
  );

  const handleTableChange = (newPagination, newFilters, newSorter) => {
    setPagination(prev => ({ ...prev, current: newPagination.current, pageSize: newPagination.pageSize }));

    const effectiveSorter = Array.isArray(newSorter) ? newSorter[0] : newSorter;
    if (effectiveSorter?.field) {
        setSorter({ field: effectiveSorter.field, order: effectiveSorter.order });
    } else {
        // Reset to default sort if no specific column is sorted
        setSorter({ field: 'strategic_score', order: 'descend' });
    }
  };

  const handleSearch = (newKeyword) => {
    setKeyword(newKeyword);
  };

  return {
    opportunities: data?.items || [],
    isLoading,
    isError,
    error,
    pagination,
    handleTableChange,
    activeStatus,
    setActiveStatus,
    handleSearch,
    refetchOpportunities: refetch,
  };
};
</file>

<file path="client/my-content-app/src/pages/OpportunitiesPage/OpportunitiesPage.css">
/* OpportunitiesPage.css */

.custom-tabs .ant-tabs-nav {
  background-color: #f0f2f5;
  padding: 0 16px;
  border-radius: 8px;
  margin-bottom: 16px;
}

.custom-tabs .ant-tabs-tab {
  padding: 12px 16px;
  font-size: 14px;
}

.custom-tabs .ant-tabs-tab-active {
  background-color: #ffffff;
  border-top-left-radius: 8px;
  border-top-right-radius: 8px;
}

.custom-tabs .ant-tabs-ink-bar {
  background-color: #1890ff;
}

.custom-tabs .ant-tag {
  font-weight: 900;
  font-size: 14px;
}
</file>

<file path="client/my-content-app/src/pages/OpportunitiesPage/OpportunitiesPage.jsx">
import React from 'react';
import { Layout, Typography, Table, Tag, Button, Space, Tooltip, Modal, Card, Tabs } from 'antd';
import { RocketOutlined, EditOutlined, DeleteOutlined, ExclamationCircleOutlined } from '@ant-design/icons';
import { useOpportunities } from './hooks/useOpportunities';
import { useNotifications } from '../../context/NotificationContext';
import { useMutation, useQueryClient } from 'react-query';
import { startFullWorkflow, rejectOpportunity } from '../../services/orchestratorService';
import JobStatusIndicator from '../../components/JobStatusIndicator';
import { useJobs } from '../../context/JobContext';
import { getJobStatus } from '../../services/jobsService';
import { useNavigate } from 'react-router-dom';
import './OpportunitiesPage.css';

const { Content } = Layout;
const { Title, Text } = Typography;
const { confirm } = Modal;
const { TabPane } = Tabs;

const MAIN_STATUSES = [
  'review', 
  'paused_for_approval', 
  'generated', 
  'rejected', 
  'failed'
];

const statusColors = {
  review: 'blue',
  paused_for_approval: 'orange',
  generated: 'green',
  rejected: 'default',
  failed: 'red',
};

const OpportunitiesPage = () => {
  const { 
    opportunities, isLoading, pagination, 
    handleTableChange, activeStatus, setActiveStatus, statusCounts
  } = useOpportunities();
  const { showNotification } = useNotifications();
  const queryClient = useQueryClient();
  const navigate = useNavigate();
  const { startJob, updateJob, completeJob } = useJobs();

  const { mutate: startWorkflowMutation, isLoading: isStartingWorkflow } = useMutation(
    ({ opportunityId, override, opportunityKeyword }) => startFullWorkflow(opportunityId, override),
    {
      onSuccess: (data, variables) => {
        const { job_id } = data;
        const { opportunityKeyword } = variables;
        startJob(job_id, `Workflow started for "${opportunityKeyword}".`);

        const poll = setInterval(async () => {
          try {
            const statusData = await getJobStatus(job_id);
            if (statusData.status === 'completed' || statusData.status === 'failed') {
              updateJob(job_id, statusData.status, statusData.error || `Workflow for "${opportunityKeyword}" finished.`);
              completeJob(job_id);
              clearInterval(poll);
              queryClient.invalidateQueries('opportunities');
            } else {
              const lastLog = statusData.progress_log?.[statusData.progress_log.length - 1];
              updateJob(job_id, 'running', lastLog?.message || 'Processing...');
            }
          } catch (error) {
            updateJob(job_id, 'failed', 'Failed to get job status.');
            completeJob(job_id);
            clearInterval(poll);
          }
        }, 5000);
      },
      onError: (err, variables) => {
        const { opportunityKeyword } = variables;
        showNotification('error', `Workflow Failed for "${opportunityKeyword}"`, err.message)
      },
    }
  );

  const { mutate: rejectOpportunityMutation, isLoading: isRejecting } = useMutation(
    (opportunityId) => rejectOpportunity(opportunityId),
    {
      onSuccess: () => {
        showNotification('success', 'Opportunity Rejected', 'The opportunity has been marked as rejected.');
        queryClient.invalidateQueries('opportunities');
      },
      onError: (err) => showNotification('error', 'Rejection Failed', err.message),
    }
  );

  const showRejectConfirm = (opportunityId) => {
    confirm({
      title: 'Are you sure you want to reject this opportunity?',
      icon: <ExclamationCircleOutlined />,
      content: 'This action cannot be undone.',
      okText: 'Yes, Reject',
      okType: 'danger',
      cancelText: 'No',
      onOk() {
        rejectOpportunityMutation(opportunityId);
      },
    });
  };

  const renderActions = (_, record) => {
    const isFailed = ['failed', 'rejected'].includes(record.status);
    const isLoading = isStartingWorkflow || isRejecting;

    const buttons = [];

    switch (activeStatus) {
      case 'review':
        buttons.push(
          <Tooltip title="Run Workflow" key="run">
            <Button
              type="primary"
              icon={<RocketOutlined />}
              onClick={(e) => { 
                e.stopPropagation(); 
                console.log('Starting workflow for opportunity:', record.id, 'with override:', isFailed);
                startWorkflowMutation({ opportunityId: record.id, override: isFailed, opportunityKeyword: record.keyword }); 
              }}
              loading={isStartingWorkflow}
              disabled={isLoading}
            />
          </Tooltip>,
          <Tooltip title="Reject Opportunity" key="reject">
            <Button
              danger
              icon={<DeleteOutlined />}
              onClick={(e) => { e.stopPropagation(); showRejectConfirm(record.id); }}
              loading={isRejecting}
              disabled={isLoading}
            />
          </Tooltip>
        );
        break;
      case 'rejected':
      case 'failed':
        buttons.push(
          <Tooltip title="Run Workflow" key="run-failed">
            <Button
              type="primary"
              icon={<RocketOutlined />}
              onClick={(e) => { e.stopPropagation(); startWorkflowMutation({ opportunityId: record.id, override: true }); }}
              loading={isStartingWorkflow}
              disabled={isLoading}
            />
          </Tooltip>
        );
        break;
      default:
        break;
    }

    buttons.push(
      <Tooltip title="View Details" key="view">
        <Button 
          icon={<EditOutlined />} 
          onClick={(e) => { e.stopPropagation(); navigate(`/opportunities/${record.id}`)}} 
        />
      </Tooltip>
    );

    return <Space>{buttons}</Space>;
  };

  const baseColumns = [
    { title: 'Keyword', dataIndex: 'keyword', key: 'keyword', sorter: true, render: (text, record) => <a onClick={(e) => { e.stopPropagation(); navigate(`/opportunities/${record.id}`)}}>{text}</a> },
    { title: 'Search Volume', dataIndex: 'search_volume', key: 'search_volume', sorter: true, render: (sv) => sv ? sv.toLocaleString() : 'N/A' },
    { title: 'KD', dataIndex: 'keyword_difficulty', key: 'keyword_difficulty', sorter: true, render: (kd) => kd != null ? kd : 'N/A' },
  ];

  const rejectedColumns = [
    ...baseColumns,
    { 
      title: 'Rejection Reason', 
      dataIndex: 'blog_qualification_reason', 
      key: 'blog_qualification_reason',
      render: (reason) => reason || <Text type="secondary">No reason provided</Text>
    },
    { title: 'Actions', key: 'actions', fixed: 'right', render: renderActions },
  ];

  const defaultColumns = [
    ...baseColumns,
    { title: 'Strategic Score', dataIndex: 'strategic_score', key: 'strategic_score', sorter: true, render: (score) => score ? <strong>{score.toFixed(1)}</strong> : 'N/A' },
    { title: 'CPC', dataIndex: 'cpc', key: 'cpc', sorter: true, render: (cpc) => cpc ? `$${cpc.toFixed(2)}` : 'N/A' },
    { title: 'Actions', key: 'actions', fixed: 'right', render: renderActions },
  ];

  const columns = activeStatus === 'rejected' ? rejectedColumns : defaultColumns;

  return (
    <Layout style={{ padding: '24px' }}><Content>
      <Title level={2}>Content Opportunities</Title>
      <Card>
        <div className="custom-tabs">
          <Tabs activeKey={activeStatus} onChange={setActiveStatus}>
            {MAIN_STATUSES.map(status => (
              <TabPane 
                tab={
                  <span>
                    {status.replace(/_/g, ' ').toUpperCase()}
                    <Tag color={statusColors[status]} style={{ marginLeft: 8 }}>
                      {statusCounts[status] || 0}
                    </Tag>
                  </span>
                }
                key={status} 
              />
            ))}
          </Tabs>
        </div>
        <Table
          columns={columns}
          dataSource={opportunities}
          rowKey="id"
          loading={isLoading}
          pagination={pagination}
          onChange={handleTableChange}
          onRow={(record) => ({
            onClick: () => navigate(`/opportunities/${record.id}`),
            style: { cursor: 'pointer' },
          })}
          rowClassName="table-row-hover"
        />
      </Card>
    </Content></Layout>
  );
};

export default OpportunitiesPage;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/ActionCenter.jsx">
import React from 'react';
import { Card, Button, Space, Alert, Modal } from 'antd';
import { CheckOutlined, ExperimentOutlined, RocketOutlined, DeleteOutlined, ExclamationCircleOutlined } from '@ant-design/icons';
import { useNotifications } from '../../../context/NotificationContext';
import { useMutation } from 'react-query';
import { approveAnalysis, startFullContentGeneration, startFullWorkflow, rejectOpportunity } from '../../../services/orchestratorService';

const { confirm } = Modal;

const ActionCenter = ({ status, opportunityId, overrides, refetch }) => {
  const { showNotification } = useNotifications();

  const { mutate: approveAnalysisMutation, isLoading: isApproving } = useMutation(
    () => approveAnalysis(opportunityId, overrides),
    {
      onSuccess: () => {
        showNotification('success', 'Analysis Approved', 'The content generation process has been initiated.');
        refetch();
      },
      onError: (error) => showNotification('error', 'Approval Failed', error.message),
    }
  );

  const { mutate: generateContentMutation, isLoading: isGenerating } = useMutation(
    (variables) => startFullContentGeneration(variables.opportunityId, variables.modelOverride, variables.temperature),
    {
      onSuccess: () => {
        showNotification('success', 'Content Generation Started', 'The AI is now generating the content.');
        refetch();
      },
      onError: (error) => showNotification('error', 'Generation Failed', error.message),
    }
  );

  const { mutate: startWorkflowMutation, isLoading: isStartingWorkflow } = useMutation(
    () => startFullWorkflow(opportunityId, ['failed', 'rejected'].includes(status)),
    {
      onSuccess: (data) => {
        showNotification('success', 'Workflow Started', `Job has been queued. Job ID: ${data.job_id}`);
        refetch();
      },
      onError: (err) => showNotification('error', 'Workflow Failed', err.message),
    }
  );

  const { mutate: rejectOpportunityMutation, isLoading: isRejecting } = useMutation(
    () => rejectOpportunity(opportunityId),
    {
      onSuccess: () => {
        showNotification('success', 'Opportunity Rejected', 'The opportunity has been marked as rejected.');
        refetch();
      },
      onError: (err) => showNotification('error', 'Rejection Failed', err.message),
    }
  );

  const showRejectConfirm = () => {
    confirm({
      title: 'Are you sure you want to reject this opportunity?',
      icon: <ExclamationCircleOutlined />,
      content: 'This action cannot be undone.',
      okText: 'Yes, Reject',
      okType: 'danger',
      cancelText: 'No',
      onOk: () => rejectOpportunityMutation(),
    });
  };

  const renderActions = () => {
    const isLoading = isApproving || isGenerating || isStartingWorkflow || isRejecting;

    switch (status) {
      case 'review':
        return (
          <Space>
            <Button type="primary" icon={<RocketOutlined />} onClick={() => startWorkflowMutation()} loading={isStartingWorkflow} disabled={isLoading}>
              Run Workflow
            </Button>
            <Button type="danger" icon={<DeleteOutlined />} onClick={showRejectConfirm} loading={isRejecting} disabled={isLoading}>
              Reject
            </Button>
          </Space>
        );
      case 'paused_for_approval':
        return (
          <Button type="primary" icon={<CheckOutlined />} onClick={() => approveAnalysisMutation()} loading={isApproving} disabled={isLoading}>
            Approve Analysis & Proceed to Content Generation
          </Button>
        );
      case 'validated':
        return (
          <Button type="primary" icon={<ExperimentOutlined />} onClick={() => generateContentMutation({ opportunityId, modelOverride: null, temperature: null })} loading={isGenerating} disabled={isLoading}>
            Generate Content
          </Button>
        );
      case 'failed':
      case 'rejected':
        return (
          <Button type="primary" icon={<RocketOutlined />} onClick={() => startWorkflowMutation()} loading={isStartingWorkflow} disabled={isLoading}>
            Rerun Workflow
          </Button>
        );
      default:
        return <Alert message="No actions available for the current status." type="info" showIcon />;
    }
  };

  return (
    <Card>
      <Space direction="vertical" style={{ width: '100%' }}>
        <Alert
          message="Next Step"
          description="This is the primary action to move this opportunity forward in the workflow."
          type="info"
          showIcon
        />
        {renderActions()}
      </Space>
    </Card>
  );
};

export default ActionCenter;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/AdditionalInsights.jsx">
import React from 'react';
import { Card, Typography, List, Tag } from 'antd';

const { Title, Paragraph } = Typography;

const AdditionalInsights = ({ serpOverview }) => {
  return (
    <Card title="Additional Insights">
      {serpOverview?.discussion_snippets?.length > 0 && (
        <>
          <Title level={5}>Discussion Snippets</Title>
          <List
            dataSource={serpOverview.discussion_snippets}
            renderItem={(item) => <List.Item>{item}</List.Item>}
            size="small"
            bordered
            style={{ marginBottom: '24px' }}
          />
        </>
      )}
    </Card>
  );
};

export default AdditionalInsights;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/ArticlePreview.jsx">
import React from 'react';
import { Card, Typography, Image, Alert } from 'antd';
import NoData from './NoData';

const { Title, Paragraph } = Typography;

const ArticlePreview = ({ aiContent, featuredImagePath }) => {
  if (!aiContent) {
    return <NoData description="No article content has been generated yet." />;
  }

  const { article_title, article_body_html } = aiContent;

  return (
    <Card>
      <Title level={2}>{article_title}</Title>
      {featuredImagePath ? (
        <div style={{ textAlign: 'center', marginBottom: '24px' }}>
          <Image
            width="50%"
            src={`/api/images/${featuredImagePath.split('/').pop()}`}
            alt={article_title}
          />
        </div>
      ) : (
        <NoData description="No featured image has been generated yet." />
      )}
      <div dangerouslySetInnerHTML={{ __html: article_body_html }} />
    </Card>
  );
};

export default ArticlePreview;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/CompetitorBacklinks.jsx">
import React from 'react';
import { Card, Statistic, Row, Col, Tooltip } from 'antd';
import { LinkOutlined, TeamOutlined, RiseOutlined } from '@ant-design/icons';

const CompetitorBacklinks = ({ avgBacklinksInfo }) => {
  if (!avgBacklinksInfo) {
    return <Card title="Competitor Backlink Analysis">No data available.</Card>;
  }

  const { backlinks, referring_domains, main_domain_rank } = avgBacklinksInfo;

  return (
    <Card title="Competitor Backlink Analysis">
      <Row gutter={16}>
        <Col span={8}>
          <Tooltip title="The average number of backlinks for the top-ranking pages.">
            <Statistic title="Avg. Backlinks" value={backlinks} prefix={<LinkOutlined />} />
          </Tooltip>
        </Col>
        <Col span={8}>
          <Tooltip title="The average number of unique domains linking to the top-ranking pages.">
            <Statistic title="Avg. Referring Domains" value={referring_domains} prefix={<TeamOutlined />} />
          </Tooltip>
        </Col>
        <Col span={8}>
          <Tooltip title="The average Domain Rank (a measure of a website's authority) of the top-ranking pages.">
            <Statistic title="Avg. Domain Rank" value={main_domain_rank} prefix={<RiseOutlined />} />
          </Tooltip>
        </Col>
      </Row>
    </Card>
  );
};

export default CompetitorBacklinks;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/ContentAuditCard.jsx">
import React from 'react';
import { Card, Typography, List, Tag, Progress } from 'antd';
import { CheckCircleOutlined, CloseCircleOutlined } from '@ant-design/icons';

const { Title, Text } = Typography;

const ContentAuditCard = ({ auditResults }) => {
  if (!auditResults) {
    return null;
  }

  const {
    flesch_kincaid_grade,
    readability_assessment,
    entity_coverage_score,
    missing_entities,
    publish_readiness_issues,
  } = auditResults;

  return (
    <Card title="Content Audit" style={{ marginTop: 24 }}>
      <Title level={5}>Readability</Title>
      <Text>{readability_assessment}</Text>
      <Text strong style={{ display: 'block', marginTop: 8 }}>
        Flesch-Kincaid Grade Level: {flesch_kincaid_grade.toFixed(1)}
      </Text>

      <Title level={5} style={{ marginTop: 16 }}>Entity Coverage</Title>
      <Progress percent={entity_coverage_score} />
      {missing_entities && missing_entities.length > 0 && (
        <>
          <Text strong style={{ display: 'block', marginTop: 8 }}>Missing Entities:</Text>
          <List
            dataSource={missing_entities}
            renderItem={(item) => (
              <List.Item>
                <CloseCircleOutlined style={{ color: 'red', marginRight: 8 }} />
                {item}
              </List.Item>
            )}
            size="small"
          />
        </>
      )}

      <Title level={5} style={{ marginTop: 16 }}>Publishing Readiness</Title>
      {publish_readiness_issues && publish_readiness_issues.length > 0 ? (
        <List
          dataSource={publish_readiness_issues}
          renderItem={(item) => (
            <List.Item>
              <CloseCircleOutlined style={{ color: 'red', marginRight: 8 }} />
              <Text>{item.issue}: {item.context}</Text>
            </List.Item>
          )}
          size="small"
        />
      ) : (
        <Space>
          <CheckCircleOutlined style={{ color: 'green' }} />
          <Text>No publishing readiness issues found.</Text>
        </Space>
      )}
    </Card>
  );
};

export default ContentAuditCard;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/ContentBlueprint.jsx">
import React, { useState, useEffect } from 'react';
import { Card, Typography, List, Tag, Descriptions, Button, Tooltip, Select } from 'antd';
import { CopyOutlined, LinkOutlined, BulbOutlined, PlusOutlined } from '@ant-design/icons';
import { useNotifications } from '../../../context/NotificationContext';
import NoData from './NoData';

const { Title, Paragraph, Text } = Typography;
const { Option } = Select;

const ContentBlueprint = ({ blueprint, overrides, setOverrides }) => {
  const { showNotification } = useNotifications();
  const [paaToAdd, setPaaToAdd] = useState(null);

  if (!blueprint) {
    return <Card><Paragraph type="secondary">No content blueprint available.</Paragraph></Card>;
  }

  const { ai_content_brief, content_intelligence, recommended_strategy } = blueprint;
  const people_also_ask = blueprint.serp_overview?.people_also_ask || [];

  const handleCopyOutline = () => {
    const outline = overrides
      .map((item) => {
        const h3s = item.h3s.map((h3) => `  - ${h3}`).join('\n');
        return `${item.h2}\n${h3s}`;
      })
      .join('\n\n');
    navigator.clipboard.writeText(outline);
    showNotification('success', 'Copied to Clipboard', 'The article outline has been copied.');
  };

  const handleAddPaa = () => {
    if (!paaToAdd) return;

    const faqSectionIndex = overrides.findIndex(sec => sec.h2.toLowerCase().includes('frequently asked questions'));
    
    if (faqSectionIndex > -1) {
      const newStructure = [...overrides];
      newStructure[faqSectionIndex].h3s.push(paaToAdd);
      setOverrides(newStructure);
      showNotification('success', 'Question Added', `"${paaToAdd}" was added to the outline.`);
    } else {
      showNotification('warning', 'Section Not Found', 'Could not find a "Frequently Asked Questions" section to add this to.');
    }
    setPaaToAdd(null);
  };
  
  return (
    <Card title="AI Content Blueprint">
      <Descriptions bordered column={1} size="small" style={{ marginBottom: '24px' }}>
        <Descriptions.Item label="Target Audience">{ai_content_brief?.target_audience_persona || 'Not available'}</Descriptions.Item>
        <Descriptions.Item label="Primary Goal">{ai_content_brief?.primary_goal || 'Not available'}</Descriptions.Item>
        <Descriptions.Item label="Target Word Count">{ai_content_brief?.target_word_count || 'Not available'}</Descriptions.Item>
      </Descriptions>

      <Title level={5}>Dynamic SERP Instructions</Title>
      <List
        dataSource={ai_content_brief.dynamic_serp_instructions}
        renderItem={(item) => <List.Item><BulbOutlined style={{ marginRight: 8 }} />{item}</List.Item>}
        style={{ marginBottom: '24px' }}
        size="small"
      />

      <Title level={5}>Content Gaps & Unique Angles</Title>
      <List
        dataSource={content_intelligence.identified_content_gaps}
        renderItem={(item) => <List.Item>{item}</List.Item>}
        style={{ marginBottom: '24px' }}
      />

      <Title level={5}>Recommended Article Structure</Title>
      <div style={{ marginBottom: 16 }}>
        <Select
          showSearch
          placeholder="Select a 'People Also Ask' question to add to your outline"
          style={{ width: 'calc(100% - 120px)', marginRight: 8 }}
          onChange={value => setPaaToAdd(value)}
          value={paaToAdd}
        >
          {people_also_ask.map(q => <Option key={q} value={q}>{q}</Option>)}
        </Select>
        <Button icon={<PlusOutlined />} onClick={handleAddPaa} disabled={!paaToAdd}>Add</Button>
        <Button
          icon={<CopyOutlined />}
          onClick={handleCopyOutline}
          style={{ float: 'right' }}
        >
          Copy Outline
        </Button>
      </div>
      <List
        dataSource={overrides}
        renderItem={(item) => (
          <List.Item>
            <List.Item.Meta
              title={item.h2}
              description={
                <div style={{ paddingLeft: '20px' }}>
                  {item.h3s.map(h3 => <p key={h3} style={{ margin: '4px 0' }}>- {h3}</p>)}
                </div>
              }
            />
          </List.Item>
        )}
        style={{ marginBottom: '24px' }}
      />

      <Title level={5}>Key Entities to Mention</Title>
      <div style={{ marginBottom: '24px' }}>
        {content_intelligence.key_entities_from_competitors.map((entity) => (
          <Tag key={entity} style={{ margin: '4px' }}>{entity}</Tag>
        ))}
      </div>

      <Title level={5}>Focus Competitors</Title>
      <List
        dataSource={recommended_strategy.focus_competitors}
        renderItem={(item) => (
          <List.Item>
            <a href={item.url} target="_blank" rel="noopener noreferrer">
              <LinkOutlined style={{ marginRight: 8 }} />
              {item.title}
            </a>
          </List.Item>
        )}
        style={{ marginBottom: '24px' }}
        bordered
        size="small"
      />

      <Title level={5}>Internal Linking Suggestions</Title>
      {blueprint.internal_linking_suggestions && blueprint.internal_linking_suggestions.length > 0 ? (
        <List
          dataSource={blueprint.internal_linking_suggestions}
          renderItem={(item) => (
            <List.Item>
              <Tooltip title={`Link to: ${item.url}`}>
                <Text>Anchor Text: "{item.anchor_text}"</Text>
              </Tooltip>
            </List.Item>
          )}
          bordered
          size="small"
        />
      ) : (
        <NoData description="No internal linking suggestions were generated." />
      )}
    </Card>
  );
};

export default ContentBlueprint;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/ErrorMessage.jsx">
import React from 'react';
import { Alert } from 'antd';

const ErrorMessage = ({ message }) => {
  if (!message || !(message.toLowerCase().includes('error') || message.toLowerCase().includes('failed'))) {
    return null;
  }

  return (
    <Alert
      message="Workflow Error"
      description={message}
      type="error"
      showIcon
      closable
    />
  );
};

export default ErrorMessage;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/ExecutiveSummary.jsx">
import React from 'react';
import { Card, Typography } from 'antd';

const { Title, Paragraph } = Typography;

const ExecutiveSummary = ({ summary }) => {
  if (!summary) {
    return null;
  }

  return (
    <Card style={{ marginTop: 24 }}>
      <Title level={4}>Executive Summary</Title>
      <Paragraph>{summary}</Paragraph>
    </Card>
  );
};

export default ExecutiveSummary;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/FactorsCard.jsx">
import React from 'react';
import { Card, Typography, List, Row, Col } from 'antd';
import { CheckCircleOutlined, CloseCircleOutlined } from '@ant-design/icons';
import NoData from './NoData';

const { Title } = Typography;

const FactorsCard = ({ positiveFactors, negativeFactors }) => {
  return (
    <Card style={{ marginTop: 24 }}>
      <Row gutter={16}>
        <Col span={12}>
          <Title level={5}>Positive Factors</Title>
          <List
            dataSource={positiveFactors}
            renderItem={(item) => (
              <List.Item>
                <CheckCircleOutlined style={{ color: 'green', marginRight: '8px' }} /> {item}
              </List.Item>
            )}
          />
        </Col>
        <Col span={12}>
          <Title level={5}>Negative Factors</Title>
          {negativeFactors && negativeFactors.length > 0 ? (
            <List
              dataSource={negativeFactors}
              renderItem={(item) => (
                <List.Item>
                  <CloseCircleOutlined style={{ color: 'red', marginRight: '8px' }} /> {item}
                </List.Item>
              )}
            />
          ) : (
            <NoData description="No negative factors identified." />
          )}
        </Col>
      </Row>
    </Card>
  );
};

export default FactorsCard;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/FeaturedSnippetCard.jsx">
import React from 'react';
import { Card, Typography } from 'antd';
import { TrophyOutlined } from '@ant-design/icons';

const { Title, Paragraph } = Typography;

const FeaturedSnippetCard = ({ blueprint }) => {
  const featuredSnippet = blueprint?.serp_overview?.featured_snippet_content;

  if (!featuredSnippet) {
    return null; // Don't render the card if there's no snippet
  }

  return (
    <Card
      title={<span><TrophyOutlined style={{ marginRight: 8 }} /> Featured Snippet Opportunity</span>}
      bordered={false}
      style={{ backgroundColor: '#e6f7ff' }}
    >
      <Paragraph>
        The following content currently holds the featured snippet position. Your goal is to provide a better, more direct answer.
      </Paragraph>
      <Paragraph blockquote="true" style={{ fontStyle: 'italic' }}>
        {featuredSnippet}
      </Paragraph>
    </Card>
  );
};

export default FeaturedSnippetCard;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/GrowthTrend.jsx">
import React from 'react';
import { Card, Statistic, Tooltip } from 'antd';
import { RiseOutlined } from '@ant-design/icons';

const GrowthTrend = ({ scoreBreakdown }) => {
  const growthTrend = scoreBreakdown?.growth_trend;

  if (!growthTrend) {
    return null;
  }

  return (
    <Card>
      <Tooltip title={growthTrend.breakdown['Growth Trend'].explanation}>
        <Statistic
          title="Year-over-Year Growth"
          value={growthTrend.breakdown['Growth Trend'].value}
          prefix={<RiseOutlined />}
          valueStyle={{ color: '#3f8600' }}
        />
      </Tooltip>
    </Card>
  );
};

export default GrowthTrend;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/IntentAnalysis.jsx">
import React from 'react';
import { Card, Typography, Tag, Tooltip } from 'antd';
import { AimOutlined, DollarOutlined } from '@ant-design/icons';

const { Title, Text } = Typography;

const IntentAnalysis = ({ searchIntentInfo }) => {
  if (!searchIntentInfo) {
    return null;
  }

  const { main_intent, foreign_intent } = searchIntentInfo;

  const intentColor = {
    informational: 'blue',
    commercial: 'gold',
    transactional: 'green',
    navigational: 'purple',
  };

  return (
    <Card title="Search Intent Analysis">
      <Tooltip title="The primary reason a user is searching for this keyword.">
        <div>
          <Text strong>Main Intent: </Text>
          <Tag color={intentColor[main_intent] || 'default'} icon={<AimOutlined />}>
            {main_intent?.toUpperCase()}
          </Tag>
        </div>
      </Tooltip>
      {foreign_intent?.length > 0 && (
        <Tooltip title="Other potential intents this keyword might satisfy.">
          <div style={{ marginTop: '16px' }}>
            <Text strong>Secondary Intents: </Text>
            {foreign_intent.map(intent => (
              <Tag key={intent} color={intentColor[intent] || 'default'} icon={<DollarOutlined />}>
                {intent.toUpperCase()}
              </Tag>
            ))}
          </div>
        </Tooltip>
      )}
    </Card>
  );
};

export default IntentAnalysis;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/KeywordMetrics.jsx">
import React from 'react';
import { Card, Statistic, Row, Col, Tooltip, Tag } from 'antd';
import { BarChartOutlined, DollarCircleOutlined, ThunderboltOutlined, InfoCircleOutlined } from '@ant-design/icons';
import { Line } from '@ant-design/plots';

const KeywordMetrics = ({ keywordInfo, keywordProperties }) => {
  if (!keywordInfo || !keywordProperties) {
    return <Card title="Keyword Metrics">No data available.</Card>;
  }

  const { search_volume, cpc, competition, monthly_searches, competition_level, low_top_of_page_bid, high_top_of_page_bid } = keywordInfo;
  const { keyword_difficulty } = keywordProperties;

  const chartData = monthly_searches?.map(item => ({
    date: `${item.year}-${item.month}`,
    volume: item.search_volume,
  }));

  const chartConfig = {
    data: chartData,
    xField: 'date',
    yField: 'volume',
    height: 200,
    point: {
      size: 5,
      shape: 'diamond',
    },
    tooltip: {
      formatter: (datum) => {
        return { name: 'Search Volume', value: datum.volume.toLocaleString() };
      },
    },
  };

  return (
    <Card title="Keyword Metrics">
      <Row gutter={[16, 24]}>
        <Col span={12}>
          <Tooltip title="The average number of times this keyword is searched for per month.">
            <Statistic title="Search Volume" value={search_volume} prefix={<BarChartOutlined />} />
          </Tooltip>
        </Col>
        <Col span={12}>
          <Tooltip title="The average cost per click for this keyword in paid search campaigns.">
            <Statistic title="CPC" value={cpc} prefix={<DollarCircleOutlined />} precision={2} />
          </Tooltip>
        </Col>
        <Col span={12}>
          <Tooltip title="The level of competition for this keyword in paid search campaigns, on a scale of 0 to 1.">
            <div>
              <Statistic title="Competition" value={competition} precision={2} />
              <Tag color={competition_level === 'LOW' ? 'green' : competition_level === 'MEDIUM' ? 'orange' : 'red'}>{competition_level}</Tag>
            </div>
          </Tooltip>
        </Col>
        <Col span={12}>
          <Tooltip title="An estimate of how difficult it would be to rank organically for this keyword, on a scale of 0 to 100.">
            <Statistic title="Keyword Difficulty" value={keyword_difficulty} prefix={<ThunderboltOutlined />} />
          </Tooltip>
        </Col>
        <Col span={12}>
          <Tooltip title="The lower range of what advertisers have historically paid for a top-of-page bid.">
            <Statistic title="Low Top of Page Bid" value={low_top_of_page_bid} precision={2} prefix="$" />
          </Tooltip>
        </Col>
        <Col span={12}>
          <Tooltip title="The higher range of what advertisers have historically paid for a top-of-page bid.">
            <Statistic title="High Top of Page Bid" value={high_top_of_page_bid} precision={2} prefix="$" />
          </Tooltip>
        </Col>
      </Row>
      <div style={{ marginTop: '24px' }}>
        {chartData && chartData.length > 0 ? (
          <Line {...chartConfig} />
        ) : (
          <div style={{ textAlign: 'center', padding: '20px' }}>
            <InfoCircleOutlined style={{ marginRight: '8px' }} />
            No monthly search volume data available to display a chart.
          </div>
        )}
      </div>
    </Card>
  );
};

export default KeywordMetrics;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/MetaInfo.jsx">
import React from 'react';
import { Card, Descriptions, Tag } from 'antd';
import { InfoCircleOutlined, DollarCircleOutlined, HistoryOutlined } from '@ant-design/icons';
import { format } from 'date-fns';

const MetaInfo = ({ blueprint, lastWorkflowStep, dateProcessed }) => {
  const metadata = blueprint?.metadata;

  return (
    <Card title="Process Metadata" icon={<InfoCircleOutlined />}>
      <Descriptions column={1} size="small" variant="outlined">
        {metadata?.blueprint_version && (
          <Descriptions.Item label="Blueprint Version">
            <Tag>{metadata.blueprint_version}</Tag>
          </Descriptions.Item>
        )}
        {lastWorkflowStep && (
          <Descriptions.Item label="Last Workflow Step">
            <Tag color="blue">{lastWorkflowStep.replace(/_/g, ' ')}</Tag>
          </Descriptions.Item>
        )}
        {dateProcessed && (
          <Descriptions.Item label="Date Processed">
            <HistoryOutlined /> {format(new Date(dateProcessed), 'MMM d, yyyy HH:mm')}
          </Descriptions.Item>
        )}
        {metadata?.total_api_cost && (
          <Descriptions.Item label="Analysis API Cost">
            <DollarCircleOutlined /> ${metadata.total_api_cost.toFixed(4)}
          </Descriptions.Item>
        )}
      </Descriptions>
    </Card>
  );
};

export default MetaInfo;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/NoData.jsx">
import React from 'react';
import { Empty } from 'antd';

const NoData = ({ description }) => {
  return (
    <div style={{ padding: '24px', textAlign: 'center' }}>
      <Empty
        image={Empty.PRESENTED_IMAGE_SIMPLE}
        description={description || 'No data available for this section.'}
      />
    </div>
  );
};

export default NoData;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/OpportunityHeader.jsx">
import React from 'react';
import { Card, Tag, Statistic, Row, Col, Typography, Progress } from 'antd';
import { format } from 'date-fns';

const { Title, Text } = Typography;

const getStatusColor = (status) => {
  if (status.includes('approved') || status === 'validated') return 'success';
  if (status.includes('paused')) return 'warning';
  if (status.includes('failed')) return 'error';
  return 'processing';
};

const OpportunityHeader = ({ keyword, strategicScore, status, dateAdded, recommendation }) => {
  return (
    <Card style={{ borderRadius: '8px' }}>
      <Row align="middle" justify="space-between">
        <Col>
          <Title level={2} style={{ margin: 0 }}>{keyword}</Title>
          <Tag color={getStatusColor(status)} style={{ marginTop: 8 }}>
            {status.replace(/_/g, ' ').toUpperCase()}
          </Tag>
        </Col>
        <Col>
          <Row align="middle" gutter={32}>
            <Col>
              <Statistic title="Date Added" value={format(new Date(dateAdded), 'MMM d, yyyy')} />
            </Col>
            {recommendation && (
              <Col>
                <Tag color={recommendation === 'Proceed' ? 'success' : 'error'} style={{ fontSize: '1.2rem', padding: '10px' }}>
                  {recommendation}
                </Tag>
              </Col>
            )}
            <Col style={{ textAlign: 'center' }}>
              {typeof strategicScore === 'number' && (
                <>
                  <Progress
                    type="circle"
                    percent={strategicScore}
                    format={(percent) => `${percent.toFixed(1)}`}
                    strokeColor={{
                      '0%': '#B8E1FF',
                      '100%': '#3D76DD',
                    }}
                    size={80}
                  />
                  <Text style={{ display: 'block', marginTop: 8 }}>Strategic Score</Text>
                </>
              )}
            </Col>
          </Row>
        </Col>
      </Row>
    </Card>
  );
};

export default OpportunityHeader;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/QualificationInfo.jsx">
import React from 'react';
import { Card, Typography, Tag } from 'antd';
import { CheckSquareOutlined } from '@ant-design/icons';

const { Text } = Typography;

const QualificationInfo = ({ status, reason }) => {
  if (!status) {
    return null;
  }

  return (
    <Card title="Initial Qualification">
      <Text strong>Status: </Text>
      <Tag color={status === 'review' ? 'orange' : 'default'}>{status.toUpperCase()}</Tag>
      <Text type="secondary" style={{ marginTop: '16px', display: 'block' }}>
        {reason}
      </Text>
    </Card>
  );
};

export default QualificationInfo;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/RecommendedStrategyCard.jsx">
import React from 'react';
import { Card, Typography, Tag } from 'antd';
import { BulbOutlined } from '@ant-design/icons';

const { Title, Paragraph } = Typography;

const RecommendedStrategyCard = ({ strategy }) => {
  if (!strategy) {
    return null;
  }

  return (
    <Card style={{ marginTop: 24 }}>
      <Title level={5}><BulbOutlined /> Recommended Strategy</Title>
      <Paragraph strong>Content Format: <Tag color="purple">{strategy.content_format}</Tag></Paragraph>
      <Paragraph>{strategy.strategic_goal}</Paragraph>
    </Card>
  );
};

export default RecommendedStrategyCard;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/SerpAnalysis.jsx">
import React from 'react';
import { Card, Table, Tag, List, Typography, Button, Statistic, Row, Col } from 'antd';
import {
  FileTextOutlined,
  VideoCameraOutlined,
  QuestionCircleOutlined,
  CommentOutlined,
  SearchOutlined,
  CopyOutlined,
  RobotOutlined,
  FileDoneOutlined,
  LinkOutlined,
  StarFilled
} from '@ant-design/icons';
import { useNotifications } from '../../../context/NotificationContext';

const { Title, Paragraph, Text } = Typography;

const SerpAnalysis = ({ blueprint }) => {
  const { showNotification } = useNotifications();

  if (!blueprint || !blueprint.serp_overview) {
    return <Card><Paragraph type="secondary">No SERP analysis available.</Paragraph></Card>;
  }

  const { top_organic_results, people_also_ask, related_searches, top_organic_sitelinks, serp_has_ai_overview, dominant_content_format } = blueprint.serp_overview;
  const { serp_item_types, se_results_count } = blueprint.winning_keyword.serp_info;

  const getSerpFeatureIcon = (item) => {
    if (item.includes('video')) return <VideoCameraOutlined />;
    if (item.includes('people_also_ask')) return <QuestionCircleOutlined />;
    if (item.includes('perspectives')) return <CommentOutlined />;
    if (item.includes('related_searches')) return <SearchOutlined />;
    return <FileTextOutlined />;
  };

  const handleCopyPaA = () => {
    navigator.clipboard.writeText(people_also_ask.join('\n'));
    showNotification('success', 'Copied to Clipboard', 'People Also Ask questions have been copied.');
  };

  const columns = [
    { title: 'Rank', dataIndex: 'rank', key: 'rank' },
    { title: 'Title', dataIndex: 'title', key: 'title', render: (text, record) => <a href={record.url} target="_blank" rel="noopener noreferrer">{text}</a> },
    { title: 'Domain', dataIndex: 'domain', key: 'domain' },
    { title: 'Page Type', dataIndex: 'page_type', key: 'page_type', render: (type) => <Tag>{type}</Tag> },
    { 
      title: 'Rating', 
      dataIndex: 'rating', 
      key: 'rating', 
      render: (rating) => rating ? <span><StarFilled style={{ color: '#fadb14' }} /> {rating.value} ({rating.votes_count})</span> : null 
    },
  ];

  const handleRelatedSearchClick = (term) => {
    // In a real app, you would likely navigate to the discovery page with this term
    console.log(`Starting a new discovery run for: ${term}`);
    showNotification('info', 'Discovery Run Started', `A new discovery run has been initiated for "${term}".`);
  };

  return (
    <Card title="SERP Analysis">
      <Row gutter={16} style={{ marginBottom: 24 }}>
        <Col span={8}>
          <Statistic title="Total Search Results" value={se_results_count} />
        </Col>
        <Col span={8}>
          <Statistic title="AI Overview in SERP" value={serp_has_ai_overview ? 'Yes' : 'No'} prefix={<RobotOutlined />} />
        </Col>
        <Col span={8}>
           <Statistic title="Dominant Content Format" value={dominant_content_format} prefix={<FileDoneOutlined />} />
        </Col>
      </Row>

      <Title level={5}>SERP Features Present</Title>
      <List
        dataSource={serp_item_types}
        renderItem={(item) => <List.Item><Tag icon={getSerpFeatureIcon(item)}>{item.replace(/_/g, ' ')}</Tag></List.Item>}
        grid={{ gutter: 16, column: 4 }}
        style={{ marginBottom: 24 }}
      />

      <Title level={5}>People Also Ask</Title>
      <Button icon={<CopyOutlined />} onClick={handleCopyPaA} style={{ float: 'right' }}>Copy</Button>
      <List dataSource={people_also_ask} renderItem={(item) => <List.Item>{item}</List.Item>} style={{ marginBottom: 24 }} />

      <Title level={5}>Related Searches</Title>
      <div style={{ marginBottom: 24 }}>
        {related_searches.map(term => (
          <Tag 
            icon={<SearchOutlined />} 
            key={term} 
            style={{ margin: 4, cursor: 'pointer' }}
            onClick={() => handleRelatedSearchClick(term)}
          >
            {term}
          </Tag>
        ))}
      </div>
      
      {top_organic_sitelinks?.length > 0 && <>
        <Title level={5}>Top Organic Sitelinks</Title>
        <List dataSource={top_organic_sitelinks} renderItem={(item) => <List.Item><LinkOutlined /> {item}</List.Item>} style={{ marginBottom: 24 }} />
      </>}

      <Title level={5}>Top 10 Organic Results</Title>
      <Table columns={columns} dataSource={top_organic_results} rowKey="url" pagination={false} size="small" />
    </Card>
  );
};

export default SerpAnalysis;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/SerpVitals.jsx">
import React from 'react';
import { Card, Row, Col, Statistic, Tooltip } from 'antd';
import { CloudSyncOutlined, AlertOutlined } from '@ant-design/icons';

const SerpVitals = ({ scoreBreakdown }) => {
  const volatility = scoreBreakdown?.serp_volatility;
  const crowding = scoreBreakdown?.serp_crowding;

  if (!volatility && !crowding) {
    return null;
  }

  return (
    <Card title="SERP Vitals">
      <Row gutter={16}>
        {volatility && (
          <Col span={12}>
            <Tooltip title={volatility.breakdown['SERP Stability'].explanation}>
              <Statistic 
                title="SERP Stability" 
                value={volatility.breakdown['SERP Stability'].value} 
                prefix={<CloudSyncOutlined />} 
              />
            </Tooltip>
          </Col>
        )}
        {crowding && (
          <Col span={12}>
            <Tooltip title={crowding.breakdown['SERP Crowding'].explanation}>
              <Statistic 
                title="SERP Crowding" 
                value={`${crowding.breakdown['SERP Crowding'].value} Features`}
                prefix={<AlertOutlined />} 
              />
            </Tooltip>
          </Col>
        )}
      </Row>
    </Card>
  );
};

export default SerpVitals;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/SocialMediaTab.jsx">
import React from 'react';
import { Card, Typography, List, Button, Tooltip } from 'antd';
import { CopyOutlined } from '@ant-design/icons';
import { useNotifications } from '../../../context/NotificationContext';

const { Title, Paragraph } = Typography;

const SocialMediaTab = ({ socialMediaPosts }) => {
  const { showNotification } = useNotifications();

  if (!socialMediaPosts || socialMediaPosts.length === 0) {
    return <Card><Paragraph>No social media posts have been generated yet.</Paragraph></Card>;
  }

  const handleCopy = (text) => {
    navigator.clipboard.writeText(text);
    showNotification('success', 'Copied to Clipboard', 'The post content has been copied.');
  };

  return (
    <List
      grid={{ gutter: 16, column: 2 }}
      dataSource={socialMediaPosts}
      renderItem={(post) => (
        <List.Item>
          <Card title={post.platform}>
            <Paragraph>{post.content}</Paragraph>
            <Tooltip title="Copy Post">
              <Button
                icon={<CopyOutlined />}
                onClick={() => handleCopy(post.content)}
              />
            </Tooltip>
          </Card>
        </List.Item>
      )}
    />
  );
};

export default SocialMediaTab;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/StrategicNotes.jsx">
import React from 'react';
import { Alert } from 'antd';
import { BulbOutlined } from '@ant-design/icons';

const StrategicNotes = ({ notes }) => {
  if (!notes) {
    return null;
  }

  return (
    <Alert
      message="Strategic Note from AI Analysis"
      description={notes}
      type="warning"
      showIcon
      icon={<BulbOutlined />}
    />
  );
};

export default StrategicNotes;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/StrategicScoreBreakdown.jsx">
import React from 'react';
import { Card, Typography, Tooltip, Progress, List, Tag } from 'antd';
import { InfoCircleOutlined } from '@ant-design/icons';

const { Title, Text, Paragraph } = Typography;

const scoreCategoryMapping = {
  'Ranking & Competition': ['ease_of_ranking', 'competitor_weakness', 'competitor_performance'],
  'Traffic & Commercial Value': ['traffic_potential', 'commercial_intent', 'growth_trend', 'volume_volatility'],
  'SERP Environment': ['serp_features', 'serp_volatility', 'serp_crowding', 'serp_threat', 'serp_freshness'],
  'Keyword Profile': ['keyword_structure'],
};

const friendlyExplanations = {
  ease_of_ranking: "This score assesses the overall difficulty of ranking on the first page of Google for this keyword. It's a composite metric that considers the authority of competing websites, the keyword's inherent difficulty, and the number of competing pages. A higher score means we've identified a path of lower resistance.",
  competitor_weakness: "We analyze the top-ranking pages to find their weak spots. This score is higher if competitors have low domain authority, few backlinks, or other vulnerabilities that we can strategically exploit to outrank them.",
  competitor_performance: "This score evaluates the technical performance of competing websites, such as their loading speed and mobile-friendliness. Slower or poorly optimized competitor sites present a clear opportunity, as search engines penalize them, making it easier for us to rank higher with a technically superior page.",
  traffic_potential: "This score estimates the potential traffic this keyword could generate. It's not just about raw search volume; it also considers the keyword's cost-per-click (CPC) value, indicating its commercial worth. A high score suggests the keyword can attract a valuable audience.",
  commercial_intent: "We analyze the language of the keyword and the types of ads on the search results page to determine the user's intent. A high score indicates that the user is likely looking to make a purchase or engage a service, making the traffic more valuable.",
  growth_trend: "This score reflects the keyword's popularity over time. We analyze search data from the past year to identify upward trends. A high score means the keyword is becoming more popular, representing a growing area of interest and a sustainable source of future traffic.",
  volume_volatility: "This score measures the stability of the keyword's search volume. A low score indicates high volatility (e.g., a seasonal trend), while a high score suggests a stable, consistent search volume, making it a more reliable target for long-term content strategy.",
  serp_features: "This score identifies opportunities within the Search Engine Results Page (SERP) itself. We look for features like 'Featured Snippets,' 'People Also Ask' boxes, and video carousels. A high score means there are multiple ways to appear on the first page beyond the standard blue links.",
  serp_volatility: "This score measures how frequently the rankings for this keyword change. A highly volatile SERP, where rankings fluctuate often, can be an opportunity to quickly gain a foothold, as it indicates that search engines are still trying to determine the best results.",
  serp_crowding: "This score assesses how 'crowded' the search results page is with non-organic results like ads, image packs, and shopping results. A lower score means the page is very crowded, which can push organic results further down the page and reduce their visibility.",
  serp_threat: "This score identifies the presence of dominant, high-authority domains (like Wikipedia, government sites, or major news outlets) that are extremely difficult to outrank. A high score indicates the absence of such threats, making it a more level playing field.",
  serp_freshness: "This score evaluates the age of the content currently ranking on the first page. If the top results are several years old, it signals a 'freshness' opportunity, where new, up-to-date content is likely to be favored by search engines.",
  keyword_structure: "This score analyzes the composition of the keyword itself. Longer, more specific keywords (long-tail keywords) are often less competitive and signal a more specific user intent, making them easier to rank for. A high score is awarded to these types of keywords.",
};

const StrategicScoreBreakdown = ({ scoreBreakdown }) => {
  if (!scoreBreakdown) {
    return null;
  }

  const getScoreColor = (score) => {
    if (score > 70) return '#52c41a'; // green
    if (score > 40) return '#faad14'; // orange
    return '#f5222d'; // red
  };

  return (
    <Card title="Strategic Score Analysis" style={{ marginTop: 24 }}>
      <Paragraph type="secondary">
        This analysis breaks down the main factors contributing to the overall Strategic Score. Each factor is scored from 0-100, where a higher score indicates a better opportunity.
      </Paragraph>
      {Object.entries(scoreCategoryMapping).map(([category, keys]) => (
        <div key={category}>
          <Title level={4} style={{ marginTop: 24, marginBottom: 16 }}>{category}</Title>
          <List
            itemLayout="vertical"
            dataSource={keys.map(key => ({ key, ...scoreBreakdown[key] })).filter(item => item.name)}
            renderItem={(factor) => (
              <List.Item key={factor.key}>
                <List.Item.Meta
                  title={
                    <div style={{ display: 'flex', alignItems: 'center', justifyContent: 'space-between' }}>
                      <span>
                        {factor.name}
                        <Tooltip title={friendlyExplanations[factor.key]}>
                          <InfoCircleOutlined style={{ marginLeft: 8, color: '#888' }} />
                        </Tooltip>
                      </span>
                      <Tag color={getScoreColor(factor.score)} style={{ fontSize: '1rem', padding: '4px 8px' }}>
                        {factor.score.toFixed(1)}
                      </Tag>
                    </div>
                  }
                  description={
                    <div>
                      <Progress
                        percent={factor.score}
                        showInfo={false}
                        strokeColor={getScoreColor(factor.score)}
                        style={{ marginBottom: 8 }}
                      />
                      {factor.breakdown.message ? (
                        <Text type="secondary">{factor.breakdown.message}</Text>
                      ) : (
                        <ul style={{ paddingLeft: 20, margin: 0 }}>
                          {Object.entries(factor.breakdown).map(([key, value]) => (
                            <li key={key}>
                              <Text strong>{key}:</Text> {value.value} - <Text type="secondary">{value.explanation}</Text>
                            </li>
                          ))}
                        </ul>
                      )}
                    </div>
                  }
                />
              </List.Item>
            )}
          />
        </div>
      ))}
    </Card>
  );
};

export default StrategicScoreBreakdown;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/VerdictCard.jsx">
import React from 'react';
import { Card, Typography, Tag } from 'antd';

const { Title, Paragraph } = Typography;

const VerdictCard = ({ recommendation, confidenceScore }) => {
  if (!recommendation) {
    return null;
  }

  return (
    <Card style={{ marginTop: 24 }}>
      <Title level={5}>The Verdict</Title>
      <Tag color={recommendation === 'Proceed' ? 'success' : 'error'} style={{ fontSize: '1.2rem', padding: '10px' }}>
        {recommendation}
      </Tag>
      <Paragraph style={{ marginTop: '10px' }}>Confidence: {confidenceScore.toFixed(1)}%</Paragraph>
    </Card>
  );
};

export default VerdictCard;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/WorkflowStatusAlert.jsx">
import React from 'react';
import { Alert } from 'antd';
import { InfoCircleOutlined } from '@ant-design/icons';

const WorkflowStatusAlert = ({ status, message }) => {
  // Only show for specific, non-error statuses that have a message
  if (status !== 'paused_for_approval' || !message) {
    return null;
  }

  // Don't show if the message looks like an error
  if (message.toLowerCase().includes('error') || message.toLowerCase().includes('failed')) {
      return null;
  }

  return (
    <Alert
      message="Current Status"
      description={message}
      type="info"
      showIcon
      icon={<InfoCircleOutlined />}
    />
  );
};

export default WorkflowStatusAlert;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/components/WorkflowTracker.jsx">
import React, { useEffect, useState } from 'react';
import { Card, Steps, Spin, Alert, Button } from 'antd';
import { useQuery, useQueryClient } from 'react-query';
import { useNavigate } from 'react-router-dom';
import { getJobStatus } from '../../../services/jobsService';
import { CheckCircleOutlined } from '@ant-design/icons';

const { Step } = Steps;

const IN_PROGRESS_STATUSES = ['processing', 'running', 'in_progress', 'pending', 'refresh_started'];

const WorkflowTracker = ({ opportunity }) => {
  const { latest_job_id, status, error_message } = opportunity;
  const navigate = useNavigate();
  const queryClient = useQueryClient();

  const { data: jobStatus, isLoading: isLoadingStatus } = useQuery(
    ['jobStatus', latest_job_id],
    () => getJobStatus(latest_job_id),
    {
      enabled: !!latest_job_id && (!jobStatus || (jobStatus.status !== 'completed' && jobStatus.status !== 'failed')),
      refetchInterval: 3000, // Poll every 3 seconds for faster updates
      onSuccess: (data) => {
        if (data?.status === 'completed' || data?.status === 'failed' || data?.status === 'paused') {
          // Invalidate queries to refetch the main opportunity data for the page
          queryClient.invalidateQueries(['opportunity', opportunity.id]);
        }
        
        if (data?.status === 'completed' && data.result?.redirect_url) {
          setTimeout(() => {
            navigate(data.result.redirect_url);
          }, 1500); // Delay for user to see the final success state
        }
      },
    }
  );

  const progressLog = jobStatus?.progress_log || [];
  const currentStepIndex = progressLog.length > 0 ? progressLog.length - 1 : 0;

  // Don't render anything if there's no job or the workflow is in a non-terminal, non-processing state
  if (!latest_job_id || (!IN_PROGRESS_STATUSES.includes(status) && status !== 'failed' && jobStatus?.status !== 'completed')) {
    return null;
  }

  const isJobRunning = jobStatus?.status === 'running' || jobStatus?.status === 'pending';

  return (
    <Card title="Workflow Status" style={{ marginTop: 24 }}>
      {isLoadingStatus && !jobStatus && <Spin tip="Initializing workflow status..." />}
      
      {progressLog.length > 0 && (
        <Steps direction="vertical" current={currentStepIndex}>
          {progressLog.map((log, index) => (
            <Step 
              key={index} 
              title={log.step} 
              description={log.message} 
              icon={isJobRunning && index === currentStepIndex ? <Spin /> : null}
            />
          ))}
        </Steps>
      )}

      {jobStatus?.status === 'completed' && (
        <Alert
          message="Workflow Completed"
          description={jobStatus.result?.message || 'The workflow finished successfully.'}
          type="success"
          showIcon
          icon={<CheckCircleOutlined />}
          action={
            jobStatus.result?.redirect_url && (
              <Button size="small" type="primary" onClick={() => navigate(jobStatus.result.redirect_url)}>
                Go to Results
              </Button>
            )
          }
        />
      )}

      {jobStatus?.status === 'failed' && (
        <Alert
          message="Workflow Failed"
          description={jobStatus.error || error_message || 'An unknown error occurred.'}
          type="error"
          showIcon
        />
      )}
    </Card>
  );
};

export default WorkflowTracker;
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/hooks/useOpportunityData.js">
import { useQuery } from 'react-query';
import { getOpportunityById } from '../../../services/opportunitiesService';

export const useOpportunityData = (opportunityId) => {
  const id = parseInt(opportunityId);
  const { data, isLoading, isError, error, refetch } = useQuery(
    ['opportunity', id],
    () => getOpportunityById(id),
    {
      enabled: !!id,
      staleTime: 5 * 60 * 1000, // 5 minutes
      cacheTime: 10 * 60 * 1000, // 10 minutes
    }
  );

  return { opportunity: data, isLoading, isError, error, refetch };
};
</file>

<file path="client/my-content-app/src/pages/opportunity-detail-page/index.jsx">
import React, { useState, useEffect } from 'react';
import { useParams } from 'react-router-dom';
import { Layout, Spin, Alert, Row, Col, Tabs } from 'antd';
import { useOpportunityData } from './hooks/useOpportunityData';
import OpportunityHeader from './components/OpportunityHeader';
import ActionCenter from './components/ActionCenter';
import ExecutiveSummary from './components/ExecutiveSummary';
import KeywordMetrics from './components/KeywordMetrics';
import StrategicScoreBreakdown from './components/StrategicScoreBreakdown';
import SerpAnalysis from './components/SerpAnalysis';
import ContentBlueprint from './components/ContentBlueprint';
import ArticlePreview from './components/ArticlePreview';
import ContentAuditCard from './components/ContentAuditCard';
import SocialMediaTab from './components/SocialMediaTab';
import VerdictCard from './components/VerdictCard';
import FactorsCard from './components/FactorsCard';
import RecommendedStrategyCard from './components/RecommendedStrategyCard';
import StrategicNotes from './components/StrategicNotes';
import CompetitorBacklinks from './components/CompetitorBacklinks';
import IntentAnalysis from './components/IntentAnalysis';
import SerpVitals from './components/SerpVitals';
import GrowthTrend from './components/GrowthTrend';

import WorkflowTracker from './components/WorkflowTracker';

const { TabPane } = Tabs;

const OpportunityDetailPageV2 = () => {
  const { opportunityId } = useParams();
  const { opportunity, isLoading, isError, error, refetch } = useOpportunityData(opportunityId);
  const [blueprintOverrides, setBlueprintOverrides] = useState(null);

  useEffect(() => {
    if (opportunity?.blueprint?.content_intelligence?.article_structure) {
      setBlueprintOverrides(opportunity.blueprint.content_intelligence.article_structure);
    }
  }, [opportunity]);

  if (isLoading) {
    return (
      <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: '100vh' }}>
        <Spin tip="Loading opportunity..." size="large" />
      </div>
    );
  }

  if (isError) {
    return <Alert message="Error" description={error.message} type="error" showIcon />;
  }

  if (!opportunity) {
    return null;
  }

  const { blueprint, ai_content, social_media_posts_json, score_breakdown, full_data } = opportunity;

  return (
    <Layout style={{ padding: '24px', background: '#f0f2f5' }}>
      <OpportunityHeader
        keyword={opportunity.keyword}
        strategicScore={opportunity.strategic_score}
        status={opportunity.status}
        dateAdded={opportunity.date_added}
        recommendation={blueprint?.final_qualification_assessment?.recommendation}
      />
      <ActionCenter 
        status={opportunity.status} 
        opportunityId={opportunity.id} 
        overrides={blueprintOverrides} 
        refetch={refetch}
        style={{ marginTop: 24, marginBottom: 24 }}
      />
      <WorkflowTracker opportunity={opportunity} />
      <Tabs defaultActiveKey="1">
        <TabPane tab="Overview" key="1">
          <Row gutter={[24, 24]}>
            <Col xs={24} lg={8}>
              <VerdictCard 
                recommendation={blueprint?.final_qualification_assessment?.recommendation}
                confidenceScore={blueprint?.final_qualification_assessment?.confidence_score}
              />
              <RecommendedStrategyCard strategy={blueprint?.recommended_strategy} />
              <KeywordMetrics 
                keywordInfo={opportunity.keyword_info} 
                keywordProperties={opportunity.keyword_properties}
              />
              <IntentAnalysis searchIntentInfo={opportunity.search_intent_info} />
              <GrowthTrend scoreBreakdown={score_breakdown} />
            </Col>
            <Col xs={24} lg={16}>
              <StrategicNotes notes={blueprint?.analysis_notes} />
              <FactorsCard 
                positiveFactors={blueprint?.final_qualification_assessment?.positive_factors}
                negativeFactors={blueprint?.final_qualification_assessment?.negative_factors}
              />
              <ExecutiveSummary summary={blueprint?.executive_summary} />
              <StrategicScoreBreakdown scoreBreakdown={score_breakdown} />
              <CompetitorBacklinks avgBacklinksInfo={full_data?.avg_backlinks_info} />
              <SerpVitals scoreBreakdown={score_breakdown} />
            </Col>
          </Row>
        </TabPane>
        <TabPane tab="SERP Analysis" key="2">
          <SerpAnalysis blueprint={blueprint} />
        </TabPane>
        <TabPane tab="Content Blueprint" key="3">
          <ContentBlueprint
            blueprint={blueprint}
            overrides={blueprintOverrides}
            setOverrides={setBlueprintOverrides}
          />
        </TabPane>
        <TabPane tab="Publishing" key="4">
          <Tabs defaultActiveKey="article">
            <TabPane tab="Article" key="article">
              <ArticlePreview
                aiContent={ai_content}
                featuredImagePath={opportunity.featured_image_local_path}
              />
            </TabPane>
            <TabPane tab="Social Media" key="social">
              <SocialMediaTab socialMediaPosts={social_media_posts_json} />
            </TabPane>
          </Tabs>
        </TabPane>
        <TabPane tab="Audit" key="5">
          <ContentAuditCard auditResults={ai_content?.audit_results} />
        </TabPane>
      </Tabs>
    </Layout>
  );
};

export default OpportunityDetailPageV2;
</file>

<file path="client/my-content-app/src/pages/RunDetailsPage/RunDetailsPage.jsx">
import React from 'react';
import { useParams, Link } from 'react-router-dom';
import { useQuery } from 'react-query';
import { Layout, Spin, Alert, Typography, Progress, Card, Descriptions, Tag, Button } from 'antd';
import { ArrowLeftOutlined } from '@ant-design/icons';
import { getDiscoveryRunById } from '../../services/discoveryService'; // Adjust import path as needed
// import OpportunityTable from '../../components/OpportunityTable'; // Assuming a reusable table component exists

const { Content } = Layout;
const { Title, Paragraph } = Typography;

const RunDetailsPage = () => {
  const { runId } = useParams();

  const { data: run, isLoading, isError, error } = useQuery(
    ['discoveryRun', runId],
    () => getDiscoveryRunById(runId),
    {
      // Poll for updates every 5 seconds if the run is still in progress
      refetchInterval: (data) => {
        const inProgress = data?.status === 'PENDING' || data?.status === 'IN_PROGRESS';
        return inProgress ? 5000 : false;
      },
      refetchOnWindowFocus: false,
    }
  );

  if (isLoading) {
    return <Spin tip="Loading run details..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (isError) {
    return <Alert message="Error" description={error.message} type="error" showIcon />;
  }

  const getStatusTag = (status) => {
    switch (status) {
      case 'COMPLETED': return <Tag color="success">Completed</Tag>;
      case 'FAILED': return <Tag color="error">Failed</Tag>;
      case 'IN_PROGRESS': return <Tag color="processing">In Progress</Tag>;
      case 'PENDING': return <Tag color="gold">Pending</Tag>;
      default: return <Tag>{status}</Tag>;
    }
  };

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Button type="link" icon={<ArrowLeftOutlined />} style={{ marginBottom: '16px', paddingLeft: 0 }}>
          <Link to="/discovery">Back to Discovery Hub</Link>
        </Button>
        <Title level={2}>Discovery Run Details</Title>
        <Card style={{ marginBottom: '24px' }}>
          <Descriptions title="Run Summary" bordered>
            <Descriptions.Item label="Run ID">{run.id}</Descriptions.Item>
            <Descriptions.Item label="Status">{getStatusTag(run.status)}</Descriptions.Item>
            <Descriptions.Item label="Seed Keyword">{run.parameters?.seed_keywords?.join(', ')}</Descriptions.Item>
            <Descriptions.Item label="Created At">{new Date(run.created_at).toLocaleString()}</Descriptions.Item>
          </Descriptions>
          {run.status === 'IN_PROGRESS' && (
            <div style={{ marginTop: '16px' }}>
              <Paragraph>{run.progress_message || 'Processing...'}</Paragraph>
              <Progress percent={run.progress_percent || 0} />
            </div>
          )}
        </Card>

        {run.status === 'COMPLETED' && (
          <Card title="Discovered Opportunities">
            {/* Assuming you have a component to display opportunities */}
            {/* <OpportunityTable opportunities={run.opportunities} /> */}
          </Card>
        )}

        {run.status === 'FAILED' && (
          <Alert message="Run Failed" description={run.error_message || 'An unknown error occurred.'} type="error" showIcon />
        )}
      </Content>
    </Layout>
  );
};

export default RunDetailsPage;
</file>

<file path="client/my-content-app/src/pages/Settings/tabs/AiContentSettingsTab.jsx">
// This is a new file. Create it with the following content:
import React from 'react';
import { Form, Input, InputNumber, Select, Switch, Slider, Typography, Row, Col, Divider, Tooltip, Alert, Space } from 'antd';
import { InfoCircleOutlined } from '@ant-design/icons';
import PromptTemplateEditor from '../../../components/PromptTemplateEditor'; // NEW

const { Title, Text } = Typography;
const { Option } = Select;

const AiContentSettingsTab = ({ settings, form }) => {
  const contentModel = Form.useWatch('ai_content_model', form); // Watch for changes in the AI content model

  return (
    <>
      <Title level={5}>AI Model & Generation</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="ai_content_model" label="AI Content Generation Model">
            <Select style={{ width: '100%' }}>
              <Option value="gpt-4o">GPT-4o (Recommended)</Option>
              <Option value="gpt-4-turbo">GPT-4 Turbo</Option>
              <Option value="gpt-3.5-turbo">GPT-3.5 Turbo (Cost-Effective)</Option>
            </Select>
          </Form.Item>
          {contentModel === 'gpt-3.5-turbo' && (
            <Alert
              message="Cost-Effective Model Selected"
              description="GPT-3.5 Turbo is cheaper but may require more prompt engineering for quality."
              type="info"
              showIcon
              style={{ marginBottom: '16px' }}
            />
          )}
        </Col>
        <Col span={12}>
          <Form.Item name="ai_generation_temperature" label="AI Creativity (Temperature)">
            <Slider min={0.0} max={1.0} step={0.01} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="expert_persona" label="AI Writer Persona">
            <Input placeholder="e.g., a certified financial planner with 15 years of experience" />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="recommended_word_count_multiplier" label="Word Count Multiplier">
            <InputNumber min={0.5} max={3.0} step={0.1} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_completion_tokens_for_generation" label="Max Output Tokens">
            <InputNumber min={1000} max={32000} step={1000} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_words_for_ai_analysis" label="Max Words for AI Analysis">
            <InputNumber min={500} max={5000} step={100} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
      </Row>

      <Divider />

      <Title level={5}>Image Generation</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="use_pexels_first" label="Use Pexels for Images" valuePropName="checked">
            <Switch checkedChildren="Enabled" unCheckedChildren="Disabled" />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="num_in_article_images" label="Number of In-Article Images">
            <InputNumber min={0} max={5} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="overlay_text_enabled" label="Add Text Overlay to Images" valuePropName="checked">
            <Switch checkedChildren="Enabled" unCheckedChildren="Disabled" />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="overlay_text_color" label="Overlay Text Color">
            <Input type="color" style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="overlay_background_color" label="Overlay Background Color">
            <Input type="color" style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="overlay_font_size" label="Overlay Font Size">
            <InputNumber min={10} max={60} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="overlay_position" label="Overlay Position">
            <Select style={{ width: '100%' }}>
              <Option value="top_left">Top Left</Option>
              <Option value="top_center">Top Center</Option>
              <Option value="top_right">Top Right</Option>
              <Option value="bottom_left">Bottom Left</Option>
              <Option value="bottom_center">Bottom Center</Option>
              <Option value="bottom_right">Bottom Right</Option>
              <Option value="center">Center</Option>
            </Select>
          </Form.Item>
        </Col>
      </Row>

      <Divider />

      <Title level={5}>Custom AI Prompt Template</Title>
      <Form.Item 
        name="custom_prompt_template" 
        label={
          <Space>
            Edit your base prompt for the AI content generator.
            <Tooltip title="This template guides the AI's writing. Use available placeholders for dynamic data.">
              <InfoCircleOutlined />
            </Tooltip>
          </Space>
        }
        style={{ marginBottom: 0 }}
      >
        <PromptTemplateEditor disabled={false} />
      </Form.Item>
    </>
  );
};

export default AiContentSettingsTab;
</file>

<file path="client/my-content-app/src/pages/Settings/tabs/DiscoverySettingsTab.jsx">
// This is a new file. Create it with the following content:
import React from 'react';
import { Form, Input, InputNumber, Select, Switch, Checkbox, Slider, Typography, Row, Col, Divider, Tooltip, Space } from 'antd';
import { InfoCircleOutlined } from '@ant-design/icons';

const { Title, Text } = Typography;
const { Option } = Select;

const DiscoverySettingsTab = ({ settings, form }) => {
  return (
    <>
      <Title level={5}>General Discovery Parameters</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="min_search_volume" label="Minimum Search Volume">
            <InputNumber min={0} max={100000} step={10} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_keyword_difficulty" label="Maximum Keyword Difficulty">
            <InputNumber min={0} max={100} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="min_keyword_word_count" label="Minimum Keyword Word Count">
            <InputNumber min={1} max={20} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_keyword_word_count" label="Maximum Keyword Word Count">
            <InputNumber min={1} max={20} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_max_pages" label="Max API Pages to Fetch">
            <InputNumber min={1} max={100} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_related_depth" label="Related Keywords Depth">
            <Slider min={0} max={4} />
          </Form.Item>
        </Col>
        <Col span={24}>
          <Form.Item name="discovery_strategies" label="Discovery Strategies">
            <Checkbox.Group
              options={[
                { label: 'Keyword Ideas (Category-based)', value: 'Keyword Ideas' },
                { label: 'Keyword Suggestions (Phrase-based)', value: 'Keyword Suggestions' },
                { label: 'Related Keywords (SERP-based)', value: 'Related Keywords' },
              ]}
            />
          </Form.Item>
        </Col>
      </Row>

      <Divider />

      <Title level={5}>Advanced Filtering & Ordering</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="closely_variants" label="Search Mode (Keyword Ideas)" valuePropName="checked">
            <Switch checkedChildren="Phrase Match" unCheckedChildren="Broad Match" />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_ignore_synonyms" label="Ignore Synonyms" valuePropName="checked">
            <Switch />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_replace_with_core_keyword" label="Replace with Core Keyword" valuePropName="checked">
            <Switch />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="min_cpc_filter" label="Minimum CPC ($)">
            <InputNumber min={0.0} step={0.1} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_cpc_filter" label="Maximum CPC ($)">
            <InputNumber min={0.0} step={0.1} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="min_competition" label="Minimum Competition (0-1)">
            <InputNumber min={0.0} max={1.0} step={0.01} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_competition" label="Maximum Competition (0-1)">
            <InputNumber min={0.0} max={1.0} step={0.01} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_competition_level" label="Max Competition Level">
            <Select style={{ width: '100%' }}>
              <Option value="LOW">LOW</Option>
              <Option value="MEDIUM">MEDIUM</Option>
              <Option value="HIGH">HIGH</Option>
            </Select>
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_order_by_field" label="Order Results By">
            <Select style={{ width: '100%' }}>
              <Option value="keyword_info.search_volume">Search Volume</Option>
              <Option value="keyword_properties.keyword_difficulty">Keyword Difficulty</Option>
              <Option value="keyword_info.cpc">CPC</Option>
              <Option value="keyword_info.competition">Competition</Option>
            </Select>
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_order_by_direction" label="Order Direction">
            <Select style={{ width: '100%' }}>
              <Option value="desc">Descending</Option>
              <Option value="asc">Ascending</Option>
            </Select>
          </Form.Item>
        </Col>
        <Col span={24}>
          <Form.Item 
            name="search_phrase_regex" 
            label={
              <Space>
                Keyword Regex Filter
                <Tooltip title="Use regular expressions to filter keywords (e.g., ^how to.*$)">
                  <InfoCircleOutlined />
                </Tooltip>
              </Space>
            }
          >
            <Input placeholder="e.g., ^best.*reviews$" />
          </Form.Item>
        </Col>
      </Row>

      <Divider />

      <Title level={5}>Intent & Qualification</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="enforce_intent_filter" label="Enforce Intent Filter" valuePropName="checked">
            <Switch />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="allowed_intents" label="Allowed Intents">
            <Select mode="multiple" style={{ width: '100%' }}>
              <Option value="informational">Informational</Option>
              <Option value="commercial">Commercial</Option>
              <Option value="transactional">Transactional</Option>
              <Option value="navigational">Navigational</Option>
            </Select>
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="prohibited_intents" label="Prohibited Secondary Intents">
            <Select mode="multiple" style={{ width: '100%' }}>
              <Option value="informational">Informational</Option>
              <Option value="commercial">Commercial</Option>
              <Option value="transactional">Transactional</Option>
              <Option value="navigational">Navigational</Option>
            </Select>
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="require_question_keywords" label="Require Question Keywords" valuePropName="checked">
            <Switch />
          </Form.Item>
        </Col>
        <Col span={24}>
          <Form.Item name="negative_keywords" label="Negative Keywords" extra="Comma-separated list of keywords to exclude (e.g., free, login)">
            <Input.TextArea rows={2} />
          </Form.Item>
        </Col>
      </Row>

      <Divider /><br/>
      <Title level={5}>SERP & Competitor Analysis Cost Controls</Title><br/>
      <Row gutter={16}><br/>
        <Col span={12}><br/>
          <Form.Item name="load_async_ai_overview" label="Load Async AI Overview" valuePropName="checked" tooltip="Set to true to obtain AI Overview items in SERPs even if they are loaded asynchronously. Costs $0.002 extra per SERP call."><br/>
            <Switch checkedChildren="Enabled" unCheckedChildren="Disabled" /><br/>
          </Form.Item><br/>
        </Col><br/>
        <Col span={12}><br/>
          <Form.Item name="people_also_ask_click_depth" label="PAA Click Depth"><br/>
            <InputNumber min={0} max={4} style={{ width: '100%' }} tooltip="Specify the depth of clicks (1 to 4) on People Also Ask elements. Costs $0.00015 extra per click per level." /><br/>
          </Form.Item><br/>
        </Col><br/>
        <Col span={12}><br/>
          <Form.Item name="onpage_enable_browser_rendering" label="Enable Browser Rendering (High Cost)" valuePropName="checked" tooltip="If true, emulates a full browser load for Core Web Vitals (LCP/CLS) and loads JavaScript/Resources automatically. Primary cost factor for Analysis."><br/>
            <Switch checkedChildren="Enabled" unCheckedChildren="Disabled" /><br/>
          </Form.Item><br/>
        </Col><br/>
        <Col span={12}><br/>
          <Form.Item name="onpage_enable_custom_js" label="Enable Custom JavaScript" valuePropName="checked" tooltip="Allows execution of custom JavaScript during OnPage crawl. Costs $0.00025 extra per page analyzed."><br/>
            <Switch checkedChildren="Enabled" unCheckedChildren="Disabled" /><br/>
          </Form.Item><br/>
        </Col><br/>
      </Row><br/><br/>
    </>
  );
};

export default DiscoverySettingsTab;
</file>

<file path="client/my-content-app/src/pages/Settings/tabs/ScoringWeightsTab.jsx">
// This is a new file. Create it with the following content:
import React from 'react';
import { Form, Slider, InputNumber, Typography, Row, Col, Divider, Tooltip, Space, Alert } from 'antd';
import { InfoCircleOutlined } from '@ant-design/icons';

const { Title, Text } = Typography;

const ScoringWeightsTab = ({ settings, form }) => {
  const allWeights = Form.useWatch([
    'ease_of_ranking_weight',
    'traffic_potential_weight',
    'commercial_intent_weight',
    'serp_features_weight',
    'growth_trend_weight',
    'serp_freshness_weight',
    'serp_volatility_weight',
    'competitor_weakness_weight'
  ], form);

  const totalWeight = Object.values(allWeights || {}).reduce((sum, current) => sum + (current || 0), 0);

  const renderWeightInput = (name, label, tooltip) => (
    <Col span={24}>
      <Form.Item 
        name={name} 
        label={
          <Space>
            {label}
            {tooltip && <Tooltip title={tooltip}><InfoCircleOutlined /></Tooltip>}
          </Space>
        }
        rules={[{ required: true, message: 'Weight is required' }]}
        style={{ marginBottom: 0 }}
      >
        <Row>
          <Col span={18}>
            <Slider min={0} max={100} step={1} style={{ margin: '0 8px' }} />
          </Col>
          <Col span={4}>
            <InputNumber min={0} max={100} step={1} style={{ width: '100%' }} />
          </Col>
        </Row>
      </Form.Item>
    </Col>
  );

  return (
    <>
      <Title level={5}>Strategic Scoring Weights (Sum to {totalWeight}%)</Title>
      {totalWeight !== 100 && (
        <Alert
          message="Warning: Total weight is not 100%"
          description="The sum of all weights should ideally be 100% for proper normalization. Consider adjusting your weights."
          type="warning"
          showIcon
          style={{ marginBottom: '16px' }}
        />
      )}
      <Row gutter={16}>
        {renderWeightInput('ease_of_ranking_weight', 'Ease of Ranking', 'How easy it is to rank (based on KD, backlinks).')}
        {renderWeightInput('traffic_potential_weight', 'Traffic Potential', 'How much traffic the keyword can bring (based on Search Volume).')}
        {renderWeightInput('commercial_intent_weight', 'Commercial Intent', 'How likely the keyword is to lead to a conversion (based on CPC, intent type).')}
        {renderWeightInput('serp_features_weight', 'SERP Features', 'Impact of rich SERP features (Featured Snippets, AI Overviews).')}
        {renderWeightInput('growth_trend_weight', 'Growth Trend', 'How quickly the search volume is growing or declining.')}
        {renderWeightInput('serp_freshness_weight', 'SERP Freshness', 'How recently the SERP was updated (opportunity if stale).')}
        {renderWeightInput('serp_volatility_weight', 'SERP Volatility', 'How often the SERP changes (opportunity if stable).')}
        {renderWeightInput('competitor_weakness_weight', 'Competitor Weakness', 'Exploitable technical or content flaws of top competitors.')}
      </Row>

      <Divider />

      <Title level={5}>Scoring Normalization Values</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="max_cpc_for_scoring" label="Max CPC for Scoring">
            <InputNumber min={0.0} step={1.0} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_sv_for_scoring" label="Max Search Volume for Scoring">
            <InputNumber min={0} step={1000} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_domain_rank_for_scoring" label="Max Domain Rank for Scoring">
            <InputNumber min={0} step={100} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_referring_domains_for_scoring" label="Max Referring Domains for Scoring">
            <InputNumber min={0} step={50} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
      </Row>
    </>
  );
};

export default ScoringWeightsTab;
</file>

<file path="client/my-content-app/src/pages/Settings/SettingsPage.jsx">
// This is a new file. Create it with the following content:
import React, { useState, useEffect } from 'react';
import { Layout, Typography, Tabs, Spin, Alert, Button, Space, Form } from 'antd';
import { useQuery, useMutation, useQueryClient } from 'react-query';
import { getClientSettings, updateClientSettings } from '../../services/clientSettingsService'; // Corrected
import { useClient } from '../../hooks/useClient'; // NEW
import { useNotifications } from '../../context/NotificationContext'; // NEW
import { SaveOutlined, ReloadOutlined } from '@ant-design/icons';
import DiscoverySettingsTab from './tabs/DiscoverySettingsTab'; // NEW
import ScoringWeightsTab from './tabs/ScoringWeightsTab'; // NEW
import AiContentSettingsTab from './tabs/AiContentSettingsTab'; // NEW

const { Content } = Layout;
const { Title, Text } = Typography;

// Placeholder tab components
const ApiKeysSettingsTab = ({ settings, form }) => <Alert message="API Keys" description="API key management will go here." type="info" />; // NEW for task 3.3.1 (API Key Management)

const SettingsPage = () => {
  console.log('Rendering SettingsPage');
  const { clientId } = useClient();
  const { showNotification } = useNotifications();
  const queryClient = useQueryClient();
  const [form] = Form.useForm();
  const [isDirty, setIsDirty] = useState(false); // To track if form has unsaved changes

  const { data: currentSettings, isLoading, isError, error, refetch } = useQuery(
    ['clientSettings', clientId],
    () => getClientSettings(clientId),
    {
      enabled: !!clientId,
      onSuccess: (data) => {
        form.setFieldsValue(data); // Populate form with fetched settings
        setIsDirty(false); // Reset dirty state on successful fetch/load
      },
      onError: (err) => {
        showNotification('error', 'Failed to Load Settings', err.message || 'An error occurred while loading settings.');
      },
      staleTime: 5 * 60 * 1000, // Consider settings stale after 5 minutes
    }
  );

  const { mutate: saveSettingsMutation, isLoading: isSavingSettings } = useMutation(
    (updatedSettings) => updateClientSettings(clientId, updatedSettings),
    {
      onSuccess: () => {
        showNotification('success', 'Settings Saved', 'Client settings updated successfully.');
        setIsDirty(false); // Mark as clean after saving
        queryClient.invalidateQueries(['clientSettings', clientId]); // Invalidate to ensure fresh data if re-fetched elsewhere
      },
      onError: (err) => {
        showNotification('error', 'Save Failed', err.message || 'An error occurred while saving settings.');
      },
    }
  );

  const handleFormChange = () => {
    setIsDirty(true); // Mark form as dirty on any change
  };

  const handleSave = () => {
    form.validateFields()
      .then(values => {
        saveSettingsMutation(values);
      })
      .catch(info => {
        showNotification('error', 'Validation Error', 'Please correct the highlighted fields.');
        console.log('Validate Failed:', info);
      });
  };

  const handleResetToCurrent = () => {
    if (currentSettings) {
      form.setFieldsValue(currentSettings);
      setIsDirty(false);
      showNotification('info', 'Reset', 'Form reset to current saved settings.');
    }
  };

  if (isLoading) {
    return <Spin tip="Loading settings..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (isError) {
    return <Alert message="Error" description={error?.message || "Failed to load settings."} type="error" showIcon />;
  }

  const tabItems = [
    {
      label: 'Discovery & Filtering',
      key: 'discovery',
      children: <DiscoverySettingsTab settings={currentSettings} form={form} />,
    },
    {
      label: 'Scoring & Weights',
      key: 'scoring',
      children: <ScoringWeightsTab settings={currentSettings} form={form} />,
    },
    {
      label: 'AI & Content Generation',
      key: 'ai-content',
      children: <AiContentSettingsTab settings={currentSettings} form={form} />,
    },
    {
      label: 'API Keys', // NEW tab
      key: 'api-keys',
      children: <ApiKeysSettingsTab settings={currentSettings} form={form} />,
    }
  ];

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Title level={2}>Client Settings: {clientId}</Title>
        <Form
          form={form}
          layout="vertical"
          onValuesChange={handleFormChange}
          onFinish={handleSave}
          initialValues={currentSettings} // Set initial values from fetched data
        >
          <Tabs defaultActiveKey="discovery" items={tabItems} style={{ marginBottom: '24px' }} />

          <Space>
            <Button 
              type="primary" 
              htmlType="submit" 
              icon={<SaveOutlined />} 
              loading={isSavingSettings} 
              disabled={!isDirty || isSavingSettings}
            >
              Save Changes
            </Button>
            <Button 
              icon={<ReloadOutlined />} 
              onClick={handleResetToCurrent} 
              disabled={!isDirty || isSavingSettings}
            >
              Reset to Current
            </Button>
          </Space>
        </Form>
      </Content>
    </Layout>
  );
};

export default SettingsPage;
</file>

<file path="client/my-content-app/src/services/apiClient.js">
import axios from 'axios';
import { ClientContext } from '../context/ClientContext'; // Import ClientContext

// Create an Axios instance for API communication
const apiClient = axios.create({
  // The base URL is handled by the Vite proxy, so we can use relative paths like /api
  headers: {
    'Content-Type': 'application/json', // Default content type for requests
  },
  // You could add a timeout here if desired
  // timeout: 10000, 
});

// Below the `axios.create` block, add the request interceptor:
apiClient.interceptors.request.use(
  (config) => {
    // Dynamically get the current client ID from localStorage
    const currentClientId = localStorage.getItem('currentClientId');
    if (currentClientId) {
      config.headers['X-Client-ID'] = currentClientId;
    }
    const token = localStorage.getItem('authToken');
    if (token) {
        config.headers.Authorization = `Bearer ${token}`;
    }
    return config;
  },
  (error) => {
    return Promise.reject(error);
  }
);

// Interceptor to handle successful responses
apiClient.interceptors.response.use(
  (response) => {
    // Axios wraps the actual data in a `data` property. We return just the data.
    return response.data;
  },
  (error) => {
    // Centralized error handling for all API calls
    console.error('API Error:', error.response || error.message);

    // Optionally, you could check for specific status codes (e.g., 401 for unauthorized)
    // and trigger global actions like showing a notification or redirecting to login.
    // if (error.response && error.response.status === 401) {
    //   // handle unauthorized
    // }

    // If the error is an AbortError from AbortController, do not treat it as a critical failure
    if (axios.isCancel(error) || error.code === 'ERR_CANCELED') {
        // Propagate as a cancelled error for specific handling in components
        const cancelledError = new Error('Request was cancelled');
        cancelledError.name = 'CanceledError';
        return Promise.reject(cancelledError);
    }

    return Promise.reject(error); // Re-throw the error for component-specific handling
  }
);

export default apiClient;
</file>

<file path="client/my-content-app/src/services/authService.js">
// This is a new file. Create it with the following content:
import apiClient from './apiClient';

/**
 * Sends login request to the backend.
 * @param {string} password - The user's password.
 * @returns {Promise<object>} A promise that resolves to the login response (user and token).
 */
export const login = (password) => {
  return apiClient.post('/api/auth/login', { password });
};

/**
 * Sends logout request to the backend.
 * @returns {Promise<object>} A promise that resolves to the logout message.
 */
export const logout = () => {
  return apiClient.post('/api/auth/logout');
};
</file>

<file path="client/my-content-app/src/services/clientService.js">
// This is a new file. Create it with the following content:
import apiClient from './apiClient';

/**
 * Fetches a list of all clients.
 * @returns {Promise<Array>} A promise that resolves to an array of client objects.
 */
export const getClients = () => {
  return apiClient.get('/api/clients');
};

/**
 * Fetches dashboard statistics for a specific client.
 * @param {string} clientId - The ID of the client.
 * @returns {Promise<Object>} A promise that resolves to the dashboard stats object.
 */
export const getDashboardStats = (clientId) => {
  return apiClient.get(`/api/clients/${clientId}/dashboard-stats`);
};

/**
 * Adds a new client to the system.
 * @param {Object} clientData - The data for the new client (client_id, client_name).
 * @returns {Promise<Object>} A promise that resolves to the new client's data.
 */
export const addClient = (clientData) => {
  return apiClient.post('/api/clients', clientData);
};


/**
 * Searches across all client-specific assets (opportunities, runs, etc.).
 * @param {string} clientId - The ID of the client.
 * @param {string} query - The search query.
 * @returns {Promise<Array>} A promise that resolves to an array of search results.
 */
export const searchAllAssets = (clientId, query) => {
  return apiClient.get(`/api/clients/${clientId}/search-all-assets?query=${encodeURIComponent(query)}`);
};

/**
 * Fetches aggregated data for the main dashboard.
 * @param {string} clientId - The ID of the client.
 * @returns {Promise<Object>} A promise that resolves to the dashboard data object.
 */
export const getDashboardData = (clientId) => {
  return apiClient.get(`/api/clients/${clientId}/dashboard`);
};
</file>

<file path="client/my-content-app/src/services/clientSettingsService.js">
// my-content-app/src/services/clientSettingsService.js
// NEW FILE
import apiClient from './apiClient';

export const getClientSettings = (clientId) => {
  return apiClient.get(`/api/settings/${clientId}`);
};

export const updateClientSettings = (clientId, settings) => {
  return apiClient.put(`/api/settings/${clientId}`, settings);
};
</file>

<file path="client/my-content-app/src/services/discoveryService.js">
import apiClient from './apiClient';

export const startDiscoveryRun = ({ clientId, runData }) => {
  return apiClient.post(`/api/clients/${clientId}/discovery-runs-async`, runData);
};

export const getDiscoveryRuns = (clientId, page = 1) => {
  return apiClient.get(`/api/clients/${clientId}/discovery-runs`, { params: { page } });
};

export const estimateCost = ({ clientId, seed_keywords, signal }) => {
  return apiClient.post(`/api/discovery/estimate-cost`, { seed_keywords }, { signal });
};

export const preCheckKeywords = ({ clientId, seed_keywords, signal }) => {
  return apiClient.post(`/api/discovery/pre-check`, { seed_keywords }, { signal });
};

export const rerunDiscoveryRun = (runId) => {
  return apiClient.post(`/api/discovery-runs/rerun/${runId}`);
};

export const getKeywordsForRun = (runId) => {
  return apiClient.get(`/api/discovery-runs/${runId}/keywords`);
};

export const getDisqualifiedKeywords = (runId, reason) => {
    return apiClient.get(`/api/discovery-runs/${runId}/keywords/${reason}`);
};

export const getDisqualificationReasons = (runId) => {
  return apiClient.get(`/api/discovery-runs/${runId}/disqualification-reasons`);
};

export const getJobStatus = (jobId) => {
  return apiClient.get(`/api/jobs/${jobId}`);
};

export const getOpportunities = (clientId, { page = 1, limit = 50, status = 'qualified', sort_by = 'strategic_score', sort_direction = 'desc' }) => {
  return apiClient.get(`/api/clients/${clientId}/opportunities`, { 
    params: { page, limit, status, sort_by, sort_direction } 
  });
};

export const getDiscoveryRunById = async (runId) => {
  const response = await apiClient.get(`/discovery/runs/${runId}`);
  return response;
};
</file>

<file path="client/my-content-app/src/services/jobsService.js">
import apiClient from './apiClient';

export const getJobStatus = (jobId) => {
  return apiClient.get(`/api/jobs/${jobId}`);
};
</file>

<file path="client/my-content-app/src/services/opportunitiesService.js">
import apiClient from './apiClient';

export const getOpportunities = (clientId, params) => {
  return apiClient.get(`/api/clients/${clientId}/opportunities/summary`, { params });
};

export const getDashboardStats = (clientId) => {
  return apiClient.get(`/api/clients/${clientId}/dashboard`);
};

export const getOpportunityById = (id) => {
  return apiClient.get(`/api/opportunities/${id}`);
};

export const updateOpportunityStatus = (opportunityId, status) => {
  return apiClient.put(`/api/opportunities/${opportunityId}/status?status=${status}`);
};

export const bulkAction = (action, opportunityIds) => {
  return apiClient.post('/api/opportunities/bulk-action', { action, opportunity_ids: opportunityIds });
};

export const compareOpportunities = (opportunityIds) => {
  return apiClient.post('/api/opportunities/compare', { opportunity_ids: opportunityIds });
};

export const updateOpportunityAiContent = (id, updatedContent) => {
  return apiClient.put(`/api/opportunities/${id}/ai-content`, updatedContent);
};

export const generateImage = (id, prompt) => {
  return apiClient.post(`/api/opportunities/${id}/generate-image`, { prompt });
};

export const updateOpportunityImages = (id, images) => {
  return apiClient.put(`/api/opportunities/${id}/images`, images);
};

export const generateSocialPosts = (id, platforms) => {
  return apiClient.post(`/api/opportunities/${id}/generate-social-posts`, { platforms });
};

export const updateOpportunitySocialPosts = (id, posts) => {
  return apiClient.put(`/api/opportunities/${id}/social-posts`, { social_media_posts: posts });
};

export const getContentHistory = (opportunityId) => {
  return apiClient.get(`/api/opportunities/${opportunityId}/content-history`);
};

export const restoreContentVersion = (opportunityId, versionTimestamp) => {
  return apiClient.post(`/api/opportunities/${opportunityId}/restore-content`, { version_timestamp: versionTimestamp });
};

export const submitContentFeedback = (opportunityId, feedbackData) => {
  return apiClient.post(`/api/opportunities/${opportunityId}/feedback`, feedbackData);
};

export const overrideDisqualification = (opportunityId) => {
  return apiClient.post(`/api/opportunities/${opportunityId}/override-disqualification`);
};

export const updateOpportunityContent = (opportunityId, newContentPayload) => {
  return apiClient.put(`/api/opportunities/${opportunityId}/content`, newContentPayload);
};

export const approveAnalysis = (opportunityId, overrides) => {
  return apiClient.post(`/api/orchestrator/approve-analysis/${opportunityId}`, overrides);
};
</file>

<file path="client/my-content-app/src/services/orchestratorService.js">
import apiClient from './apiClient';

export const runAnalysis = (opportunityId, selectedUrls) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/run-analysis-async`, { selected_competitor_urls: selectedUrls });
};

export const startFullContentGeneration = (opportunityId, modelOverride = null, temperature = null) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/run-generation-async`, { model_override: modelOverride, temperature: temperature });
};

export const approveAnalysis = (opportunityId, overrides = null) => {
  const payload = {
    overrides: {
      additional_instructions: JSON.stringify(overrides),
    },
  };
  return apiClient.post(`/api/orchestrator/approve-analysis/${opportunityId}`, payload);
};

export const refreshContentWorkflow = (opportunityId) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/refresh-content-async`);
};

export const startFullAnalysis = (opportunityId, modelOverride) => {
    return apiClient.post(`/api/orchestrator/run-full-analysis/${opportunityId}`, { model_override: modelOverride });
};

export const getJobStatus = (jobId) => {
    return apiClient.get(`/api/jobs/${jobId}/status`);
};

export const estimateActionCost = (actionType, opportunityId = null, discoveryParams = null) => {
  const url = opportunityId 
    ? `/api/orchestrator/estimate-cost/${opportunityId}` 
    : '/api/orchestrator/estimate-cost';
  
  const payload = {
    action_type: actionType,
    discovery_params: discoveryParams,
  };
  
  return apiClient.post(url, payload);
};

export const getSerpDataLive = (opportunityId) => {
    return apiClient.get(`/api/orchestrator/${opportunityId}/serp-preview`);
};

export const refreshSerpData = (opportunityId) => {
            return apiClient.post(`/api/orchestrator/${opportunityId}/rerun-analysis-async`);
        };
export const getAllJobs = () => {
    return apiClient.get('/api/jobs');
};

export const cancelJob = (jobId) => {
    return apiClient.post(`/api/jobs/${jobId}/cancel`);
};

// Update startFullWorkflow to accept overrideValidation
export const startFullWorkflow = (opportunityId, overrideValidation = false) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/run-full-auto-async`, {
    override_validation: overrideValidation
  });
};

// Add refineContent to call the backend refinement endpoint
export const refineContent = (opportunityId, htmlContent, command) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/refine-content`, {
    html_content: htmlContent,
    command: command
  });
};

export const generateContentOverride = (opportunityId) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/generate-content-override`);
};

// New service function to reject an opportunity
export const rejectOpportunity = (opportunityId) => {
  return apiClient.post(`/api/orchestrator/reject-opportunity/${opportunityId}`);
};

export const updateSocialMediaPostsStatus = (opportunityId, newStatus) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/social-media-status`, { new_status: newStatus });
};

 // Add this new function

 export const startFullAutomationWorkflow = (opportunityId, overrideValidation = false) => {

   return apiClient.post(`/api/orchestrator/${opportunityId}/run-full-automation-async`, {

     override_validation: overrideValidation

   });

 };



 export const getFullPrompt = (opportunityId) => {



   return apiClient.get(`/api/orchestrator/${opportunityId}/full-prompt`);



 };



 



 export const getScoreNarrative = (opportunityId) => {



   return apiClient.get(`/api/orchestrator/${opportunityId}/score-narrative`);



 };
</file>

<file path="client/my-content-app/src/services/settingsService.js">
import apiClient from './apiClient';

export const getDiscoveryStrategies = async () => {
  // This is a placeholder. In a real app, you might fetch this from the backend.
  return ["Keyword Ideas", "Keyword Suggestions", "Related Keywords"];
};

export const getAvailableDiscoveryFilters = async () => {
  return apiClient.get('/api/discovery/available-filters');
};
</file>

<file path="client/my-content-app/src/App.jsx">
import React from 'react';
import { Routes, Route } from 'react-router-dom';
import MainLayout from './components/layout/MainLayout';
import DiscoveryPage from './pages/DiscoveryPage/DiscoveryPage';
import RunDetailsPage from './pages/RunDetailsPage/RunDetailsPage';
import OpportunitiesPage from './pages/OpportunitiesPage/OpportunitiesPage';
import { useAuth } from './context/AuthContext';
import LoginPage from './pages/Auth/LoginPage';
import { Spin } from 'antd'; // For loading state
import DashboardPage from './pages/Dashboard/DashboardPage';
import ClientDashboardPage from './pages/ClientDashboard/ClientDashboardPage';
import OpportunityDetailPage from './pages/opportunity-detail-page/index.jsx';
import ActivityLogPage from './pages/ActivityLog/ActivityLogPage';
import SettingsPage from './pages/Settings/SettingsPage';
import NotFoundPage from './pages/NotFoundPage/NotFoundPage';

import { JobProvider } from './context/JobContext';
import BlogPage from './pages/BlogPage/BlogPage';

// REPLACE the existing `function App() { ... }` block with this:
function App() {
  const { isAuthenticated, loading } = useAuth();

  if (loading) {
    return (
      <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: '100vh' }}>
        <Spin size="large" tip="Loading authentication..." />
      </div>
    );
  }

  return (
    <Routes>
      {isAuthenticated ? (
        <>
          <Route path="/" element={<MainLayout />}>
            <Route index element={<DashboardPage />} />
            <Route path="/dashboard" element={<DashboardPage />} />
            <Route path="/clients" element={<ClientDashboardPage />} />
            <Route path="/opportunities" element={<OpportunitiesPage />} />
            <Route path="/opportunities/:opportunityId" element={<OpportunityDetailPage />} />
            <Route path="/discovery/run/:runId" element={<RunDetailsPage />} />
            <Route path="/activity-log" element={<ActivityLogPage />} />
            <Route path="/settings" element={<SettingsPage />} />
            <Route path="/discovery" element={<DiscoveryPage />} />
            <Route path="*" element={<NotFoundPage />} />
          </Route>
          <Route path="/blog/:opportunityId" element={<BlogPage />} />
        </>
      ) : (
        <>
          <Route path="/login" element={<LoginPage />} />
          <Route path="*" element={<LoginPage />} />
        </>
      )}
    </Routes>
  );
}

export default App;
</file>

<file path="client/my-content-app/src/index.css">
/* Global styles for the application */
body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  background-color: #f0f2f5; /* Light grey background for the app */
}

/* Add specific Ant Design overrides if needed */
/* For example, to make tags slightly smaller globally */
.ant-tag {
  font-size: 12px;
  height: auto;
  line-height: 1.8;
  padding: 0 7px;
}
</file>

<file path="client/my-content-app/src/main.jsx">
import React from 'react';
import ReactDOM from 'react-dom/client';
import { BrowserRouter } from 'react-router-dom';
import { QueryClient, QueryClientProvider } from 'react-query';
import { ClientProvider } from './context/ClientContext';
import { NotificationProvider } from './context/NotificationContext';
import { AuthProvider } from './context/AuthContext';
import App from './App';
import 'antd/dist/reset.css'; // Ant Design's base styles
import './index.css'; // Your global custom styles

import { JobProvider } from './context/JobContext';
import GlobalJobTracker from './components/GlobalJobTracker';

// ... (other imports)

const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      refetchOnWindowFocus: false, // Don't refetch automatically on window focus
      retry: 1, // Retry failed queries once
    },
  },
});

ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <BrowserRouter>
      <QueryClientProvider client={queryClient}>
        <NotificationProvider>
          <AuthProvider>
            <ClientProvider>
              <JobProvider>
                <App />
                <GlobalJobTracker />
              </JobProvider>
            </ClientProvider>
          </AuthProvider>
        </NotificationProvider>
      </QueryClientProvider>
    </BrowserRouter>
  </React.StrictMode>
);
</file>

<file path="client/my-content-app/.dockerignore">
# Ignore dependencies, as they are installed inside the container
node_modules

# Ignore build output
dist
build

# Ignore miscellaneous files
.env
.eslintignore
.eslintrc.json
npm-debug.log
README.md
</file>

<file path="client/my-content-app/.eslintignore">
/node_modules
/dist
/vite.config.js
/.eslintrc.json
</file>

<file path="client/my-content-app/.eslintrc.json">
{
    "env": {
        "browser": true,
        "es2021": true
    },
    "extends": [
        "eslint:recommended",
        "plugin:react/recommended",
        "plugin:react-hooks/recommended"
    ],
    "parserOptions": {
        "ecmaFeatures": {
            "jsx": true
        },
        "ecmaVersion": 12,
        "sourceType": "module"
    },
    "plugins": [
        "react",
        "react-hooks"
    ],
    "rules": {
        "react/prop-types": "off"
    },
    "settings": {
        "react": {
            "version": "detect"
        }
    }
}
</file>

<file path="client/my-content-app/Dockerfile">
# Stage 1: Build the React application
FROM node:18-alpine as build

WORKDIR /app

COPY package.json ./

RUN npm install --legacy-peer-deps

COPY . .

RUN npm run build

# Stage 2: Serve the application with Nginx
FROM nginx:stable-alpine

# Copy the built files from the build stage
COPY --from=build /app/dist /usr/share/nginx/html

# Copy the custom Nginx configuration
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Expose port 80
EXPOSE 80

# Start Nginx
CMD ["nginx", "-g", "daemon off;"]
</file>

<file path="client/my-content-app/index.html">
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + React</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>
</file>

<file path="client/my-content-app/nginx.conf">
server {
  listen 80;

  location / {
    root   /usr/share/nginx/html;
    index  index.html index.htm;
    try_files $uri $uri/ /index.html;
  }

  location /api {
    proxy_pass http://backend:8000;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
  }

  error_page   500 502 503 504  /50x.html;
  location = /50x.html {
    root   /usr/share/nginx/html;
  }
}
</file>

<file path="client/my-content-app/package.json">
{
  "name": "my-content-app",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint . --ext js,jsx --report-unused-disable-directives --max-warnings 0",
    "preview": "vite preview"
  },
  "dependencies": {
    "@ant-design/charts": "^2.6.5",
    "@ant-design/icons": "^5.3.7",
    "antd": "^5.17.4",
    "axios": "^1.7.2",
    "date-fns": "^3.6.0",
    "file-saver": "^2.0.5",
    "jszip": "^3.10.1",
    "react": "^18.2.0",
    "react-diff-viewer": "^3.1.1",
    "react-dom": "^18.2.0",
    "react-query": "^3.39.3",
    "react-quill": "^2.0.0",
    "react-router-dom": "^6.23.1",
    "recharts": "^2.12.7"
  },
  "devDependencies": {
    "@types/react": "^18.2.66",
    "@types/react-dom": "^18.2.22",
    "@vitejs/plugin-react": "^4.2.1",
    "eslint": "^8.57.0",
    "eslint-plugin-react": "^7.34.1",
    "eslint-plugin-react-hooks": "^4.6.0",
    "eslint-plugin-react-refresh": "^0.4.6",
    "vite": "^5.2.0"
  }
}
</file>

<file path="client/my-content-app/vite.config.js">
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react';

// https://vitejs.dev/config/
export default defineConfig({
  plugins: [react()],
  server: {
    port: 3000, // Frontend will run on port 3000
    proxy: {
      // Proxy API requests to the FastAPI backend
      '/api': {
        target: 'http://localhost:8000', // Backend address
        changeOrigin: true,
        rewrite: (path) => path.replace(/^\/api/, ''), // Strip /api prefix
      },
    },
  },
});
</file>

<file path="docs/plan.md">
Of course. This is the logical next step. I will now create the detailed, granular table of contents for the 15-task roadmap.

This document is built by rigorously comparing each task against your actual codebase. Each sub-task represents a single, atomic code change, specifying the exact file, the code to be altered, and the consequences of that change. This is designed to serve as a precise, developer-ready implementation plan.

---

### **Definitive Implementation Plan: Table of Contents**

#### **Tier 1: The "Smart Funnel" Foundation**

---

**Task No:** 1
**Task Higher Overview:** Implement an API-side filter to automatically exclude keywords found in "hostile" SERP environments (e.g., dominated by shopping or local results), saving significant cost and processing time.
**Task Total Sub-tasks:** 3

*   **Sub Task No:** 1.1
*   **Sub Task Overview:** Add a new configuration setting in `settings.ini` to define which SERP features are considered "hostile" to blog content.
*   **Sub Task File:** `backend/app_config/settings.ini`
*   **Sub Task Exact code changes:**
    ```ini
    # In the [DISQUALIFICATION_RULES] section, ADD the following line:
    hostile_serp_features = shopping,local_pack,google_flights,google_hotels,popular_products,local_services,app,jobs
    ```
*   **Sub Task Verification:** After restarting the application, check that the `hostile_serp_features` list is present in the `client_cfg` dictionary used by the orchestrator.
*   **Sub Task Side Effects or Ripple Effects:** None. This is a configuration addition.

*   **Sub Task No:** 1.2
*   **Sub Task Overview:** Modify the discovery workflow to read the new `hostile_serp_features` setting and dynamically construct an API-side filter to exclude them.
*   **Sub Task File:** `backend/pipeline/step_01_discovery/run_discovery.py`
*   **Sub Task Exact code changes:**
    ```python
    # Inside the `run_discovery_phase` function, BEFORE the `expander.expand_seed_keyword` call:

    # ADD THIS BLOCK to dynamically add the hostile SERP filter
    hostile_features = client_cfg.get("hostile_serp_features", [])
    if hostile_features:
        hostile_filter = ["serp_info.serp_item_types", "not_in", hostile_features]
        
        # Ensure filters are initialized as a list if None
        if filters is None:
            filters = []
        
        # Add the hostile filter and an 'and' operator if other filters exist
        if filters:
            filters.append("and")
        filters.append(hostile_filter)
        
        logger.info(f"Applying API-side hostile SERP filter for: {hostile_features}")

    # The `filters` variable passed to the expander will now contain the new rule.
    expansion_result = expander.expand_seed_keyword(...) 
    ```
*   **Sub Task Verification:** Run a discovery for a keyword known to have shopping results (e.g., "best running shoes"). Check the application logs to confirm the "Applying API-side hostile SERP filter" message appears. Verify the final list of discovered keywords does not contain keywords dominated by shopping results.
*   **Sub Task Side Effects or Ripple Effects:** This will reduce the number of keywords returned by the API, lowering `total_raw_count` and subsequent processing load. This is the intended effect.

*   **Sub Task No:** 1.3
*   **Sub Task Overview:** Remove the now-redundant Python-side hostile SERP check from the disqualification rules.
*   **Sub Task File:** `backend/pipeline/step_01_discovery/disqualification_rules.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `apply_disqualification_rules` function, REMOVE the following block:

    # REMOVE THIS >>>
    # Rule: Check for hostile SERP environment
    is_hostile, hostile_reason = _check_hostile_serp_environment(opportunity)
    if is_hostile:
        return True, hostile_reason, True
    # <<< REMOVE THIS
    ```    ```python
    # At the bottom of the file, REMOVE the entire helper function:

    # REMOVE THIS >>>
    def _check_hostile_serp_environment(opportunity: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
        # ... entire function body ...
    # <<< REMOVE THIS
    ```
*   **Sub Task Verification:** Verify that the "Rule 16: SERP is hostile to blog content" reason no longer appears in discovery run summaries, as this filtering is now done at the API level.
*   **Sub Task Side Effects or Ripple Effects:** Simplifies the Python code and makes the discovery process slightly faster by removing a redundant check.

---

**Task No:** 2
**Task Higher Overview:** Implement an AI-powered assessment of competitor content quality using only the titles and descriptions available from the SERP API call, replacing the expensive On-Page analysis.
**Task Total Sub-tasks:** 2

*   **Sub Task No:** 2.1
*   **Sub Task Overview:** Create a new method in the `ContentAnalyzer` to build a prompt and call the AI for a SERP-only competitive quality assessment.
*   **Sub Task File:** `backend/pipeline/step_04_analysis/content_analyzer.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `ContentAnalyzer` class, ADD a new method:

    def assess_competitor_quality_from_serp(self, keyword: str, serp_overview: Dict[str, Any]) -> Tuple[Dict[str, Any], float]:
        """
        Uses an AI call to assess competitor quality based only on SERP titles and descriptions.
        """
        self.logger.info(f"Running SERP-only competitor quality assessment for '{keyword}'.")
        
        top_results = serp_overview.get("top_organic_results", [])[:5]
        if not top_results:
            return {"assessment": "No organic results to analyze.", "weaknesses": []}, 0.0

        competitor_snippets = "\\n".join([f"- Title: {r.get('title')}\\n  Description: {r.get('description')}" for r in top_results])

        prompt = f"""
        You are an expert SEO analyst. Based *only* on the following SERP titles and descriptions for the keyword "{keyword}", assess the overall quality and likely weaknesses of the competition.

        Competitor Snippets:
        {competitor_snippets}

        Your Task:
        1.  **Assessment:** Write a one-sentence summary of the competitive landscape. (e.g., "Competition appears to be mostly low-quality, list-based articles.")
        2.  **Weaknesses:** List 2-3 specific, actionable weaknesses you can infer from these snippets. (e.g., "Content seems outdated," "Titles are not compelling," "Descriptions lack depth.")

        Return a JSON object with two keys: "assessment" and "weaknesses".
        """

        schema = {
            "name": "assess_competitor_quality",
            "type": "object",
            "properties": {
                "assessment": {"type": "string"},
                "weaknesses": {"type": "array", "items": {"type": "string"}},
            },
            "required": ["assessment", "weaknesses"],
        }

        response, error = self.openai_client.call_chat_completion(
            messages=[{"role": "user", "content": prompt}],
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
        )
        cost = self.openai_client.latest_cost

        if error or not response:
            self.logger.error(f"Failed to get AI competitor assessment: {error}")
            return {"assessment": "AI analysis failed.", "weaknesses": []}, cost

        return response, cost
    ```
*   **Sub Task Verification:** This is a new internal method. Verification will happen in the next step when it's integrated.
*   **Sub Task Side Effects or Ripple Effects:** Adds a new AI-powered capability to the system.

*   **Sub Task No:** 2.2
*   **Sub Task Overview:** Integrate the new AI assessment method into the main analysis orchestrator, replacing the On-Page analysis call.
*   **Sub Task File:** `backend/pipeline/orchestrator/analysis_orchestrator.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `run_analysis_phase` method:

    # REMOVE or comment out the entire "Conditional Competitor OnPage Analysis" block:
    # REMOVE THIS >>>
    # competitor_analysis = []
    # competitor_api_cost = 0.0
    # if self.client_cfg.get("enable_deep_competitor_analysis", False):
    #     # ... entire block ...
    #     competitor_analysis, competitor_api_cost = competitor_analyzer.analyze_competitors(...)
    # total_api_cost += competitor_api_cost
    # <<< REMOVE THIS

    # ADD THIS NEW BLOCK in its place:
    self.logger.info(f"Running SERP-only AI competitor assessment for '{keyword}'.")
    from pipeline.step_04_analysis.content_analyzer import ContentAnalyzer
    content_analyzer = ContentAnalyzer(self.openai_client, self.client_cfg)
    
    ai_competitor_assessment, assessment_cost = content_analyzer.assess_competitor_quality_from_serp(keyword, live_serp_data)
    total_api_cost += assessment_cost
    
    # Store this new data in the analysis_data dictionary
    analysis_data = {
        "serp_overview": live_serp_data,
        "competitor_analysis": [], # This is now always empty
        "ai_competitor_assessment": ai_competitor_assessment, # Store the new AI assessment
        "content_intelligence": content_intelligence,
        "recommended_strategy": recommended_strategy,
    }
    # ... continue with blueprint factory ...
    ```
*   **Sub Task Verification:** Run a full analysis on an opportunity. Verify that no On-Page API calls are made (check logs). Check the final `blueprint.json` file to ensure it contains the new `ai_competitor_assessment` object.
*   **Sub Task Side Effects or Ripple Effects:** This is a major strategic change. The `competitor_analysis` object will now always be empty. Any downstream logic that relies on it (like the `competitor_performance` scoring component) will be affected and needs to be disabled or refactored.

... and so on for the remaining 13 tasks. I will now generate the complete, detailed table for all 15 tasks.

---
Here is the complete, detailed Table of Contents for the 15-task roadmap.

### **Tier 1: The "Smart Funnel" Foundation**

---
**Task No:** 1
**Task Higher Overview:** Implement an API-side filter to automatically exclude keywords found in "hostile" SERP environments (e.g., dominated by shopping or local results), saving significant cost and processing time.
**Task Total Sub-tasks:** 3

*   **Sub Task No:** 1.1
*   **Sub Task Overview:** Add a new configuration setting in `settings.ini` to define which SERP features are considered "hostile" to blog content.
*   **Sub Task File:** `backend/app_config/settings.ini`
*   **Sub Task Exact code changes:**
    ```ini
    # In the [DISQUALIFICATION_RULES] section, ADD the following line:
    hostile_serp_features = shopping,local_pack,google_flights,google_hotels,popular_products,local_services,app,jobs
    ```
*   **Sub Task Verification:** After restarting the application, check that the `hostile_serp_features` list is present in the `client_cfg` dictionary used by the orchestrator.
*   **Sub Task Side Effects or Ripple Effects:** None. This is a configuration addition.

*   **Sub Task No:** 1.2
*   **Sub Task Overview:** Modify the discovery workflow to read the new `hostile_serp_features` setting and dynamically construct an API-side filter to exclude them.
*   **Sub Task File:** `backend/pipeline/step_01_discovery/run_discovery.py`
*   **Sub Task Exact code changes:**
    ```python
    # Inside the `run_discovery_phase` function, BEFORE the `expander.expand_seed_keyword` call:

    # ADD THIS BLOCK to dynamically add the hostile SERP filter
    hostile_features = client_cfg.get("hostile_serp_features", [])
    if hostile_features:
        hostile_filter = ["serp_info.serp_item_types", "not_in", hostile_features]
        
        # Ensure filters are initialized as a list if None
        if filters is None:
            filters = []
        
        # Add the hostile filter and an 'and' operator if other filters exist
        if filters:
            filters.append("and")
        filters.append(hostile_filter)
        
        logger.info(f"Applying API-side hostile SERP filter for: {hostile_features}")

    # The `filters` variable passed to the expander will now contain the new rule.
    expansion_result = expander.expand_seed_keyword(...) 
    ```
*   **Sub Task Verification:** Run a discovery for a keyword known to have shopping results (e.g., "best running shoes"). Check the application logs to confirm the "Applying API-side hostile SERP filter" message appears. Verify the final list of discovered keywords does not contain keywords dominated by shopping results.
*   **Sub Task Side Effects or Ripple Effects:** This will reduce the number of keywords returned by the API, lowering `total_raw_count` and subsequent processing load. This is the intended effect.

*   **Sub Task No:** 1.3
*   **Sub Task Overview:** Remove the now-redundant Python-side hostile SERP check from the disqualification rules.
*   **Sub Task File:** `backend/pipeline/step_01_discovery/disqualification_rules.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `apply_disqualification_rules` function, REMOVE the following block:
    # REMOVE THIS >>>
    # Rule: Check for hostile SERP environment
    is_hostile, hostile_reason = _check_hostile_serp_environment(opportunity)
    if is_hostile:
        return True, hostile_reason, True
    # <<< REMOVE THIS
    ```
    ```python
    # At the bottom of the file, REMOVE the entire helper function:
    # REMOVE THIS >>>
    def _check_hostile_serp_environment(opportunity: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
        # ... entire function body ...
    # <<< REMOVE THIS
    ```*   **Sub Task Verification:** Verify that the "Rule 16: SERP is hostile to blog content" reason no longer appears in discovery run summaries, as this filtering is now done at the API level.
*   **Sub Task Side Effects or Ripple Effects:** Simplifies the Python code and makes the discovery process slightly faster by removing a redundant check.

---
**Task No:** 2
**Task Higher Overview:** Implement an AI-powered assessment of competitor content quality using only the titles and descriptions available from the SERP API call, replacing the expensive On-Page analysis.
**Task Total Sub-tasks:** 2

*   **Sub Task No:** 2.1
*   **Sub Task Overview:** Create a new method in the `ContentAnalyzer` to build a prompt and call the AI for a SERP-only competitive quality assessment.
*   **Sub Task File:** `backend/pipeline/step_04_analysis/content_analyzer.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `ContentAnalyzer` class, ADD a new method:

    def assess_competitor_quality_from_serp(self, keyword: str, serp_overview: Dict[str, Any]) -> Tuple[Dict[str, Any], float]:
        """
        Uses an AI call to assess competitor quality based only on SERP titles and descriptions.
        """
        self.logger.info(f"Running SERP-only competitor quality assessment for '{keyword}'.")
        
        top_results = serp_overview.get("top_organic_results", [])[:5]
        if not top_results:
            return {"assessment": "No organic results to analyze.", "weaknesses": []}, 0.0

        competitor_snippets = "\\n".join([f"- Title: {r.get('title')}\\n  Description: {r.get('description')}" for r in top_results])

        prompt = f"""
        You are an expert SEO analyst. Based *only* on the following SERP titles and descriptions for the keyword "{keyword}", assess the overall quality and likely weaknesses of the competition.

        Competitor Snippets:
        {competitor_snippets}

        Your Task:
        1.  **Assessment:** Write a one-sentence summary of the competitive landscape. (e.g., "Competition appears to be mostly low-quality, list-based articles.")
        2.  **Weaknesses:** List 2-3 specific, actionable weaknesses you can infer from these snippets. (e.g., "Content seems outdated," "Titles are not compelling," "Descriptions lack depth.")

        Return a JSON object with two keys: "assessment" and "weaknesses".
        """

        schema = {
            "name": "assess_competitor_quality",
            "type": "object",
            "properties": {
                "assessment": {"type": "string"},
                "weaknesses": {"type": "array", "items": {"type": "string"}},
            },
            "required": ["assessment", "weaknesses"],
        }

        response, error = self.openai_client.call_chat_completion(
            messages=[{"role": "user", "content": prompt}],
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
        )
        cost = self.openai_client.latest_cost

        if error or not response:
            self.logger.error(f"Failed to get AI competitor assessment: {error}")
            return {"assessment": "AI analysis failed.", "weaknesses": []}, cost

        return response, cost
    ```
*   **Sub Task Verification:** This is a new internal method. Verification will happen in the next step when it's integrated.
*   **Sub Task Side Effects or Ripple Effects:** Adds a new AI-powered capability to the system.

*   **Sub Task No:** 2.2
*   **Sub Task Overview:** Integrate the new AI assessment method into the main analysis orchestrator, replacing the On-Page analysis call.
*   **Sub Task File:** `backend/pipeline/orchestrator/analysis_orchestrator.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `run_analysis_phase` method:

    # REMOVE or comment out the entire "Conditional Competitor OnPage Analysis" block:
    # REMOVE THIS >>>
    # competitor_analysis = []
    # competitor_api_cost = 0.0
    # if self.client_cfg.get("enable_deep_competitor_analysis", False):
    #     # ... entire block ...
    #     competitor_analysis, competitor_api_cost = competitor_analyzer.analyze_competitors(...)
    # total_api_cost += competitor_api_cost
    # <<< REMOVE THIS

    # ADD THIS NEW BLOCK in its place:
    self.logger.info(f"Running SERP-only AI competitor assessment for '{keyword}'.")
    from pipeline.step_04_analysis.content_analyzer import ContentAnalyzer
    content_analyzer = ContentAnalyzer(self.openai_client, self.client_cfg)
    
    ai_competitor_assessment, assessment_cost = content_analyzer.assess_competitor_quality_from_serp(keyword, live_serp_data)
    total_api_cost += assessment_cost
    
    # Store this new data in the analysis_data dictionary
    analysis_data = {
        "serp_overview": live_serp_data,
        "competitor_analysis": [], # This is now always empty
        "ai_competitor_assessment": ai_competitor_assessment, # Store the new AI assessment
        "content_intelligence": content_intelligence,
        "recommended_strategy": recommended_strategy,
    }
    # ... continue with blueprint factory ...
    ```
*   **Sub Task Verification:** Run a full analysis on an opportunity. Verify that no On-Page API calls are made (check logs). Check the final `blueprint.json` file to ensure it contains the new `ai_competitor_assessment` object.
*   **Sub Task Side Effects or Ripple Effects:** This is a major strategic change. The `competitor_analysis` object will now always be empty. Any downstream logic that relies on it (like the `competitor_performance` scoring component) will be affected and needs to be disabled or refactored.

---
**Task No:** 3
**Task Higher Overview:** Implement an AI-powered "inferred" competitor outline using only SERP data to provide a content gap analysis tool without expensive On-Page calls.
**Task Total Sub-tasks:** 2

*   **Sub Task No:** 3.1
*   **Sub Task Overview:** Create a new method in `ContentAnalyzer` to generate an inferred "master outline" from SERP titles and descriptions.
*   **Sub Task File:** `backend/pipeline/step_04_analysis/content_analyzer.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `ContentAnalyzer` class, ADD a new method:

    def infer_competitor_outline_from_serp(self, keyword: str, serp_overview: Dict[str, Any]) -> Tuple[Dict[str, Any], float]:
        """
        Uses an AI call to infer a common outline from SERP titles and descriptions.
        """
        self.logger.info(f"Inferring competitor outline from SERP for '{keyword}'.")
        
        top_results = serp_overview.get("top_organic_results", [])[:7] # Use top 7 for broader context
        if not top_results:
            return {"inferred_outline": []}, 0.0

        competitor_snippets = "\\n".join([f"- Title: {r.get('title')}\\n  Description: {r.get('description')}" for r in top_results])

        prompt = f"""
        You are an SEO analyst. Your task is to reverse-engineer the likely content structure of the top-ranking articles for the keyword "{keyword}" using only their SERP titles and descriptions.

        SERP Snippets:
        {competitor_snippets}

        Your Task:
        Synthesize these snippets to generate a single, logical "master outline" of H2 and H3 headings that a comprehensive article on this topic should cover. Group related concepts.

        Return a JSON object with a single key "inferred_outline", which is an array of strings representing the headings.
        """

        schema = {
            "name": "infer_competitor_outline",
            "type": "object",
            "properties": {
                "inferred_outline": {"type": "array", "items": {"type": "string"}},
            },
            "required": ["inferred_outline"],
        }

        response, error = self.openai_client.call_chat_completion(
            messages=[{"role": "user", "content": prompt}],
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
        )
        cost = self.openai_client.latest_cost

        if error or not response:
            self.logger.error(f"Failed to get AI-inferred outline: {error}")
            return {"inferred_outline": []}, cost

        return response, cost
    ```
*   **Sub Task Verification:** Internal method; verified upon integration.
*   **Sub Task Side Effects or Ripple Effects:** None.

*   **Sub Task No:** 3.2
*   **Sub Task Overview:** Integrate the inferred outline into the `analysis_orchestrator` and save it to the blueprint.
*   **Sub Task File:** `backend/pipeline/orchestrator/analysis_orchestrator.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `run_analysis_phase` method, after the `ai_competitor_assessment` block:
    
    # ADD THIS NEW BLOCK
    inferred_outline, inferred_outline_cost = content_analyzer.infer_competitor_outline_from_serp(keyword, live_serp_data)
    total_api_cost += inferred_outline_cost
    
    # In the `analysis_data` dictionary, add the new key:
    analysis_data = {
        "serp_overview": live_serp_data,
        "competitor_analysis": [],
        "ai_competitor_assessment": ai_competitor_assessment,
        "inferred_competitor_outline": inferred_outline.get("inferred_outline", []), # Add this line
        "content_intelligence": content_intelligence,
        "recommended_strategy": recommended_strategy,
    }
    ```
*   **Sub Task Verification:** Run a full analysis. Check the final blueprint JSON for the new `inferred_competitor_outline` array.
*   **Sub Task Side Effects or Ripple Effects:** Adds a new data point to the blueprint, which can now be used by the frontend UI to display a content gap analysis.

---

**Task No:** 4
**Task Higher Overview:** Auto-determine the correct `content_format` (e.g., "Review Article") based on structured data (schema) found in the SERP, making the strategic recommendation more accurate.
**Task Total Sub-tasks:** 2

*   **Sub Task No:** 4.1
*   **Sub Task Overview:** Enhance the `FullSerpAnalyzer` to parse and count the `@type` from the `microdata` object in each organic result.
*   **Sub Task File:** `backend/core/serp_analyzer.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `FullSerpAnalyzer.analyze_serp` method, add a new key to the initial `analysis` dictionary:
    analysis = {
        # ... existing keys ...
        "microdata_type_counts": {}, # ADD THIS LINE
    }

    # Inside the `for item in serp_results.get("items") or []:` loop, within the `if item_type == "organic":` block:
    # ADD THIS BLOCK to process microdata
    if item.get("microdata"):
        for microdata_item in item["microdata"].get("items", []):
            item_type = microdata_item.get("@type")
            if item_type:
                # Simplify type (e.g., "Review" from ["Thing", "CreativeWork", "Review"])
                simple_type = item_type[-1] if isinstance(item_type, list) else item_type
                analysis["microdata_type_counts"][simple_type] = analysis["microdata_type_counts"].get(simple_type, 0) + 1
    ```
*   **Sub Task Verification:** Run an analysis on a keyword known for reviews (e.g., "best air fryer"). Inspect the `serp_overview` object in the final blueprint and verify that `microdata_type_counts` contains entries like `{"Review": 4}`.
*   **Sub Task Side Effects or Ripple Effects:** Adds a new, valuable data point to the `serp_overview` object for strategic use.

*   **Sub Task No:** 4.2
*   **Sub Task Overview:** Update the `StrategicDecisionEngine` to use the new `microdata_type_counts` to programmatically set the `content_format`.
*   **Sub Task File:** `backend/pipeline/step_05_strategy/decision_engine.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `determine_strategy` method, at the beginning:
    
    # ADD THIS BLOCK to override content_format based on microdata
    microdata_counts = serp_overview.get("microdata_type_counts", {})
    top_results_count = len(serp_overview.get("top_organic_results", [])[:5])

    if microdata_counts.get("Review", 0) >= top_results_count / 2: # If 50%+ of top results are reviews
        content_format = "Review Article"
        strategic_goal = "Produce an authoritative review that leverages social proof and rich snippets, as the SERP is dominated by review formats."
    elif microdata_counts.get("FAQPage", 0) >= top_results_count / 2:
        content_format = "FAQ Article"
        strategic_goal = "Create a comprehensive FAQ page that directly answers user questions to target rich snippets."
    elif microdata_counts.get("Recipe", 0) >= 1:
        content_format = "Recipe Article"
        strategic_goal = "Provide a high-quality recipe with structured data to capture the recipe carousel."
    # ... keep existing logic as a fallback
    ```
*   **Sub Task Verification:** Run an analysis on a review-heavy keyword. Verify that the `recommended_strategy.content_format` in the final blueprint is correctly set to "Review Article".
*   **Sub Task Side Effects or Ripple Effects:** Makes the system's strategic recommendations significantly more accurate and data-driven.

---
**Task No:** 5
**Task Higher Overview:** Implement incremental and failure-aware API cost tracking to ensure all API expenditures are accurately recorded, even for partially completed jobs.
**Task Total Sub-tasks:** 2

*   **Sub Task No:** 5.1
*   **Sub Task Overview:** Create a new method in `DatabaseManager` to atomically increment the `total_api_cost` for an opportunity.
*   **Sub Task File:** `backend/data_access/database_manager.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `DatabaseManager` class, ADD a new method:

    def increment_opportunity_cost(self, opportunity_id: int, cost_to_add: float):
        """Atomically adds a cost value to the opportunity's total_api_cost."""
        if cost_to_add > 0:
            self.logger.info(f"Adding ${cost_to_add:.4f} to total cost for opportunity {opportunity_id}.")
            conn = self._get_conn()
            with conn:
                conn.execute(
                    "UPDATE opportunities SET total_api_cost = total_api_cost + ? WHERE id = ?",
                    (cost_to_add, opportunity_id),
                )
    ```
*   **Sub Task Verification:** Internal method, verified upon integration.
*   **Sub Task Side Effects or Ripple Effects:** None.

*   **Sub Task No:** 5.2
*   **Sub Task Overview:** Refactor all orchestrator methods to call `increment_opportunity_cost` immediately after every single API call that incurs a cost.
*   **Sub Task File:** `backend/pipeline/orchestrator/analysis_orchestrator.py`, `content_orchestrator.py`, etc.
*   **Sub Task Exact code changes (Example):**
    ```python
    # In `analysis_orchestrator.run_analysis_phase`, MODIFY the API call blocks:

    # --- Before ---
    # live_serp_data, serp_api_cost = serp_analyzer.analyze_serp(keyword)
    # total_api_cost += serp_api_cost

    # --- After ---
    live_serp_data, serp_api_cost = serp_analyzer.analyze_serp(keyword)
    self.db_manager.increment_opportunity_cost(opportunity_id, serp_api_cost) # ADD THIS LINE
    # total_api_cost += serp_api_cost # REMOVE local tracking variable

    # Repeat this pattern for every API call in every orchestrator file.
    # The final cost will be read from the database at the end of the job.
    ```*   **Sub Task Verification:** Intentionally cause a workflow to fail midway (e.g., by providing invalid credentials for a later API). Check the database and verify that the `total_api_cost` for that opportunity reflects the cost of the calls that succeeded before the failure.
*   **Sub Task Side Effects or Ripple Effects:** This will increase the number of database writes but will provide far more accurate cost tracking. Local `total_api_cost` variables should be removed from orchestrator methods.

---
... and so on for the remaining 10 tasks. I will now generate the complete, detailed table for all 15 tasks.

### **Tier 2: The "Intelligent Blueprint" Engine (Continued)**

---
**Task No:** 6
**Task Higher Overview:** Inject Google's own knowledge sources (Knowledge Graph source URL and "About this Result" terms) directly into the AI prompt to boost E-E-A-T and topical relevance.
**Task Total Sub-tasks:** 2

*   **Sub Task No:** 6.1
*   **Sub Task Overview:** Enhance the `FullSerpAnalyzer` to extract the `source.url` from the `knowledge_graph` item.
*   **Sub Task File:** `backend/core/serp_analyzer.py`
*   **Sub Task Exact code changes:**
    ```python
    # In `FullSerpAnalyzer.analyze_serp`, find the `elif item_type == "knowledge_graph":` block.
    # ADD the following lines inside this block:

    analysis["knowledge_graph_data"] = {
        "title": item.get("title"),
        "description": item.get("description"),
        "url": item.get("url"),
        "image_url": item.get("image_url"),
        "source_url": item.get("source", {}).get("url") # ADD THIS LINE
    }
    ```
*   **Sub Task Verification:** Run an analysis for a keyword with a prominent Knowledge Graph (e.g., "Albert Einstein"). Inspect the `serp_overview.knowledge_graph_data` object in the final blueprint and verify that `source_url` contains the URL to the Wikipedia page.
*   **Sub Task Side Effects or Ripple Effects:** Adds a new data point to the `knowledge_graph_data` object.

*   **Sub Task No:** 6.2
*   **Sub Task Overview:** Update the `BriefAssembler` and `DynamicPromptAssembler` to inject the KG source URL and the existing "About This Result" terms into the final AI prompt.
*   **Sub Task File:** `backend/agents/brief_assembler.py` and `backend/agents/prompt_assembler.py`
*   **Sub Task Exact code changes:**
    1.  **In `backend/agents/brief_assembler.py`:**
        ```python
        # In the `assemble_brief` method, inside the `brief` dictionary, add these new keys:
        brief = {
            # ... existing keys ...
            "knowledge_graph_source_url": serp_overview.get("knowledge_graph_data", {}).get("source_url"), # ADD THIS
            "google_understanding_terms": list(set( (serp_overview.get("google_understanding_search_terms") or []) + (serp_overview.get("google_understanding_related_terms") or []) )), # ADD THIS
        }
        ```
    2.  **In `backend/agents/prompt_assembler.py`:**
        ```python
        # In the `build_prompt` method, add logic to inject these new fields into `base_instructions`:

        if brief.get("knowledge_graph_source_url"):
            base_instructions += f"\\n- **Authoritative Source:** Reference or cite this Google-trusted source: {brief['knowledge_graph_source_url']}"

        if brief.get("google_understanding_terms"):
            terms_str = ", ".join(brief["google_understanding_terms"])
            base_instructions += f"\\n- **Topical Relevance:** Google explicitly associates this topic with the following terms. Ensure they are naturally integrated: {terms_str}"
        ```
*   **Sub Task Verification:** Generate a full prompt for an opportunity (`/orchestrator/{id}/full-prompt` endpoint). Verify that the prompt text contains the new "Authoritative Source" and "Topical Relevance" instructions.
*   **Sub Task Side Effects or Ripple Effects:** The AI prompt will be more detailed, leading to more factually aligned and topically complete content.

---
**Task No:** 7
**Task Higher Overview:** Enhance the rule-based `SummaryGenerator` by having it make a final, cheap AI call to generate a more nuanced and human-readable narrative explaining the keyword's strategic value.
**Task Total Sub-tasks:** 2

*   **Sub Task No:** 7.1
*   **Sub Task Overview:** Modify the `SummaryGenerator` to include a new AI-powered method.
*   **Sub Task File:** `backend/agents/summary_generator.py`
*   **Sub Task Exact code changes:**
    ```python
    # ADD imports and modify the class
    from backend.external_apis.openai_client import OpenAIClientWrapper

    class SummaryGenerator:
        def __init__(self, openai_client: OpenAIClientWrapper): # MODIFY constructor
            self.logger = logging.getLogger(self.__class__.__name__)
            self.openai_client = openai_client # ADD this line

        def generate_ai_summary(self, score_breakdown: Dict[str, Any]) -> Tuple[str, float]:
            if not score_breakdown:
                return "No scoring data available to generate a summary.", 0.0

            summary_data = {
                factor: f"{data['name']}: {data['score']}/100" 
                for factor, data in score_breakdown.items()
            }

            prompt = f"""
            You are an expert SEO strategist. Given the following scoring breakdown for a keyword, write a concise, 2-3 sentence executive summary explaining the strategic value. Focus on the most important factors.

            Scoring Data:
            {json.dumps(summary_data, indent=2)}

            Example: "This is a strong opportunity due to its high traffic potential and clear competitor weaknesses. However, be aware of the crowded SERP environment, which will require a compelling title to capture clicks."

            Return a JSON object with a single key "summary".
            """
            
            schema = {"name": "generate_summary", "type": "object", "properties": {"summary": {"type": "string"}}, "required": ["summary"]}

            response, error = self.openai_client.call_chat_completion(
                messages=[{"role": "user", "content": prompt}],
                schema=schema, model="gpt-5-nano"
            )
            cost = self.openai_client.latest_cost

            if error or not response:
                return "AI summary generation failed.", cost
            
            return response.get("summary", "AI summary generation failed."), cost
    ```
*   **Sub Task Verification:** Internal method, verified upon integration.
*   **Sub Task Side Effects or Ripple Effects:** Introduces a dependency on `OpenAIClientWrapper` for the `SummaryGenerator`.

*   **Sub Task No:** 7.2
*   **Sub Task Overview:** Integrate the new AI summary generation into the `analysis_orchestrator`.
*   **Sub Task File:** `backend/pipeline/orchestrator/analysis_orchestrator.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `run_analysis_phase` method, AFTER the `scoring_engine.calculate_score` call:
    
    # ADD THIS BLOCK
    from agents.summary_generator import SummaryGenerator
    summary_generator = SummaryGenerator(self.openai_client)
    ai_summary, summary_cost = summary_generator.generate_ai_summary(final_score_breakdown)
    total_api_cost += summary_cost

    # In the `blueprint` creation call, add the new summary:
    blueprint = self.blueprint_factory.create_blueprint(
        # ... existing args ...
    )
    blueprint["executive_summary"] = ai_summary # OVERWRITE the placeholder
    ```
*   **Sub Task Verification:** Run a full analysis. Verify that the `executive_summary` field in the final blueprint contains a human-like narrative instead of the old placeholder text.
*   **Sub Task Side Effects or Ripple Effects:** Adds a small AI cost to the analysis phase but significantly improves the actionability of the blueprint for the user.

---
**Task No:** 8
**Task Higher Overview:** Add a configuration setting to allow the user to completely disable the `InternalLinkingSuggester` to save on API costs, as per your request to de-prioritize this feature.
**Task Total Sub-tasks:** 2

*   **Sub Task No:** 8.1
*   **Sub Task Overview:** Add a boolean setting to `settings.ini` to control the internal linking feature.
*   **Sub Task File:** `backend/app_config/settings.ini`
*   **Sub Task Exact code changes:**
    ```ini
    # In the [APP_SETTINGS] section, MODIFY the existing line:
    enable_automated_internal_linking = false
    ```
*   **Sub Task Verification:** Ensure the application correctly loads this setting into the `client_cfg`.
*   **Sub Task Side Effects or Ripple Effects:** None.

*   **Sub Task No:** 8.2
*   **Sub Task Overview:** In the `BlueprintFactory`, check the new setting before calling the `InternalLinkingSuggester`.
*   **Sub Task File:** `backend/core/blueprint_factory.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `create_blueprint` method, WRAP the internal linking logic in a conditional:
    
    # ADD THIS CONDITIONAL WRAPPER >>>
    if self.client_cfg.get("enable_automated_internal_linking", False):
        brief_text_for_linking = json.dumps(blueprint_data["ai_content_brief"])
        target_domain = self.client_cfg.get("target_domain")
        key_entities = blueprint_data.get("ai_content_brief", {}).get("key_entities_to_mention", [])

        if brief_text_for_linking and target_domain:
            suggestions, linking_cost = self.internal_linking_suggester.suggest_links(
                brief_text_for_linking, key_entities, target_domain, client_id
            )
            blueprint_data["internal_linking_suggestions"] = suggestions
            blueprint_data["metadata"]["total_api_cost"] = round(
                blueprint_data["metadata"]["total_api_cost"] + linking_cost, 4
            )
    else: # ADD THIS ELSE BLOCK
        blueprint_data["internal_linking_suggestions"] = []
        self.logger.info("Internal linking suggestion is disabled by client configuration.")
    # <<< END CONDITIONAL
    ```
*   **Sub Task Verification:** Set `enable_automated_internal_linking = false`. Run a full analysis and verify from the logs that the internal linking step is skipped. Check that the final blueprint contains an empty `internal_linking_suggestions` array.
*   **Sub Task Side Effects or Ripple Effects:** Provides a direct way to control API costs for a non-critical feature.

---

### **Tier 3: The "Auto-Polish & Finalization" Workflow**

---
**Task No:** 9
**Task Higher Overview:** Implement a prompt template management system to enable specialized agents for different content formats, ensuring higher quality and more structured output.
**Task Total Sub-tasks:** 2

*   **Sub Task No:** 9.1
*   **Sub Task Overview:** Create a UI in the Settings page for managing multiple prompt templates.
*   **Sub Task File:** `client/my-content-app/src/pages/Settings/tabs/AiContentSettingsTab.jsx`
*   **Sub Task Exact code changes:** This involves significant frontend work. The current single `PromptTemplateEditor` needs to be wrapped in a stateful component that allows creating, selecting, and saving multiple named templates (e.g., `default_prompt`, `review_article_prompt`, `how_to_guide_prompt`).
*   **Sub Task Verification:** The UI should allow a user to create a new template named "Review Article Prompt," edit its content, save it, and see it in a list of available templates.
*   **Sub Task Side Effects or Ripple Effects:** Requires new API endpoints to save/load/delete these templates from the database.

*   **Sub Task No:** 9.2
*   **Sub Task Overview:** Modify the `DynamicPromptAssembler` to select the appropriate prompt template based on the blueprint's `content_format`.
*   **Sub Task File:** `backend/agents/prompt_assembler.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `build_prompt` method, CHANGE how the template is selected:

    # --- Before ---
    # template = client_cfg.get("custom_prompt_template")
    
    # --- After ---
    content_format = strategy.get('content_format', 'Comprehensive Article')
    format_key = content_format.lower().replace(' ', '_') + '_prompt' # e.g., 'review_article_prompt'
    
    # Look for a specific template, fall back to the custom one, then to a hardcoded default
    template = client_cfg.get(format_key)
    if not template:
        template = client_cfg.get("custom_prompt_template")

    if not template or not template.strip():
        # ... existing hardcoded default template ...
    ```
*   **Sub Task Verification:** Create a custom prompt for "Review Article" in the new UI. Run an analysis on a keyword that results in a "Review Article" `content_format`. Verify that the full prompt generated for content creation uses the custom review template, not the default one.
*   **Sub Task Side Effects or Ripple Effects:** This is a major enhancement to the system's intelligence, allowing for highly tailored content generation based on strategic decisions.

---
**Task No:** 10
**Task Higher Overview:** Implement dynamic `Schema.org` generation in the `HtmlFormatter` based on the blueprint's `content_format`.
**Task Total Sub-tasks:** 1

*   **Sub Task No:** 10.1
*   **Sub Task Overview:** Extend the `_generate_schema_org` method to handle different content formats.
*   **Sub Task File:** `backend/agents/html_formatter.py`
*   **Sub Task Exact code changes:**
    ```python
    # In the `_generate_schema_org` method, replace the existing logic with a more dynamic one:

    # ... inside the method ...
    content_format = opportunity.get("blueprint", {}).get("recommended_strategy", {}).get("content_format", "BlogPosting")

    # Base Article Schema (always present)
    article_schema = { "@type": "BlogPosting", ... }
    schema_graph.append(article_schema)

    # Dynamic Schema based on format
    if content_format == "Review Article":
        # Logic to find ratings in the text or use a default
        review_schema = {
            "@type": "Review",
            "itemReviewed": {"@type": "Thing", "name": opportunity.get("keyword")},
            "reviewRating": {"@type": "Rating", "ratingValue": "4.5", "bestRating": "5"},
            "author": article_schema["author"]
        }
        schema_graph.append(review_schema)
    
    # The existing HowTo schema logic can remain as is, since it's detected from headings
    # ... existing HowTo logic ...
    ```
*   **Sub Task Verification:** Run a workflow for a "Review Article." Inspect the `final_package.json`. The `schema_org_json` field should contain both a `BlogPosting` object and a `Review` object in its `@graph`.
*   **Sub Task Side Effects or Ripple Effects:** Significantly improves the on-page SEO of generated content at no extra cost, increasing the likelihood of rich snippets.

---
**Task No:** 11
**Task Higher Overview:** Add an AI-powered step to generate a "Key Takeaways" summary box for the top of the article.
**Task Total Sub-tasks:** 1

*   **Sub Task No:** 11.1
*   **Sub Task Overview:** In the content orchestrator, after the final HTML is generated and audited, make one last AI call to summarize the content, and prepend it to the final HTML.
*   **Sub Task File:** `backend/pipeline/orchestrator/content_orchestrator.py`
*   **Sub Task Exact code changes:**
    ```python
    # In `_run_full_content_generation_background`, just BEFORE the `html_formatter.format_final_package` call:
    
    # ADD THIS BLOCK
    self.job_manager.update_job_status(job_id, "running", progress=88, result={"step": "Generating Key Takeaways"})
    summary_prompt = f"""
    Read the following article HTML and generate a 3-5 bullet point 'Key Takeaways' summary inside a `<div>` with the class 'key-takeaways'.

    Article HTML:
    {current_html}
    """
    takeaways_html, takeaways_cost = self.openai_client.call_chat_completion(
        messages=[{"role": "user", "content": summary_prompt}]
        # No schema needed, just raw HTML output
    )
    total_api_cost += takeaways_cost
    if takeaways_html:
        current_html = takeaways_html + "\\n" + current_html
        opportunity["ai_content"]["article_body_html"] = current_html
    # <<< END ADD
    
    # Now call the formatter with the prepended HTML
    final_package = self.html_formatter.format_final_package(...)
    ```
*   **Sub Task Verification:** Run a full content generation workflow. Inspect the `final_package.json` and verify that the `article_html_final` starts with a `div.key-takeaways` containing a bulleted list summary.
*   **Sub Task Side Effects or Ripple Effects:** Adds a small AI cost but improves content quality and user experience.

---
**Task No:** 12
**Task Higher Overview:** Generate more relevant in-article images by using the context surrounding the image placeholder as the search prompt.
**Task Total Sub-tasks:** 1

*   **Sub Task No:** 12.1
*   **Sub Task Overview:** Modify the `content_orchestrator` to extract contextual prompts before calling the `ImageGenerator`.
*   **Sub Task File:** `backend/pipeline/orchestrator/content_orchestrator.py`
*   **Sub Task Exact code changes:**
    ```python
    # In `_run_full_content_generation_background`, this task requires a more significant change.
    # The call to `image_generator` needs to happen AFTER the main HTML is generated, not before.

    # 1. First, instruct the ArticleGenerator to insert placeholders like [[IMAGE: A descriptive prompt]]
    # (This is already in your `prompt_assembler.py`)

    # 2. After the `final_article_html` is assembled, parse it to find these placeholders.
    import re
    image_prompts = re.findall(r'\[\[IMAGE: (.*?)\]\]', final_article_html)

    # 3. Then, call the image generator with these extracted prompts.
    if image_prompts:
        in_article_images_data, image_cost = self.image_generator.generate_images_from_prompts(image_prompts)
        total_api_cost += image_cost

    # 4. Finally, in the `html_formatter`, replace the placeholders with the actual <img> tags.
    # This logic will need to be added to `html_formatter.format_final_package`.
    ```
*   **Sub Task Verification:** Generate an article. The AI should insert placeholders like `[[IMAGE: A bar chart showing SEO growth]]`. The system should then search Pexels for "bar chart SEO growth" and replace the placeholder with the resulting image.
*   **Sub Task Side Effects or Ripple Effects:** This is a significant logic change, as image generation must move to a later stage in the workflow. It makes the images much more relevant.

---
**Task No:** 13
**Task Higher Overview:** Implement graceful degradation to make the system more resilient, allowing workflows to complete even if a non-critical external API (like Pexels) fails.
**Task Total Sub-tasks:** 1

*   **Sub Task No:** 13.1
*   **Sub Task Overview:** Wrap non-critical API calls in the orchestrators with try/except blocks that log the error but do not raise an exception to crash the job.
*   **Sub Task File:** `backend/pipeline/orchestrator/content_orchestrator.py`
*   **Sub Task Exact code changes:**
    ```python
    # In `_run_full_content_generation_background`:

    # WRAP the image and social media calls in try/except blocks
    try:
        featured_image_data, image_cost = self.image_generator.generate_featured_image(opportunity)
        total_api_cost += image_cost
    except Exception as e:
        self.logger.error(f"Featured image generation failed but workflow will continue: {e}")
        featured_image_data, image_cost = None, 0.0

    try:
        social_posts, social_cost = self.social_crafter.craft_posts(opportunity)
        total_api_cost += social_cost
    except Exception as e:
        self.logger.error(f"Social post generation failed but workflow will continue: {e}")
        social_posts, social_cost = None, 0.0
    ```
*   **Sub Task Verification:** Manually introduce an error in the `PexelsClient` (e.g., a wrong API key). Run a full content generation workflow. The job should complete successfully, the article should be generated, but the logs should show an error for image generation, and no image should be present.
*   **Sub Task Side Effects or Ripple Effects:** Makes the entire system more robust and less prone to complete failure due to transient issues with external services.

---
**Task No:** 14
**Task Higher Overview:** Implement a configurable "Auto-Proceed" path to allow for true end-to-end automation for high-confidence opportunities.
**Task Total Sub-tasks:** 2

*   **Sub Task No:** 14.1
*   **Sub Task Overview:** Add settings to `settings.ini` to control the auto-proceed feature.
*   **Sub Task File:** `backend/app_config/settings.ini`
*   **Sub Task Exact code changes:**
    ```ini
    # In the [APP_SETTINGS] section, ADD the following lines:
    enable_auto_proceed = false
    auto_proceed_confidence_threshold = 85
    ```
*   **Sub Task Verification:** Verify settings are loaded into `client_cfg`.
*   **Sub Task Side Effects or Ripple Effects:** None.

*   **Sub Task No:** 14.2
*   **Sub Task Overview:** Modify the `analysis_orchestrator` to check these settings and decide whether to pause or continue to content generation.
*   **Sub Task File:** `backend/pipeline/orchestrator/analysis_orchestrator.py`
*   **Sub Task Exact code changes:**
    ```python
    # In `run_analysis_phase`, at the very end, REPLACE the final DB update:

    # --- Before ---
    # self.db_manager.update_opportunity_workflow_state(opportunity_id, "analysis_completed", "paused_for_approval")
    # return { "status": "success", "message": "Analysis phase completed...", ... }

    # --- After ---
    enable_auto_proceed = self.client_cfg.get("enable_auto_proceed", False)
    confidence_threshold = self.client_cfg.get("auto_proceed_confidence_threshold", 85)
    confidence_score = blueprint.get("final_qualification_assessment", {}).get("confidence_score", 0)

    if enable_auto_proceed and confidence_score >= confidence_threshold:
        self.logger.info(f"Auto-proceeding to content generation for opportunity {opportunity_id} (Confidence: {confidence_score})")
        # This now requires calling the content generation logic directly or queuing a new job.
        # For simplicity, we'll just update the status to trigger a potential worker.
        self.db_manager.update_opportunity_workflow_state(opportunity_id, "analysis_completed", "in_progress") # Status to trigger generation
        return { "status": "success_auto_proceeded", "message": "Analysis completed, auto-proceeding to content generation.", "api_cost": total_api_cost }
    else:
        self.db_manager.update_opportunity_workflow_state(opportunity_id, "analysis_completed", "paused_for_approval")
        return { "status": "success", "message": "Analysis completed. Awaiting user approval.", "api_cost": total_api_cost }
    ```
*   **Sub Task Verification:** Set `enable_auto_proceed = true`. Run an analysis on an opportunity that you know will have a high confidence score. Verify that the final status is `in_progress` and the workflow continues, rather than `paused_for_approval`.
*   **Sub Task Side Effects or Ripple Effects:** This is a major workflow change that enables full automation. It requires a worker/scheduler that can pick up the `in_progress` status and automatically start the content generation job.

---
**Task No:** 15
**Task Higher Overview:** Create a `/api/health` endpoint for simple, effective monitoring of the application's core dependencies.
**Task Total Sub-tasks:** 1

*   **Sub Task No:** 15.1
*   **Sub Task Overview:** Add a new router and endpoint in the FastAPI application.
*   **Sub Task File:** Create `backend/api/routers/health.py` and register it in `main.py`.
*   **Sub Task Exact code changes:**
    1.  **Create `backend/api/routers/health.py`:**
        ```python
        from fastapi import APIRouter, Depends
        from data_access.database_manager import DatabaseManager
        from ..dependencies import get_db

        router = APIRouter()

        @router.get("/health")
        async def health_check(db: DatabaseManager = Depends(get_db)):
            db_status = "ok"
            try:
                # A simple, non-blocking query to check DB connection
                conn = db._get_conn()
                conn.execute("SELECT 1")
            except Exception:
                db_status = "error"
            
            # In a real app, you would also check external API connectivity here
            return {"status": "ok", "dependencies": {"database": db_status}}
        ```
    2.  **In `backend/api/main.py`:**
        ```python
        # In startup_event, import and register the new router
        from .routers import health # ADD THIS
        app.include_router(health.router, prefix="/api") # ADD THIS
        ```
*   **Sub Task Verification:** Start the application and navigate to `http://localhost:8000/api/health`. It should return a JSON response like `{"status": "ok", "dependencies": {"database": "ok"}}`.
*   **Sub Task Side Effects or Ripple Effects:** Provides a crucial endpoint for production monitoring, uptime checks, and automated container orchestration health checks.
</file>

<file path="docs/serp.md">
## Live Google Organic SERP Advanced

‌

### **Note:** the default value for the `depth` parameter has been updated from 100 to 10. Corresponding pricing changes are already in effect. [Full details >>](https://dataforseo.com/update/organic-serp-api-pricing-changes-now-in-effect)

Live SERP provides real-time data on top search engine results for the specified keyword, search engine, and location. This endpoint will supply a complete overview of featured snippets and other extra elements of SERPs.

> Instead of ‘login’ and ‘password’ use your credentials from https://app.dataforseo.com/api-access

```

Plain text

Copy to clipboard

Open code in new window

EnlighterJS 3 Syntax Highlighter

# Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access \

login="login"

password="password"

cred="$(printf ${login}:${password} | base64)"

curl --location --request POST "https://api.dataforseo.com/v3/serp/google/organic/live/advanced" \

--header "Authorization: Basic ${cred}"  \

--header "Content-Type: application/json" \

--data-raw '[\
\
  {\
\
      "language_code": "en",\
\
      "location_code": 2840,\
\
      "keyword": "albert einstein",\
\
      "calculate_rectangles": true\
\
  }\
\
]'
# Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access \
login="login"
password="password"
cred="$(printf ${login}:${password} | base64)"
curl --location --request POST "https://api.dataforseo.com/v3/serp/google/organic/live/advanced" \
--header "Authorization: Basic ${cred}"  \
--header "Content-Type: application/json" \
--data-raw '[\
  {\
      "language_code": "en",\
      "location_code": 2840,\
      "keyword": "albert einstein",\
      "calculate_rectangles": true\
  }\
]'
# Instead of 'login' and 'password' use your credentials from https://app.dataforseo.com/api-access \
login="login"
password="password"
cred="$(printf ${login}:${password} | base64)"
curl --location --request POST "https://api.dataforseo.com/v3/serp/google/organic/live/advanced" \
--header "Authorization: Basic ${cred}"  \
--header "Content-Type: application/json" \
--data-raw '[\
  {\
      "language_code": "en",\
      "location_code": 2840,\
      "keyword": "albert einstein",\
      "calculate_rectangles": true\
  }\
]'

```

```
All POST data should be sent in the [JSON](https://en.wikipedia.org/wiki/JSON) format (UTF-8 encoding). When setting a task, you should send all task parameters in the task array of the generic POST array. You can send up to 2000 API calls per minute, each Live SERP API call can contain only one task.\
\
Below you will find a detailed description of the fields you can use for setting a task.\
\
**Description of the fields for setting a task:**\
\
| Field name | Type | Description |\
| --- | --- | --- |\
| `keyword` | string | _keyword_<br>**required field**<br>you can specify **up to 700 characters** in the `keyword` field<br>all %## will be decoded (plus character ‘+’ will be decoded to a space character)<br>if you need to use the “%” character for your `keyword`, please specify it as “%25”;<br>if you need to use the “+” character for your `keyword`, please specify it as “%2B”;<br>if this field contains such parameters as _‘allinanchor:’, ‘allintext:’, ‘allintitle:’, ‘allinurl:’, ‘define:’, ‘definition:’, ‘filetype:’, ‘id:’, ‘inanchor:’, ‘info:’, ‘intext:’, ‘intitle:’, ‘inurl:’, ‘link:’, ‘site:’_, **the charge per task will be multiplied by 5**<br>**Note:** queries containing the ‘cache:’ parameter are not supported and will return a validation error<br>learn more about rules and limitations of `keyword` and `keywords` fields in DataForSEO APIs in this [Help Center article](https://dataforseo.com/help-center/rules-and-limitations-of-keyword-and-keywords-fields-in-dataforseo-apis) |  |\
| `url` | string | _direct URL of the search query_<br>optional field<br>you can specify a direct URL and we will sort it out to the necessary fields. Note that this method is the most difficult for our API to process and also requires you to specify the exact language and location in the URL. In most cases, we wouldn’t recommend using this method.<br>example:<br>`https://www.google.co.uk/search?q=%20rank%20tracker%20api&hl=en&gl=GB&uule=w+CAIQIFISCXXeIa8LoNhHEZkq1d1aOpZS` |  |\
| `depth` | integer | _parsing depth_<br>optional field<br>number of results in SERP<br>**default value: `10`**<br>max value: `700`<br>**Note:** your account will be billed per each SERP containing up to 10 results;<br>thus, setting a depth above `10` may result in additional charges if the search engine returns more than 10 results;<br>if the specified depth is higher than the number of results in the response, the difference will be refunded automatically to your account balance |  |\
| `max_crawl_pages` | integer | _page crawl limit_<br>optional field<br>number of search results pages to crawl<br>max value: `100`<br>**Note:** you will be charged for each page crawled (10 organic results per page);<br>learn more about pricing on our [Pricing](https://dataforseo.com/pricing/serp/google-organic-serp-api) page;<br>**Note#2:** the `max_crawl_pages` and `depth` parameters complement each other;<br>learn more at [our help center](https://dataforseo.com/help-center/what-is-max-crawl-pages-and-how-does-it-work) |  |\
| `location_name` | string | _full name of search engine location_<br>**required field if you don’t specify** `location_code` or `location_coordinate`<br>**if you use this field, you don’t need to specify `location_code` or `location_coordinate`**<br>you can receive the list of available locations of the search engine with their `location_name` by making a separate request to the `https://api.dataforseo.com/v3/serp/google/locations`<br>example:<br>`London,England,United Kingdom` |  |\
| `location_code` | integer | _search engine location code_<br>**required field if you don’t specify** `location_name` or `location_coordinate`<br>**if you use this field, you don’t need to specify `location_name` or `location_coordinate`**<br>you can receive the list of available locations of the search engines with their `location_code` by making a separate request to the `https://api.dataforseo.com/v3/serp/google/locations`<br>example:<br>`2840` |  |\
| `location_coordinate` | string | _GPS coordinates of a location_<br>optional field if you specify `location_name` or `location_code`<br>**if you use this field, you don’t need to specify `location_name` or `location_code`**<br>`location_coordinate` parameter should be specified in the _“latitude,longitude,radius”_ format<br>the maximum number of decimal digits for _“latitude”_ and _“longitude”_: 7<br>the minimum value for _“radius”_: 199.9 (mm)<br>the maximum value for _“radius”_: 199999 (mm)<br>example:<br>`53.476225,-2.243572,200` |  |\
| `language_name` | string | _full name of search engine language_<br>optional field if you specify `language_code`<br>**if you use this field, you don’t need to specify `language_code`**<br>you can receive the list of available languages of the search engine with their `language_name` by making a separate request to the `https://api.dataforseo.com/v3/serp/google/languages`<br>example:<br>`English` |  |\
| `language_code` | string | _search engine language code_<br>optional field if you specify `language_name`<br>**if you use this field, you don’t need to specify `language_name`**<br>you can receive the list of available languages of the search engine with their `language_code` by making a separate request to the `https://api.dataforseo.com/v3/serp/google/languages` example: `en` |  |\
| `se_domain` | string | _search engine domain_<br>optional field<br>we choose the relevant search engine domain automatically according to the location and language you specify<br>however, you can set a custom search engine domain in this field<br>example:<br>_google.co.uk_, `google.com.au`, `google.de`, etc. |  |\
| `device` | string | _device type_<br>optional field<br>can take the values: `desktop`, `mobile`<br>default value: `desktop` |  |\
| `os` | string | _device operating system_<br>optional field<br>if you specify `desktop` in the `device` field, choose from the following values: `windows`, `macos`<br>default value: `windows`<br>if you specify `mobile` in the `device` field, choose from the following values: `android`, `ios`<br>default value: `android` |  |\
| `target` | string | _target domain, subdomain, or webpage to get results for_<br>optional field<br>a domain or a subdomain should be specified without `https://` and `www.`<br>note that the results of `target`-specific tasks will only include SERP elements that contain a `url` string;<br>you can also use a wildcard (‘\*’) character to specify the search pattern in SERP and narrow down the results;<br>examples:<br>**`example.com`** – returns results for the website’s home page with URLs, such as `https://example.com`, or `https://www.example.com/`, or `https://example.com/`;<br>**`example.com*`** – returns results for the domain, including all its pages;<br>**`*example.com*`** – returns results for the entire domain, including all its pages and subdomains;<br>**`*example.com`** – returns results for the home page regardless of the subdomain, such as `https://en.example.com`;<br>**`example.com/example-page`** – returns results for the exact URL;<br>**`example.com/example-page*`** – returns results for all domain’s URLs that start with the specified string |  |\
| `group_organic_results` | boolean | _display related results_<br>optional field<br>if set to `true`, the `related_result` element in the response will be provided as a snippet of its parent organic result;<br>if set to `false`, the `related_result` element will be provided as a separate organic result;<br>default value: `true` |  |\
| `calculate_rectangles` | boolean | _calcualte pixel rankings for SERP elements in advanced results_<br>optional field<br>pixel ranking refers to the distance between the result snippet and top left corner of the screen;<br>[Visit Help Center to learn more>>](https://dataforseo.com/help-center/pixel-ranking-in-serp-api)<br>by default, the parameter is set to `false`;<br>**Note:** you will be charged extra $0.002 for using this parameter |  |\
| `browser_screen_width` | integer | _browser screen width_<br>optional field<br>you can set a custom browser screen width to calculate pixel rankings for a particular device;<br>by default, the parameter is set to:<br>`1920` for `desktop`;<br>`360` for `mobile` on `android`;<br>`375` for `mobile` on `iOS`;<br>**Note:** to use this parameter, set `calculate_rectangles` to `true` |  |\
| `browser_screen_height` | integer | _browser screen height_<br>optional field<br>you can set a custom browser screen height to calculate pixel rankings for a particular device;<br>by default, the parameter is set to:<br>`1080` for `desktop`;<br>`640` for `mobile` on `android`;<br>`812` for `mobile` on `iOS`;<br>**Note:** to use this parameter, set `calculate_rectangles` to `true` |  |\
| `browser_screen_resolution_ratio` | integer | _browser screen resolution ratio_<br>optional field<br>you can set a custom browser screen resolution ratio to calculate pixel rankings for a particular device;<br>possible values: from `1` to `3`;<br>by default, the parameter is set to:<br>`1` for `desktop`;<br>`3` for `mobile` on `android`;<br>`3` for `mobile` on `iOS`;<br>**Note:** to use this parameter, set `calculate_rectangles` to `true` |  |\
| `people_also_ask_click_depth` | integer | _clicks on the corresponding element_<br>optional field<br>specify the click depth on the `people_also_ask` element to get additional `people_also_ask_element` items;<br>**Note** your account will be billed $0.00015 extra for each click;<br>if the element is absent or we perform fewer clicks than you specified, all extra charges will be returned to your account balance<br>possible values: from `1` to `4` |  |\
| `load_async_ai_overview` | boolean | _load asynchronous ai overview_<br>optional field<br>set to `true` to obtain `ai_overview` items is SERPs even if they are loaded asynchronically;<br>if set to `false`, you will only obtain `ai_overview` items from cache;<br>default value: `false`<br>**Note:** you will be charged extra $0.002 for using this parameter;<br>if the element is absent or contains `"asynchronous_ai_overview": false`, all extra charges will be returned to your account balance |  |\
| `search_param` | string | _additional parameters of the search query_<br>optional field<br>[get the list of available parameters and additional details here](https://dataforseo.com/help-center/google-search-engine-parameters-and-how-to-use-them) |  |\
| `remove_from_url` | array | _remove specific parameters from URLs_<br>optional field<br>using this field, you can specify up to 10 parameters to remove from URLs in the result<br>example:<br>`"remove_from_url": ["srsltid"]`<br>**Note:** if the `target` field is specified, the specified URL parameters will be removed before the search |  |\
| `tag` | string | _user-defined task identifier_<br>optional field<br>_the character limit is 255_<br>you can use this parameter to identify the task and match it with the result<br>you will find the specified `tag` value in the `data` object of the response |  |\
\
‌‌‌\
\
As a response of the API server, you will receive [JSON](https://en.wikipedia.org/wiki/JSON)-encoded data containing a `tasks` array with the information specific to the set tasks.\
\
**Description of the fields in the results array:**\
\
| Field name | Type | Description |\
| --- | --- | --- |\
| `version` | string | _the current version of the API_ |  |\
| `status_code` | integer | _general status code_<br>you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors)<br>**Note:** we strongly recommend designing a necessary system for handling related exceptional or error conditions |  |\
| `status_message` | string | _general informational message_<br>you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix/errors) |  |\
| `time` | string | _execution time, seconds_ |  |\
| `cost` | float | _total tasks cost, USD_ |  |\
| `tasks_count` | integer | _the number of tasks in the **`tasks`** array_ |  |\
| `tasks_error` | integer | _the number of tasks in the **`tasks`** array returned with an error_ |  |\
| **`tasks`** | array | _array of tasks_ |  |\
| `id` | string | _task identifier_<br>**unique task identifier in our system in the [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier) format** |  |\
| `status_code` | integer | _status code of the task_<br>generated by DataForSEO; can be within the following range: 10000-60000<br>you can find the full list of the response codes [here](https://docs.dataforseo.com/v3/appendix/errors) |  |\
| `status_message` | string | _informational message of the task_<br>you can find the full list of general informational messages [here](https://docs.dataforseo.com/v3/appendix/errors) |  |\
| `time` | string | _execution time, seconds_ |  |\
| `cost` | float | _cost of the task, USD_ |  |\
| `result_count` | integer | _number of elements in the `result` array_ |  |\
| `path` | array | _URL path_ |  |\
| `data` | object | _contains the same parameters that you specified in the POST request_ |  |\
| **`result`** | array | _array of results_ |  |\
| `keyword` | string | _keyword received in a POST array_ **the keyword is returned with decoded %## (plus character ‘+’ will be decoded to a space character)** |  |\
| `type` | string | _search engine type in a POST array_ |  |\
| `se_domain` | string | _search engine domain in a POST array_ |  |\
| `location_code` | integer | _location code in a POST array_ |  |\
| `language_code` | string | _language code in a POST array_ |  |\
| `check_url` | string | _direct URL to search engine results_<br>you can use it to make sure that we provided accurate results |  |\
| `datetime` | string | _date and time when the result was received_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `spell` | object | _autocorrection of the search engine_<br>if the search engine provided results for a keyword that was corrected, we will specify the keyword corrected by the search engine and the type of autocorrection |  |\
| `keyword` | string | _keyword obtained as a result of search engine autocorrection_<br>the results will be provided for the corrected keyword |  |\
| `type` | string | _type of autocorrection_<br>possible values:<br>`did_you_mean`, `showing_results_for`, `no_results_found_for`, `including_results_for` |  |\
| `refinement_chips` | object | _search refinement chips_ |  |\
| `type` | string | _type of element = **‘refinement\_chips’**_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _items of the element_ |  |\
| `type` | string | _type of element = **‘refinement\_chips\_element’**_ |  |\
| `title` | string | _title of the element_ |  |\
| `url` | string | _search URL with refinement parameters_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `options` | array | _further search refinement options_ |  |\
| `type` | string | _type of element = **‘refinement\_chips\_option’**_ |  |\
| `title` | string | _title of the element_ |  |\
| `url` | string | _search URL with refinement parameters_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `item_types` | array | _types of search results in SERP_<br>contains types of search results ( `items`) found in SERP.<br>possible item types:<br>[`answer_box`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#answer_box), [`app`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#app), [`carousel`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#carousel), [`multi_carousel`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#multi_carousel), [`featured_snippet`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#featured_snippet), [`google_flights`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#google_flights), [`google_reviews`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#google_reviews), [`third_party_reviews`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#third_party_reviews), [`google_posts`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#google_posts), [`images`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#images), [`jobs`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#jobs), [`knowledge_graph`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#knowledge_graph), [`local_pack`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#local_pack), [`hotels_pack`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#hotels_pack), [`map`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#map), [`organic`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#organic), [`paid`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#paid), [`people_also_ask`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#people_also_ask), [`related_searches`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#related_searches), [`people_also_search`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#people_also_search), [`shopping`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#shopping), [`top_stories`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#top_stories), [`twitter`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#twitter), [`video`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#video), [`events`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#events), [`mention_carousel`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#mention_carousel), [`recipes`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#recipes), [`top_sights`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#top_sights), [`scholarly_articles`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#scholarly_articles), [`popular_products`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#popular_products), [`podcasts`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#podcasts), [`questions_and_answers`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#questions_and_answers), [`find_results_on`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#find_results_on), [`stocks_box`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#stocks_box), [`visual_stories`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#visual_stories), [`commercial_units`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#commercial_units), [`local_services`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#local_services), [`google_hotels`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#google_hotels), [`math_solver`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#math_solver), [`currency_box`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#currency_box), [`product_considerations`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#product_considerations), [`found_on_web`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#found_on_web), [`short_videos`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#short_videos), [`refine_products`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#refine_products), [`explore_brands`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#explore_brands), [`perspectives`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#perspectives), [`discussions_and_forums`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#discussions_and_forums), [`compare_sites`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#compare_sites), [`courses`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#courses), [`ai_overview`](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#ai_overview) |  |\
| `se_results_count` | integer | _total number of results in SERP_ |  |\
| `pages_count` | integer | _total search results pages retrieved_<br>total number of retrieved SERPs in the result |  |\
| `items_count` | integer | _the number of results returned in the **`items`** array_ |  |\
| **`items`** | array | _elements of search results found in SERP_ |  |\
| **[‘organic’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-organic-1.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘organic’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _relevant URL in SERP_ |  |\
| `cache_url` | string | _cached version of the page_ |  |\
| `related_search_url` | string | _URL to a similar search_<br>URL to a new search for the same keyword(s) [on related sites](https://support.google.com/websearch/answer/2466433?hl=en#:~:text=Search%20for%20related%20sites) |  |\
| `breadcrumb` | string | _breadcrumb in SERP_ |  |\
| `website_name` | string | _name of the website in SERP_ |  |\
| `is_image` | boolean | _indicates whether the element contains an `image`_ |  |\
| `is_video` | boolean | _indicates whether the element contains a `video`_ |  |\
| `is_featured_snippet` | boolean | _indicates whether the element is a `featured_snippet`_ |  |\
| `is_malicious` | boolean | _indicates whether the element is marked as malicious_ |  |\
| `is_web_story` | boolean | _indicates whether the element is marked as Google web story_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `pre_snippet` | string | _includes additional information appended before the result description in SERP_ |  |\
| `extended_snippet` | string | _includes additional information appended after the result description in SERP_ |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `amp_version` | boolean | _Accelerated Mobile Pages_<br>indicates whether an item has the Accelerated Mobile Page (AMP) version |  |\
| `rating` | object | _the item’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of_ _rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | float | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `price` | object | _pricing details_<br>contains the pricing details of the product or service featured in the result |  |\
| `current` | float | _current price_<br>indicates the current price of the product or service featured in the result |  |\
| `regular` | float | _regular price_<br>indicates the regular price of the product or service with no discounts applied |  |\
| `max_value` | float | _the maximum price_<br>the maximum price of the product or service as indicated in the result |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `highlighted` | array | _words highlighted in bold within the results `description`_ |  |\
| `links` | array | _sitelinks_<br>the links shown below some of Google’s search results<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `url` | string | _sitelink URL_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `faq` | object | _frequently asked questions_<br>questions and answers extension shown below some of Google’s search results<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **faq\_box**‘_ |  |\
| `items` | array | _items featured in the faq\_box_ |  |\
| `type` | string | _type of element = ‘ **faq\_box\_element**‘_ |  |\
| `title` | string | _question related to the result_ |  |\
| `description` | string | _answer provided in the drop-down block_ |  |\
| `links` | array | _links featured in the faq\_box\_element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _link anchor text_ |  |\
| `url` | string | _link URL_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `extended_people_also_search` | array | _extension of the organic element_<br>extension of the organic result containing related search queries<br>**Note:** extension appears in SERP upon clicking on the result and then bouncing back to search results |  |\
| `about_this_result` | object | _contains information from the ‘About this result’ panel_<br>[‘About this result’ panel](https://blog.google/products/search/learn-more-and-get-more-from-search/) provides additional context about why Google returned this result for the given query;<br>this feature appears after clicking on the three dots next to most results |  |\
| `type` | string | _type of element = ‘ **about\_this\_result\_element**‘_ |  |\
| `url` | string | _result’s URL_ |  |\
| `source` | string | _source of additional information about the result_ |  |\
| `source_info` | string | _additional information about the result_<br>description of the website from Wikipedia or another additional context |  |\
| `source_url` | string | _URL to full information from the `source`_ |  |\
| `language` | string | _the language of the result_ |  |\
| `location` | string | _location for which the result is relevant_ |  |\
| `search_terms` | array | _matching search terms that appear in the result_ |  |\
| `related_terms` | array | _related search terms that appear in the result_ |  |\
| `related_result` | array | _related result from the same domain_<br>related result from the same domain appears as a part of the main result snippet;<br>you can derive the `related_result` snippets as `"type": "organic"` results by setting the `group_organic_results` parameter to `false` in the POST request |  |\
| `type` | string | _type of element = ‘ **related\_result**‘_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `domain` | string | _relevant domain_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _relevant URL in SERP_ |  |\
| `cache_url` | string | _cached version of the page_ |  |\
| `related_search_url` | string | _URL to a similar search_<br>URL to a new search for the same keyword(s) [on related sites](https://support.google.com/websearch/answer/2466433?hl=en#:~:text=Search%20for%20related%20sites) |  |\
| `breadcrumb` | string | _breadcrumb in SERP_ |  |\
| `is_image` | boolean | _indicates whether the element contains an `image`_ |  |\
| `is_video` | boolean | _indicates whether the element contains a `video`_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `pre_snippet` | string | _includes additional information appended before the result description in SERP_ |  |\
| `extended_snippet` | string | _includes additional information appended after the result description in SERP_ |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `amp_version` | boolean | _Accelerated Mobile Pages_<br>indicates whether an item has the Accelerated Mobile Page (AMP) version |  |\
| `rating` | object | _the item’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of_ _rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | float | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `price` | object | _pricing details_<br>contains the pricing details of the product or service featured in the result |  |\
| `current` | float | _current price_<br>indicates the current price of the product or service featured in the result |  |\
| `regular` | float | _regular price_<br>indicates the regular price of the product or service with no discounts applied |  |\
| `max_value` | float | _the maximum price_<br>the maximum price of the product or service as indicated in the result |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `highlighted` | array | _words highlighted in bold within the results `description`_ |  |\
| `about_this_result` | object | _contains information from the ‘About this result’ panel_<br>[‘About this result’ panel](https://blog.google/products/search/learn-more-and-get-more-from-search/) provides additional context about why Google returned this result for the given query;<br>this feature appears after clicking on the three dots next to most results |  |\
| `type` | string | _type of element = ‘ **about\_this\_result\_element**‘_ |  |\
| `url` | string | _result’s URL_ |  |\
| `source` | string | _source of additional information about the result_ |  |\
| `source_info` | string | _additional information about the result_<br>description of the website from Wikipedia or another additional context |  |\
| `source_url` | string | _URL to full information from the `source`_ |  |\
| `language` | string | _the language of the result_ |  |\
| `location` | string | _location for which the result is relevant_ |  |\
| `search_terms` | array | _matching search terms that appear in the result_ |  |\
| `related_terms` | array | _related search terms that appear in the result_ |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘paid’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-paid-1.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘paid’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `domain` | string | _domain in SERP of the ad element_ |  |\
| `website_name` | string | _name of the website in the ad element_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `url` | string | _relevant URL of the Ad element in SERP_ |  |\
| `breadcrumb` | string | _breadcrumb of the Ad element in SERP_ |  |\
| `is_image` | boolean | _indicates whether the element contains an `image`_ |  |\
| `is_video` | boolean | _indicates whether the element contains a `video`_ |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `highlighted` | array | _words highlighted in bold within the results `description`_ |  |\
| `extra` | object | _additional information about the result_ |  |\
| `ad_aclk` | string | _the identifier of the ad_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `description_rows` | array | _extended description_<br>if there is none, equals `null` |  |\
| `links` | array | _sitelinks_<br>the links shown below some of Google’s search results<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of the link element_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `url` | string | _URL link_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `ad_aclk` | string | _the identifier of the ad_ |  |\
| `price` | object | _pricing details_<br>contains the pricing details of the product or service featured in the result |  |\
| `current` | float | _current price_<br>indicates the current price of the product or service featured in the result |  |\
| `regular` | float | _regular price_<br>indicates the regular price of the product or service with no discounts applied |  |\
| `max_value` | float | _the maximum price_<br>the maximum price of the product or service as indicated in the result |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `rating` | object | _the element’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of_ _rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | float | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘carousel’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-carousel-1.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘carousel’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **carousel\_element**‘_ |  |\
| `title` | string | _title of the item_ |  |\
| `subtitle` | string | _subtitle of the item_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘multi\_carousel’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/05/mobile-element-multi-carousel.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘multi\_carousel’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **multi\_carousel\_element**‘_ |  |\
| `title` | string | _title of the item_ |  |\
| `multi_carousel_snippets` | array | _`multi_carousel_snippet` results_ |  |\
| `type` | string | _type of element = ‘ **multi\_carousel\_snippet**‘_ |  |\
| `title` | string | _title of a particular item_ |  |\
| `subtitle` | string | _subtitle of the item_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘answer\_box’ element in SERP![](https://dataforseo.com/wp-content/uploads/2023/10/Answer_box-img.webp)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘answer\_box’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `text` | array | _text_<br>if there is none, equals `null` |  |\
| `links` | array | _sitelinks_<br>the links shown below some of Google’s search results<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of the link_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `url` | string | _URL link_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘related\_searches’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-related-searches.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘related\_searches’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘people\_also\_search’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-people-also-search.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘people\_also\_search’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘local\_pack’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-local-pack.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘local\_pack’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `phone` | string | _phone number_ |  |\
| `url` | string | _relevant URL_ |  |\
| `is_paid` | boolean | _indicates whether the element is an ad_ |  |\
| `rating` | object | _the item’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | float | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `cid` | string | _google-defined client id_<br>unique id of a local establishment;<br>can be used with [Google Reviews API](https://docs.dataforseo.com/v3/reviews/google/overview/?php) to get a full list of reviews |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘hotels\_pack’ element in SERP![](https://dataforseo.com/wp-content/uploads/2021/03/window-feature-hotel-1.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘hotels\_pack’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `date_from` | string | _starting date of stay_<br>in the format “year-month-date”<br>example:<br>2019-11-15 |  |\
| `date_to` | string | _ending date of stay_<br>in the format “year-month-date”<br>example:<br>2019-11-17 |  |\
| `items` | array | _contains results featured in the ‘hotels\_pack’ element of SERP_ |  |\
| `type` | string | _type of element = **‘hotels\_pack\_element’**_ |  |\
| `price` | object | _price of booking a place for the specified dates of stay_ |  |\
| `current` | float | _current price_<br>indicates the current price of booking a place for the specified dates of stay |  |\
| `regular` | float | _regular price_<br>indicates the regular price of booking a place for the specified dates of stay |  |\
| `max_value` | float | _the maximum price_<br>the maximum price of booking a place for the specified dates of stay |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `title` | string | _title of the place_ |  |\
| `desription` | string | _description of the place in SERP_ |  |\
| `hotel_identifier` | string | _unique hotel identifier_<br>unique hotel identifier assigned by Google;<br>example: `"CgoIjaeSlI6CnNpVEAE"` |  |\
| `domain` | string | _domain in SERP_ |  |\
| `url` | string | _relevant URL_ |  |\
| `is_paid` | boolean | _indicates whether the element is an ad_ |  |\
| `rating` | object | _the item’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | float | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘knowledge\_graph’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-Knowledge-Graph-2.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  | **note** that `knowledge_graph` items in `mobile` results may be separated by other elements;<br>in such cases, the API response returns several `knowledge_graph` elements, each containing distinct items, and each placed according to the items’ placement in SERP |  |\
| `type` | string | _type of element = **‘knowledge\_graph’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `subtitle` | string | _subtitle of the item_ |  |\
| `description` | string | _description_ |  |\
| `card_id` | string | _card id_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image from knowledge graph_ |  |\
| `logo_url` | string | _URL of the logo from knowledge graph_ |  |\
| `cid` | string | _google-defined client id_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_images\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `link` | object | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _contains arrays of specific images_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_images\_element**‘_ |  |\
| `url` | string | _image source URL_ |  |\
| `domain` | string | _website domain_ |  |\
| `alt` | string | _alt attribute_ |  |\
| `image_url` | string | _URL of a specific image_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_list\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the element_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`action:listen_artist` |  |\
| `link` | object | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _contains arrays of elements available in the list_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_list\_element**‘_ |  |\
| `title` | string | _title of the element_ |  |\
| `subtitle` | string | _subtitle of the element_ |  |\
| `url` | string | _URL of element_ |  |\
| `domain` | string | _website domain_ |  |\
| `image_url` | string | _URL of the image_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_ai\_overview\_item‘**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `asynchronous_ai_overview` | boolean | _indicates whether the element is loaded asynchronically_<br>if `true`, the `ai_overview` element is loaded asynchronically;<br>if `false`, the `ai_overview` element is loaded from cache; |  |\
| `items` | array | _items present in the element_ |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_element**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `title` | string | _title of the element_ |  |\
| `text` | string | _text or description of the element in SERP_ |  |\
| `markdown` | string | _content of the element in markdown format_ |  |\
| `links` | array | _website links featured in the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _link anchor text_ |  |\
| `description` | string | _link description_ |  |\
| `url` | string | _link URL_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `images` | array | _images of the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `references` | array | _references relevant to the element_<br>includes references to webpages that were used to generate the `ai_overview_element` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_reference**‘_ |  |\
| `source` | string | _reference source name or title_ |  |\
| `domain` | string | _domain name of the reference_ |  |\
| `url` | string | _reference page URL_ |  |\
| `title` | string | _reference page title_ |  |\
| `text` | string | _reference text_<br>text snippet from the page that was used to generate the `ai_overview_element` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_video\_element**‘_ |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `title` | string | _title of the element_ |  |\
| `snippet` | string | _additional information for the video_ |  |\
| `url` | string | _URL of the link to the video_ |  |\
| `domain` | string | _domain of the website hosting the video_ |  |\
| `image_url` | string | _URL to the image thumbnail of the video_ |  |\
| `source` | string | _name of the source of the video_ |  |\
| `date` | string | _date when the video was published or indexed_<br>example:<br>`Apr 26, 2024` |  |\
| `timestamp` | string | _date and time when the video was published or indexed_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_table\_element**‘_ |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `markdown` | string | _content of the element in markdown format_ |  |\
| `table` | object | _table present in the element_<br>the header and content of the table present in the element |  |\
| `table_header` | array | _content in the header of the table_ |  |\
| `table_content` | array | _array of contents of the table present in the element_<br>each array represents the table row |  |\
| `references` | array | _references relevant to the element_<br>includes references to webpages that were used to generate the `ai_overview_element` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_reference**‘_ |  |\
| `source` | string | _reference source name or title_ |  |\
| `domain` | string | _domain name of the reference_ |  |\
| `url` | string | _reference page URL_ |  |\
| `title` | string | _reference page title_ |  |\
| `text` | string | _reference text_<br>text snippet from the page that was used to generate the `ai_overview_element` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_expanded\_element**‘_ |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `title` | string | _title of the element in SERP_ |  |\
| `text` | string | _additional text of the element in SERP_ |  |\
| `components` | array | _array of components of the element_ |  |\
| `type` | string | _type of component = ‘ **ai\_overview\_expanded\_component**‘_ |  |\
| `title` | string | _title of the element in SERP_ |  |\
| `text` | string | _text of the component_ |  |\
| `markdown` | string | _text of the component in the markdwon format_ |  |\
| `images` | array | _images of the component_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `links` | array | _sitelinks_<br>the links shown below some of Google’s search results<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of the link_ |  |\
| `description` | string | _description of the link_ |  |\
| `url` | string | _URL in link_ |  |\
| `domain` | string | _domain in link_ |  |\
| `references` | array | _additional references relevant to the item_<br>includes references to webpages that may have been used to generate the `ai_overview` |  |\
| `type` | string | _type of element = ‘ **ai\_overview\_reference**‘_ |  |\
| `source` | string | _reference source name or title_ |  |\
| `domain` | string | _domain name of the reference_ |  |\
| `url` | string | _reference page URL_ |  |\
| `title` | string | _reference page title_ |  |\
| `text` | string | _reference text_<br>text snippet from the page that was used to generate the `ai_overview_element` |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_description\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `text` | string | _description content_ |  |\
| `links` | array | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_row\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the row_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`ss:/webfacts:net_worth` |  |\
| `text` | string | _row content_ |  |\
| `links` | array | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_carousel\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the carousel item_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`kc:/common/topic:social media presence` |  |\
| `link` | object | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _contains arrays of elements available in the list_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_carousel\_element**‘_ |  |\
| `title` | string | _title of the element_ |  |\
| `subtitle` | string | _subtitle of the element_ |  |\
| `url` | string | _URL of element_ |  |\
| `domain` | string | _website domain_ |  |\
| `image_url` | string | _URL of the image_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_part\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the item_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`kc:/local:place qa` |  |\
| `text` | string | _content within the item_ |  |\
| `links` | array | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **link\_element**‘_ |  |\
| `title` | string | _title of a given link element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain where a link points_ |  |\
| `snippet` | string | _text alongside the link title_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_expanded\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the item_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`kc:/local:place qa` |  |\
| `expanded_element` | array | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_expanded\_element**‘_ |  |\
| `featured_title` | string | _title of a given element_ |  |\
| `url` | string | _source URL_ |  |\
| `domain` | string | _source domain_ |  |\
| `title` | string | _source title_ |  |\
| `snippet` | string | _text alongside the title_ |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `table` | object | _table element_ |  |\
| `table_element` | string | _name assigned to the table element_<br>possible values:<br>`table_element` |  |\
| `table_header` | array | _column names_ |  |\
| `table_content` | array | _the content of the table_<br>one line of the table in this element of the array |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| ``**type** | string | _type of element = ‘ **knowledge\_graph\_shopping\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the item_ |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`kc:/shopping/gpc:organic-offers` |  |\
| `items` | array | _link of the element_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_shopping\_element**‘_ |  |\
| `title` | string | _title of a given shopping element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain in url_ |  |\
| `price` | object | _price indicated in the element_ |  |\
| `current` | float | _current price_<br>refers to the current price indicated in the element |  |\
| `regular` | float | _regular price_<br>refers to the regular price indicated in the element |  |\
| `max_value` | float | _the maximum price_<br>refers to the maximum price indicated in the element |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `source` | string | _web source of the shopping element_<br>indicates the source of information included in the element |  |\
| `snippet` | string | _description of the shopping element_ |  |\
| `marketplace` | string | _merchant account provider_<br>ecommerce site that hosts products or websites of individual sellers under the same merchant account<br>example:<br>`by Google` |  |\
| `marketplace_url` | string | _URL to the merchant account provider_<br>ecommerce site that hosts products or websites of individual sellers under the same merchant account |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_hotels\_booking\_item**‘_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values;<br>positions of elements with different `type` values are omitted from `rank_group`;<br>always equals `0` for `desktop` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP<br>always equals `0` for `desktop` |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the element_ |  |\
| `date_from` | string | _starting date of stay_<br>in the format “year-month-date”<br>example:<br>2019-11-15 |  |\
| `date_to` | string | _ending date of stay_<br>in the format “year-month-date”<br>example:<br>2019-11-17 |  |\
| `data_attrid` | string | _google defined data attribute ID_<br>example:<br>`kc:/local:hotel booking` |  |\
| `items` | array | _contains arrays of elements available in the list_ |  |\
| `type` | string | _type of element = ‘ **knowledge\_graph\_hotels\_booking\_element**‘_ |  |\
| `source` | string | _web source of the hotel booking element_<br>indicates the source of information included in the element |  |\
| `description` | string | _description of the hotel booking element_ |  |\
| `url` | string | _URL_ |  |\
| `domain` | string | _domain in the URL_ |  |\
| `price` | object | _price indicated in the element_ |  |\
| `current` | float | _current price_<br>refers to the current price indicated in the element |  |\
| `regular` | float | _regular price_<br>refers to the regular price indicated in the element |  |\
| `max_value` | float | _the maximum price_<br>refers to the maximum price indicated in the element |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `is_paid` | boolean | _indicates whether the element is an ad_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘featured\_snippet’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-featured_snippet.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘featured\_snippet’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `featured_title` | string | _the title of the featured snippets source page_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `url` | string | _relevant URL_ |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `table` | object | _results table_<br>if there are none, equals `null` |  |\
| `table_header` | array | _column names_ |  |\
| `table_content` | array | _the content of the table_<br>one line of the table in this element of the array |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘top\_stories’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-top-stories.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘top\_stories’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `page` | integer | _search results page number_<br>indicates the number of the SERP page on which the element is located |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the element in SERP_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **top\_stories\_element**‘_ |  |\
| `source` | string | _source of the element_<br>indicates the source of information included in the `top_stories_element` |  |\
| `domain` | string | _domain in SERP_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `date` | string | _the date when the page source of the element was published_ |  |\
| `amp_version` | boolean | _Accelerated Mobile Pages_<br>indicates whether an item has the Accelerated Mobile Page (AMP) version |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `url` | string | _URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original resource or DataForSEO storage (in case the original source is not available) |  |\
| `badges` | array | _badges relevant to the element_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘twitter’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-twitters.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘twitter’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _URL_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **twitter** **\_element**‘_ |  |\
| `tweet` | string | _tweet message_ |  |\
| `date` | string | _the posting date_ |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `url` | string | _URL_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘map’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-map.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘map’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _URL_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘google\_flights’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-google-flights.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘google\_flights’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _URL_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **google\_flights\_element**‘_ |  |\
| `description` | string | _description_ |  |\
| `url` | string | _URL_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘google\_reviews’ element in SERP![](https://dataforseo.com/wp-content/uploads/2021/03/desktop-element-google_reviews.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘google\_reviews’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `reviews_count` | integer | _the number of reviews_ |  |\
| `rating` | object | _the element’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of_ _rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | integer | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `place_id` | string | _the identifier of a place_ |  |\
| `feature` | string | _the additional feature of the review_ |  |\
| `cid` | string | _google-defined client id_<br>unique id of a local establishment;<br>can be used with [Google Reviews API](https://docs.dataforseo.com/v3/reviews/google/overview/?php) to get a full list of reviews |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘third\_party\_reviews’ element in SERP![](https://docs.dataforseo.com/wp-content/uploads/2025/02/external_reviews.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘third\_party\_reviews’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `reviews_count` | integer | _the number of reviews_ |  |\
| `title` | string | _name of the third-party review source_ |  |\
| `url` | string | _URL of the third-party review source_ |  |\
| `rating` | object | _the element’s rating_<br>the popularity rate based on reviews and displayed in SERP |  |\
| `rating_type` | string | _the type of_ _rating_<br>here you can find the following elements: `Max5`, `Percents`, `CustomMax` |  |\
| `value` | integer | _the value of the rating_ |  |\
| `votes_count` | integer | _the amount of_ _feedback_ |  |\
| `rating_max` | integer | _the maximum value for a `rating_type`_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘google\_posts’ element in SERP![](https://dataforseo.com/wp-content/uploads/2021/03/desktop-element-google_posts.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘google\_posts’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `posts_id` | string | _the identifier of the google\_posts feature_ |  |\
| `feature` | string | _the additional feature of the review_ |  |\
| `cid` | string | _google-defined client id_<br>unique id of a local establishment;<br>can be used with [Google Reviews API](https://docs.dataforseo.com/v3/reviews/google/overview/?php) to get a full list of reviews |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘video’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-video.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘video’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **video\_element**‘_ |  |\
| `source` | string | _source of the element_<br>indicates the source of the video |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `timestamp` | string | _date and time when the result was published_<br>in the UTC format: “yyyy-mm-dd hh-mm-ss +00:00”<br>example:<br>`2019-11-15 12:57:46 +00:00` |  |\
| `url` | string | _URL_ |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘app’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/05/mobile-element-app.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘app’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **app\_element**‘_ |  |\
| `description` | string | _description of the results element in SERP_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `url` | string | _URL_ |  |\
| `price` | object | _price of the app element_ |  |\
| `current` | float | _current price_<br>refers to the current price indicated in the app element |  |\
| `regular` | float | _regular price_<br>refers to the regular price indicated in the app element |  |\
| `max_value` | float | _the maximum price_<br>refers to the maximum price indicated in the app element |  |\
| `currency` | string | _currency of the listed price_<br>ISO code of the currency applied to the price |  |\
| `is_price_range` | boolean | _price is provided as a range_<br>indicates whether a price is provided in a range |  |\
| `displayed_price` | string | _price string in the result_<br>raw price string as provided in the result |  |\
| `rectangle` | object | _rectangle parameters_<br>contains cartesian coordinates and pixel dimensions of the result’s snippet in SERP<br>equals `null` if `calculate_rectangles` in the POST request is not set to `true` |  |\
| `x` | float | _x-axis coordinate_<br>x-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `y` | float | _y-axis coordinate_<br>y-axis coordinate of the top-left corner of the result’s snippet, where top-left corner of the screen is the origin |  |\
| `width` | float | _width of the element in pixels_ |  |\
| `height` | float | _height of the element in pixels_ |  |\
| **[‘people\_also\_ask’ element in SERP![](https://dataforseo.com/wp-content/uploads/2020/02/window-feature-paa.png)](https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/#)** |  |  |  |\
| `type` | string | _type of element = **‘people\_also\_ask’**_ |  |\
| `rank_group` | integer | _group rank in SERP_<br>position within a group of elements with identical `type` values<br>positions of elements with different `type` values are omitted from `rank_group` |  |\
| `rank_absolute` | integer | _absolute rank in SERP_<br>absolute position among all the elements in SERP |  |\
| `position` | string | _the alignment of the element in SERP_<br>can take the following values:<br>`left`, `right` |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `items` | array | _additional items present in the element_<br>if there are none, equals `null` |  |\
| `type` | string | _type of element = ‘ **people\_also\_ask\_element**‘_ |  |\
| `title` | string | _title of the result in SERP_ |  |\
| `seed_question` | string | _question that triggered additional expanded elements_ |  |\
| `xpath` | string | _the [XPath](https://en.wikipedia.org/wiki/XPath) of the element_ |  |\
| `expanded_element` | array | _expanded element_ |  |\
| `type` | string | _type of element = ‘ **people\_also\_ask\_expanded\_element**‘_ |  |\
| `featured_title` | string | _title_ |  |\
| `url` | string | _relevant URL_ |  |\
| `domain` | string | _domain in SERP_ |  |\
| code>title | string | _title of the result in SERP_ |\
| `description` | string | _description of the results element in SERP_ |  |\
| `images` | array | _images of the element_ |  |\
| `type` | string | _type of element = ‘ **images\_element**‘_ |  |\
| `alt` | string | _alt tag of the image_ |  |\
| `url` | string | _relevant URL_ |  |\
| `image_url` | string | _URL of the image_<br>the URL leading to the image on the original |  |



## DataForSEO API v.3: OnPage API Instant Pages

This function allows you to retrieve page-specific data with detailed information on how well a particular page is optimized for organic search. This endpoint operates based on the **Live method**, meaning it does not require a separate GET request to obtain task results.

### API Endpoint

**POST `https://api.dataforseo.com/v3/on_page/instant_pages`**

Your account will be charged for each request made to this endpoint.

### Authentication

To authenticate, you should use your credentials from `https://app.dataforseo.com/api-access`.

You will need to encode your `login:password` combination using Base64 for the `Authorization` header.

**Example Authentication Header:**
`Authorization: Basic {base64_encoded_login:password}`

### Request Parameters

All POST data must be sent in **JSON format** with **UTF-8 encoding**. The task setting is done using the POST method, where all task parameters are sent within a `task` array of the generic POST array.

You can send up to **2000 API requests per minute**, with each request containing no more than **20 tasks**. The maximum number of simultaneous requests is limited to **30**. In a single request, you can set up to 20 tasks, each containing one URL, but these URLs cannot contain more than **5 identical domains**.

**Request Body Example:**

```json
[
{
  "url": "https://dataforseo.com/blog",
  "enable_javascript": true,
  "custom_js": "meta = {}; meta.url = document.URL; meta;"
}
]
```
_This example shows setting a task with additional parameters._

**Description of Fields for Setting a Task:**

| Field Name                   | Type      | Description                                                                                                                                                                                       |url|string|*target page url* **required field**
absolute URL of the target page;
**Note #1:** results will be returned for the specified URL only;
**Note #2:** to prevent denial-of-service events, tasks that contain a duplicate crawl host will be returned with a 40501 error;
to prevent this error from occurring, avoid setting tasks with the same domain if at least one of your previous tasks with this domain (including a page URL on the domain) is still in a crawling queue|
|`custom_user_agent`          |string     |*custom user agent* optional field
custom user agent for crawling a website
example: `Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36`
default value: `Mozilla/5.0 (compatible; RSiteAuditor)`|
|`browser_preset`             |string     |*preset for browser screen parameters* optional field
If you use this field, you do not need to indicate `browser_screen_width`, `browser_screen_height`, `browser_screen_scale_factor`.
Possible values: `desktop`, `mobile`, `tablet`
`desktop` preset will apply: `browser_screen_width: 1920`, `browser_screen_height: 1080`, `browser_screen_scale_factor: 1`
`mobile` preset will apply: `browser_screen_width: 390`, `browser_screen_height: 844`, `browser_screen_scale_factor: 3`
`tablet` preset will apply: `browser_screen_width: 1024`, `browser_screen_height: 1366`, `browser_screen_scale_factor: 2`
**Note:** To use this parameter, set `enable_javascript` or `enable_browser_rendering` to `true`|
|`browser_screen_width`       |integer    |*browser screen width* optional field
You can set a custom browser screen width to perform an audit for a particular device.
If you use this field, `browser_preset` will be ignored.
**Note:** To use this parameter, set `enable_javascript` or `enable_browser_rendering` to `true`.
Minimum value, in pixels: `240`
Maximum value, in pixels: `9999`|
|`browser_screen_height`      |integer    |*browser screen height* optional field
You can set a custom browser screen height to perform an audit for a particular device.
If you use this field, `browser_preset` will be ignored.
**Note:** To use this parameter, set `enable_javascript` or `enable_browser_rendering` to `true`.
Minimum value, in pixels: `240`
Maximum value, in pixels: `9999`|
|`browser_screen_scale_factor`|float      |*browser screen scale factor* optional field
You can set a custom browser screen resolution ratio to perform an audit for a particular device.
If you use this field, `browser_preset` will be ignored.
**Note:** To use this parameter, set `enable_javascript` or `enable_browser_rendering` to `true`.
Minimum value: `0.5`
Maximum value: `3`|
|`store_raw_html`             |boolean    |*store HTML of a crawled page* optional field
Set to `true` if you want to get the HTML of the page using the OnPage Raw HTML endpoint.
Default value: `false`|
|`accept_language`            |string     |*language header for accessing the website* optional field
All locale formats are supported (xx, xx-XX, xxx-XX, etc.).
**Note:** If you do not specify this parameter, some websites may deny access; in this case, pages will be returned with `"type":"broken"` in the response array|
|`load_resources`             |boolean    |*load resources* optional field
Set to `true` if you want to load images, stylesheets, scripts, and broken resources.
Default value: `false`
**Note:** If you use this parameter, additional charges will apply. The cost can be calculated on the Pricing Page|
|`enable_javascript`          |boolean    |*load javascript on a page* optional field
Set to `true` if you want to load the scripts available on a page.
Default value: `false`
**Note:** If you use this parameter, additional charges will apply. The cost can be calculated on the Pricing Page|
|`enable_browser_rendering`   |boolean    |*emulate browser rendering to measure Core Web Vitals* optional field
By using this parameter, you will be able to emulate a browser when loading a web page.
`enable_browser_rendering` loads styles, images, fonts, animations, videos, and other resources on a page.
Default value: `false`
Set to `true` to obtain Core Web Vitals (FID, CLS, LCP) metrics in the response.
**If you use this field, parameters `enable_javascript` and `load_resources` are enabled automatically.**
**Note:** If you use this parameter, additional charges will apply. The cost can be calculated on the Pricing Page|
|`disable_cookie_popup`       |boolean    |*disable the cookie popup* optional field
Set to `true` if you want to disable the popup requesting cookie consent from the user.
Default value: `false`|
|`return_despite_timeout`     |boolean    |*return data on pages despite the timeout error* optional field
If `true`, data will be provided on pages that failed to load within 120 seconds and responded with a timeout error.
Default value: `false`|
|`enable_xhr`                 |boolean    |*enable XMLHttpRequest on a page* optional field
Set to `true` if you want our crawler to request data from a web server using the XMLHttpRequest object.
Default value: `false`
**Note:** If you use this field, `enable_javascript` must be set to `true`|
|`custom_js`                  |string     |*custom javascript* optional field
The execution time for the script you enter here should be **700 ms maximum**.
For example, you can use a JS snippet to check if the website contains Google Tag Manager.
The returned value depends on what you specify. For instance, `meta = {}; meta.url = document.URL; meta.test = 'test'; meta;` will return `"custom_js_response": { "url": "https://dataforseo.com/", "test": "test" }`.
**Note:** If you use this parameter, additional charges will apply. The cost can be calculated on the Pricing Page|
|`validate_micromarkup`       |boolean    |*enable microdata validation* optional field
If set to `true`, you can use the OnPage API Microdata endpoint with the `id` of the task.
Default value: `false`|
|`check_spell`                |boolean    |*check spelling* optional field
Set to `true` to check spelling on a website using the Hunspell library.
Default value: `false`|
|`checks_threshold`           |array      |*custom threshold values for checks* optional field
You can specify custom threshold values for the parameters included in the `checks` array of OnPage API responses.
**Note:** Only integer threshold values can be modified|
|`switch_pool`                |boolean    |*switch proxy pool* optional field
If `true`, additional proxy pools will be used to obtain the requested data.
This parameter can be used if a multitude of tasks is set simultaneously, resulting in occasional `rate-limit` and/or `site_unreachable` errors|
|`ip_pool_for_scan`           |string     |*proxy pool* optional field
You can choose a location of the proxy pool that will be used to obtain the requested data.
This parameter can be used if page content is inaccessible in one of the locations, resulting in occasional `site_unreachable` errors.
Possible values: `us`, `de`|

### Response Parameters

As a response, the API server will return JSON-encoded data containing a `tasks` array with information specific to the set tasks.

**General Response Fields:**

| Field Name     | Type    | Description                                                                                                                                              |
| :------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `version`      | string  | **The current version of the API**.                                                                                                                 |
| `status_code`  | integer | **General status code** of the request. A full list of response codes is available.                                                            |
| `status_message` | string  | **General informational message**. A full list of general informational messages is available.                                                 |
| `time`         | string  | **Execution time**, in seconds.                                                                                                                     |
| `cost`         | float   | Total **cost of tasks**, in USD.                                                                                                                    |
| `tasks_count`  | integer | The number of tasks in the `tasks` array.                                                                                                           |
| `tasks_error`  | integer | The number of tasks in the `tasks` array that returned with an error.                                                                               |
| `tasks`        | array   | **Array of tasks**. Each element represents a task submitted in the POST request.                                                                   |

**Fields within the `tasks` Array (for each task):**

| Field Name     | Type    | Description                                                                                                                                              |
| :------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `id`           | string  | **Task identifier**, a unique UUID in our system.                                                                                                   |
| `status_code`  | integer | **Status code of the task**, generated by DataForSEO (range: 10000-60000).                                                                        |
| `status_message` | string  | **Informational message of the task**.                                                                                                            |
| `time`         | string  | **Execution time** for the task, in seconds.                                                                                                        |
| `cost`         | float   | **Cost of the task**, in USD.                                                                                                                       |
| `result_count` | integer | **Number of elements in the `result` array**.                                                                                                       |
| `path`         | array   | **URL path** for the API endpoint (e.g., `["v3", "on_page", "instant_pages"]`).                                                                 |
| `data`         | object  | **Contains the same parameters that you specified in the POST request**.                                                                        |
| `result`       | array   | **Array of results**.                                                                                                                           |

**Fields within the `result` Array:**

The `result` array contains items with different `resource_type` values, each having specific fields.

---

#### **`resource_type`: 'html' (for HTML pages)**

| Field Name                     | Type    | Description                                                                                                                                                                                                                                                                     |
| :----------------------------- | :------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `crawl_progress`               | string  | **Status of the crawling session**. Possible values: `in_progress`, `finished`.                                                                                                                                                                                |
| `crawl_status`                 | object  | **Details of the crawling session**. In this case, the value will be `null`.                                                                                                                                                                                   |
| `crawl_gateway_address`        | string  | **Crawler IP address**. Displays the IP address used by the crawler to initiate the current crawling session.                                                                                                                                                 |
| `items_count`                  | integer | **Number of items** in the `items` array.                                                                                                                                                                                                                    |
| `items`                        | array   | **Items array**, representing the 'html' page.                                                                                                                                                                                                                 |
| `items.resource_type`          | string  | **Type of the returned resource**. Equals `'html'`.                                                                                                                                                                                                                |
| `items.status_code`            | integer | **Status code of the page** (e.g., 200).                                                                                                                                                                                                                       |
| `items.location`               | string  | **Location header**. Indicates the URL to redirect a page to.                                                                                                                                                                                                  |
| `items.url`                    | string  | **Page URL**.                                                                                                                                                                                                                                                  |
| `items.meta`                   | object  | **Page properties**.                                                                                                                                                                                                                                           |
| `items.meta.title`             | string  | **Page title**.                                                                                                                                                                                                                                                |
| `items.meta.charset`           | integer | **Code page** (e.g., 65001).                                                                                                                                                                                                                                   |
| `items.meta.follow`            | boolean | **Indicates whether a page’s ‘meta robots’ allows crawlers to follow the links on the page**. If `false`, the page’s ‘meta robots’ tag contains “nofollow”.                                                                                                 |
| `items.meta.generator`         | string  | **Meta tag generator**.                                                                                                                                                                                                                                        |
| `items.meta.htags`             | object  | **HTML header tags** (e.g., `h4`, `h1`, `h2` and their content).                                                                                                                                                                                              |
| `items.meta.description`       | string  | **Content of the meta description tag**.                                                                                                                                                                                                                       |
| `items.meta.favicon`           | string  | **Favicon of the page**.                                                                                                                                                                                                                                       |
| `items.meta.meta_keywords`     | string  | **Content of the `keywords` meta tag**.                                                                                                                                                                                                                        |
| `items.meta.canonical`         | string  | **Canonical page URL**.                                                                                                                                                                                                                                        |
| `items.meta.internal_links_count` | integer | **Number of internal links** on the page.                                                                                                                                                                                                                      |
| `items.meta.external_links_count` | integer | **Number of external links** on the page.                                                                                                                                                                                                                      |
| `items.meta.inbound_links_count` | integer | **Number of internal links pointing at the page**.                                                                                                                                                                                                             |
| `items.meta.images_count`      | integer | **Number of images** on the page.                                                                                                                                                                                                                              |
| `items.meta.images_size`       | integer | **Total size of images** on the page measured in bytes.                                                                                                                                                                                                        |
| `items.meta.scripts_count`     | integer | **Number of scripts** on the page.                                                                                                                                                                                                                             |
| `items.meta.scripts_size`      | integer | **Total size of scripts** on the page measured in bytes.                                                                                                                                                                                                       |
| `items.meta.stylesheets_count` | integer | **Number of stylesheets** on the page.                                                                                                                                                                                                                         |
| `items.meta.stylesheets_size`  | integer | **Total size of stylesheets** on the page measured in bytes.                                                                                                                                                                                                   |
| `items.meta.title_length`      | integer | **Length of the `title` tag** in characters.                                                                                                                                                                                                                   |
| `items.meta.description_length` | integer | **Length of the `description` tag** in characters.                                                                                                                                                                                                             |
| `items.meta.render_blocking_scripts_count` | integer | **Number of scripts on the page that block page rendering**.                                                                                                                                                                                           |
| `items.meta.render_blocking_stylesheets_count` | integer | **Number of CSS styles on the page that block page rendering**.                                                                                                                                                                                  |
| `items.meta.cumulative_layout_shift` | float | **Core Web Vitals metric measuring the layout stability of the page**. Sum total of all individual layout shift scores for every unexpected layout shift during the page's lifespan.                                                                       |
| `items.meta.meta_title`        | string  | **Meta title of the page**. The meta tag in the head section of an HTML document that defines the title of a page.                                                                                                                                              |
| `items.meta.content`           | object  | **Overall information about content of the page**.                                                                                                                                                                                                             |
| `items.meta.content.plain_text_size` | integer | **Total size of the text** on the page measured in bytes.                                                                                                                                                                                                      |
| `items.meta.content.plain_text_rate` | integer | **Plaintext rate value**. `plain_text_size` to `size` ratio.                                                                                                                                                                                                   |
| `items.meta.content.plain_text_word_count` | float | **Number of words** on the page.                                                                                                                                                                                                                               |
| `items.meta.content.automated_readability_index` | float | **Automated Readability Index**.                                                                                                                                                                                                                             |
| `items.meta.content.coleman_liau_readability_index` | float | **Coleman–Liau Index**.                                                                                                                                                                                                                                        |
| `items.meta.content.dale_chall_readability_index` | float | **Dale–Chall Readability Index**.                                                                                                                                                                                                                              |
| `items.meta.content.flesch_kincaid_readability_index` | float | **Flesch–Kincaid Readability Index**.                                                                                                                                                                                                                          |
| `items.meta.content.smog_readability_index` | float | **SMOG Readability Index**.                                                                                                                                                                                                                                |
| `items.meta.content.description_to_content_consistency` | float | **Consistency of the meta `description` tag with the page content**, measured from 0 to 1.                                                                                                                                                                 |
| `items.meta.content.title_to_content_consistency` | float | **Consistency of the meta `title` tag with the page content**, measured from 0 to 1.                                                                                                                                                                       |
| `items.meta.content.meta_keywords_to_content_consistency` | float | **Consistency of meta `keywords` tag with the page content**, measured from 0 to 1.                                                                                                                                                                |
| `items.meta.deprecated_tags`   | array   | **Deprecated tags** on the page.                                                                                                                                                                                                                               |
| `items.meta.duplicate_meta_tags` | array   | **Duplicate meta tags** on the page (e.g., "generator").                                                                                                                                                                                                         |
| `items.meta.spell`             | object  | **Spellcheck** results, including Hunspell spellcheck errors.                                                                                                                                                                                                  |
| `items.meta.spell.hunspell_language_code` | string  | **Spellcheck language code**.                                                                                                                                                                                                                                  |
| `items.meta.spell.misspelled`  | array   | **Array of misspelled words**.                                                                                                                                                                                                                                   |
| `items.meta.spell.misspelled.word` | string  | **Misspelled word**.                                                                                                                                                                                                                                               |
| `items.meta.social_media_tags` | object  | **Object of social media tags** found on the page, containing tags and their content (e.g., Open Graph and Twitter card tags like `og:locale`, `og:type`, `twitter:card`, etc.).                                                                         |
| `page_timing`                  | object  | **Object of page load metrics**.                                                                                                                                                                                                                               |
| `page_timing.time_to_interactive` | integer | **Time To Interactive (TTI) metric**. The time until the user can interact with a page (in milliseconds).                                                                                                                                                      |
| `page_timing.dom_complete`     | integer | **Time to load resources**. The time until the page and all of its subresources are downloaded (in milliseconds).                                                                                                                                              |
| `page_timing.largest_contentful_paint` | float | **Core Web Vitals metric measuring how fast the largest above-the-fold content element is displayed** (in milliseconds).                                                                                                                                   |
| `page_timing.first_input_delay` | float   | **Core Web Vitals metric indicating the responsiveness of a page**. Time from user interaction to browser response (in milliseconds).                                                                                                                           |
| `page_timing.connection_time`  | integer | **Time to connect to a server** (in milliseconds).                                                                                                                                                                                                             |
| `page_timing.time_to_secure_connection` | integer | **Time to establish a secure connection** (in milliseconds).                                                                                                                                                                                                 |
| `page_timing.request_sent_time` | integer | **Time to send a request to a server** (in milliseconds).                                                                                                                                                                                                      |
| `page_timing.waiting_time`     | integer | **Time to first byte (TTFB)** in milliseconds.                                                                                                                                                                                                                 |
| `page_timing.download_time`    | integer | **Time it takes for a browser to receive a response** (in milliseconds).                                                                                                                                                                                       |
| `page_timing.duration_time`    | integer | **Total time it takes until a browser receives a complete response from a server** (in milliseconds).                                                                                                                                                          |
| `page_timing.fetch_start`      | integer | **Time to start downloading the HTML resource**. The amount of time the browser needs to start downloading a page.                                                                                                                                               |
| `page_timing.fetch_end`        | integer | **Time to complete downloading the HTML resource**. The amount of time the browser needs to complete downloading a page.                                                                                                                                           |
| `onpage_score`                 | float   | **Shows how page is optimized on a 100-point scale**. 100 is the highest possible score.                                                                                                                                                                       |
| `total_dom_size`               | integer | **Total DOM size of a page**.                                                                                                                                                                                                                                  |
| `custom_js_response`           | string/object/integer | **The result of executing a specified JS script**. The field type and value depend on the script specified in the `custom_js` field. Results can be filtered by this value.                                                                 |
| `custom_js_client_exception`   | string  | **Error when executing a custom JS**. If an error occurred, the error message will be displayed here.                                                                                                                                                          |
| `resource_errors`              | object  | **Resource errors and warnings**.                                                                                                                                                                                                                              |
| `resource_errors.errors`       | array   | **Resource errors**.                                                                                                                                                                                                                                           |
| `resource_errors.errors.line`  | integer | **Line where the error was found**.                                                                                                                                                                                                                            |
| `resource_errors.errors.column` | integer | **Column where the error was found**.                                                                                                                                                                                                                          |
| `resource_errors.errors.message` | string  | **Text message of the error**. Possible HTML errors are available.                                                                                                                                                                                      |
| `resource_errors.errors.status_code` | integer | **Status code of the error**. Possible values: `0` (Unidentified), `501` (Html Parse Error), `1501` (JS Parse Error), `2501` (CSS Parse Error), `3501` (Image Parse Error), `3502` (Image Scale Is Zero), `3503` (Image Size Is Zero), `3504` (Image Format Invalid). |
| `resource_errors.warnings`     | array   | **Resource warnings**.                                                                                                                                                                                                                                         |
| `resource_errors.warnings.line` | integer | **Line the warning relates to**. `0` means the warning relates to the whole page.                                                                                                                                                                             |
| `resource_errors.warnings.column` | integer | **Column the warning relates to**. `0` means the warning relates to the whole page.                                                                                                                                                                           |
| `resource_errors.warnings.message` | string  | **Text message of the warning**. Possible messages: "Has node with more than 60 childs.", "Has more that 1500 nodes.", "HTML depth more than 32 tags.".                                                                                                    |
| `resource_errors.warnings.status_code` | integer | **Status code of the warning**. Possible values: `0` (Unidentified), `1` (Has node with more than 60 childs), `2` (Has more that 1500 nodes), `3` (HTML depth more than 32 tags).                                                                     |
| `broken_resources`             | boolean | **Indicates whether a page contains broken resources**.                                                                                                                                                                                                        |
| `broken_links`                 | boolean | **Indicates whether a page contains broken links**.                                                                                                                                                                                                            |
| `duplicate_title`              | boolean | **Indicates whether a page has duplicate `title` tags**.                                                                                                                                                                                                       |
| `duplicate_description`        | boolean | **Indicates whether a page has a duplicate description**.                                                                                                                                                                                                      |
| `duplicate_content`            | boolean | **Indicates whether a page has duplicate content**.                                                                                                                                                                                                            |
| `click_depth`                  | integer | **Number of clicks it takes to get to the page** from the homepage.                                                                                                                                                                                            |
| `size`                         | integer | **Resource size** in bytes.                                                                                                                                                                                                                                    |
| `encoded_size`                 | integer | **Page size after encoding** in bytes.                                                                                                                                                                                                                         |
| `total_transfer_size`          | integer | **Compressed page size** in bytes.                                                                                                                                                                                                                             |
| `fetch_time`                   | string  | **Date and time when a resource was fetched** in UTC format: “yyyy-mm-dd hh-mm-ss +00:00”.                                                                                                                                                              |
| `cache_control`                | object  | **Instructions for caching**.                                                                                                                                                                                                                                  |
| `cache_control.cachable`       | boolean | **Indicates whether the page is cacheable**.                                                                                                                                                                                                                   |
| `cache_control.ttl`            | integer | **Time to live**. The amount of time the browser caches a resource.                                                                                                                                                                                            |
| `checks`                       | object  | **Website checks** related to the page.                                                                                                                                                                                                                        |
| `checks.no_content_encoding`   | boolean | **Page with no content encoding**. Indicates whether a page has no compression algorithm of the content.                                                                                                                                                       |
| `checks.high_loading_time`     | boolean | **Page with high loading time**. Indicates whether a page loading time exceeds 3 seconds.                                                                                                                                                                      |
| `checks.is_redirect`           | boolean | **Page with redirects**. Indicates whether a page has `3XX` redirects to other pages.                                                                                                                                                                          |
| `checks.is_4xx_code`           | boolean | **Page with `4xx` status codes**. Indicates whether a page has a `4xx` response code.                                                                                                                                                                        |
| `checks.is_5xx_code`           | boolean | **Page with `5xx` status codes**. Indicates whether a page has a `5xx` response code.                                                                                                                                                                        |
| `checks.is_broken`             | boolean | **Broken page**. Indicates whether a page returns a response code less than `200` or greater than `400`.                                                                                                                                                   |
| `checks.is_www`                | boolean | **Page with www**. Indicates whether a page is on a `www` subdomain.                                                                                                                                                                                           |
| `checks.is_https`              | boolean | **Page with the https protocol**.                                                                                                                                                                                                                              |
| `checks.is_http`               | boolean | **Page with the http protocol**.                                                                                                                                                                                                                               |
| `checks.high_waiting_time`     | boolean | **Page with high waiting time**. Indicates whether a page waiting time (Time to First Byte) exceeds 1.5 seconds.                                                                                                                                              |
| `checks.has_micromarkup`       | boolean | **Page contains microdata markup**.                                                                                                                                                                                                                                |
| `checks.has_micromarkup_errors` | boolean | **Page contains microdata markup errors**.                                                                                                                                                                                                                         |
| `checks.no_doctype`            | boolean | **Page with no doctype**. Indicates whether a page is without the `<!DOCTYPE HTML>` declaration.                                                                                                                                                            |
| `checks.has_html_doctype`      | boolean | **Page with HTML doctype declaration**. `true` if the page has HTML `DOCTYPE` declaration.                                                                                                                                                                   |
| `checks.canonical`             | boolean | **Page is canonical**.                                                                                                                                                                                                                                         |
| `checks.no_encoding_meta_tag`  | boolean | **Page with no meta tag encoding**. Indicates whether a page is without `Content-Type`. Available for pages with `canonical` check set to `true`.                                                                                                       |
| `checks.no_h1_tag`             | boolean | **Page with empty or absent h1 tags**. Available for pages with `canonical` check set to `true`.                                                                                                                                                            |
| `checks.https_to_http_links`   | boolean | **HTTPS page has links to HTTP pages**. `true` if this HTTPS page has links to HTTP pages. Available for pages with `canonical` check set to `true`.                                                                                                        |
| `checks.size_greater_than_3mb` | boolean | **Page with size larger than 3 MB**. `true` if the page size is exceeding 3 MB. Available for pages with `canonical` check set to `true`.                                                                                                                   |
| `checks.meta_charset_consistency` | boolean | **Consistency between charset encoding and page charset**. `true` if the page’s charset encoding doesn’t match the actual charset of the page. Available for pages with `canonical` check set to `true`.                                                      |
| `checks.has_meta_refresh_redirect` | boolean | **Pages with meta refresh redirect**. `true` if the page has `<meta http-equiv=”refresh”>` tag. Available for pages with `canonical` check set to `true`.                                                                                                 |
| `checks.has_render_blocking_resources` | boolean | **Page with render-blocking resources**. `true` if the page has render-blocking scripts or stylesheets. Available for pages with `canonical` check set to `true`.                                                                                      |
| `checks.low_content_rate`      | boolean | **Page with low content rate**. Indicates whether a page has the `plaintext size` to `page size` ratio of less than 0.1. Available for pages with `canonical` check set to `true`.                                                                            |
| `checks.high_content_rate`     | boolean | **Page with high content rate**. Indicates whether a page has the `plaintext size` to `page size` ratio of more than 0.9. Available for pages with `canonical` check set to `true`.                                                                           |
| `checks.low_character_count`   | boolean | **Indicates whether the page has less than 1024 characters**. Available for pages with `canonical` check set to `true`.                                                                                                                                      |
| `checks.high_character_count`  | boolean | **Indicates whether the page has more than 256,000 characters**. Available for pages with `canonical` check set to `true`.                                                                                                                                     |
| `checks.small_page_size`       | boolean | **Indicates whether a page is too small**. `true` if a page size is smaller than 1024 bytes. Available for pages with `canonical` check set to `true`.                                                                                                       |
| `checks.large_page_size`       | boolean | **Indicates whether a page is too heavy**. `true` if a page size exceeds 1 megabyte. Available for pages with `canonical` check set to `true`.                                                                                                               |
| `checks.low_readability_rate`  | boolean | **Page with a low readability rate**. Indicates whether a page is scored less than 15 points on the Flesch–Kincaid readability test. Available for pages with `canonical` check set to `true`.                                                                |
| `checks.irrelevant_description` | boolean | **Page with irrelevant description**. Indicates whether a page `description` tag is irrelevant to the content of a page (relevance threshold is 0.2). Available for pages with `canonical` check set to `true`.                                                 |
| `checks.irrelevant_title`      | boolean | **Page with irrelevant title**. Indicates whether a page `title` tag is irrelevant to the content of the page (relevance threshold is 0.3). Available for pages with `canonical` check set to `true`.                                                         |
| `checks.irrelevant_meta_keywords` | boolean | **Page with irrelevant meta keywords**. Indicates whether a page `keywords` tags are irrelevant to the content of a page (relevance threshold is 0.6). Available for pages with `canonical` check set to `true`.                                              |
| `checks.title_too_long`        | boolean | **Page with a long title**. Indicates whether the content of the `title` tag exceeds 65 characters. Available for pages with `canonical` check set to `true`.                                                                                                 |
| `checks.has_meta_title`        | boolean | **Page has a meta title**. Indicates whether the HTML of a page contains the `meta_title` tag. Available for pages with `canonical` check set to `true`.                                                                                                   |
| `checks.title_too_short`       | boolean | **Page with short titles**. Indicates whether the content of `title` tag is shorter than 30 characters. Available for pages with `canonical` check set to `true`.                                                                                             |
| `checks.deprecated_html_tags`  | boolean | **Page with deprecated tags**. Indicates whether a page has deprecated HTML tags. Available for pages with `canonical` check set to `true`.                                                                                                                 |
| `checks.duplicate_meta_tags`   | boolean | **Page with duplicate meta tags**. Indicates whether a page has more than one meta tag of the same type. Available for pages with `canonical` check set to `true`.                                                                                          |
| `checks.duplicate_title_tag`   | boolean | **Page with more than one title tag**. Indicates whether a page has more than one `title` tag. Available for pages with `canonical` check set to `true`.                                                                                                    |
| `checks.no_image_alt`          | boolean | **Images without `alt` tags**. Available for pages with `canonical` check set to `true`.                                                                                                                                                                     |
| `checks.no_image_title`        | boolean | **Images without `title` tags**. Available for pages with `canonical` check set to `true`.                                                                                                                                                                   |
| `checks.no_description`        | boolean | **Pages with no description**. Indicates whether a page has an empty or absent `description` meta tag. Available for pages with `canonical` check set to `true`.                                                                                             |
| `checks.no_title`              | boolean | **Page with no title**. Indicates whether a page has an empty or absent `title` tag. Available for pages with `canonical` check set to `true`.                                                                                                             |
| `checks.no_favicon`            | boolean | **Page with no favicon**. Available for pages with `canonical` check set to `true`.                                                                                                                                                                          |
| `checks.seo_friendly_url`      | boolean | **Page with seo-friendly URL**. Checked by four parameters: relative path length < 120 chars, no special characters, no dynamic parameters, URL relevance to the page. If any fail, URL is not SEO-friendly. Available for pages with `canonical` check set to `true`. |
| `checks.flash`                 | boolean | **Page with flash**. Indicates whether a page has flash elements.                                                                                                                                                                                              |
| `checks.frame`                 | boolean | **Page with frames**. Indicates whether a page contains `frame`, `iframe`, `frameset` tags.                                                                                                                                                                  |
| `checks.lorem_ipsum`           | boolean | **Page with lorem ipsum**. Indicates whether a page has *lorem ipsum* content. Available for pages with `canonical` check set to `true`.                                                                                                                 |
| `checks.has_misspelling`       | boolean | **Page with misspelling**. Indicates whether a page has spelling mistakes. Informative if `check_spell` was set to `true` in the POST array.                                                                                                                  |
| `checks.seo_friendly_url_characters_check` | boolean | **URL characters check-up**. Indicates whether a page URL contains only uppercase/lowercase Latin characters, digits, and dashes.                                                                                                                  |
| `checks.seo_friendly_url_dynamic_check` | boolean | **URL dynamic check-up**. `true` if a page has no dynamic parameters in the URL.                                                                                                                                                                     |
| `checks.seo_friendly_url_keywords_check` | boolean | **URL keyword check-up**. Indicates whether a page URL is consistent with the `title` meta tag.                                                                                                                                                  |
| `checks.seo_friendly_url_relative_length_check` | boolean | **URL length check-up**. `true` if a page URL is no longer than 120 characters.                                                                                                                                                              |
| `content_encoding`             | string  | **Type of encoding** (e.g., "br").                                                                                                                                                                                                                             |
| `media_type`                   | string  | **Types of media** used to display a page (e.g., "text/html").                                                                                                                                                                                                 |
| `server`                       | string  | **Server version** (e.g., "cloudflare").                                                                                                                                                                                                                       |
| `is_resource`                  | boolean | **Indicates whether a page is a single resource**.                                                                                                                                                                                                             |
| `url_length`                   | integer | **Page URL length** in characters.                                                                                                                                                                                                                                 |
| `relative_url_length`          | integer | **Relative URL length** in characters.                                                                                                                                                                                                                             |
| `last_modified`                | object  | **Contains data on changes related to the resource**. `null` if no data.                                                                                                                                                                                       |
| `last_modified.header`         | string  | **Date and time when the header was last modified** in UTC format. `null` if no data.                                                                                                                                                                     |
| `last_modified.sitemap`        | string  | **Date and time when the sitemap was last modified** in UTC format. `null` if no data.                                                                                                                                                                    |
| `last_modified.meta_tag`       | string  | **Date and time when the meta tag was last modified** in UTC format. `null` if no data.                                                                                                                                                                   |

---

#### **`resource_type`: 'broken' (for broken pages)**

| Field Name                   | Type    | Description                                                                                                                                                                                  |
| :--------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `resource_type`              | string  | Type of the returned resource. Equals `'broken'`.                                                                                                                                       |
| `status_code`                | integer | **Status code of the page**.                                                                                                                                                            |
| `location`                   | string  | **Location header**. Indicates the URL to redirect a page to.                                                                                                                           |
| `url`                        | string  | **Page URL**.                                                                                                                                                                           |
| `size`                       | integer | **Resource size** in bytes.                                                                                                                                                             |
| `encoded_size`               | integer | **Page size after encoding** in bytes.                                                                                                                                                  |
| `total_transfer_size`        | integer | **Compressed page size** in bytes.                                                                                                                                                      |
| `fetch_time`                 | string  | **Date and time when a resource was fetched** in UTC format.                                                                                                                            |
| `fetch_timing`               | object  | **Time range within which a result was fetched**.                                                                                                                                       |
| `fetch_timing.duration_time` | integer | **Indicates how many seconds it took to download a page**.                                                                                                                              |
| `fetch_timing.fetch_start`   | integer | **Time to start downloading the HTML resource**.                                                                                                                                        |
| `fetch_timing.fetch_end`     | integer | **Time to complete downloading the HTML resource**.                                                                                                                                     |
| `cache_control`              | object  | **Instructions for caching**.                                                                                                                                                           |
| `cache_control.cachable`     | boolean | **Indicates whether the page is cacheable**.                                                                                                                                            |
| `cache_control.ttl`          | integer | **Time to live**. The amount of time the browser caches a resource.                                                                                                                     |
| `checks`                     | object  | **On-page check-ups**.                                                                                                                                                                  |
| `checks.no_content_encoding` | boolean | **Page with no content encoding**.                                                                                                                                                      |
| `checks.high_loading_time`   | boolean | **Page with high loading time**.                                                                                                                                                        |
| `checks.is_redirect`         | boolean | **Page with redirects**.                                                                                                                                                                |
| `checks.is_4xx_code`         | boolean | **Page with `4xx` status codes**.                                                                                                                                                       |
| `checks.is_5xx_code`         | boolean | **Page with `5xx` status codes**.                                                                                                                                                       |
| `checks.is_broken`           | boolean | **Broken page**.                                                                                                                                                                        |
| `checks.is_www`              | boolean | **Page with www**.                                                                                                                                                                      |
| `checks.is_https`            | boolean | **Page with the https protocol**.                                                                                                                                                       |
| `checks.is_http`             | boolean | **Page with the http protocol**.                                                                                                                                                        |
| `resource_errors`            | object  | **Resource errors and warnings**. (Same sub-fields as for 'html' resource type: `errors`, `errors.line`, `errors.column`, `errors.message`, `errors.status_code`, `warnings`, `warnings.line`, `warnings.column`, `warnings.message`, `warnings.status_code`). |
| `content_encoding`           | string  | **Type of encoding**.                                                                                                                                                                   |
| `media_type`                 | string  | **Types of media** used to display a page (e.g., "text/html").                                                                                                                          |
| `server`                     | string  | **Server version**.                                                                                                                                                                     |
| `is_resource`                | boolean | **Indicates whether a page is a single resource**.                                                                                                                                      |
| `last_modified`              | object  | **Contains data on changes related to the resource**. `null` if no data. (Same sub-fields as for 'html' resource type: `header`, `sitemap`, `meta_tag`).                           |

---

#### **`resource_type`: 'redirect' (for redirect pages)**

| Field Name                   | Type    | Description                                                                                                                                                                                  |
| :--------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `resource_type`              | string  | Type of the returned resource. Equals `'redirect'`.                                                                                                                                     |
| `status_code`                | integer | **Status code of the page**.                                                                                                                                                            |
| `location`                   | string  | **Target URL** for "redirect" resources.                                                                                                                                                |
| `url`                        | string  | **Source URL** for "redirect" resources.                                                                                                                                                |
| `size`                       | integer | **Resource size** in bytes. Equals `0` for "redirect" resources.                                                                                                                        |
| `encoded_size`               | integer | **Page size after encoding**. Equals `0` for "redirect" resources.                                                                                                                      |
| `total_transfer_size`        | integer | **Compressed page size** in bytes.                                                                                                                                                      |
| `fetch_time`                 | string  | **Date and time when a resource was fetched** in UTC format.                                                                                                                            |
| `fetch_timing`               | object  | **Time range within which a result was fetched**.                                                                                                                                       |
| `fetch_timing.duration_time` | integer | **Indicates how many seconds it took to download a page**.                                                                                                                              |
| `fetch_timing.fetch_start`   | integer | **Time to start downloading the HTML resource**.                                                                                                                                        |
| `fetch_timing.fetch_end`     | integer | **Time to complete downloading the HTML resource**.                                                                                                                                     |
| `resource_errors`            | object  | **Resource errors and warnings**. (Same sub-fields as for 'html' resource type: `errors`, `errors.line`, `errors.column`, `errors.message`, `errors.status_code`, `warnings`, `warnings.line`, `warnings.column`, `warnings.message`, `warnings.status_code`). |
| `cache_control`              | object  | **Instructions for caching**. (Same sub-fields as for 'html' resource type: `cachable`, `ttl`).                                                                                   |
| `checks`                     | object  | **On-page check-ups**. (Same sub-fields as for 'html' resource type regarding loading/status codes/protocols: `no_content_encoding`, `high_loading_time`, `is_redirect`, `is_4xx_code`, `is_5xx_code`, `is_broken`, `is_www`, `is_https`, `is_http`). |
| `content_encoding`           | string  | **Type of encoding**.                                                                                                                                                                   |
| `media_type`                 | string  | **Types of media** used to display a page (e.g., "text/html").                                                                                                                          |
| `server`                     | string  | **Server version**.                                                                                                                                                                     |
| `is_resource`                | boolean | **Indicates whether a page is a single resource**.                                                                                                                                      |
| `last_modified`              | object  | **Contains data on changes related to the resource**. `null` if no data. (Same sub-fields as for 'html' resource type: `header`, `sitemap`, `meta_tag`).                           |

---

#### **`resource_type`: 'script', 'image', 'stylesheet' (for resources)**
(Note: These types of resources are displayed only if the first URL to crawl is a script, image, or stylesheet.)

| Field Name                   | Type    | Description                                                                                                                                                                                  |
| :--------------------------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `resource_type`              | string  | **Type of the returned resource**. Possible types: `script`, `image`, `stylesheet`.                                                                                                     |
| `meta`                       | object  | **Resource properties**. Available only for items with `resource_type: 'image'`.                                                                                                        |
| `meta.alternative_text`      | string  | **Content of the image `alt` attribute**.                                                                                                                                               |
| `meta.title`                 | string  | **Title**.                                                                                                                                                                              |
| `meta.original_width`        | integer | **Original image width** in px.                                                                                                                                                         |
| `meta.original_height`       | integer | **Original image height** in px.                                                                                                                                                        |
| `meta.width`                 | integer | **Image width** in px.                                                                                                                                                                  |
| `meta.height`                | integer | **Image height** in px.                                                                                                                                                                 |
| `status_code`                | integer | **Status code of the page** where a given resource is located.                                                                                                                          |
| `location`                   | string  | **Location header**. Indicates the URL to redirect a page to.                                                                                                                           |
| `url`                        | string  | **Resource URL**.                                                                                                                                                                       |
| `size`                       | integer | **Resource size** in bytes.                                                                                                                                                             |
| `encoded_size`               | integer | **Resource size after encoding** in bytes.                                                                                                                                              |
| `total_transfer_size`        | integer | **Compressed resource size** in bytes.                                                                                                                                                  |
| `fetch_time`                 | string  | **Date and time when a resource was fetched** in UTC format.                                                                                                                            |
| `fetch_timing`               | object  | **Resource fetching time range**.                                                                                                                                                       |
| `fetch_timing.duration_time` | integer | **Indicates how many milliseconds it took to fetch a resource**.                                                                                                                        |
| `fetch_timing.fetch_start`   | integer | **Time to start downloading the resource**.                                                                                                                                             |
| `fetch_timing.fetch_end`     | integer | **Time to complete downloading the resource**.                                                                                                                                          |
| `cache_control`              | object  | **Instructions for caching**. (Same sub-fields as for 'html' resource type: `cachable`, `ttl`).                                                                                   |
| `checks`                     | object  | **Resource check-ups**. Contents depend on the `resource_type`.                                                                                                                         |
| `checks.no_content_encoding` | boolean | **Resource with no content encoding**.                                                                                                                                                  |
| `checks.high_loading_time`   | boolean | **Resource with high loading time**.                                                                                                                                                    |
| `checks.is_redirect`         | boolean | **Resource with redirects**.                                                                                                                                                            |
| `checks.is_4xx_code`         | boolean | **Resource with `4xx` status codes**.                                                                                                                                                   |
| `checks.is_5xx_code`         | boolean | **Resource with `5xx` status codes**.                                                                                                                                                   |
| `checks.is_broken`           | boolean | **Broken resource**.                                                                                                                                                                    |
| `checks.is_www`              | boolean | **Page with www**.                                                                                                                                                                      |
| `checks.is_https`            | boolean | **Page with the https protocol**.                                                                                                                                                       |
| `checks.is_http`             | boolean | **Page with the http protocol**.                                                                                                                                                        |
| `checks.is_minified`         | boolean | **Resource is minified**. Indicates whether the content of a stylesheet or script is minified. Available for `stylesheet`, `script`.                                                      |
| `checks.has_redirect`        | boolean | **Resource has a redirect**. Available for `script`, `image`. Indicates redirects pointing at the resource or if the script contains a redirect.                                        |
| `checks.has_subrequests`     | boolean | **Resource contains subrequests**. Indicates whether the content of a stylesheet or script contain additional requests. Available for `stylesheet`, `script`.                           |
| `checks.original_size_displayed` | boolean | **Image displayed in its original size**. Available only for `image`.                                                                                                                   |
| `resource_errors`            | object  | **Resource errors and warnings**. (Same sub-fields as for 'html' resource type: `errors`, `errors.line`, `errors.column`, `errors.message`, `errors.status_code`, `warnings`, `warnings.line`, `warnings.column`, `warnings.message`, `warnings.status_code`). |
| `content_encoding`           | string  | **Type of encoding**.                                                                                                                                                                   |
| `media_type`                 | string  | **Types of media** used to display a resource.                                                                                                                                          |
| `accept_type`                | string  | **Indicates the expected type of resource**. For a broken resource, indicates its original type. Possible values: `any`, `none`, `image`, `sitemap`, `robots`, `script`, `stylesheet`, `redirect`, `html`, `text`, `other`, `font`. |
| `server`                     | string  | **Server version**.                                                                                                                                                                     |
| `last_modified`              | object  | **Contains data on changes related to the resource**. `null` if no data. (Same sub-fields as for 'html' resource type: `header`, `sitemap`, `meta_tag`).                           |

--- 

Based on the sources provided, this documentation covers two distinct endpoints within the DataForSEO Labs API: `dataforseo_labs/google/keyword_ideas/live` and `dataforseo_labs/google/serp_competitors/live`.

***


DocsDocs](https://platform.openai.com/docs) [API referenceAPI](https://platform.openai.com/docs/api-reference/responses)
Log in [Sign up](https://platform.openai.com/signup)
Search`` `K`
Get started
[Overview](https://platform.openai.com/docs/overview)
[Quickstart](https://platform.openai.com/docs/quickstart)
[Models](https://platform.openai.com/docs/models)
[Pricing](https://platform.openai.com/docs/pricing)
[Libraries](https://platform.openai.com/docs/libraries)
Core concepts
[Text generation](https://platform.openai.com/docs/guides/text)
[Images and vision](https://platform.openai.com/docs/guides/images-vision)
[Audio and speech](https://platform.openai.com/docs/guides/audio)
[Structured output](https://platform.openai.com/docs/guides/structured-outputs)
[Function calling](https://platform.openai.com/docs/guides/function-calling)
[Using GPT-5](https://platform.openai.com/docs/guides/latest-model)
[Migrate to Responses API](https://platform.openai.com/docs/guides/migrate-to-responses)
Agents
[Overview](https://platform.openai.com/docs/guides/agents)
Build agents
Deploy in your product
Optimize
[Voice agents](https://platform.openai.com/docs/guides/voice-agents)
Tools
[Using tools](https://platform.openai.com/docs/guides/tools)
[Connectors and MCP](https://platform.openai.com/docs/guides/tools-connectors-mcp)
[Web search](https://platform.openai.com/docs/guides/tools-web-search)
[Code interpreter](https://platform.openai.com/docs/guides/tools-code-interpreter)
File search and retrieval
More tools
Run and scale
[Conversation state](https://platform.openai.com/docs/guides/conversation-state)
[Background mode](https://platform.openai.com/docs/guides/background)
[Streaming](https://platform.openai.com/docs/guides/streaming-responses)
[Webhooks](https://platform.openai.com/docs/guides/webhooks)
[File inputs](https://platform.openai.com/docs/guides/pdf-files)
Prompting
Reasoning
Evaluation
[Getting started](https://platform.openai.com/docs/guides/evaluation-getting-started)
[Working with evals](https://platform.openai.com/docs/guides/evals)
[Prompt optimizer](https://platform.openai.com/docs/guides/prompt-optimizer)
[External models](https://platform.openai.com/docs/guides/external-models)
[Best practices](https://platform.openai.com/docs/guides/evaluation-best-practices)
Realtime API
[Overview](https://platform.openai.com/docs/guides/realtime)
Connect
Usage
Model optimization
[Optimization cycle](https://platform.openai.com/docs/guides/model-optimization)
Fine-tuning
[Graders](https://platform.openai.com/docs/guides/graders)
Specialized models
[Image generation](https://platform.openai.com/docs/guides/image-generation)
[Video generation](https://platform.openai.com/docs/guides/video-generation)
[Text to speech](https://platform.openai.com/docs/guides/text-to-speech)
[Speech to text](https://platform.openai.com/docs/guides/speech-to-text)
[Deep research](https://platform.openai.com/docs/guides/deep-research)
[Embeddings](https://platform.openai.com/docs/guides/embeddings)
[Moderation](https://platform.openai.com/docs/guides/moderation)
Coding agents
[Codex cloud](https://developers.openai.com/codex/cloud)
[Agent internet access](https://developers.openai.com/codex/cloud/agent-internet)
[Local shell tool](https://platform.openai.com/docs/guides/tools-local-shell)
[Codex CLI](https://developers.openai.com/codex/cli)
[Codex IDE](https://developers.openai.com/codex/ide)
[Codex changelog](https://developers.openai.com/codex/changelog)
Going live
[Production best practices](https://platform.openai.com/docs/guides/production-best-practices)
Latency optimization
Cost optimization
[Accuracy optimization](https://platform.openai.com/docs/guides/optimizing-llm-accuracy)
Safety
Specialized APIs
Assistants API
Resources
[Terms and policies](https://openai.com/policies)
[Changelog](https://platform.openai.com/docs/changelog)
[Your data](https://platform.openai.com/docs/guides/your-data)
[Rate limits](https://platform.openai.com/docs/guides/rate-limits)
[Deprecations](https://platform.openai.com/docs/deprecations)
[MCP for deep research](https://platform.openai.com/docs/mcp)
[Developer mode](https://platform.openai.com/docs/guides/developer-mode)
ChatGPT Actions
[Cookbook](https://cookbook.openai.com/) [Forum](https://community.openai.com/categories)
</file>

<file path=".gitignore">
# Python
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
env/
venv/
pip-wheel-metadata/
.eggs/
*.egg-info/
*.egg

# Node
node_modules/
npm-debug.log
yarn-error.log
package-lock.json

# General
.DS_Store
.env
data/
.vscode/
.idea/
*.log

# Repomix output
repomix-output.xml
backend/repomix-output.xml
repomix-output.xml
</file>

<file path="docker-compose.yml">
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    env_file:
      - ./.env
    volumes:
      - ./data:/app/data

  frontend:
    build: ./client/my-content-app
    ports:
      - "3000:80"
    depends_on:
      - backend
</file>

<file path="get_opportunity.py">
import json
from backend.data_access.database_manager import DatabaseManager
from backend.app_config.manager import ConfigManager


def get_opportunity_data(opportunity_id: int):
    """Connects to the database and fetches a single opportunity, including job status."""
    config_manager = ConfigManager()
    db_manager = DatabaseManager(cfg_manager=config_manager)

    opportunity = db_manager.get_opportunity_by_id(opportunity_id)

    if opportunity:
        # The data is already a dict-like object, so we can just print it.
        # The `full_data` field is a JSON string, so we should parse it.
        if opportunity.get("full_data") and isinstance(opportunity["full_data"], str):
            try:
                opportunity["full_data"] = json.loads(opportunity["full_data"])
            except json.JSONDecodeError:
                opportunity["full_data"] = "Error: Could not decode JSON."

        # Fetch job information if a job ID exists
        job_id = opportunity.get("latest_job_id")
        if job_id:
            job_info = db_manager.get_job(job_id)
            if job_info:
                opportunity["job_status"] = {
                    "id": job_info.get("id"),
                    "status": job_info.get("status"),
                    "progress": job_info.get("progress"),
                    "error": job_info.get("error"),
                    "started_at": job_info.get("started_at"),
                    "finished_at": job_info.get("finished_at"),
                }

        # Check for ai_content before writing
        if opportunity.get("ai_content"):
            print("ai_content is present in the fetched data.")
        else:
            print("ai_content is MISSING from the fetched data.")

        output_filename = f"opportunity_{opportunity_id}.json"
        with open(output_filename, "w") as f:
            json.dump(opportunity, f, indent=4)
        print(f"Opportunity data saved to {output_filename}")
    else:
        error_message = {"error": f"Opportunity with ID {opportunity_id} not found."}
        output_filename = f"opportunity_{opportunity_id}_error.json"
        with open(output_filename, "w") as f:
            json.dump(error_message, f, indent=4)
        print(f"Error message saved to {output_filename}")


if __name__ == "__main__":
    get_opportunity_data(3)
</file>

<file path="openai_audit.md">
# OpenAI API Call Audit Report

This document audits all calls to the OpenAI API within the codebase to ensure they are accurate, efficient, and use the correct parameters.

## 1. `backend/agents/brief_assembler.py`

- **Function:** `_generate_dynamic_brief_attributes`
- **Purpose:** Generates a target audience persona and a primary goal for the content based on SERP data.
- **Model:** `gpt-5-nano` (Hardcoded)
- **Schema:** `generate_brief_attributes`
- **Audit Findings:**
    - **[FIXED]** The schema was missing the mandatory `type: "object"` at its root.
    - **[CORRECT]** The schema correctly uses `"required"` fields.
    - **[NEEDS FIX]** The schema is missing `"additionalProperties": False` at the root level, which is required for strict schema enforcement.
- **Status:** <span style="color:orange">**Needs Correction**</span>

## 2. `backend/pipeline/step_04_analysis/content_analysis_modules/ai_intelligence_caller.py`

- **Function:** `get_ai_content_analysis`
- **Purpose:** Performs the core content intelligence analysis based on a comprehensive SERP and competitor data prompt.
- **Model:** Configurable (`default_model`)
- **Schema:** `extract_deep_content_insights`
- **Audit Findings:**
    - **[CORRECT]** The schema correctly defines `"type": "object"` at the root.
    - **[CORRECT]** The schema correctly uses `"required"` fields.
    - **[CORRECT]** The schema correctly enforces `"additionalProperties": False`.
- **Status:** <span style="color:green">**OK**</span>

## 3. `backend/pipeline/step_04_analysis/content_analyzer.py`

- **Function:** `generate_ai_outline`
- **Purpose:** Generates the structured article outline (H2s and H3s) based on the content intelligence.
- **Model:** Configurable (`default_model`)
- **Schema:** `generate_structured_content_outline`
- **Audit Findings:**
    - **[CORRECT]** The schema correctly defines `"type": "object"` at the root.
    - **[CORRECT]** The schema correctly uses `"required"` fields.
    - **[CORRECT]** The schema correctly enforces `"additionalProperties": False` at all levels.
- **Status:** <span style="color:green">**OK**</span>

## 4. `backend/agents/article_generator.py`

- **Function:** `_generate_component` (used by `generate_introduction`, `generate_conclusion`, `generate_section`)
- **Purpose:** Generates individual sections of the article (intro, body, conclusion) as HTML.
- **Model:** Configurable (`default_model`)
- **Schema:** `generate_html_content`
- **Audit Findings:**
    - **[CORRECT]** The schema correctly defines `"type": "object"` at the root.
    - **[CORRECT]** The schema correctly uses `"required"` fields.
    - **[CORRECT]** The schema correctly enforces `"additionalProperties": False`.
- **Status:** <span style="color:green">**OK**</span>

## 5. `backend/agents/social_media_crafter.py`

- **Function:** `craft_posts`
- **Purpose:** Generates social media posts for various platforms based on the final article.
- **Model:** Configurable (`default_model`)
- **Schema:** `social_media_posts`
- **Audit Findings:**
    - **[CORRECT]** The schema correctly defines `"type": "object"` at the root.
    - **[CORRECT]** The schema correctly uses `"required"` fields.
    - **[CORRECT]** The schema correctly enforces `"additionalProperties": False` at all levels.
- **Status:** <span style="color:green">**OK**</span>

## 6. `backend/agents/internal_linking_suggester.py`

- **Function:** `suggest_links`
- **Purpose:** Suggests relevant internal links by analyzing the article text against existing published content.
- **Model:** Configurable (`default_model`)
- **Schema:** `suggest_contextual_internal_links`
- **Audit Findings:**
    - **[CORRECT]** The schema correctly defines `"type": "object"` at the root.
    - **[CORRECT]** The schema correctly uses `"required"` fields.
    - **[CORRECT]** The schema correctly enforces `"additionalProperties": False` at all levels.
- **Status:** <span style="color:green">**OK**</span>

## 7. `backend/agents/image_generator.py`

- **Function:** `_simplify_prompt_for_pexels`
- **Purpose:** Extracts keywords from a descriptive prompt to use for searching a stock photo library. This is a non-critical, helper function.
- **Model:** Configurable (`default_model`)
- **Schema:** `extract_keywords`
- **Audit Findings:**
    - **[CORRECT]** The schema correctly defines `"type": "object"` at the root.
    - **[CORRECT]** The schema correctly uses `"required"` fields.
    - **[NEEDS FIX]** The schema is missing `"additionalProperties": False`. While not critical for this function, it's best practice to add it for consistency.
- **Status:** <span style="color:orange">**Needs Correction**</span>

## 8. `backend/pipeline/orchestrator/content_orchestrator.py` & `backend/api/routers/orchestrator.py`

- **Function:** `refine_content`
- **Purpose:** Refines a snippet of HTML based on a user's command.
- **Model:** Configurable (`default_model`)
- **Schema:** **None**
- **Audit Findings:**
    - **[CORRECT]** This is a freeform text-to-text call and does not use a JSON schema, which is appropriate for its purpose.
- **Status:** <span style="color:green">**OK**</span>

---

## Summary & Action Items

The audit has confirmed that most OpenAI calls are correctly implemented. However, two files require minor corrections to enforce strict schema validation, which is the likely cause of the final remaining errors.

1.  **`backend/agents/brief_assembler.py`**: Add `"additionalProperties": False` to the root of the schema.
2.  **`backend/agents/image_generator.py`**: Add `"additionalProperties": False` to the root of the schema.

I will now apply these two fixes.
</file>

<file path="opportunity_3.json">
{
    "id": 3,
    "keyword": "can chatgpt make videos",
    "status": "generated",
    "client_id": "Lark_Main_Site",
    "date_added": "2025-10-20T15:12:28.852287",
    "date_processed": "2025-10-21T01:23:48.899707",
    "strategic_score": 56.83,
    "blog_qualification_status": "review",
    "blog_qualification_reason": "Review: Moderate strategic score.",
    "keyword_info": {
        "se_type": "google",
        "last_updated_time": "2025-09-17T11:40:43",
        "competition": 0.09,
        "competition_level": "LOW",
        "cpc": 4.45,
        "search_volume": 2400,
        "low_top_of_page_bid": 0.92,
        "high_top_of_page_bid": 4.5,
        "categories": [
            10010
        ],
        "monthly_searches": [
            {
                "year": 2025,
                "month": 8,
                "search_volume": 6600
            },
            {
                "year": 2025,
                "month": 7,
                "search_volume": 6600
            },
            {
                "year": 2025,
                "month": 6,
                "search_volume": 5400
            },
            {
                "year": 2025,
                "month": 5,
                "search_volume": 3600
            },
            {
                "year": 2025,
                "month": 4,
                "search_volume": 2900
            },
            {
                "year": 2025,
                "month": 3,
                "search_volume": 880
            },
            {
                "year": 2025,
                "month": 2,
                "search_volume": 720
            },
            {
                "year": 2025,
                "month": 1,
                "search_volume": 590
            },
            {
                "year": 2024,
                "month": 12,
                "search_volume": 590
            },
            {
                "year": 2024,
                "month": 11,
                "search_volume": 480
            },
            {
                "year": 2024,
                "month": 10,
                "search_volume": 390
            },
            {
                "year": 2024,
                "month": 9,
                "search_volume": 320
            }
        ],
        "search_volume_trend": {
            "monthly": 0,
            "quarterly": 22,
            "yearly": 2438
        }
    },
    "keyword_properties": {
        "se_type": "google",
        "core_keyword": null,
        "synonym_clustering_algorithm": "text_processing",
        "keyword_difficulty": 0,
        "detected_language": "en",
        "is_another_language": false,
        "intent": "informational"
    },
    "search_intent_info": {
        "se_type": "google",
        "main_intent": "informational",
        "foreign_intent": [],
        "last_updated_time": "2023-08-31T03:14:40"
    },
    "serp_overview": null,
    "score_breakdown": {
        "ease_of_ranking": {
            "name": "Ease of Ranking",
            "score": 65,
            "breakdown": {
                "Keyword Difficulty": {
                    "value": 0,
                    "score": 100,
                    "explanation": "Lower is better."
                },
                "Avg. Domain Rank": {
                    "value": "531",
                    "score": 24,
                    "explanation": "Normalized against a max of 700. Lower is better."
                },
                "Avg. Page Rank": {
                    "value": "49",
                    "score": 51,
                    "explanation": "Represents page-level authority. Lower is better."
                },
                "Dofollow Ratio": {
                    "value": "14.8%",
                    "score": 85,
                    "explanation": "Ratio of dofollow links. A lower ratio indicates weaker competitor backlink profiles."
                },
                "Total Results": {
                    "value": "493,000,000",
                    "score": 3,
                    "explanation": "Log-normalized. Fewer competing pages is better."
                }
            },
            "weight": 40
        },
        "traffic_potential": {
            "name": "Traffic Potential",
            "score": 16,
            "breakdown": {
                "Traffic Potential": {
                    "value": "2400 SV | $4.45 CPC",
                    "score": 16,
                    "explanation": "Blended score: 70% from Est. Traffic Value ($10,680) and 30% from Raw SV (2400)."
                }
            },
            "weight": 15
        },
        "commercial_intent": {
            "name": "Commercial Intent",
            "score": 66,
            "breakdown": {
                "CPC & Competition": {
                    "value": "$4.45 (LOW)",
                    "score": 42,
                    "explanation": "Normalized CPC, with bonuses for low competition and wide bid spread."
                },
                "Strategic Intent": {
                    "value": "Informational",
                    "score": 90,
                    "explanation": "Base score for 'informational' intent is 75. Bonus for being a question keyword."
                }
            },
            "weight": 5
        },
        "growth_trend": {
            "name": "Growth Trend",
            "score": 73,
            "breakdown": {
                "Growth Trend": {
                    "value": "2438% YoY",
                    "score": 73,
                    "explanation": "Weighted score from trends (Y:2438%, Q:22%, M:0%) and search volume."
                }
            },
            "weight": 5
        },
        "serp_features": {
            "name": "SERP Opportunity",
            "score": 75.0,
            "breakdown": {
                "SERP Opportunity": {
                    "value": 3,
                    "score": 75.0,
                    "explanation": "Score reflects SERP opportunities. Featured Snippet (+40), People Also Ask (+25), Video Results (-15)"
                }
            },
            "weight": 5
        },
        "serp_volatility": {
            "name": "SERP Volatility",
            "score": 50.0,
            "breakdown": {
                "SERP Stability": {
                    "value": "53 days",
                    "score": 50.0,
                    "explanation": "SERP updated every 53 days. More frequent updates can signal an opportunity."
                }
            },
            "weight": 5
        },
        "competitor_weakness": {
            "name": "Competitor Weakness",
            "score": 48,
            "breakdown": {
                "Avg. Domain Rank": {
                    "value": "531",
                    "score": 24,
                    "explanation": "Normalized against a max of 700. Lower is better."
                },
                "Avg. Referring Domains": {
                    "value": "33.6",
                    "score": 83,
                    "explanation": "Normalized against a max of 200. Lower is better."
                }
            },
            "weight": 20
        },
        "serp_crowding": {
            "name": "SERP Crowding",
            "score": 50.0,
            "breakdown": {
                "SERP Crowding": {
                    "value": 3,
                    "score": 50.0,
                    "explanation": "3 attention-grabbing features found. A lower count is better."
                }
            },
            "weight": 5
        },
        "keyword_structure": {
            "name": "Keyword Structure",
            "score": 100.0,
            "breakdown": {
                "Keyword Structure": {
                    "value": "4 words (Depth: 0)",
                    "score": 100.0,
                    "explanation": "Keyword has 4 words and search depth of 0. The 4-6 word range is the sweet spot."
                }
            },
            "weight": 5
        },
        "serp_threat": {
            "name": "SERP Threat",
            "score": 100,
            "breakdown": {
                "SERP Threat": {
                    "value": "0%",
                    "score": 100,
                    "explanation": "No major threats found."
                }
            },
            "weight": 5
        },
        "volume_volatility": {
            "name": "Volume Volatility",
            "score": 0,
            "breakdown": {
                "Volatility": {
                    "value": "99.49%",
                    "score": 0,
                    "explanation": "Coefficient of Variation: 99.49%. Lower is more stable."
                }
            },
            "weight": 0
        },
        "serp_freshness": {
            "name": "SERP Freshness",
            "score": 40.0,
            "breakdown": {
                "Freshness": {
                    "value": "21 days",
                    "score": 40.0,
                    "explanation": "SERP last updated 21 days ago. Older SERPs are better opportunities."
                }
            },
            "weight": 5
        },
        "competitor_performance": {
            "name": "Competitor Tech Performance",
            "score": 50.0,
            "breakdown": {
                "message": "No competitor analysis available for performance scoring."
            },
            "weight": 5.0
        }
    },
    "full_data": {
        "se_type": "google",
        "keyword": "can chatgpt make videos",
        "location_code": 2840,
        "language_code": "en",
        "keyword_info": {
            "se_type": "google",
            "last_updated_time": "2025-09-17T11:40:43",
            "competition": 0.09,
            "competition_level": "LOW",
            "cpc": 4.45,
            "search_volume": 2400,
            "low_top_of_page_bid": 0.92,
            "high_top_of_page_bid": 4.5,
            "categories": [
                10010
            ],
            "monthly_searches": [
                {
                    "year": 2025,
                    "month": 8,
                    "search_volume": 6600
                },
                {
                    "year": 2025,
                    "month": 7,
                    "search_volume": 6600
                },
                {
                    "year": 2025,
                    "month": 6,
                    "search_volume": 5400
                },
                {
                    "year": 2025,
                    "month": 5,
                    "search_volume": 3600
                },
                {
                    "year": 2025,
                    "month": 4,
                    "search_volume": 2900
                },
                {
                    "year": 2025,
                    "month": 3,
                    "search_volume": 880
                },
                {
                    "year": 2025,
                    "month": 2,
                    "search_volume": 720
                },
                {
                    "year": 2025,
                    "month": 1,
                    "search_volume": 590
                },
                {
                    "year": 2024,
                    "month": 12,
                    "search_volume": 590
                },
                {
                    "year": 2024,
                    "month": 11,
                    "search_volume": 480
                },
                {
                    "year": 2024,
                    "month": 10,
                    "search_volume": 390
                },
                {
                    "year": 2024,
                    "month": 9,
                    "search_volume": 320
                }
            ],
            "search_volume_trend": {
                "monthly": 0,
                "quarterly": 22,
                "yearly": 2438
            }
        },
        "keyword_info_normalized_with_bing": null,
        "keyword_info_normalized_with_clickstream": null,
        "clickstream_keyword_info": null,
        "keyword_properties": {
            "se_type": "google",
            "core_keyword": null,
            "synonym_clustering_algorithm": "text_processing",
            "keyword_difficulty": 0,
            "detected_language": "en",
            "is_another_language": false
        },
        "serp_info": {
            "se_type": "google",
            "check_url": "https://www.google.com/search?q=can%20chatgpt%20make%20videos&hl=en&gl=US&ie=UTF-8&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",
            "serp_item_types": [
                "featured_snippet",
                "people_also_ask",
                "organic",
                "video",
                "perspectives",
                "people_also_search",
                "related_searches"
            ],
            "se_results_count": 493000000,
            "last_updated_time": "2025-09-29T10:25:44",
            "previous_updated_time": "2025-08-07T08:29:16"
        },
        "avg_backlinks_info": {
            "se_type": "google",
            "backlinks": 46.1,
            "dofollow": 6.8,
            "referring_pages": 42.6,
            "referring_domains": 33.8,
            "referring_main_domains": 33.6,
            "rank": 49.3,
            "main_domain_rank": 531.4,
            "last_updated_time": "2025-09-29T10:25:48"
        },
        "search_intent_info": {
            "se_type": "google",
            "main_intent": "informational",
            "foreign_intent": [],
            "last_updated_time": "2023-08-31T03:14:40"
        },
        "discovery_source": "keyword_ideas",
        "depth": 0,
        "strategic_score": 56.83,
        "score_breakdown": {
            "ease_of_ranking": {
                "name": "Ease of Ranking",
                "score": 65,
                "breakdown": {
                    "Keyword Difficulty": {
                        "value": 0,
                        "score": 100,
                        "explanation": "Lower is better."
                    },
                    "Avg. Domain Rank": {
                        "value": "531",
                        "score": 24,
                        "explanation": "Normalized against a max of 700. Lower is better."
                    },
                    "Avg. Page Rank": {
                        "value": "49",
                        "score": 51,
                        "explanation": "Represents page-level authority. Lower is better."
                    },
                    "Dofollow Ratio": {
                        "value": "14.8%",
                        "score": 85,
                        "explanation": "Ratio of dofollow links. A lower ratio indicates weaker competitor backlink profiles."
                    },
                    "Total Results": {
                        "value": "493,000,000",
                        "score": 3,
                        "explanation": "Log-normalized. Fewer competing pages is better."
                    }
                },
                "weight": 40
            },
            "traffic_potential": {
                "name": "Traffic Potential",
                "score": 16,
                "breakdown": {
                    "Traffic Potential": {
                        "value": "2400 SV | $4.45 CPC",
                        "score": 16,
                        "explanation": "Blended score: 70% from Est. Traffic Value ($10,680) and 30% from Raw SV (2400)."
                    }
                },
                "weight": 15
            },
            "commercial_intent": {
                "name": "Commercial Intent",
                "score": 66,
                "breakdown": {
                    "CPC & Competition": {
                        "value": "$4.45 (LOW)",
                        "score": 42,
                        "explanation": "Normalized CPC, with bonuses for low competition and wide bid spread."
                    },
                    "Strategic Intent": {
                        "value": "Informational",
                        "score": 90,
                        "explanation": "Base score for 'informational' intent is 75. Bonus for being a question keyword."
                    }
                },
                "weight": 5
            },
            "growth_trend": {
                "name": "Growth Trend",
                "score": 73,
                "breakdown": {
                    "Growth Trend": {
                        "value": "2438% YoY",
                        "score": 73,
                        "explanation": "Weighted score from trends (Y:2438%, Q:22%, M:0%) and search volume."
                    }
                },
                "weight": 5
            },
            "serp_features": {
                "name": "SERP Opportunity",
                "score": 75.0,
                "breakdown": {
                    "SERP Opportunity": {
                        "value": 3,
                        "score": 75.0,
                        "explanation": "Score reflects SERP opportunities. Featured Snippet (+40), People Also Ask (+25), Video Results (-15)"
                    }
                },
                "weight": 5
            },
            "serp_volatility": {
                "name": "SERP Volatility",
                "score": 50.0,
                "breakdown": {
                    "SERP Stability": {
                        "value": "53 days",
                        "score": 50.0,
                        "explanation": "SERP updated every 53 days. More frequent updates can signal an opportunity."
                    }
                },
                "weight": 5
            },
            "competitor_weakness": {
                "name": "Competitor Weakness",
                "score": 48,
                "breakdown": {
                    "Avg. Domain Rank": {
                        "value": "531",
                        "score": 24,
                        "explanation": "Normalized against a max of 700. Lower is better."
                    },
                    "Avg. Referring Domains": {
                        "value": "33.6",
                        "score": 83,
                        "explanation": "Normalized against a max of 200. Lower is better."
                    }
                },
                "weight": 20
            },
            "serp_crowding": {
                "name": "SERP Crowding",
                "score": 50.0,
                "breakdown": {
                    "SERP Crowding": {
                        "value": 3,
                        "score": 50.0,
                        "explanation": "3 attention-grabbing features found. A lower count is better."
                    }
                },
                "weight": 5
            },
            "keyword_structure": {
                "name": "Keyword Structure",
                "score": 100.0,
                "breakdown": {
                    "Keyword Structure": {
                        "value": "4 words (Depth: 0)",
                        "score": 100.0,
                        "explanation": "Keyword has 4 words and search depth of 0. The 4-6 word range is the sweet spot."
                    }
                },
                "weight": 5
            },
            "serp_threat": {
                "name": "SERP Threat",
                "score": 100,
                "breakdown": {
                    "SERP Threat": {
                        "value": "0%",
                        "score": 100,
                        "explanation": "No major threats found."
                    }
                },
                "weight": 5
            },
            "volume_volatility": {
                "name": "Volume Volatility",
                "score": 0,
                "breakdown": {
                    "Volatility": {
                        "value": "99.49%",
                        "score": 0,
                        "explanation": "Coefficient of Variation: 99.49%. Lower is more stable."
                    }
                },
                "weight": 0
            },
            "serp_freshness": {
                "name": "SERP Freshness",
                "score": 40.0,
                "breakdown": {
                    "Freshness": {
                        "value": "21 days",
                        "score": 40.0,
                        "explanation": "SERP last updated 21 days ago. Older SERPs are better opportunities."
                    }
                },
                "weight": 5
            },
            "competitor_performance": {
                "name": "Competitor Tech Performance",
                "score": 50.0,
                "breakdown": {
                    "message": "Invalid opportunity data for scoring."
                },
                "weight": 5.0
            }
        },
        "status": "review",
        "blog_qualification_status": "review",
        "blog_qualification_reason": "Review: Moderate strategic score."
    },
    "ai_content_model": "gpt-4o",
    "featured_image_url": "https://images.pexels.com/photos/7046698/pexels-photo-7046698.jpeg?auto=compress&cs=tinysrgb&fit=crop&h=627&w=1200",
    "featured_image_local_path": "generated_images/pexels-featured-can-chatgpt-make-videos-7046698-overlay.jpeg",
    "in_article_images_data": [],
    "social_media_posts_json": [
        {
            "platform": "Twitter",
            "content": "\ud83d\ude80 Discover the power of AI video creation! Dive into our latest article, \"The Article on Untitled\", exploring how ChatGPT is transforming video production. \ud83d\udcf9 Learn how tools like Sora, Invideo, and more are integrated for seamless content creation. \ud83d\udca1\ud83c\udf1f #AI #VideoProduction #ChatGPT [LINK]"
        },
        {
            "platform": "LinkedIn",
            "content": "\ud83c\udfa5\ud83d\udcc8 Unlock the future of video content with AI! Our new article, \"The Article on Untitled\", delves into the fascinating capabilities of ChatGPT in video production. Explore how integrating ChatGPT with tools like Sora, Invideo, and Pictory is revolutionizing the industry. Gain insights from successful case studies and learn how to leverage AI for creating compelling video content on platforms like YouTube and Instagram Reels. \ud83d\ude80\ud83d\udcbc Read more and transform your video strategy today! \ud83d\udc49 [LINK] #AI #VideoMarketing #Innovation #ChatGPT #OpenAI"
        }
    ],
    "social_media_posts_status": "draft",
    "last_workflow_step": "generation_complete",
    "error_message": null,
    "wordpress_payload_json": null,
    "final_package_json": {
        "meta_title": "No Title",
        "meta_description": "",
        "article_html_final": "<div><ul class=\"toc-list\"><li><a href=\"#introduction\">Introduction</a></li><li><a href=\"#understanding-chatgpt-and-its-capabilities\">Understanding ChatGPT and Its Capabilities</a></li><li><a href=\"#can-chatgpt-create-videos\">Can ChatGPT Create Videos?</a></li><li><a href=\"#integrating-chatgpt-with-other-ai-tools-for-video-creation\">Integrating ChatGPT with Other AI Tools for Video Creation</a></li><li><a href=\"#case-studies-successful-uses-of-chatgpt-in-video-production\">Case Studies: Successful Uses of ChatGPT in Video Production</a></li><li><a href=\"#expert-tips-for-using-ai-in-video-editing\">Expert Tips for Using AI in Video Editing</a></li><li><a href=\"#frequently-asked-questions\">Frequently Asked Questions</a></li><li><a href=\"#conclusion\">Conclusion</a></li></ul><h2 id=\"table-of-contents\">Table of Contents</h2><h2 id=\"introduction\">Introduction</h2>\n<p>Imagine a world where creating engaging video content is as effortless as having a conversation with a friend. In an era where 85% of internet users in the United States alone watch online videos monthly, video content is king. Yet, for many professionals, the thought of producing high-quality videos is daunting, time-consuming, and often expensive. Enter ChatGPT, a cutting-edge AI tool that promises to transform the way we create and consume video content.</p>\n<p>For seasoned financial planners like you, who have spent years honing their craft, the ability to seamlessly translate complex financial concepts into easily digestible video content can be a game-changer. But can ChatGPT really create videos, and if so, how does it integrate with your existing skill set? These questions are more than just intriguing\u2014they're crucial in a world increasingly dominated by digital content.</p>\n<p>In this post, we'll delve into the capabilities of ChatGPT in video creation, explore its potential impact on the financial industry, and offer practical tips for leveraging this technology to enhance your content strategy. Buckle up, as we unravel the future of AI-driven video production and what it means for professionals like you.</p>\n<h2 id=\"understanding-chatgpt-and-its-capabilities\">Understanding ChatGPT and Its Capabilities</h2>\n<p>Understanding the potential of ChatGPT begins with unraveling its fundamental nature. ChatGPT, developed by OpenAI, is a state-of-the-art language model designed to generate human-like text based on the input it receives. Trained on diverse datasets, it can simulate conversations, answer questions, and even assist in creative writing. In essence, ChatGPT serves as a virtual assistant capable of interacting with users in a remarkably conversational manner.</p>\n<h3>What is ChatGPT?</h3>\n<p>At its core, ChatGPT is a machine learning model that leverages Natural Language Processing (NLP) to understand and generate text. Powered by the groundbreaking Generative Pre-trained Transformer architecture, it can process and produce coherent responses that mimic human conversation. This makes it an invaluable tool in various domains, from customer service to educational content creation. However, when it comes to video production, its role becomes more nuanced.</p>\n<h3>The Role of ChatGPT in Content Creation</h3>\n<p>In the realm of content creation, ChatGPT is a versatile player. It can aid in scriptwriting, brainstorming ideas, and even generating detailed outlines for video content. For a financial planner seeking to convey complex investment strategies through video, ChatGPT can help draft engaging scripts that break down intricate topics into viewer-friendly narratives. This ability to ease the script development process empowers professionals to focus on delivering high-quality visual content with clear and compelling messages.</p>\n<p>Moreover, ChatGPT can serve as a collaborative partner, offering alternative perspectives and suggestions during the ideation phase. By providing diverse content angles, it facilitates the creation of rich, multifaceted video presentations that resonate with varied audiences. However, while ChatGPT excels in text generation, there are inherent limitations to its capabilities in video production.</p>\n<h3>Limitations of ChatGPT in Video Production</h3>\n<p>Despite its prowess in text-based tasks, ChatGPT does not possess the ability to directly create or edit video content. It lacks the visual processing capabilities necessary to manipulate video elements or integrate audio-visual components. For instance, while ChatGPT can help draft an informative script for a video on retirement planning, it cannot transform that script into a polished video presentation.</p>\n<p>To effectively produce video content, ChatGPT must be integrated with video editing software or platforms that translate its textual outputs into visual narratives. This often involves collaboration with tools that handle video rendering, animation, and post-production editing. As such, while ChatGPT offers a significant advantage in content ideation and planning, it remains a complementary tool rather than a standalone video production solution.</p>\n<p>In conclusion, understanding ChatGPT's capabilities and limitations is crucial for professionals seeking to leverage AI in video content strategy. By harnessing its strengths in content creation and pairing it with powerful video technologies, financial planners can streamline their production processes, ultimately delivering impactful and accessible financial insights through video.</p>\n<h2 id=\"can-chatgpt-create-videos\">Can ChatGPT Create Videos?</h2>\n<p>Moving from scriptwriting to video production, the question arises: Can ChatGPT create videos? The answer isn't straightforward. Understanding the capabilities and limitations of ChatGPT in the realm of video creation requires delving into the synergy between AI-driven text generation and video production technologies.</p>\n<h3>Exploring the Possibilities with ChatGPT</h3>\n<p>At its core, ChatGPT excels in generating text-based content, offering substantial support in ideating and scripting video concepts. When paired with complementary technologies, it can be a powerful ally in video creation. Here\u2019s how:</p>\n<ul>\n<li><strong>Scriptwriting and Storyboarding:</strong> ChatGPT can draft engaging scripts and create detailed storyboards that convey the vision and flow of the video. This foundation is crucial for effective video production.</li>\n<li><strong>Content Personalization:</strong> With its adaptability, ChatGPT can tailor content to specific audiences, ensuring that the video resonates with viewers, whether it\u2019s explaining complex financial terms or providing insights into market trends.</li>\n<li><strong>Idea Generation:</strong> By analyzing market data and trends, ChatGPT can suggest innovative video topics that align with audience interests and organizational goals, ensuring a steady stream of fresh content ideas.</li>\n</ul>\n<h3>What ChatGPT Can and Cannot Do in Video Creation</h3>\n<p>While ChatGPT is invaluable in the pre-production phase, it's essential to acknowledge its limitations in the actual video creation process:</p>\n<ul>\n<li><strong>Video Rendering:</strong> ChatGPT cannot directly render or edit video footage. Specialized software, such as Adobe Premiere Pro or Final Cut Pro, is needed to transform scripts into visual content.</li>\n<li><strong>Animation and Visual Effects:</strong> AI tools like ChatGPT lack the capability to create animations or visual effects. These tasks require animation software like After Effects, often necessitating human creativity and expertise.</li>\n<li><strong>Audio and Visual Synchronization:</strong> Synchronizing audio tracks with video clips is beyond ChatGPT\u2019s scope. This requires precise timing and editing skills that are best handled by dedicated video editing software.</li>\n</ul>\n<p>To sum up, while ChatGPT is not equipped to independently create videos, its contributions to content creation and planning are undeniable. By leveraging its strengths in generating ideas and crafting scripts, professionals can streamline video production processes when combined with the right technological tools. The future of video creation lies in the seamless integration of AI with traditional production methods, offering endless possibilities for storytelling and engagement.</p>\n<h2 id=\"integrating-chatgpt-with-other-ai-tools-for-video-creation\">Integrating ChatGPT with Other AI Tools for Video Creation</h2>\n<p>As the landscape of video production evolves, the integration of ChatGPT with other AI tools presents a new frontier in creating dynamic and engaging video content. While ChatGPT excels in generating scripts and storyboards, combining it with specialized AI platforms like Sora, Invideo, Pictory, and Dream Machine can unleash a new wave of creativity and efficiency.</p>\n<h3>Working with Sora</h3>\n<p>Sora, an AI-driven video editing tool, complements ChatGPT by transforming text-based scripts into visual content. By feeding ChatGPT-generated scripts into Sora, creators can automate the editing process, allowing the tool to select appropriate footage, transitions, and effects. Sora's ability to analyze textual content ensures that the video narrative aligns with the intended message, providing a seamless way to bring script ideas to life.</p>\n<h3>Using Invideo for Video Creation</h3>\n<p>Invideo offers a user-friendly platform for turning ChatGPT's scripts into polished videos. With its extensive library of templates and stock footage, Invideo simplifies the creation process. By integrating ChatGPT-generated content, users can easily customize these templates, ensuring that each video is unique and tailored to their specific needs. Invideo's intuitive interface allows even those with minimal editing experience to produce professional-quality videos.</p>\n<h3>Combining ChatGPT with Pictory and Dream Machine</h3>\n<p>When it comes to enhancing visual storytelling, Pictory and Dream Machine stand out as innovative AI tools that work harmoniously with ChatGPT. Pictory specializes in creating short, engaging video snippets from longer text, making it ideal for crafting teasers or promotional content from comprehensive scripts written by ChatGPT. Meanwhile, Dream Machine excels in generating imaginative and surreal visuals, offering creators the opportunity to incorporate unique visual elements into their videos, adding depth and intrigue to their narratives.</p>\n<p>By leveraging the power of these AI tools, creators can harness the combined strengths of text generation and video production, paving the way for more engaging, efficient, and innovative video content. This integration not only enhances productivity but also expands the creative possibilities, enabling storytellers to push the boundaries of traditional video creation.</p>\n<h2 id=\"case-studies-successful-uses-of-chatgpt-in-video-production\">Case Studies: Successful Uses of ChatGPT in Video Production</h2>\n<h3>Success Story 1: Creating Instagram Reels</h3>\n<p>The dynamic world of Instagram Reels is a testament to the immense potential of AI-driven content creation. A digital marketing agency, <strong>InstaSpark</strong>, leveraged ChatGPT to revolutionize their approach to producing Reels for fashion brands. By using ChatGPT to draft compelling scripts, the agency was able to enhance engagement with captivating narratives that resonated with the target audience. These scripts were then paired with Pictory's capabilities to create visually appealing short videos that captured attention within seconds. Not only did this approach streamline their production process, but it also doubled their clients' engagement rates, proving the effectiveness of AI in maximizing social media impact.</p>\n<h3>Success Story 2: YouTube Video Production</h3>\n<p><strong>ViralTech</strong>, a burgeoning tech review channel on YouTube, faced the challenge of producing content that was both informative and engaging. By incorporating ChatGPT, the team was able to generate detailed video scripts that broke down complex tech jargon into easy-to-understand language. This not only expanded their viewership but also improved viewer retention rates. The integration didn't stop with scripting; using Dream Machine, ViralTech added visually stunning AI-generated graphics to their videos, making their content visually appealing and informative, thereby setting a new standard in tech reviews.</p>\n<h3>Success Story 3: AI-Generated Video Monetization</h3>\n<p>In the niche world of culinary arts, <strong>ChefBytes</strong> has carved out a unique space by combining traditional cooking with innovative technology. Utilizing ChatGPT to create engaging storylines and scripts for their cooking tutorials, ChefBytes was able to enhance viewer interaction and subscription rates. The channel further capitalized on AI by using Dream Machine to generate eye-catching thumbnails and engaging intro sequences. This not only increased their ad revenue but also opened new monetization streams through sponsorships and collaborations, highlighting the potential of AI in transforming video content into a profitable venture.</p>\n<p>These case studies underscore the transformative power of AI in video production. By employing tools like ChatGPT, Pictory, and Dream Machine, creators can enhance creativity, improve efficiency, and unlock new revenue streams, crafting a future where AI is an indispensable partner in content creation.</p>\n<h2 id=\"expert-tips-for-using-ai-in-video-editing\">Expert Tips for Using AI in Video Editing</h2>\n<p>With the transformative power of AI technologies in video production established, let's delve into expert strategies for integrating AI into video editing. As tools like ChatGPT, Pictory, and Dream Machine continue to evolve, they offer new avenues to enhance creativity, efficiency, and storytelling in filmmaking. Here are some expert tips to harness the potential of AI in video editing effectively:</p>\n<h3>Video Editing Tips: Secrets for Success</h3>\n<ul>\n<li><strong>Start with a Vision:</strong> Before integrating AI into your editing process, have a clear vision of the story you want to tell. AI tools can aid in realizing this vision by providing automated suggestions, but they work best when aligned with a well-defined creative direction.</li>\n<li><strong>Utilize AI for Routine Tasks:</strong> Leverage AI to handle repetitive tasks like color correction, transitions, and audio syncing. This enables editors to focus more on the creative aspects that require human intuition and artistic touch.</li>\n<li><strong>Experiment with AI-generated Effects:</strong> Use AI to test new visual effects and transitions that might not have been considered otherwise. Many AI tools offer libraries of effects that can be customized to fit your video\u2019s unique style.</li>\n</ul>\n<h3>Understanding the 80/20 Rule in Video Editing</h3>\n<p>The Pareto Principle, or the 80/20 Rule, posits that 80% of outcomes come from 20% of the efforts. In the context of video editing, this can be a game-changer when using AI:</p>\n<ul>\n<li><strong>Focus on the Core Elements:</strong> Identify the key components of your video that will have the most significant impact on your audience and use AI to streamline the rest. This could include automating tedious tasks to dedicate more time to perfecting the narrative and engaging the viewer.</li>\n<li><strong>Leverage AI Analytics:</strong> Use AI-driven analytics to understand which parts of your videos resonate most with viewers and refine your editing process accordingly. This data-driven approach allows for more targeted improvements and greater viewer satisfaction.</li>\n</ul>\n<h3>Maximizing the Potential of AI in Filmmaking</h3>\n<p>To truly maximize the potential of AI in filmmaking, it\u2019s essential to see it as a collaborative tool rather than a replacement for human creativity:</p>\n<ul>\n<li><strong>Incorporate AI in Pre-Production:</strong> Use AI to analyze scripts and storyboard layouts for pacing and plot coherence before filming begins. This preemptive step can save time and resources during the editing phase.</li>\n<li><strong>Customize AI Models:</strong> Train AI models with your unique style and preferences to get more personalized suggestions and edits that align with your vision.</li>\n<li><strong>Stay Updated:</strong> The field of AI in video editing is rapidly evolving. Regularly update your tools and skills to stay ahead of trends and incorporate the latest advancements into your workflow.</li>\n</ul>\n<p>By thoughtfully integrating AI into your video editing process, you can unlock unprecedented efficiencies and creative possibilities, ensuring your content not only captivates but also leaves a lasting impact on your audience.</p>\n<h2 id=\"frequently-asked-questions\">Frequently Asked Questions</h2>\n<h3>Can ChatGPT Make Video Reels?</h3>\n<p>While ChatGPT is an advanced AI language model developed by OpenAI, it does not directly generate video content, including reels. However, it can assist creators in the ideation phase. For instance, you can use ChatGPT to brainstorm video concepts, generate scripts, or even suggest optimal hashtags and captions that can enhance the visibility of your reels on platforms like Instagram.</p>\n<h3>Do AI-Generated Videos Make Money?</h3>\n<p>Absolutely, AI-generated videos can be monetized across various platforms. Content creators often use AI tools to enhance video quality, automate editing processes, and even generate subtitles, all of which contribute to a more engaging viewer experience. Monetization strategies include ad revenue from platforms like YouTube, sponsored content, and direct sales of AI-enhanced video services to clients.</p>\n<h3>What is the Best AI for Filmmaking?</h3>\n<p>The \"best\" AI for filmmaking depends on your specific needs. Tools like RunwayML and Synthesia are popular among creators for their advanced features. RunwayML offers a collaborative platform for video editing and visual effects, while Synthesia specializes in AI-generated video content, including avatars that can deliver scripted content in multiple languages, enhancing global reach.</p>\n<h3>How to Make Money with AI Videos?</h3>\n<p>There are several pathways to monetize AI-produced videos. Start by creating content for your own social media channels, leveraging AI to boost production quality and efficiency. Additionally, consider offering AI video production services to businesses looking to streamline their content creation. Platforms like Patreon or Ko-fi can also help creators earn income through fan support and exclusive content offerings.</p>\n<h3>How I Use ChatGPT to Create Instagram Reels</h3>\n<p>Using ChatGPT for Instagram reels involves tapping into its natural language processing capabilities. I often start by asking ChatGPT for trendy topics or script ideas. Once I settle on a concept, I use its suggestions to craft engaging captions and select hashtags that increase discoverability. This strategic approach ensures that my reels are not only creative but also optimized for audience engagement.</p>\n<h3>How Many Views Do You Need on YouTube to Make $1000 a Month?</h3>\n<p>To earn $1000 a month on YouTube, creators typically need around 100,000 views per month, assuming an average RPM (Revenue Per Mille) of $10. This figure can vary based on factors like audience demographics, content niche, and engagement rates. Therefore, combining AI tools for video optimization with effective marketing strategies can help achieve these viewership targets more efficiently.</p>\n<h3>Is Video GPT Free?</h3>\n<p>As of now, Video GPT is not a standalone, freely available tool. It usually refers to AI-driven video solutions offered by various companies, many of which come with subscription fees. However, platforms like OpenAI do offer free trials for some of their language models, enabling users to explore their capabilities before committing to a paid plan.</p>\n<h3>Is There Any Free AI Video Generator?</h3>\n<p>Yes, there are several free AI video generators available. Tools like Lumen5 and InVideo offer free plans with basic features, which can be a great starting point for creators who are new to AI-enhanced video production. These platforms allow users to convert text into video, making them ideal for creating quick, engaging content without a significant financial investment.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In summary, ChatGPT has proven to be a revolutionary tool in the realm of content creation, especially when it comes to scriptwriting and ideation for video production. Its ability to transform complex topics into engaging narratives provides a significant advantage for professionals looking to captivate their audience with clarity and precision. While ChatGPT shines in the pre-production phase, its integration with other AI tools is essential for creating fully-fledged video content, thus marking it as an invaluable asset in the digital storyteller's toolkit.</p>\n<p>To truly harness the power of ChatGPT, professionals should embrace it as a collaborative partner that complements their creative vision. By combining its strengths with comprehensive video editing software or platforms, you can elevate your content strategy to new heights. Whether you're a financial planner aiming to demystify investment strategies or a content creator exploring new narratives, the future of video production beckons with endless possibilities. Now is the time to dive in and explore the potential of AI-driven innovation. Visit <a href=\"#\">[insert link here]</a> to discover more about how you can integrate ChatGPT into your workflow and redefine your approach to video content creation.</p></div>",
        "schema_org_json": {
            "@context": "https://schema.org",
            "@graph": [
                {
                    "@type": "BlogPosting",
                    "@id": "can-chatgpt-make-videos-1760982517#article",
                    "mainEntityOfPage": {
                        "@id": "can-chatgpt-make-videos-1760982517"
                    },
                    "headline": "can chatgpt make videos",
                    "author": {
                        "@type": "Organization",
                        "name": "Author"
                    },
                    "publisher": {
                        "@type": "Organization",
                        "name": "Publisher Name"
                    },
                    "datePublished": "2025-10-21T01:23:48.871753",
                    "dateModified": "2025-10-21T01:23:48.871767",
                    "additionalProperties": false
                }
            ]
        },
        "featured_image_path": null,
        "featured_image_relative_path": null,
        "social_media_posts": null
    },
    "slug": null,
    "run_id": 2,
    "keyword_info_normalized_with_bing": null,
    "keyword_info_normalized_with_clickstream": null,
    "monthly_searches": [
        {
            "year": 2025,
            "month": 8,
            "search_volume": 6600
        },
        {
            "year": 2025,
            "month": 7,
            "search_volume": 6600
        },
        {
            "year": 2025,
            "month": 6,
            "search_volume": 5400
        },
        {
            "year": 2025,
            "month": 5,
            "search_volume": 3600
        },
        {
            "year": 2025,
            "month": 4,
            "search_volume": 2900
        },
        {
            "year": 2025,
            "month": 3,
            "search_volume": 880
        },
        {
            "year": 2025,
            "month": 2,
            "search_volume": 720
        },
        {
            "year": 2025,
            "month": 1,
            "search_volume": 590
        },
        {
            "year": 2024,
            "month": 12,
            "search_volume": 590
        },
        {
            "year": 2024,
            "month": 11,
            "search_volume": 480
        },
        {
            "year": 2024,
            "month": 10,
            "search_volume": 390
        },
        {
            "year": 2024,
            "month": 9,
            "search_volume": 320
        }
    ],
    "traffic_value": 0.0,
    "check_url": "https://www.google.com/search?q=can%20chatgpt%20make%20videos&hl=en&gl=US&ie=UTF-8&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",
    "related_keywords": "null",
    "keyword_categories": "[10010]",
    "core_keyword": null,
    "published_url": null,
    "last_seen_at": "2025-10-20T15:12:28.852379",
    "metrics_history": "[]",
    "keyword_id": 3,
    "latest_job_id": null,
    "cluster_name": null,
    "cpc": 4.45,
    "competition": 0.09,
    "main_intent": "informational",
    "search_volume_trend_json": {
        "monthly": 0,
        "quarterly": 22,
        "yearly": 2438
    },
    "competitor_social_media_tags_json": null,
    "competitor_page_timing_json": null,
    "total_api_cost": 0.0,
    "search_volume": 2400,
    "keyword_difficulty": 0,
    "search_volume_trend": {
        "monthly": 0,
        "quarterly": 22,
        "yearly": 2438
    },
    "blueprint": {
        "metadata": {
            "seed_topic": "can chatgpt make videos",
            "blueprint_version": "6.0",
            "generated_at": "2025-10-20T22:48:37.974606",
            "total_api_cost": 0.0222,
            "client_id": "Lark_Main_Site"
        },
        "winning_keyword": {
            "se_type": "google",
            "keyword": "can chatgpt make videos",
            "location_code": 2840,
            "language_code": "en",
            "keyword_info": {
                "se_type": "google",
                "last_updated_time": "2025-09-17T11:40:43",
                "competition": 0.09,
                "competition_level": "LOW",
                "cpc": 4.45,
                "search_volume": 2400,
                "low_top_of_page_bid": 0.92,
                "high_top_of_page_bid": 4.5,
                "categories": [
                    10010
                ],
                "monthly_searches": [
                    {
                        "year": 2025,
                        "month": 8,
                        "search_volume": 6600
                    },
                    {
                        "year": 2025,
                        "month": 7,
                        "search_volume": 6600
                    },
                    {
                        "year": 2025,
                        "month": 6,
                        "search_volume": 5400
                    },
                    {
                        "year": 2025,
                        "month": 5,
                        "search_volume": 3600
                    },
                    {
                        "year": 2025,
                        "month": 4,
                        "search_volume": 2900
                    },
                    {
                        "year": 2025,
                        "month": 3,
                        "search_volume": 880
                    },
                    {
                        "year": 2025,
                        "month": 2,
                        "search_volume": 720
                    },
                    {
                        "year": 2025,
                        "month": 1,
                        "search_volume": 590
                    },
                    {
                        "year": 2024,
                        "month": 12,
                        "search_volume": 590
                    },
                    {
                        "year": 2024,
                        "month": 11,
                        "search_volume": 480
                    },
                    {
                        "year": 2024,
                        "month": 10,
                        "search_volume": 390
                    },
                    {
                        "year": 2024,
                        "month": 9,
                        "search_volume": 320
                    }
                ],
                "search_volume_trend": {
                    "monthly": 0,
                    "quarterly": 22,
                    "yearly": 2438
                }
            },
            "keyword_info_normalized_with_bing": null,
            "keyword_info_normalized_with_clickstream": null,
            "clickstream_keyword_info": null,
            "keyword_properties": {
                "se_type": "google",
                "core_keyword": null,
                "synonym_clustering_algorithm": "text_processing",
                "keyword_difficulty": 0,
                "detected_language": "en",
                "is_another_language": false
            },
            "serp_info": {
                "se_type": "google",
                "check_url": "https://www.google.com/search?q=can%20chatgpt%20make%20videos&hl=en&gl=US&ie=UTF-8&uule=w+CAIQIFISCQs2MuSEtepUEUK33kOSuTsc",
                "serp_item_types": [
                    "featured_snippet",
                    "people_also_ask",
                    "organic",
                    "video",
                    "perspectives",
                    "people_also_search",
                    "related_searches"
                ],
                "se_results_count": 493000000,
                "last_updated_time": "2025-09-29T10:25:44",
                "previous_updated_time": "2025-08-07T08:29:16"
            },
            "avg_backlinks_info": {
                "se_type": "google",
                "backlinks": 46.1,
                "dofollow": 6.8,
                "referring_pages": 42.6,
                "referring_domains": 33.8,
                "referring_main_domains": 33.6,
                "rank": 49.3,
                "main_domain_rank": 531.4,
                "last_updated_time": "2025-09-29T10:25:48"
            },
            "search_intent_info": {
                "se_type": "google",
                "main_intent": "informational",
                "foreign_intent": [],
                "last_updated_time": "2023-08-31T03:14:40"
            },
            "discovery_source": "keyword_ideas",
            "depth": 0,
            "strategic_score": 56.83,
            "score_breakdown": {
                "ease_of_ranking": {
                    "name": "Ease of Ranking",
                    "score": 65,
                    "breakdown": {
                        "Keyword Difficulty": {
                            "value": 0,
                            "score": 100,
                            "explanation": "Lower is better."
                        },
                        "Avg. Domain Rank": {
                            "value": "531",
                            "score": 24,
                            "explanation": "Normalized against a max of 700. Lower is better."
                        },
                        "Avg. Page Rank": {
                            "value": "49",
                            "score": 51,
                            "explanation": "Represents page-level authority. Lower is better."
                        },
                        "Dofollow Ratio": {
                            "value": "14.8%",
                            "score": 85,
                            "explanation": "Ratio of dofollow links. A lower ratio indicates weaker competitor backlink profiles."
                        },
                        "Total Results": {
                            "value": "493,000,000",
                            "score": 3,
                            "explanation": "Log-normalized. Fewer competing pages is better."
                        }
                    },
                    "weight": 40
                },
                "traffic_potential": {
                    "name": "Traffic Potential",
                    "score": 16,
                    "breakdown": {
                        "Traffic Potential": {
                            "value": "2400 SV | $4.45 CPC",
                            "score": 16,
                            "explanation": "Blended score: 70% from Est. Traffic Value ($10,680) and 30% from Raw SV (2400)."
                        }
                    },
                    "weight": 15
                },
                "commercial_intent": {
                    "name": "Commercial Intent",
                    "score": 66,
                    "breakdown": {
                        "CPC & Competition": {
                            "value": "$4.45 (LOW)",
                            "score": 42,
                            "explanation": "Normalized CPC, with bonuses for low competition and wide bid spread."
                        },
                        "Strategic Intent": {
                            "value": "Informational",
                            "score": 90,
                            "explanation": "Base score for 'informational' intent is 75. Bonus for being a question keyword."
                        }
                    },
                    "weight": 5
                },
                "growth_trend": {
                    "name": "Growth Trend",
                    "score": 73,
                    "breakdown": {
                        "Growth Trend": {
                            "value": "2438% YoY",
                            "score": 73,
                            "explanation": "Weighted score from trends (Y:2438%, Q:22%, M:0%) and search volume."
                        }
                    },
                    "weight": 5
                },
                "serp_features": {
                    "name": "SERP Opportunity",
                    "score": 75.0,
                    "breakdown": {
                        "SERP Opportunity": {
                            "value": 3,
                            "score": 75.0,
                            "explanation": "Score reflects SERP opportunities. Featured Snippet (+40), People Also Ask (+25), Video Results (-15)"
                        }
                    },
                    "weight": 5
                },
                "serp_volatility": {
                    "name": "SERP Volatility",
                    "score": 50.0,
                    "breakdown": {
                        "SERP Stability": {
                            "value": "53 days",
                            "score": 50.0,
                            "explanation": "SERP updated every 53 days. More frequent updates can signal an opportunity."
                        }
                    },
                    "weight": 5
                },
                "competitor_weakness": {
                    "name": "Competitor Weakness",
                    "score": 48,
                    "breakdown": {
                        "Avg. Domain Rank": {
                            "value": "531",
                            "score": 24,
                            "explanation": "Normalized against a max of 700. Lower is better."
                        },
                        "Avg. Referring Domains": {
                            "value": "33.6",
                            "score": 83,
                            "explanation": "Normalized against a max of 200. Lower is better."
                        }
                    },
                    "weight": 20
                },
                "serp_crowding": {
                    "name": "SERP Crowding",
                    "score": 50.0,
                    "breakdown": {
                        "SERP Crowding": {
                            "value": 3,
                            "score": 50.0,
                            "explanation": "3 attention-grabbing features found. A lower count is better."
                        }
                    },
                    "weight": 5
                },
                "keyword_structure": {
                    "name": "Keyword Structure",
                    "score": 100.0,
                    "breakdown": {
                        "Keyword Structure": {
                            "value": "4 words (Depth: 0)",
                            "score": 100.0,
                            "explanation": "Keyword has 4 words and search depth of 0. The 4-6 word range is the sweet spot."
                        }
                    },
                    "weight": 5
                },
                "serp_threat": {
                    "name": "SERP Threat",
                    "score": 100,
                    "breakdown": {
                        "SERP Threat": {
                            "value": "0%",
                            "score": 100,
                            "explanation": "No major threats found."
                        }
                    },
                    "weight": 5
                },
                "volume_volatility": {
                    "name": "Volume Volatility",
                    "score": 0,
                    "breakdown": {
                        "Volatility": {
                            "value": "99.49%",
                            "score": 0,
                            "explanation": "Coefficient of Variation: 99.49%. Lower is more stable."
                        }
                    },
                    "weight": 0
                },
                "serp_freshness": {
                    "name": "SERP Freshness",
                    "score": 40.0,
                    "breakdown": {
                        "Freshness": {
                            "value": "21 days",
                            "score": 40.0,
                            "explanation": "SERP last updated 21 days ago. Older SERPs are better opportunities."
                        }
                    },
                    "weight": 5
                },
                "competitor_performance": {
                    "name": "Competitor Tech Performance",
                    "score": 50.0,
                    "breakdown": {
                        "message": "Invalid opportunity data for scoring."
                    },
                    "weight": 5.0
                }
            },
            "status": "review",
            "blog_qualification_status": "review",
            "blog_qualification_reason": "Review: Moderate strategic score."
        },
        "serp_overview": {
            "serp_has_ai_overview": false,
            "has_popular_products": false,
            "people_also_ask": [
                "Can ChatGPT make video reels?",
                "Video editing tips: These are the secrets you need to know! - Voice123",
                "Do AI-generated videos make money?",
                "What is the best AI for filmmaking?",
                "How to Make Money with AI Videos (Honest Guide) - SendShort",
                "Can you use ChatGPT to create videos?",
                "What is the 80 20 rule in video editing?",
                "What is the best AI for creating videos?",
                "How I Use ChatGPT to Create Instagram Reels | by Bernard Builds",
                "How many views do you need on YouTube to make $1000 a month?",
                "Video GPT | Generate Videos in Minutes - Invideo AI",
                "Is video GPT free?",
                "Is there any free AI video generator?",
                "VideoGPT: Free AI Video Generator App - Create Anime Video in Seconds"
            ],
            "paa_questions": [
                "Can ChatGPT make video reels?",
                "Video editing tips: These are the secrets you need to know! - Voice123",
                "Do AI-generated videos make money?",
                "What is the best AI for filmmaking?",
                "How to Make Money with AI Videos (Honest Guide) - SendShort",
                "Can you use ChatGPT to create videos?",
                "What is the 80 20 rule in video editing?",
                "What is the best AI for creating videos?",
                "How I Use ChatGPT to Create Instagram Reels | by Bernard Builds",
                "How many views do you need on YouTube to make $1000 a month?",
                "Video GPT | Generate Videos in Minutes - Invideo AI",
                "Is video GPT free?",
                "Is there any free AI video generator?",
                "VideoGPT: Free AI Video Generator App - Create Anime Video in Seconds"
            ],
            "top_organic_results": [
                {
                    "rank": 3,
                    "url": "https://chatgpt.com/g/g-h8l4uLHFQ-video-ai-by-invideo",
                    "title": "Video AI by invideo",
                    "domain": "chatgpt.com",
                    "description": "- AI video maker GPT (Supercharged with Sora 2) - generate engaging videos with voiceovers in any language! Sign up to chat. Sign up or Log in to chat.",
                    "page_type": "Blog/Article"
                },
                {
                    "rank": 4,
                    "url": "https://www.reddit.com/r/ChatGPT/comments/1dgpfxf/have_you_tried_chatgpt_for_creating_videos_here/",
                    "title": "Have you tried Chatgpt for creating videos? Here is 3 ways ...",
                    "domain": "www.reddit.com",
                    "description": "The best way that I've found for creating videos with AI is to use Dream Machine to generate video footages and then give those footages to\u00a0...",
                    "page_type": "Blog/Article"
                },
                {
                    "rank": 6,
                    "url": "https://invideo.io/ai/video-gpt/",
                    "title": "Video GPT | Generate Videos in Minutes",
                    "domain": "invideo.io",
                    "description": "Our ChatGPT video generator changes the way you make videos for your YouTube, social media, and other channels. Simply specify your desired video idea and get\u00a0...",
                    "page_type": "Blog/Article",
                    "rating": {
                        "value": 4.7,
                        "votes_count": 530,
                        "rating_max": 5
                    }
                },
                {
                    "rank": 7,
                    "url": "https://chatgpt.com/g/g-CPgdui5Ib-text-to-video-maker-video-ai-scripting",
                    "title": "Text to Video Maker Video AI Scripting",
                    "domain": "chatgpt.com",
                    "description": "Create stunning videos with AI Video Maker powered by VideoGPT. Just enter your text, write your script, pick a style, and get full videos with subtitles,\u00a0...",
                    "page_type": "Blog/Article"
                },
                {
                    "rank": 8,
                    "url": "https://www.youtube.com/watch?v=pkLTvvualis",
                    "title": "How to Make AI Video | ChatGPT + invideo",
                    "domain": "www.youtube.com",
                    "description": null,
                    "page_type": "Homepage/Landing Page"
                },
                {
                    "rank": 9,
                    "url": "https://www.veed.io/tools/video-gpt/chatgpt-video-generator",
                    "title": "ChatGPT Video Generator - AI Text to Video",
                    "domain": "www.veed.io",
                    "description": "Use our AI text-to-video tool to create videos instantly. Just type your video topic, generate a script or input your own, and watch the AI work its magic.",
                    "page_type": "Blog/Article",
                    "rating": {
                        "value": 4.9,
                        "votes_count": 79,
                        "rating_max": 5
                    }
                },
                {
                    "rank": 10,
                    "url": "https://pictory.ai/chatgpt-video-generator",
                    "title": "ChatGPT Video Generator",
                    "domain": "pictory.ai",
                    "description": "Create videos directly from your ideas with Pictory's ChatGPT Video Generator. Collaborate with AI to bring your concepts to life.",
                    "page_type": "Homepage/Landing Page"
                },
                {
                    "rank": 11,
                    "url": "https://www.youtube.com/shorts/JQjzhbyTBQE",
                    "title": "Create an Entire Video with ChatGPT? #invideoAiPartner",
                    "domain": "www.youtube.com",
                    "description": "Did you know you can create an entire youtube video or tiktok just using a chatgpt prompt ? Invideo ai's VideoMaker allows you to do just\u00a0...",
                    "page_type": "Blog/Article"
                }
            ],
            "related_searches": [
                "Can chatgpt make videos reddit",
                "Can ChatGPT create videos for free",
                "ChatGPT video generator",
                "AI video generator",
                "Free AI video generator",
                "ChatGPT text to video",
                "ChatGPT video editor",
                "ChatGPT for video Summary"
            ],
            "knowledge_graph_data": {},
            "knowledge_graph_facts": [],
            "paid_ad_copy": [],
            "ai_overview_content": null,
            "ai_overview_sources": [],
            "top_organic_faqs": [],
            "top_organic_sitelinks": [
                "Can any GPT's actually create original video clips from text?",
                "Can videos be made in ChatGPT currently? : r/OpenAI ...",
                "More results from www.reddit.com"
            ],
            "discussion_snippets": [
                "Make Videos Faster with Chat GPT - Beginner\u2019s Guide for 2025",
                "How to Create Cinematic AI Videos (No-BS Guide)",
                "10 Ways to Use ChatGPT So Well It Feels Illegal (Tutorial)",
                "How do y'all make videos with AI?",
                "How to Create AI Videos Using ChatGPT-4o: Quick Steps for AI-Generated Content",
                "Can You Create Videos In ChatGPT?",
                "\ud83d\udea8BREAKING: ChatGPT can now edit and create videos for free.\n\nYou don\u2019t need fancy software anymore.\n\nHere\u2019s how to do it (in 3 simple steps) \ud83d\udc47",
                "How I Use ChatGPT-5 to Create Videos That Print Money \ud83d\udcb0",
                "How to use ChatGPT - 4o as video editor"
            ],
            "product_considerations_summary": null,
            "refinement_chips": [],
            "extracted_serp_features": [],
            "serp_last_updated_days_ago": 0,
            "serp_update_interval_days": null,
            "dominant_content_format": "Article",
            "serp_has_featured_snippet": true,
            "featured_snippet_content": "Can you make videos with ChatGPT? You can create videos with AI, but ChatGPT itself doesn't generate them. OpenAI recently released Sora, a powerful text-to-video model capable of generating high-quality clips from simple prompts.",
            "serp_has_video_results": false,
            "pixel_ranking_summary": null,
            "raw_pixel_ranking_data": [],
            "first_organic_y_pixel": null,
            "is_disqualified": false,
            "disqualification_reason": null
        },
        "content_intelligence": {
            "unique_angles_to_include": [
                "Exploring the limitations and potential of ChatGPT in video creation, focusing on what it can and cannot do directly.",
                "A step-by-step guide on integrating ChatGPT with other AI tools like Sora and Invideo to create seamless video content.",
                "Case studies or success stories of users who have effectively used ChatGPT in their video production workflow."
            ],
            "key_entities_from_competitors": [
                "ChatGPT",
                "Sora",
                "Invideo",
                "Pictory",
                "Dream Machine",
                "VideoGPT",
                "OpenAI",
                "AI video maker",
                "YouTube",
                "Instagram Reels"
            ],
            "core_questions_answered_by_competitors": [
                "Can ChatGPT make videos or video reels?",
                "How can ChatGPT be used in video editing and creation?",
                "What are the best AI tools for creating videos?",
                "How to make money with AI-generated videos?",
                "Is there a free AI video generator available?",
                "What is the process to create videos using ChatGPT and other AI tools?",
                "What are the limitations of ChatGPT in video creation?"
            ],
            "identified_content_gaps": [
                "Detailed comparison of ChatGPT with other AI video tools like Sora and VideoGPT.",
                "In-depth tutorial on using ChatGPT in conjunction with other AI tools for video editing.",
                "Exploration of the monetization potential of AI-generated videos, including case studies.",
                "Discussion on the ethical implications and copyright issues related to AI-generated video content.",
                "Step-by-step guide on creating specific types of videos (e.g., cinematic, tutorial) using ChatGPT and other AI tools."
            ],
            "article_structure": [
                {
                    "h2": "Introduction",
                    "h3s": []
                },
                {
                    "h2": "Understanding ChatGPT and Its Capabilities",
                    "h3s": [
                        "What is ChatGPT?",
                        "The Role of ChatGPT in Content Creation",
                        "Limitations of ChatGPT in Video Production"
                    ]
                },
                {
                    "h2": "Can ChatGPT Create Videos?",
                    "h3s": [
                        "Exploring the Possibilities with ChatGPT",
                        "What ChatGPT Can and Cannot Do in Video Creation"
                    ]
                },
                {
                    "h2": "Integrating ChatGPT with Other AI Tools for Video Creation",
                    "h3s": [
                        "Working with Sora",
                        "Using Invideo for Video Creation",
                        "Combining ChatGPT with Pictory and Dream Machine"
                    ]
                },
                {
                    "h2": "Case Studies: Successful Uses of ChatGPT in Video Production",
                    "h3s": [
                        "Success Story 1: Creating Instagram Reels",
                        "Success Story 2: YouTube Video Production",
                        "Success Story 3: AI-Generated Video Monetization"
                    ]
                },
                {
                    "h2": "Expert Tips for Using AI in Video Editing",
                    "h3s": [
                        "Video Editing Tips: Secrets for Success",
                        "Understanding the 80/20 Rule in Video Editing",
                        "Maximizing the Potential of AI in Filmmaking"
                    ]
                },
                {
                    "h2": "Frequently Asked Questions",
                    "h3s": [
                        "Can ChatGPT make video reels?",
                        "Do AI-generated videos make money?",
                        "What is the best AI for filmmaking?",
                        "How to Make Money with AI Videos?",
                        "How I Use ChatGPT to Create Instagram Reels",
                        "How many views do you need on YouTube to make $1000 a month?",
                        "Is Video GPT free?",
                        "Is there any free AI video generator?"
                    ]
                },
                {
                    "h2": "Conclusion",
                    "h3s": []
                }
            ]
        },
        "competitor_analysis": [],
        "recommended_strategy": {
            "content_format": "Article",
            "strategic_goal": "Create a definitive guide that outranks competitors through superior depth and quality.",
            "focus_competitors": [
                {
                    "url": "https://chatgpt.com/g/g-h8l4uLHFQ-video-ai-by-invideo",
                    "title": "Video AI by invideo"
                },
                {
                    "url": "https://www.reddit.com/r/ChatGPT/comments/1dgpfxf/have_you_tried_chatgpt_for_creating_videos_here/",
                    "title": "Have you tried Chatgpt for creating videos? Here is 3 ways ..."
                },
                {
                    "url": "https://invideo.io/ai/video-gpt/",
                    "title": "Video GPT | Generate Videos in Minutes"
                }
            ],
            "final_qualification_assessment": {
                "scorecard": {
                    "hostility_score": 0,
                    "is_hostile_serp_environment": false,
                    "has_ai_overview": false,
                    "average_competitor_weaknesses": 2,
                    "has_clear_content_angle": true,
                    "is_intent_well_defined": true
                },
                "recommendation": "Proceed",
                "confidence_score": 100,
                "positive_factors": [
                    "A unique content angle has been identified.",
                    "User intent is well-defined by SERP features."
                ],
                "negative_factors": []
            }
        },
        "final_qualification_assessment": {
            "scorecard": {
                "hostility_score": 0,
                "is_hostile_serp_environment": false,
                "has_ai_overview": false,
                "average_competitor_weaknesses": 2,
                "has_clear_content_angle": true,
                "is_intent_well_defined": true
            },
            "recommendation": "Proceed",
            "confidence_score": 100,
            "positive_factors": [
                "A unique content angle has been identified.",
                "User intent is well-defined by SERP features."
            ],
            "negative_factors": []
        },
        "analysis_notes": "No qualified article-based competitors were found in the top results after rigorous qualification. This SERP may be dominated by social media, video, or other non-article formats, making it a challenging topic to rank for with a standard blog post.",
        "executive_summary": "The executive summary will be generated by the AI based on the full analysis when implementation is complete.",
        "ai_content_brief": {
            "target_keyword": "can chatgpt make videos",
            "content_type": "Article",
            "target_audience_persona": "a certified financial planner with 15 years of experience who writes like a human",
            "primary_goal": "To provide a comprehensive and helpful resource that ranks for 'can chatgpt make videos'.",
            "target_word_count": 1800,
            "mandatory_sections": [],
            "unique_angles_to_cover": [
                "Exploring the limitations and potential of ChatGPT in video creation, focusing on what it can and cannot do directly.",
                "A step-by-step guide on integrating ChatGPT with other AI tools like Sora and Invideo to create seamless video content.",
                "Case studies or success stories of users who have effectively used ChatGPT in their video production workflow."
            ],
            "questions_to_answer_directly": [
                "Can ChatGPT make video reels?",
                "Video editing tips: These are the secrets you need to know! - Voice123",
                "Do AI-generated videos make money?",
                "What is the best AI for filmmaking?",
                "How to Make Money with AI Videos (Honest Guide) - SendShort",
                "Can you use ChatGPT to create videos?",
                "What is the 80 20 rule in video editing?",
                "What is the best AI for creating videos?",
                "How I Use ChatGPT to Create Instagram Reels | by Bernard Builds",
                "How many views do you need on YouTube to make $1000 a month?",
                "Video GPT | Generate Videos in Minutes - Invideo AI",
                "Is video GPT free?",
                "Is there any free AI video generator?",
                "VideoGPT: Free AI Video Generator App - Create Anime Video in Seconds"
            ],
            "key_entities_to_mention": [
                "ChatGPT",
                "Sora",
                "Invideo",
                "Pictory",
                "Dream Machine",
                "VideoGPT",
                "OpenAI",
                "AI video maker",
                "YouTube",
                "Instagram Reels"
            ],
            "compelling_arguments_to_integrate": [],
            "core_questions_competitors_answer": [
                "Can ChatGPT make videos or video reels?",
                "How can ChatGPT be used in video editing and creation?",
                "What are the best AI tools for creating videos?",
                "How to make money with AI-generated videos?",
                "Is there a free AI video generator available?",
                "What is the process to create videos using ChatGPT and other AI tools?",
                "What are the limitations of ChatGPT in video creation?"
            ],
            "related_topics_to_include": [
                "Can chatgpt make videos reddit",
                "Can ChatGPT create videos for free",
                "ChatGPT video generator",
                "AI video generator",
                "Free AI video generator",
                "ChatGPT text to video",
                "ChatGPT video editor",
                "ChatGPT for video Summary"
            ],
            "google_preferred_answers": [],
            "dynamic_serp_instructions": [
                "Create a concise, clear paragraph early in the article that directly answers the main query to target the featured snippet.",
                "The SERP is fresh (0 days old). Ensure content reflects the latest information.",
                "Mention key entities identified from competitors: ChatGPT, Sora, Invideo, Pictory, Dream Machine, VideoGPT, OpenAI, AI video maker, YouTube, Instagram Reels."
            ],
            "source_and_inspiration_content": {
                "competitors_urls": []
            },
            "client_id": "Lark_Main_Site",
            "top_organic_sitelinks": [
                "Can any GPT's actually create original video clips from text?",
                "Can videos be made in ChatGPT currently? : r/OpenAI ...",
                "More results from www.reddit.com"
            ],
            "discussion_snippets": [
                "Make Videos Faster with Chat GPT - Beginner\u2019s Guide for 2025",
                "How to Create Cinematic AI Videos (No-BS Guide)",
                "10 Ways to Use ChatGPT So Well It Feels Illegal (Tutorial)",
                "How do y'all make videos with AI?",
                "How to Create AI Videos Using ChatGPT-4o: Quick Steps for AI-Generated Content",
                "Can You Create Videos In ChatGPT?",
                "\ud83d\udea8BREAKING: ChatGPT can now edit and create videos for free.\n\nYou don\u2019t need fancy software anymore.\n\nHere\u2019s how to do it (in 3 simple steps) \ud83d\udc47",
                "How I Use ChatGPT-5 to Create Videos That Print Money \ud83d\udcb0",
                "How to use ChatGPT - 4o as video editor"
            ]
        },
        "internal_linking_suggestions": [],
        "slug": "can-chatgpt-make-videos-1760982517"
    },
    "ai_content": {
        "article_body_html": "<h2>Introduction</h2>\n<p>Imagine a world where creating engaging video content is as effortless as having a conversation with a friend. In an era where 85% of internet users in the United States alone watch online videos monthly, video content is king. Yet, for many professionals, the thought of producing high-quality videos is daunting, time-consuming, and often expensive. Enter ChatGPT, a cutting-edge AI tool that promises to transform the way we create and consume video content.</p>\n\n<p>For seasoned financial planners like you, who have spent years honing their craft, the ability to seamlessly translate complex financial concepts into easily digestible video content can be a game-changer. But can ChatGPT really create videos, and if so, how does it integrate with your existing skill set? These questions are more than just intriguing\u2014they're crucial in a world increasingly dominated by digital content.</p>\n\n<p>In this post, we'll delve into the capabilities of ChatGPT in video creation, explore its potential impact on the financial industry, and offer practical tips for leveraging this technology to enhance your content strategy. Buckle up, as we unravel the future of AI-driven video production and what it means for professionals like you.</p>\n<h2>Understanding ChatGPT and Its Capabilities</h2>\n<p>Understanding the potential of ChatGPT begins with unraveling its fundamental nature. ChatGPT, developed by OpenAI, is a state-of-the-art language model designed to generate human-like text based on the input it receives. Trained on diverse datasets, it can simulate conversations, answer questions, and even assist in creative writing. In essence, ChatGPT serves as a virtual assistant capable of interacting with users in a remarkably conversational manner.</p>\n\n<h3>What is ChatGPT?</h3>\n<p>At its core, ChatGPT is a machine learning model that leverages Natural Language Processing (NLP) to understand and generate text. Powered by the groundbreaking Generative Pre-trained Transformer architecture, it can process and produce coherent responses that mimic human conversation. This makes it an invaluable tool in various domains, from customer service to educational content creation. However, when it comes to video production, its role becomes more nuanced.</p>\n\n<h3>The Role of ChatGPT in Content Creation</h3>\n<p>In the realm of content creation, ChatGPT is a versatile player. It can aid in scriptwriting, brainstorming ideas, and even generating detailed outlines for video content. For a financial planner seeking to convey complex investment strategies through video, ChatGPT can help draft engaging scripts that break down intricate topics into viewer-friendly narratives. This ability to ease the script development process empowers professionals to focus on delivering high-quality visual content with clear and compelling messages.</p>\n\n<p>Moreover, ChatGPT can serve as a collaborative partner, offering alternative perspectives and suggestions during the ideation phase. By providing diverse content angles, it facilitates the creation of rich, multifaceted video presentations that resonate with varied audiences. However, while ChatGPT excels in text generation, there are inherent limitations to its capabilities in video production.</p>\n\n<h3>Limitations of ChatGPT in Video Production</h3>\n<p>Despite its prowess in text-based tasks, ChatGPT does not possess the ability to directly create or edit video content. It lacks the visual processing capabilities necessary to manipulate video elements or integrate audio-visual components. For instance, while ChatGPT can help draft an informative script for a video on retirement planning, it cannot transform that script into a polished video presentation.</p>\n\n<p>To effectively produce video content, ChatGPT must be integrated with video editing software or platforms that translate its textual outputs into visual narratives. This often involves collaboration with tools that handle video rendering, animation, and post-production editing. As such, while ChatGPT offers a significant advantage in content ideation and planning, it remains a complementary tool rather than a standalone video production solution.</p>\n\n<p>In conclusion, understanding ChatGPT's capabilities and limitations is crucial for professionals seeking to leverage AI in video content strategy. By harnessing its strengths in content creation and pairing it with powerful video technologies, financial planners can streamline their production processes, ultimately delivering impactful and accessible financial insights through video.</p>\n<h2>Can ChatGPT Create Videos?</h2>\n<p>Moving from scriptwriting to video production, the question arises: Can ChatGPT create videos? The answer isn't straightforward. Understanding the capabilities and limitations of ChatGPT in the realm of video creation requires delving into the synergy between AI-driven text generation and video production technologies.</p>\n\n<h3>Exploring the Possibilities with ChatGPT</h3>\n<p>At its core, ChatGPT excels in generating text-based content, offering substantial support in ideating and scripting video concepts. When paired with complementary technologies, it can be a powerful ally in video creation. Here\u2019s how:</p>\n<ul>\n  <li><strong>Scriptwriting and Storyboarding:</strong> ChatGPT can draft engaging scripts and create detailed storyboards that convey the vision and flow of the video. This foundation is crucial for effective video production.</li>\n  <li><strong>Content Personalization:</strong> With its adaptability, ChatGPT can tailor content to specific audiences, ensuring that the video resonates with viewers, whether it\u2019s explaining complex financial terms or providing insights into market trends.</li>\n  <li><strong>Idea Generation:</strong> By analyzing market data and trends, ChatGPT can suggest innovative video topics that align with audience interests and organizational goals, ensuring a steady stream of fresh content ideas.</li>\n</ul>\n\n<h3>What ChatGPT Can and Cannot Do in Video Creation</h3>\n<p>While ChatGPT is invaluable in the pre-production phase, it's essential to acknowledge its limitations in the actual video creation process:</p>\n<ul>\n  <li><strong>Video Rendering:</strong> ChatGPT cannot directly render or edit video footage. Specialized software, such as Adobe Premiere Pro or Final Cut Pro, is needed to transform scripts into visual content.</li>\n  <li><strong>Animation and Visual Effects:</strong> AI tools like ChatGPT lack the capability to create animations or visual effects. These tasks require animation software like After Effects, often necessitating human creativity and expertise.</li>\n  <li><strong>Audio and Visual Synchronization:</strong> Synchronizing audio tracks with video clips is beyond ChatGPT\u2019s scope. This requires precise timing and editing skills that are best handled by dedicated video editing software.</li>\n</ul>\n<p>To sum up, while ChatGPT is not equipped to independently create videos, its contributions to content creation and planning are undeniable. By leveraging its strengths in generating ideas and crafting scripts, professionals can streamline video production processes when combined with the right technological tools. The future of video creation lies in the seamless integration of AI with traditional production methods, offering endless possibilities for storytelling and engagement.</p>\n<h2>Integrating ChatGPT with Other AI Tools for Video Creation</h2>\n<p>As the landscape of video production evolves, the integration of ChatGPT with other AI tools presents a new frontier in creating dynamic and engaging video content. While ChatGPT excels in generating scripts and storyboards, combining it with specialized AI platforms like Sora, Invideo, Pictory, and Dream Machine can unleash a new wave of creativity and efficiency.</p>\n\n<h3>Working with Sora</h3>\n<p>Sora, an AI-driven video editing tool, complements ChatGPT by transforming text-based scripts into visual content. By feeding ChatGPT-generated scripts into Sora, creators can automate the editing process, allowing the tool to select appropriate footage, transitions, and effects. Sora's ability to analyze textual content ensures that the video narrative aligns with the intended message, providing a seamless way to bring script ideas to life.</p>\n\n<h3>Using Invideo for Video Creation</h3>\n<p>Invideo offers a user-friendly platform for turning ChatGPT's scripts into polished videos. With its extensive library of templates and stock footage, Invideo simplifies the creation process. By integrating ChatGPT-generated content, users can easily customize these templates, ensuring that each video is unique and tailored to their specific needs. Invideo's intuitive interface allows even those with minimal editing experience to produce professional-quality videos.</p>\n\n<h3>Combining ChatGPT with Pictory and Dream Machine</h3>\n<p>When it comes to enhancing visual storytelling, Pictory and Dream Machine stand out as innovative AI tools that work harmoniously with ChatGPT. Pictory specializes in creating short, engaging video snippets from longer text, making it ideal for crafting teasers or promotional content from comprehensive scripts written by ChatGPT. Meanwhile, Dream Machine excels in generating imaginative and surreal visuals, offering creators the opportunity to incorporate unique visual elements into their videos, adding depth and intrigue to their narratives.</p>\n\n<p>By leveraging the power of these AI tools, creators can harness the combined strengths of text generation and video production, paving the way for more engaging, efficient, and innovative video content. This integration not only enhances productivity but also expands the creative possibilities, enabling storytellers to push the boundaries of traditional video creation.</p>\n<h2>Case Studies: Successful Uses of ChatGPT in Video Production</h2>\n<h3>Success Story 1: Creating Instagram Reels</h3>\n<p>The dynamic world of Instagram Reels is a testament to the immense potential of AI-driven content creation. A digital marketing agency, <strong>InstaSpark</strong>, leveraged ChatGPT to revolutionize their approach to producing Reels for fashion brands. By using ChatGPT to draft compelling scripts, the agency was able to enhance engagement with captivating narratives that resonated with the target audience. These scripts were then paired with Pictory's capabilities to create visually appealing short videos that captured attention within seconds. Not only did this approach streamline their production process, but it also doubled their clients' engagement rates, proving the effectiveness of AI in maximizing social media impact.</p>\n\n<h3>Success Story 2: YouTube Video Production</h3>\n<p><strong>ViralTech</strong>, a burgeoning tech review channel on YouTube, faced the challenge of producing content that was both informative and engaging. By incorporating ChatGPT, the team was able to generate detailed video scripts that broke down complex tech jargon into easy-to-understand language. This not only expanded their viewership but also improved viewer retention rates. The integration didn't stop with scripting; using Dream Machine, ViralTech added visually stunning AI-generated graphics to their videos, making their content visually appealing and informative, thereby setting a new standard in tech reviews.</p>\n\n<h3>Success Story 3: AI-Generated Video Monetization</h3>\n<p>In the niche world of culinary arts, <strong>ChefBytes</strong> has carved out a unique space by combining traditional cooking with innovative technology. Utilizing ChatGPT to create engaging storylines and scripts for their cooking tutorials, ChefBytes was able to enhance viewer interaction and subscription rates. The channel further capitalized on AI by using Dream Machine to generate eye-catching thumbnails and engaging intro sequences. This not only increased their ad revenue but also opened new monetization streams through sponsorships and collaborations, highlighting the potential of AI in transforming video content into a profitable venture.</p>\n\n<p>These case studies underscore the transformative power of AI in video production. By employing tools like ChatGPT, Pictory, and Dream Machine, creators can enhance creativity, improve efficiency, and unlock new revenue streams, crafting a future where AI is an indispensable partner in content creation.</p>\n<h2>Expert Tips for Using AI in Video Editing</h2>\n<p>With the transformative power of AI technologies in video production established, let's delve into expert strategies for integrating AI into video editing. As tools like ChatGPT, Pictory, and Dream Machine continue to evolve, they offer new avenues to enhance creativity, efficiency, and storytelling in filmmaking. Here are some expert tips to harness the potential of AI in video editing effectively:</p>\n\n<h3>Video Editing Tips: Secrets for Success</h3>\n<ul>\n  <li><strong>Start with a Vision:</strong> Before integrating AI into your editing process, have a clear vision of the story you want to tell. AI tools can aid in realizing this vision by providing automated suggestions, but they work best when aligned with a well-defined creative direction.</li>\n  <li><strong>Utilize AI for Routine Tasks:</strong> Leverage AI to handle repetitive tasks like color correction, transitions, and audio syncing. This enables editors to focus more on the creative aspects that require human intuition and artistic touch.</li>\n  <li><strong>Experiment with AI-generated Effects:</strong> Use AI to test new visual effects and transitions that might not have been considered otherwise. Many AI tools offer libraries of effects that can be customized to fit your video\u2019s unique style.</li>\n</ul>\n\n<h3>Understanding the 80/20 Rule in Video Editing</h3>\n<p>The Pareto Principle, or the 80/20 Rule, posits that 80% of outcomes come from 20% of the efforts. In the context of video editing, this can be a game-changer when using AI:</p>\n<ul>\n  <li><strong>Focus on the Core Elements:</strong> Identify the key components of your video that will have the most significant impact on your audience and use AI to streamline the rest. This could include automating tedious tasks to dedicate more time to perfecting the narrative and engaging the viewer.</li>\n  <li><strong>Leverage AI Analytics:</strong> Use AI-driven analytics to understand which parts of your videos resonate most with viewers and refine your editing process accordingly. This data-driven approach allows for more targeted improvements and greater viewer satisfaction.</li>\n</ul>\n\n<h3>Maximizing the Potential of AI in Filmmaking</h3>\n<p>To truly maximize the potential of AI in filmmaking, it\u2019s essential to see it as a collaborative tool rather than a replacement for human creativity:</p>\n<ul>\n  <li><strong>Incorporate AI in Pre-Production:</strong> Use AI to analyze scripts and storyboard layouts for pacing and plot coherence before filming begins. This preemptive step can save time and resources during the editing phase.</li>\n  <li><strong>Customize AI Models:</strong> Train AI models with your unique style and preferences to get more personalized suggestions and edits that align with your vision.</li>\n  <li><strong>Stay Updated:</strong> The field of AI in video editing is rapidly evolving. Regularly update your tools and skills to stay ahead of trends and incorporate the latest advancements into your workflow.</li>\n</ul>\n\n<p>By thoughtfully integrating AI into your video editing process, you can unlock unprecedented efficiencies and creative possibilities, ensuring your content not only captivates but also leaves a lasting impact on your audience.</p>\n<h2>Frequently Asked Questions</h2>\n<h3>Can ChatGPT Make Video Reels?</h3>\n<p>While ChatGPT is an advanced AI language model developed by OpenAI, it does not directly generate video content, including reels. However, it can assist creators in the ideation phase. For instance, you can use ChatGPT to brainstorm video concepts, generate scripts, or even suggest optimal hashtags and captions that can enhance the visibility of your reels on platforms like Instagram.</p>\n\n<h3>Do AI-Generated Videos Make Money?</h3>\n<p>Absolutely, AI-generated videos can be monetized across various platforms. Content creators often use AI tools to enhance video quality, automate editing processes, and even generate subtitles, all of which contribute to a more engaging viewer experience. Monetization strategies include ad revenue from platforms like YouTube, sponsored content, and direct sales of AI-enhanced video services to clients.</p>\n\n<h3>What is the Best AI for Filmmaking?</h3>\n<p>The \"best\" AI for filmmaking depends on your specific needs. Tools like RunwayML and Synthesia are popular among creators for their advanced features. RunwayML offers a collaborative platform for video editing and visual effects, while Synthesia specializes in AI-generated video content, including avatars that can deliver scripted content in multiple languages, enhancing global reach.</p>\n\n<h3>How to Make Money with AI Videos?</h3>\n<p>There are several pathways to monetize AI-produced videos. Start by creating content for your own social media channels, leveraging AI to boost production quality and efficiency. Additionally, consider offering AI video production services to businesses looking to streamline their content creation. Platforms like Patreon or Ko-fi can also help creators earn income through fan support and exclusive content offerings.</p>\n\n<h3>How I Use ChatGPT to Create Instagram Reels</h3>\n<p>Using ChatGPT for Instagram reels involves tapping into its natural language processing capabilities. I often start by asking ChatGPT for trendy topics or script ideas. Once I settle on a concept, I use its suggestions to craft engaging captions and select hashtags that increase discoverability. This strategic approach ensures that my reels are not only creative but also optimized for audience engagement.</p>\n\n<h3>How Many Views Do You Need on YouTube to Make $1000 a Month?</h3>\n<p>To earn $1000 a month on YouTube, creators typically need around 100,000 views per month, assuming an average RPM (Revenue Per Mille) of $10. This figure can vary based on factors like audience demographics, content niche, and engagement rates. Therefore, combining AI tools for video optimization with effective marketing strategies can help achieve these viewership targets more efficiently.</p>\n\n<h3>Is Video GPT Free?</h3>\n<p>As of now, Video GPT is not a standalone, freely available tool. It usually refers to AI-driven video solutions offered by various companies, many of which come with subscription fees. However, platforms like OpenAI do offer free trials for some of their language models, enabling users to explore their capabilities before committing to a paid plan.</p>\n\n<h3>Is There Any Free AI Video Generator?</h3>\n<p>Yes, there are several free AI video generators available. Tools like Lumen5 and InVideo offer free plans with basic features, which can be a great starting point for creators who are new to AI-enhanced video production. These platforms allow users to convert text into video, making them ideal for creating quick, engaging content without a significant financial investment.</p>\n<h2>Conclusion</h2>\n<p>In summary, ChatGPT has proven to be a revolutionary tool in the realm of content creation, especially when it comes to scriptwriting and ideation for video production. Its ability to transform complex topics into engaging narratives provides a significant advantage for professionals looking to captivate their audience with clarity and precision. While ChatGPT shines in the pre-production phase, its integration with other AI tools is essential for creating fully-fledged video content, thus marking it as an invaluable asset in the digital storyteller's toolkit.</p>\n\n<p>To truly harness the power of ChatGPT, professionals should embrace it as a collaborative partner that complements their creative vision. By combining its strengths with comprehensive video editing software or platforms, you can elevate your content strategy to new heights. Whether you're a financial planner aiming to demystify investment strategies or a content creator exploring new narratives, the future of video production beckons with endless possibilities. Now is the time to dive in and explore the potential of AI-driven innovation. Visit <a href=\"#\">[insert link here]</a> to discover more about how you can integrate ChatGPT into your workflow and redefine your approach to video content creation.</p>",
        "audit_results": {
            "flesch_kincaid_grade": 15.240637183136275,
            "smog_index": 17.425139229785795,
            "coleman_liau_index": 15.75543136570031,
            "readability_assessment": "Flesch-Kincaid Grade Level: 15.2. Assessment: Appropriate complexity for an expert audience.",
            "refinement_command": null,
            "entity_coverage_score": 80.0,
            "missing_entities": [
                "VideoGPT",
                "AI video maker"
            ],
            "covered_sections": null,
            "publish_readiness_issues": [
                {
                    "issue": "word_count_deviation",
                    "context": "Actual count (2865) deviates from target (1800) by more than 20%."
                }
            ]
        }
    }
}
</file>

<file path="run_dev.sh">
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# --- Cleanup Logic ---
echo "Attempting to free up ports 3000 and 8000..."
lsof -ti:3000 | xargs kill -9 || true
lsof -ti:8000 | xargs kill -9 || true
echo "Ports have been cleared."
# --- End Cleanup ---

# Function to clean up all child processes on exit
cleanup() {
    echo "Shutting down servers..."
    # The pkill command is more reliable for finding and killing all child processes.
    pkill -P $$
}
trap cleanup EXIT

# 1. Create a logs directory and start the backend in the background
echo "Starting FastAPI backend..."
mkdir -p logs
LOG_FILE="logs/backend-$(date +'%Y-%m-%d_%H-%M-%S').log"
(uvicorn backend.api.main:app --host 0.0.0.0 --port 8000 --reload) > "$LOG_FILE" 2>&1 &
echo "Backend logs are being written to: $LOG_FILE"

# 2. Wait for the backend to be ready
echo "Waiting for backend to start on port 8000..."
while ! nc -z localhost 8000; do
  sleep 0.1 # wait for a moment before checking again
done
echo "Backend is ready."

# 3. Start the frontend in the foreground
echo "Starting React frontend..."
(cd client/my-content-app && npm run dev)

# When the user presses Ctrl+C, the trap will run the cleanup function.
</file>

<file path="workflow_analysis.md">
# Workflow Analysis Report

This report details the findings from a comprehensive review of the backend workflow, focusing on the keyword discovery and analysis pipeline. It identifies key weaknesses that could be improved for efficiency and reliability, as well as missed opportunities for new features and cost savings.

---

## 10 Weaknesses in the Current Workflow

1.  **Excessive API Cost in Discovery:** The single biggest weakness is making an expensive `serp_advanced` call for *every* keyword that passes the initial metric filter. This is highly inefficient and burns through API credits on keywords that may still be poor fits.
2.  **Redundant `on_page_content` Calls:** The discovery phase immediately fetches the on-page content for all competitors of a "blog opportunity." This data is not used until the much later, manual `Analysis` phase. This is a premature, wasteful API call.
3.  **Single Point of Failure in Orchestrator:** The main `WorkflowOrchestrator` class is a monolith that inherits from all other orchestrator classes. This creates a tightly coupled system where a failure or change in one part (e.g., `ImageOrchestrator`) can break the entire application, even parts that are unrelated.
4.  **Lack of a Staging/Validation Step:** Keywords go directly from automated discovery to a list awaiting manual approval. There is no intermediate step where a user can trigger a more in-depth (but still automated) validation, forcing a binary choice between a full, expensive analysis or nothing.
5.  **Hardcoded "Blog Opportunity" Logic:** The `SerpAnalysisService` hardcodes the definition of a blog opportunity as "at least 3 blog/news articles in the top 10." This is inflexible and cannot be customized for different client strategies (e.g., a client who is happy to compete if there's only one blog, or one who wants to see forums).
6.  **No API Retry Logic for Transient Errors:** The `_post_request` function in the `DataForSEOClientV2` has retry logic, but it's basic. It doesn't differentiate between error types. A 500 server error from DataForSEO is treated the same as a 429 rate limit error, leading to unnecessary retries or premature failures.
7.  **Orchestrator Instantiated on Every API Call:** The `get_orchestrator` dependency creates a new `WorkflowOrchestrator` instance for every single API request. This is inefficient as it re-initializes all clients and services (like the DataForSEO and OpenAI clients) repeatedly, adding unnecessary overhead.
8.  **In-Memory Filtering Limitations:** The `KeywordExpander` fetches a large list of keywords and then filters them in memory. This is not scalable. For a very broad seed keyword, this could lead to high memory consumption and potentially crash the application.
9.  **Lack of Granular Cost Tracking:** The system tracks the `total_cost` of a discovery run, but it doesn't break it down by API call type (e.g., `keyword_ideas` vs. `serp_advanced`). This makes it difficult to identify which parts of the process are the most expensive and where to optimize.
10. **Configuration Mixed Between DB and Files:** Some configuration (like API keys) is stored in the database, while other settings are in `settings.ini`. This split can make configuration management confusing and error-prone, especially in a multi-client environment.

---

## 10 Missed Opportunities in the Current Workflow

1.  **Manual SERP Validation Step:** The most significant missed opportunity. Introduce a manual "Qualify" button for each opportunity. This would trigger the expensive `serp_advanced` and `on_page_content` calls on-demand, giving the user full control over API spending.
2.  **Leverage SERP `serp_item_types` for Deeper Scoring:** The initial Keywords Data API call returns a `serp_item_types` list. The current scoring engine only uses this superficially. This could be used to create much richer scoring components, such as a "Video Opportunity Score" (if `video` is present) or a "Local Intent Score" (if `local_pack` is present).
3.  **Cache API Responses More Intelligently:** The current caching is based on the exact request payload. A smarter cache could recognize that a `serp_advanced` call for "how to bake a cake" is the same regardless of which discovery run initiated it, leading to significant cross-run cost savings.
4.  **Use AI to Summarize Competitor Content:** Instead of just downloading competitor content, use an OpenAI call to generate a summary of the top 3 articles. This could be displayed in the UI, giving the user a much faster way to assess the competitive landscape without having to read the full articles.
5.  **Create a "Keyword Clustering" Feature:** After discovery, the system could automatically group semantically related keywords (e.g., "how to bake a cake," "cake baking recipe," "easy cake recipe") into clusters. This would help the user plan content more effectively and avoid cannibalization.
6.  **Analyze SERP Volatility Over Time:** The `serp_info` object contains `last_updated_time` and `previous_updated_time`. This data is currently unused. It could be used to calculate a SERP volatility score, identifying keywords where the rankings are unstable and easier to break into.
7.  **Automated Internal Linking Suggestions:** The system has an `InternalLinkingSuggester`, but it's not integrated into the main workflow. After a new article is approved, the system could automatically scan existing content and suggest relevant internal linking opportunities.
8.  **"Low-Hanging Fruit" Dashboard Widget:** Create a dashboard component that automatically highlights opportunities that have a high Strategic Score but low Keyword Difficulty and a weak set of competitors (based on the initial `avg_backlinks_info`).
9.  **Track Keyword Ranking Post-Publication:** Once an article is published, the workflow could be extended to periodically run a `serp_advanced` call for its target keyword to track its ranking performance over time, providing valuable feedback on the success of the content.
10. **Dynamic Disqualification Rules:** The disqualification rules are currently hardcoded in Python. These could be moved to the database and managed via the UI, allowing users to create their own custom rules (e.g., "reject any keyword containing the word 'free'") without needing to change the code.
</file>

</files>