Review the Discovery Page and all the related files in the baceknd and find out all the issues you can spot and also give a list of 10 weaknesses in the current discovery page and its related backend which can effect the 
actual user experiecne and funcionaity. 



FRONTEND:

This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-10-30T20:52:24.029Z

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Info

# Directory Structure
```
my-content-app/
  src/
    components/
      layout/
        AppSidebar.jsx
        MainLayout.jsx
      ContentDiffViewer.jsx
      CostConfirmationModal.jsx
      GlobalJobTracker.jsx
      JobStatusIndicator.jsx
      OpportunityTable.jsx
      PromptTemplateEditor.jsx
    context/
      AuthContext.jsx
      ClientContext.jsx
      JobContext.jsx
      NotificationContext.jsx
    hooks/
      useClient.js
      useDebounce.js
    pages/
      ActivityLog/
        ActivityLogPage.jsx
      Auth/
        LoginPage.jsx
      BlogPage/
        BlogPage.css
        BlogPage.jsx
      ClientDashboard/
        AddNewClientModal.jsx
        ClientDashboardPage.jsx
      ClientSettings/
        ClientSettingsPage.jsx
      Dashboard/
        DashboardPage.jsx
      DiscoveryPage/
        components/
          DiscoveryForm.jsx
          DiscoveryHistory.jsx
          DiscoveryStatsBreakdown.jsx
          FilterBuilder.jsx
          FunnelChart.css
          FunnelChart.jsx
          PieChartCard.jsx
          RunDetailsModal.jsx
          RunDetailsPage.jsx
        hooks/
          useDiscoveryFilters.js
          useDiscoveryRuns.js
        DiscoveryPage.jsx
      NotFoundPage/
        NotFoundPage.jsx
      OpportunitiesPage/
        components/
          ScoreBreakdownModal.jsx
        hooks/
          useOpportunities.js
          useOpportunities.refactored.js
        OpportunitiesPage.css
        OpportunitiesPage.jsx
      opportunity-detail-page/
        components/
          ActionCenter.jsx
          AdditionalInsights.jsx
          ArticlePreview.jsx
          CompetitorBacklinks.jsx
          ContentAuditCard.jsx
          ContentBlueprint.jsx
          ErrorMessage.jsx
          ExecutiveSummary.jsx
          FactorsCard.jsx
          FeaturedSnippetCard.jsx
          GrowthTrend.jsx
          IntentAnalysis.jsx
          KeywordMetrics.jsx
          MetaInfo.jsx
          NoData.jsx
          OpportunityHeader.jsx
          QualificationInfo.jsx
          RecommendedStrategyCard.jsx
          SerpAnalysis.jsx
          SerpVitals.jsx
          SocialMediaTab.jsx
          StrategicNotes.jsx
          StrategicScoreBreakdown.jsx
          VerdictCard.jsx
          WorkflowStatusAlert.jsx
          WorkflowTracker.jsx
        hooks/
          useOpportunityData.js
        index.jsx
      RunDetailsPage/
        RunDetailsPage.jsx
      Settings/
        tabs/
          AiContentSettingsTab.jsx
          DiscoverySettingsTab.jsx
          ScoringWeightsTab.jsx
        SettingsPage.jsx
    services/
      apiClient.js
      authService.js
      clientService.js
      clientSettingsService.js
      discoveryService.js
      jobsService.js
      opportunitiesService.js
      orchestratorService.js
      settingsService.js
    App.jsx
    index.css
    main.jsx
  .dockerignore
  .eslintignore
  .eslintrc.json
  Dockerfile
  index.html
  nginx.conf
  package.json
  vite.config.js
```

# Files

## File: my-content-app/src/components/layout/AppSidebar.jsx
```javascript
import React from 'react';
import { Layout, Menu, Typography } from 'antd';
import { RocketOutlined, BulbOutlined } from '@ant-design/icons';
import { NavLink, useLocation } from 'react-router-dom';

const { Sider } = Layout;
const { Title } = Typography;

// Menu items for the sidebar
const menuItems = [
  {
    key: '/',
    icon: <RocketOutlined />,
    label: <NavLink to="/">Discovery</NavLink>,
  },
  {
    key: '/opportunities',
    icon: <BulbOutlined />,
    label: <NavLink to="/opportunities">Opportunities</NavLink>,
  },
  // Add other navigation links here as you build new pages
  // {
  //   key: '/pipeline',
  //   icon: <BranchesOutlined />,
  //   label: <NavLink to="/pipeline">Content Pipeline</NavLink>,
  // },
];

const AppSidebar = () => {
  const location = useLocation(); // To highlight the active menu item

  return (
    <Sider collapsible breakpoint="lg" collapsedWidth="0">
      <div style={{ padding: '16px', textAlign: 'center' }}>
        <Title level={4} style={{ color: 'white', margin: 0 }}>Content AI</Title>
      </div>
      <Menu
        theme="dark"
        mode="inline"
        selectedKeys={[location.pathname]} // Highlight the current path
        items={menuItems}
      />
    </Sider>
  );
};

export default AppSidebar;
```

## File: my-content-app/src/components/layout/MainLayout.jsx
```javascript
import { Menu, Layout, Typography, Button, Space, Select, Spin, Input, Card, Tag } from 'antd'; // Add Button and Typography
import { DashboardOutlined, SettingOutlined, LogoutOutlined, BulbOutlined, RocketOutlined, ClockCircleOutlined } from '@ant-design/icons'; // Add new icons
import { NavLink, useLocation, useNavigate, Outlet } from 'react-router-dom'; // Add useNavigate
import { useAuth } from '../../context/AuthContext'; // NEW
import { useClient } from '../../context/ClientContext'; // NEW
import { getClients, searchAllAssets } from '../../services/clientService'; // NEW
import { useQuery } from 'react-query'; // NEW
import useDebounce from '../../hooks/useDebounce'; // Assuming you have a useDebounce hook
import React, { useState, useEffect } from 'react';

const { Sider, Header, Content } = Layout;
const { Title, Text } = Typography;

// REPLACE the existing `menuItems` definition with this:
const menuItems = [
  {
    key: '/dashboard',
    icon: <DashboardOutlined />,
    label: <NavLink to="/dashboard">Dashboard</NavLink>,
  },
  {
    key: '/opportunities',
    icon: <BulbOutlined />,
    label: <NavLink to="/opportunities">Opportunities</NavLink>,
  },
  {
    key: '/discovery',
    icon: <RocketOutlined />,
    label: <NavLink to="/discovery">Discovery</NavLink>,
  },
  {
    key: '/activity-log',
    icon: <ClockCircleOutlined />,
    label: <NavLink to="/activity-log">Activity Log</NavLink>,
  },
  {
    key: '/settings',
    icon: <SettingOutlined />,
    label: <NavLink to="/settings">Settings</NavLink>,
  },
];

// REPLACE the existing `MainLayout` component with this:
const MainLayout = () => {
  const location = useLocation();
  const navigate = useNavigate();
  const { logout } = useAuth();
  const { clientId, setClientId } = useClient();
  const { data: clients = [], isLoading: isLoadingClients } = useQuery('clients', getClients);

  const [globalSearchResults, setGlobalSearchResults] = useState([]);
  const [isSearchLoading, setIsSearchLoading] = useState(false);
  const [searchTerm, setSearchTerm] = useState('');
  const debouncedSearchTerm = useDebounce(searchTerm, 500);

  useEffect(() => {
    const handleGlobalSearch = async () => {
      if (!debouncedSearchTerm || debouncedSearchTerm.length < 3 || !clientId) {
        setGlobalSearchResults([]);
        return;
      }
      setIsSearchLoading(true);
      try {
        const results = await searchAllAssets(clientId, debouncedSearchTerm);
        setGlobalSearchResults(results);
      } catch (error) {
        console.error("Global search failed:", error);
        setGlobalSearchResults([]);
      } finally {
        setIsSearchLoading(false);
      }
    };

    handleGlobalSearch();
  }, [debouncedSearchTerm, clientId]);

  const handleSearchResultClick = (e) => {
    const [type, id] = e.key.split('-');
    setGlobalSearchResults([]); // Clear search results after selection
    if (type === 'opportunity') navigate(`/opportunities/${id}`);
    if (type === 'discovery_run') navigate(`/discovery-run/${id}`);
    // Add more navigation logic for other types if needed
  };

  const handleLogout = async () => {
    await logout();
    navigate('/login');
  };

  const handleSelectClientFromDropdown = (value) => {
    setClientId(value);
    localStorage.setItem('clientId', value);
    navigate('/dashboard'); // CRITICAL FIX: Redirect to a safe page (dashboard) after switching client
  };

  return (
    <Layout style={{ minHeight: '100vh' }}>
      <Sider collapsible breakpoint="lg" collapsedWidth="0">
        <div style={{ padding: '16px', textAlign: 'center' }}>
          <Title level={4} style={{ color: 'white', margin: 0 }}>Content AI</Title>
          <Text style={{ color: 'rgba(255,255,255,0.6)', fontSize: '0.8em' }}>
            {isLoadingClients ? 'Loading Clients...' : `Client: ${clients.find(c => c.client_id === clientId)?.client_name || clientId}`}
          </Text>
        </div>
        <Menu
          theme="dark"
          mode="inline"
          selectedKeys={[location.pathname]}
          items={menuItems}
        />
      </Sider>
      <Layout>
        <Header style={{ background: '#fff', padding: '0 16px', display: 'flex', justifyContent: 'space-between', alignItems: 'center', borderBottom: '1px solid #f0f0f0' }}>
<div style={{ padding: '0 16px', color: 'rgba(255, 255, 255, 0.65)' }}>
  <Space>
    {isLoadingClients ? <Spin size="small" /> : `Client: ${clients.find(c => c.client_id === clientId)?.client_name || 'N/A'}`}
  </Space>
</div>
          <div style={{ flexGrow: 1, margin: '0 20px', maxWidth: '400px' }}>
            <Input.Search
              placeholder="Search keywords, opportunities, runs..."
              onChange={(e) => setSearchTerm(e.target.value)}
              style={{ width: '100%' }}
              allowClear
              loading={isSearchLoading}
            />
            {globalSearchResults.length > 0 && (
              <Card
                size="small"
                style={{ position: 'absolute', zIndex: 100, width: 'inherit', marginTop: '5px' }}
                bodyStyle={{ padding: '0px' }}
              >
                <Menu
  onClick={handleSearchResultClick}
  style={{ width: '100%', borderRight: 0 }}
>
  {globalSearchResults.map(item => {
    const key = `${item.type}-${item.id}`;
    let labelText = item.keyword || item.name || 'N/A';
    let typeTag = item.type.replace(/_/g, ' ').toUpperCase();

    return (
      <Menu.Item key={key} icon={item.type === 'opportunity' ? <BulbOutlined /> : <RocketOutlined />}>
        <Space size="small">
          <Text ellipsis={true} style={{ maxWidth: '200px' }}>{labelText}</Text>
          <Tag color={item.type === 'opportunity' ? 'blue' : 'purple'}>{typeTag}</Tag>
        </Space>
      </Menu.Item>
    );
  })}
</Menu>
              </Card>
            )}
          </div>
          <Space>
            <Select
              value={clientId}
              style={{ width: 180 }}
              onChange={handleSelectClientFromDropdown}
              loading={isLoadingClients}
              disabled={isLoadingClients || clients.length === 0}
              options={clients.map(c => ({ value: c.client_id, label: c.client_name }))}
            />
            <Button icon={<LogoutOutlined />} onClick={handleLogout}>Logout</Button>
          </Space>
        </Header>
        <Content style={{ margin: '16px' }}>
          <Outlet />
        </Content>
      </Layout>
    </Layout>
  );
};

export default MainLayout;
```

## File: my-content-app/src/components/ContentDiffViewer.jsx
```javascript
// This is a new file. Create it with the following content:
// You'll need to install react-diff-viewer: npm install react-diff-viewer
import React from 'react';
import DiffViewer from 'react-diff-viewer';
import { Typography } from 'antd';

const { Text } = Typography;

const ContentDiffViewer = ({ oldValue, newValue, oldTitle = "Previous Version", newTitle = "Current Version" }) => {
  if (!oldValue && !newValue) {
    return <Text type="secondary">No content to compare.</Text>;
  }
  if (oldValue === newValue) {
    return <Text type="success">Content is identical.</Text>;
  }

  return (
    <DiffViewer
      oldValue={oldValue || ''}
      newValue={newValue || ''}
      splitView={true}
      leftTitle={oldTitle}
      rightTitle={newTitle}
      showDiffOnly={false} // Show entire file with diffs highlighted
      use={true} // Enable styling
      styles={{
        variables: {
          dark: {
            diffViewerBackground: '#262626',
            diffViewerColor: '#f0f2f5',
            addedBackground: '#003a29',
            removedBackground: '#320a0b',
            wordAddedBackground: '#006d3d',
            wordRemovedBackground: '#9e2b2f',
          },
        },
        diffContainer: {
          fontSize: '12px',
          fontFamily: 'monospace',
        },
        line: {
          wordBreak: 'break-word',
        }
      }}
    />
  );
};

export default ContentDiffViewer;
```

## File: my-content-app/src/components/CostConfirmationModal.jsx
```javascript
// This is a new file. Create it with the following content:
import React from 'react';
import { Modal, Typography, Spin, Alert, List } from 'antd';
import { useQuery } from 'react-query';
import { estimateActionCost } from '../services/orchestratorService'; // NEW

const { Title, Text, Paragraph } = Typography;

const CostConfirmationModal = ({ open, onCancel, onConfirm, opportunityId, actionType }) => {
  const { data: costEstimate, isLoading, isError, error } = useQuery(
    ['costEstimate', opportunityId, actionType],
    () => estimateActionCost(opportunityId, actionType),
    {
      enabled: open && !!opportunityId && !!actionType, // Only fetch when modal is open
      staleTime: 5 * 60 * 1000, // Cost estimates don't change frequently
    }
  );

  const getActionTitle = (type) => {
    switch (type) {
      case 'analyze': return 'Run Deep-Dive Analysis';
      case 'generate': return 'Generate Full Content Package';
      case 'refresh': return 'Refresh Content (Analysis & Generation)';
      case 'validate': return 'Run Live SERP Validation';
      default: return 'Perform Action';
    }
  };

  return (
    <Modal
      title={getActionTitle(actionType)}
      open={open}
      onCancel={onCancel}
      onOk={onConfirm}
      confirmLoading={isLoading}
      okText="Confirm & Proceed"
      cancelText="Cancel"
    >
      {isLoading ? (
        <Spin tip="Estimating API costs..." />
      ) : isError ? (
        <Alert
          message="Error Estimating Cost"
          description={error?.message || 'Could not fetch cost estimation. Proceed with caution.'}
          type="error"
          showIcon
        />
      ) : (
        <>
          <Paragraph>
            This action will incur API costs. Please review the estimate below before proceeding.
          </Paragraph>
          <List
            size="small"
            bordered
            dataSource={costEstimate?.breakdown || []}
            renderItem={item => (
    <List.Item
        actions={[
            <Text key="cost" strong>
                {item.cost ? `$${item.cost.toFixed(4)}` : item.cost === 0 ? '$0.0000' : 'N/A'}
            </Text>
        ]}
    >
        <List.Item.Meta
            title={item.service}
            description={item.details}
        />
    </List.Item>
)}
footer={
    <Paragraph strong style={{ fontSize: '1.2em' }}>Estimated Total: <Text code>${costEstimate?.total_cost?.toFixed(4) || '0.0000'}</Text> USD</Paragraph>
}
/>
<Alert
            message="This is an estimate. Actual costs may vary."
            type="info"
            showIcon
            style={{ marginTop: '16px' }}
          />
        </>
      )}
    </Modal>
  );
};

export default CostConfirmationModal;
```

## File: my-content-app/src/components/GlobalJobTracker.jsx
```javascript
import React from 'react';
import { Alert, Spin } from 'antd';
import { useJobs } from '../context/JobContext';
import { CheckCircleOutlined, CloseCircleOutlined } from '@ant-design/icons';

const GlobalJobTracker = () => {
  const { activeJobs } = useJobs();

  if (Object.keys(activeJobs).length === 0) {
    return null;
  }

  return (
    <div style={{ position: 'fixed', bottom: 24, right: 24, zIndex: 1000, display: 'flex', flexDirection: 'column', gap: '16px' }}>
      {Object.entries(activeJobs).map(([jobId, job]) => {
        let icon, type;
        switch (job.status) {
          case 'running':
            icon = <Spin />;
            type = 'info';
            break;
          case 'completed':
            icon = <CheckCircleOutlined />;
            type = 'success';
            break;
          case 'failed':
            icon = <CloseCircleOutlined />;
            type = 'error';
            break;
          default:
            icon = <Spin />;
            type = 'info';
        }

        return (
          <Alert
            key={jobId}
            message={`Job Status: ${job.status.charAt(0).toUpperCase() + job.status.slice(1)}`}
            description={job.message}
            type={type}
            showIcon
            icon={icon}
            style={{ boxShadow: '0 2px 8px rgba(0,0,0,0.15)' }}
          />
        );
      })}
    </div>
  );
};

export default GlobalJobTracker;
```

## File: my-content-app/src/components/JobStatusIndicator.jsx
```javascript
import React from 'react';
import { useQuery } from 'react-query';
import { getJobStatus } from '../services/orchestratorService'; // This service function needs to be created
import { Progress, Tag, Tooltip } from 'antd';
import { LoadingOutlined, CheckCircleOutlined, CloseCircleOutlined, ClockCircleOutlined, PauseCircleOutlined } from '@ant-design/icons'; // ADD PauseCircleOutlined

const JobStatusIndicator = ({ jobId }) => {
  const { data: job, isLoading } = useQuery(
    ['jobStatus', jobId],
    () => getJobStatus(jobId),
    {
      refetchInterval: (data) => (data?.status === 'running' || data?.status === 'pending' || data?.status === 'paused' ? 3000 : false), // Refetch for paused jobs too
      enabled: !!jobId,
    }
  );

  if (isLoading && !job) return <Tag icon={<LoadingOutlined spin />}>Loading...</Tag>;
  if (!job) return <Tag>Unknown</Tag>;

  const statusConfig = {
    pending: { icon: <LoadingOutlined />, color: 'default', text: 'Pending' },
    running: { icon: <LoadingOutlined spin />, color: 'processing', text: job.result?.step || 'Running...' },
    completed: { icon: <CheckCircleOutlined />, color: 'success', text: 'Completed' },
    failed: { icon: <CloseCircleOutlined />, color: 'error', text: 'Failed' },
    paused: { icon: <PauseCircleOutlined />, color: 'warning', text: 'Paused (Awaiting Approval)' }, // NEW text
  };
  
  const config = statusConfig[job.status] || { icon: <ClockCircleOutlined />, color: 'default', text: 'Queued' };

  return (
    <Tooltip title={job.error || job.result?.message || job.status_message}> {/* Include job.status_message */}
      <div style={{ display: 'flex', alignItems: 'center', gap: 4 }}>
        <Tag icon={config.icon} color={config.color}>{config.text}</Tag>
        {(job.status === 'running' || job.status === 'paused') && ( // Show progress for paused as well
          <Progress percent={job.progress} size="small" status="active" showInfo={false} style={{ width: '100px', margin: 0 }} />
        )}
      </div>
    </Tooltip>
  );
};
export default JobStatusIndicator;
```

## File: my-content-app/src/components/OpportunityTable.jsx
```javascript
import React from 'react';
import { Table, Tag } from 'antd';
import { Link } from 'react-router-dom';

const OpportunityTable = ({ opportunities, isLoading }) => {
  const columns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
      render: (text, record) => <Link to={`/opportunities/${record.id}`}>{text}</Link>,
    },
    {
      title: 'Search Volume',
      dataIndex: ['full_data', 'keyword_info', 'search_volume'],
      key: 'search_volume',
      sorter: (a, b) => a.full_data.keyword_info.search_volume - b.full_data.keyword_info.search_volume,
    },
    {
      title: 'Keyword Difficulty',
      dataIndex: ['full_data', 'keyword_properties', 'keyword_difficulty'],
      key: 'keyword_difficulty',
      sorter: (a, b) => a.full_data.keyword_properties.keyword_difficulty - b.full_data.keyword_properties.keyword_difficulty,
    },
    {
      title: 'CPC',
      dataIndex: ['full_data', 'keyword_info', 'cpc'],
      key: 'cpc',
      sorter: (a, b) => a.full_data.keyword_info.cpc - b.full_data.keyword_info.cpc,
    },
    {
      title: 'Status',
      dataIndex: 'status',
      key: 'status',
      render: status => {
        let color = 'geekblue';
        if (status === 'qualified') {
          color = 'green';
        } else if (status === 'rejected') {
          color = 'volcano';
        }
        return (
          <Tag color={color} key={status}>
            {status.toUpperCase()}
          </Tag>
        );
      },
    },
    {
      title: 'Rejected Reason',
      dataIndex: 'blog_qualification_reason',
      key: 'blog_qualification_reason',
      render: (text, record) => record.status === 'rejected' ? text : '-',
    },
  ];

  return (
    <Table
      columns={columns}
      dataSource={opportunities}
      loading={isLoading}
      rowKey="id"
    />
  );
};

export default OpportunityTable;
```

## File: my-content-app/src/components/PromptTemplateEditor.jsx
```javascript
// This is a new file. Create it with the following content:
import React, { useRef, useState, useEffect } from 'react';
import { Input, Button, Space, Tooltip, Typography, List, Divider, Row, Col, Card } from 'antd';
import { BulbOutlined, CopyOutlined } from '@ant-design/icons';

const { TextArea } = Input;
const { Text } = Typography;

const PLACEHOLDERS = [
  { name: "[TOPIC]", desc: "The main keyword or topic of the article." },
  { name: "[PRIMARY KEYWORD]", desc: "The exact target keyword for SEO." },
  { name: "[LSI/secondary keywords]", desc: "List of related keywords/entities to include." },
  { name: "[WORD_COUNT]", desc: "The target word count for the article." },
  { name: "[CTA_URL]", desc: "The call-to-action URL to promote." },
  { name: "[[IMAGE: <prompt>]]", desc: "Placeholder for in-article images (AI will fill <prompt>)." },
  // Add other placeholders as they become relevant
];

const PromptTemplateEditor = ({ value, onChange, disabled }) => {
  const textAreaRef = useRef(null);
  const [copied, setCopied] = useState(false);

  useEffect(() => {
    if (copied) {
      const timer = setTimeout(() => setCopied(false), 2000);
      return () => clearTimeout(timer);
    }
  }, [copied]);

  const handleInsertPlaceholder = (placeholder) => {
    const textarea = textAreaRef.current?.resizableTextArea?.textArea;
    if (textarea) {
      const start = textarea.selectionStart;
      const end = textarea.selectionEnd;
      const newValue = value.substring(0, start) + placeholder + value.substring(end, value.length);
      onChange(newValue);
      // Move cursor after inserted placeholder
      setTimeout(() => {
        textarea.selectionStart = textarea.selectionEnd = start + placeholder.length;
        textarea.focus();
      }, 0);
    }
  };

  return (
    <Row gutter={16}>
      <Col span={18}>
        <TextArea
          ref={textAreaRef}
          value={value}
          onChange={(e) => onChange(e.target.value)}
          rows={20}
          placeholder="Enter your custom AI prompt template here..."
          disabled={disabled}
          autoSize={{ minRows: 15, maxRows: 30 }}
        />
        <Space style={{ marginTop: '10px' }}>
          <Tooltip title="Copy entire prompt to clipboard">
            <Button icon={<CopyOutlined />} onClick={() => { handleInsertPlaceholder(value); setCopied(true); }}>
              {copied ? 'Copied!' : 'Copy Full Prompt'}
            </Button>
          </Tooltip>
        </Space>
      </Col>
      <Col span={6}>
        <Card title={<Space><BulbOutlined /> Available Placeholders</Space>} size="small">
          <List
            size="small"
            dataSource={PLACEHOLDERS}
            renderItem={item => (
              <List.Item
                actions={[
                  <Button
                    key="insert"
                    type="link"
                    size="small"
                    onClick={() => handleInsertPlaceholder(item.name)}
                    disabled={disabled}
                  >
                    Insert
                  </Button>
                ]}
              >
                <List.Item.Meta
                  title={<Text code>{item.name}</Text>}
                  description={<Text type="secondary" ellipsis={{tooltip: item.desc}}>{item.desc}</Text>}
                />
              </List.Item>
            )}
          />
          <Divider style={{ margin: '16px 0' }} />
          <Text type="secondary" style={{fontSize: '0.8em'}}>
            These placeholders will be dynamically replaced with data from your opportunity blueprint.
          </Text>
        </Card>
      </Col>
    </Row>
  );
};

export default PromptTemplateEditor;
```

## File: my-content-app/src/context/AuthContext.jsx
```javascript
// This is a new file. Create it with the following content:
import React, { createContext, useContext, useState, useEffect } from 'react';
import { login as apiLogin, logout as apiLogout } from '../services/authService';

export const AuthContext = createContext();

export const useAuth = () => useContext(AuthContext);

export const AuthProvider = ({ children }) => {
  const [isAuthenticated, setIsAuthenticated] = useState(false);
  const [authToken, setAuthToken] = useState(null);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    try {
      const storedToken = localStorage.getItem('authToken');
      if (storedToken) {
        setAuthToken(storedToken);
        setIsAuthenticated(true);
      }
    } catch (error) {
      console.error("Failed to read authToken from localStorage:", error);
    } finally {
      setLoading(false);
    }
  }, []);

  const login = async (password) => {
    const response = await apiLogin(password);
    if (response && response.token) {
      try {
        localStorage.setItem('authToken', response.token);
        setAuthToken(response.token);
        setIsAuthenticated(true);
      } catch (error) {
        console.error("Failed to store authToken in localStorage:", error);
        // Even if localStorage fails, keep session in memory for current tab
        setAuthToken(response.token);
        setIsAuthenticated(true);
      }
    } else {
      throw new Error('No token received');
    }
  };

  const logout = async () => {
    try {
      await apiLogout();
      localStorage.removeItem('authToken');
      setAuthToken(null);
      setIsAuthenticated(false);
    } catch (error) {
      console.error("Logout failed:", error);
      // Even if API logout fails, clear local state
      localStorage.removeItem('authToken');
      setAuthToken(null);
      setIsAuthenticated(false);
      throw error; // Re-throw to allow component to show error
    }
  };

  return (
    <AuthContext.Provider value={{ isAuthenticated, authToken, login, logout, loading }}>
      {children}
    </AuthContext.Provider>
  );
};
```

## File: my-content-app/src/context/ClientContext.jsx
```javascript
import React, { createContext, useContext, useState, useEffect } from 'react';

export const ClientContext = createContext();

export const useClient = () => useContext(ClientContext);

export const ClientProvider = ({ children }) => {
    const [clientId, setClientIdState] = useState(() => { // Renamed local state setter
        try {
            const storedClientId = localStorage.getItem('clientId'); // Changed key from 'currentClientId'
            return storedClientId ? storedClientId : 'Lark_Main_Site'; // Use a consistent default if not found
        } catch (error) {
            console.error("Could not access localStorage for clientId:", error);
            return 'Lark_Main_Site';
        }
    });

    useEffect(() => {
        try {
            localStorage.setItem('clientId', clientId); // Changed key from 'currentClientId'
        } catch (error) {
            console.error("Could not write to localStorage for clientId:", error);
        }
    }, [clientId]);
      
    // New function to update clientId and persist to localStorage
    const updateClientId = (newClientId) => {
        setClientIdState(newClientId);
        try {
            localStorage.setItem('clientId', newClientId); // Changed key from 'currentClientId'
        } catch (error) {
            console.error("Could not write to localStorage for clientId:", error);
        }
    };

    const value = { clientId, setClientId: updateClientId }; // Provide the wrapped setter

    return (
        <ClientContext.Provider value={value}>
            {children}
        </ClientContext.Provider>
    );
};
```

## File: my-content-app/src/context/JobContext.jsx
```javascript
import React, { createContext, useState, useContext } from 'react';

const JobContext = createContext();

export const useJobs = () => useContext(JobContext);

export const JobProvider = ({ children }) => {
  const [activeJobs, setActiveJobs] = useState({});

  const startJob = (jobId, message) => {
    setActiveJobs(prev => ({ ...prev, [jobId]: { status: 'running', message } }));
  };

  const updateJob = (jobId, status, message) => {
    setActiveJobs(prev => {
      if (!prev[jobId]) return prev;
      return { ...prev, [jobId]: { ...prev[jobId], status, message } };
    });
  };

  const completeJob = (jobId) => {
    setTimeout(() => {
      setActiveJobs(prev => {
        const newJobs = { ...prev };
        delete newJobs[jobId];
        return newJobs;
      });
    }, 5000); // Remove after 5 seconds
  };

  return (
    <JobContext.Provider value={{ activeJobs, startJob, updateJob, completeJob }}>
      {children}
    </JobContext.Provider>
  );
};
```

## File: my-content-app/src/context/NotificationContext.jsx
```javascript
import React, { useContext } from 'react';
import { notification } from 'antd';

// Create a context to hold the notification logic
const NotificationContext = React.createContext();

// Custom hook to access the notification context
export const useNotifications = () => useContext(NotificationContext);

// Provider component that wraps your app
export const NotificationProvider = ({ children }) => {
  const showNotification = (type, message, description) => {
    notification[type]({
      message,
      description,
    });
  };

  return (
    <NotificationContext.Provider value={{ showNotification }}>
      {children}
    </NotificationContext.Provider>
  );
};
```

## File: my-content-app/src/hooks/useClient.js
```javascript
import { useContext } from 'react';
import { ClientContext } from '../context/ClientContext';

// Simple wrapper hook to ensure useClient is used within its Provider
export const useClient = () => {
    const context = useContext(ClientContext);
    if (context === undefined) {
        throw new Error('useClient must be used within a ClientProvider');
    }
    return context;
};
```

## File: my-content-app/src/hooks/useDebounce.js
```javascript
import { useState, useEffect } from 'react';

function useDebounce(value, delay) {
  const [debouncedValue, setDebouncedValue] = useState(value);

  useEffect(() => {
    const handler = setTimeout(() => {
      setDebouncedValue(value);
    }, delay);

    return () => {
      clearTimeout(handler);
    };
  }, [value, delay]);

  return debouncedValue;
}

export default useDebounce;
```

## File: my-content-app/src/pages/ActivityLog/ActivityLogPage.jsx
```javascript
// This is a new file. Create it with the following content:
import React from 'react';
import { Layout, Typography, Table, Space, Alert, Spin, Button, Tooltip } from 'antd';
import { useQuery, useMutation, useQueryClient } from 'react-query';
import { getAllJobs, cancelJob } from '../../services/orchestratorService';
import { formatDistanceToNow, format } from 'date-fns';
import { CloseCircleOutlined } from '@ant-design/icons';
import { useNotifications } from '../../context/NotificationContext';
import JobStatusIndicator from '../../components/JobStatusIndicator'; // NEW

const { Content } = Layout;
const { Title, Text } = Typography;



const ActivityLogPage = () => {
  const queryClient = useQueryClient();
  const { showNotification } = useNotifications();

  const { data: jobs = [], isLoading, isError, error } = useQuery(
    'allJobs',
    getAllJobs,
    {
      refetchInterval: (data) =>
        data?.some((job) => job.status === 'running' || job.status === 'pending') ? 3000 : false, // Refetch every 3s if any job is running
    }
  );

  const { mutate: cancelJobMutation, isLoading: isCancelling } = useMutation(
    (jobId) => cancelJob(jobId),
    {
      onSuccess: () => {
        showNotification('info', 'Job Cancellation', 'Job cancellation request sent.');
        queryClient.invalidateQueries('allJobs'); // Refetch to show updated status
      },
      onError: (err) => {
        showNotification('error', 'Cancellation Failed', err.message || 'An error occurred during cancellation.');
      },
    }
  );

  const columns = [
    {
      title: 'Job ID',
      dataIndex: 'id',
      key: 'id',
      render: (text) => <Text code>{text.substring(0, 8)}...</Text>,
    },
    {
      title: 'Type',
      dataIndex: 'function_name',
      key: 'function_name',
      render: (text) => text ? text.replace(/_background$/, '').replace(/_/g, ' ').replace('run ', '').replace('run', '').trim().toUpperCase() : 'N/A',
    },
    {
      title: 'Status',
      dataIndex: 'status',
      key: 'status',
      render: (_, record) => <JobStatusIndicator jobId={record.id} />, // Use the new component
    },
    {
      title: 'Current Step',
      dataIndex: 'result',
      key: 'step',
      render: (result) => result?.step || 'N/A',
    },
    {
      title: 'Started',
      dataIndex: 'started_at',
      key: 'started_at',
      render: (timestamp) => timestamp ? formatDistanceToNow(new Date(timestamp * 1000), { addSuffix: true }) : 'N/A',
    },
    {
      title: 'Finished',
      dataIndex: 'finished_at',
      key: 'finished_at',
      render: (timestamp) => timestamp ? format(new Date(timestamp * 1000), 'MMM d, hh:mm a') : 'N/A',
    },
    {
      title: 'Error',
      dataIndex: 'error',
      key: 'error',
      render: (errorMsg) => errorMsg ? (
        <Tooltip title={errorMsg}>
          <Text type="danger" ellipsis>{errorMsg}</Text>
        </Tooltip>
      ) : 'N/A',
    },
    {
      title: 'Actions',
      key: 'actions',
      render: (_, record) => (
        <Space size="small">
          {record.status === 'running' && (
            <Tooltip title="Cancel Job">
              <Button 
                danger 
                icon={<CloseCircleOutlined />} 
                size="small" 
                onClick={() => cancelJobMutation(record.id)} 
                loading={isCancelling} 
              />
            </Tooltip>
          )}
        </Space>
      ),
    },
  ];

  if (isLoading) {
    return <Spin tip="Loading activity log..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (isError) {
    return <Alert message="Error" description={error?.message || "Failed to load activity log. Please try again."} type="error" showIcon />;
  }

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Title level={2}>Activity Log</Title>
        <Table
          columns={columns}
          dataSource={jobs}
          rowKey="id"
          pagination={{ pageSize: 10 }}
          scroll={{ x: 1000 }}
        />
      </Content>
    </Layout>
  );
};

export default ActivityLogPage;
```

## File: my-content-app/src/pages/Auth/LoginPage.jsx
```javascript
// This is a new file. Create it with the following content:
import React, { useState } from 'react';
import { Form, Input, Button, Card, Typography, Alert, Space } from 'antd';
import { LockOutlined } from '@ant-design/icons';
import { useAuth } from '../../context/AuthContext';
import { useNavigate } from 'react-router-dom';

const { Title } = Typography;

const LoginPage = () => {
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);
  const { login } = useAuth();
  const navigate = useNavigate();

  const onFinish = async (values) => {
    setLoading(true);
    setError(null);
    try {
      await login(values.password); // Only password is required for dummy login
      navigate('/dashboard'); // Redirect to the main dashboard on success
    } catch (err) {
      setError(err.message || 'Login failed. Please check your password.');
    } finally {
      setLoading(false);
    }
  };

  return (
    <Space
      direction="vertical"
      align="center"
      style={{
        width: '100%',
        minHeight: '100vh',
        justifyContent: 'center',
        background: '#f0f2f5',
      }}
    >
      <Card style={{ width: 350, textAlign: 'center' }}>
        <Title level={3}>Content AI Login</Title>
        {error && (
          <Alert message="Authentication Failed" description={error} type="error" showIcon style={{ marginBottom: 20 }} />
        )}
        <Form
          name="login"
          initialValues={{ remember: true }}
          onFinish={onFinish}
        >
          <Form.Item
            name="password"
            rules={[{ required: true, message: 'Please enter your password!' }]}
          >
            <Input.Password prefix={<LockOutlined />} placeholder="Password" />
          </Form.Item>

          <Form.Item>
            <Button type="primary" htmlType="submit" loading={loading} block>
              Log in
            </Button>
          </Form.Item>
        </Form>
      </Card>
    </Space>
  );
};

export default LoginPage;
```

## File: my-content-app/src/pages/BlogPage/BlogPage.css
```css
.blog-layout {
    background: #fff;
    padding: 40px 0;
}

.blog-content {
    max-width: 800px;
    margin: 0 auto;
    padding: 0 24px;
}

.blog-post-container {
    border-radius: 8px;
    overflow: hidden;
}

.blog-title {
    font-size: 3em;
    font-weight: 700;
    line-height: 1.2;
    margin-bottom: 24px;
    color: #1a1a1a;
}

.author-info {
    display: flex;
    align-items: center;
    gap: 12px;
    margin-bottom: 32px;
}

.featured-image {
    width: 100%;
    height: auto;
    border-radius: 8px;
    margin-bottom: 32px;
    object-fit: cover;
}

.blog-body {
    font-size: 1.1em;
    line-height: 1.8;
    color: #333;
}

.blog-body h1, .blog-body h2, .blog-body h3 {
    font-weight: 600;
    margin-top: 2em;
    margin-bottom: 1em;
    color: #1a1a1a;
}

.blog-body p {
    margin-bottom: 1.5em;
}

.blog-body a {
    color: #1890ff;
    text-decoration: none;
}

.blog-body a:hover {
    text-decoration: underline;
}

.blog-body ul, .blog-body ol {
    padding-left: 2em;
    margin-bottom: 1.5em;
}

.blog-body blockquote {
    border-left: 4px solid #e8e8e8;
    padding-left: 1em;
    margin: 2em 0;
    color: #666;
    font-style: italic;
}

.loading-container {
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100vh;
}

/* Styles to make the Quill Editor body match the blog body for a WYSIWYG experience */
.blog-body .ql-editor {
    font-size: 1.1em;
    line-height: 1.8;
    color: #333;
    min-height: 600px;
}

.blog-body .ql-editor h1, .blog-body .ql-editor h2, .blog-body .ql-editor h3 {
    font-weight: 600;
    margin-top: 2em;
    margin-bottom: 1em;
    color: #1a1a1a;
}

.blog-body .ql-editor p {
    margin-bottom: 1.5em;
}

.blog-body .ql-editor blockquote {
    border-left: 4px solid #e8e8e8;
    padding-left: 1em;
    margin: 2em 0;
    color: #666;
    font-style: italic;
}
```

## File: my-content-app/src/pages/BlogPage/BlogPage.jsx
```javascript
import React from 'react';
import { useParams } from 'react-router-dom';
import { useQuery } from 'react-query';
import { getOpportunityById } from '../../services/opportunitiesService';
import { Layout, Typography, Spin, Alert, Result, Button, Avatar, Space } from 'antd';
import { UserOutlined } from '@ant-design/icons';
import { Link } from 'react-router-dom';
import './BlogPage.css';

const { Content } = Layout;
const { Title, Text } = Typography;

const BlogPage = () => {
  const { opportunityId } = useParams();
  const { data: opportunity, isLoading, isError, error } = useQuery(
    ['opportunity', opportunityId],
    () => getOpportunityById(opportunityId)
  );

  if (isLoading) {
    return (
      <div className="loading-container">
        <Spin size="large" tip="Loading article..." />
      </div>
    );
  }

  if (isError) {
    return <Alert message="Error" description={error.message} type="error" showIcon />;
  }

  const {
    keyword,
    final_package_json,
    author_name = 'AI Assistant',
    publication_date = new Date().toLocaleDateString(),
  } = opportunity || {};

  if (!final_package_json || !final_package_json.article_html_final) {
    return (
      <Result
        status="404"
        title="Content Not Found"
        subTitle="The final content package for this opportunity has not been generated yet."
        extra={<Button type="primary"><Link to={`/opportunities/${opportunityId}`}>Go Back</Link></Button>}
      />
    );
  }

  return (
    <Layout className="blog-layout">
      <Content className="blog-content">
        <div className="blog-post-container">
          <Title level={1} className="blog-title">{final_package_json.meta_title || keyword}</Title>
          <div className="author-info">
            <Avatar icon={<UserOutlined />} />
            <Space direction="vertical" size={0}>
              <Text strong>{author_name}</Text>
              <Text type="secondary">Published on {publication_date}</Text>
            </Space>
          </div>

          {final_package_json.featured_image_relative_path && (
            <img src={final_package_json.featured_image_relative_path} alt={keyword} className="featured-image" />
          )}

          <div
            className="blog-body"
            dangerouslySetInnerHTML={{ __html: final_package_json.article_html_final }}
          />
        </div>
      </Content>
    </Layout>
  );
};

export default BlogPage;
```

## File: my-content-app/src/pages/ClientDashboard/AddNewClientModal.jsx
```javascript
// This is a new file. Create it with the following content:
import React from 'react';
import { Modal, Form, Input } from 'antd';

const AddNewClientModal = ({ open, onCancel, onAddClient, loading }) => {
  const [form] = Form.useForm();

  const handleOk = () => {
    form.validateFields()
      .then(values => {
        onAddClient(values);
        form.resetFields();
      })
      .catch(info => {
        console.log('Validate Failed:', info);
      });
  };

  return (
    <Modal
      title="Add New Client"
      open={open}
      onOk={handleOk}
      onCancel={onCancel}
      confirmLoading={loading}
      okText="Add Client"
    >
      <Form
        form={form}
        layout="vertical"
        name="add_client_form"
      >
        <Form.Item
          name="client_name"
          label="Client Name"
          rules={[{ required: true, message: 'Please enter the client\'s name!' }]}
        >
          <Input autoFocus />
        </Form.Item>
        <Form.Item
          name="client_id"
          label="Client ID"
          rules={[{ required: true, message: 'Please enter a unique client ID!' }]}
          extra="This should be a unique identifier for the client (e.g., my_company_name)."
        >
          <Input />
        </Form.Item>
      </Form>
    </Modal>
  );
};

export default AddNewClientModal;
```

## File: my-content-app/src/pages/ClientDashboard/ClientDashboardPage.jsx
```javascript
// This is a new file. Create it with the following content:
import React, { useState } from 'react';
import { Layout, Typography, Spin, Alert, Card, Row, Col, Statistic, Button, Space } from 'antd';
import { useQuery, useMutation, useQueryClient } from 'react-query';
import { getClients, getDashboardStats, addClient } from '../../services/clientService';
import { useClient } from '../../hooks/useClient';
import { useNotifications } from '../../context/NotificationContext';
import AddNewClientModal from './AddNewClientModal';
import { useNavigate } from 'react-router-dom'; // NEW

const { Content } = Layout;
const { Title } = Typography;

const ClientCard = ({ client, onSelectClient, isActive }) => {
  const { data: stats, isLoading: isLoadingStats, isError: isErrorStats } = useQuery(
    ['dashboardStats', client.client_id],
    () => getDashboardStats(client.client_id),
    {
      enabled: !!client.client_id,
      staleTime: 5 * 60 * 1000, // 5 minutes
      cacheTime: 10 * 60 * 1000, // 10 minutes
    }
  );

  return (
    <Card
      title={<Title level={5} style={{ margin: 0 }}>{client.client_name}</Title>}
      extra={
        <Button
          type={isActive ? "primary" : "default"}
          onClick={() => onSelectClient(client.client_id)}
          disabled={isActive}
        >
          {isActive ? 'Current Client' : 'Select'}
        </Button>
      }
      style={{ marginBottom: '16px' }}
      loading={isLoadingStats}
    >
      <Row gutter={16}>
        <Col span={8}>
          <Statistic title="Opportunities" value={stats?.status_counts?.all || 0} />
        </Col>
        <Col span={8}>
          <Statistic title="Qualified" value={stats?.status_counts?.validated || 0} />
        </Col>
        <Col span={8}>
          <Statistic title="Generated" value={stats?.status_counts?.generated || 0} />
        </Col>
      </Row>
      {isErrorStats && (
        <Alert
          message="Error loading stats"
          description={isErrorStats?.message || 'Failed to load dashboard statistics.'}
          type="error"
          style={{ marginTop: '16px' }}
        />
      )}
    </Card>
  );
};

const ClientDashboardPage = () => {
  const { clientId, setClientId } = useClient();
  const { showNotification } = useNotifications();
  const queryClient = useQueryClient();
  const navigate = useNavigate(); // NEW
  const [isModalVisible, setIsModalVisible] = useState(false);

  const { data: clients = [], isLoading, isError, error } = useQuery(
    'clients',
    getClients,
  );

  const { mutate: addClientMutation, isLoading: isAddingClient } = useMutation(
    (newClient) => addClient(newClient),
    {
      onSuccess: () => {
        showNotification('success', 'Client Added', 'New client has been successfully added.');
        queryClient.invalidateQueries('clients'); // Invalidate to refetch client list
        setIsModalVisible(false);
      },
      onError: (err) => {
        showNotification('error', 'Failed to Add Client', err.message || 'An error occurred while adding the client.');
      },
    }
  );

  const handleSelectClient = (selectedClientId) => {
    setClientId(selectedClientId);
    navigate('/dashboard'); // Navigate to the new dashboard page after selecting
  };

  if (isLoading) {
    return <Spin tip="Loading clients..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (isError) {
    return <Alert message="Error" description={error?.message || "Failed to load clients. Please try again."} type="error" showIcon />;
  }

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Title level={2}>Client Dashboard</Title>
        <Space style={{ marginBottom: '16px' }}>
          <Button type="primary" onClick={() => setIsModalVisible(true)}>Add New Client</Button>
        </Space>

        <Row gutter={[16, 16]}>
          {clients.map(client => (
            <Col xs={24} sm={12} md={8} lg={6} key={client.client_id}>
              <ClientCard 
                client={client} 
                onSelectClient={handleSelectClient} 
                isActive={client.client_id === clientId} 
              />
            </Col>
          ))}
        </Row>
      </Content>
      <AddNewClientModal
        open={isModalVisible}
        onCancel={() => setIsModalVisible(false)}
        onAddClient={addClientMutation}
        loading={isAddingClient}
      />
    </Layout>
  );
};

export default ClientDashboardPage;
```

## File: my-content-app/src/pages/ClientSettings/ClientSettingsPage.jsx
```javascript
// my-content-app/src/pages/ClientSettings/ClientSettingsPage.jsx
// NEW FILE
import React from 'react';
import { Layout, Typography, Form, Input, Button, Spin, Alert } from 'antd';
import { useQuery, useMutation, useQueryClient } from 'react-query';
import { getClientSettings, updateClientSettings } from '../../services/clientSettingsService';
import { useNotifications } from '../../context/NotificationContext';

const { Content } = Layout;
const { Title } = Typography;
const { TextArea } = Input;

const ClientSettingsPage = ({ clientId = 'default' }) => {
  const [form] = Form.useForm();
  const queryClient = useQueryClient();
  const { showNotification } = useNotifications();

  const { data: settings, isLoading, isError, error } = useQuery(
    ['clientSettings', clientId],
    () => getClientSettings(clientId),
    {
      enabled: !!clientId,
      onSuccess: (data) => {
        form.setFieldsValue(data);
      }
    }
  );

  const { mutate: updateSettingsMutation, isLoading: isUpdating } = useMutation(
    (newSettings) => updateClientSettings(clientId, newSettings),
    {
      onSuccess: () => {
        showNotification('success', 'Settings Updated', 'Client settings have been saved successfully.');
        queryClient.invalidateQueries(['clientSettings', clientId]);
      },
      onError: (err) => {
        showNotification('error', 'Update Failed', err.message || 'An error occurred while saving settings.');
      }
    }
  );

  const onFinish = (values) => {
    updateSettingsMutation(values);
  };

  if (isLoading) return <Spin tip="Loading settings..." />;
  if (isError) return <Alert message="Error" description={error.message} type="error" showIcon />;

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Title level={2}>Client AI & Content Settings</Title>
        <Form
          form={form}
          layout="vertical"
          onFinish={onFinish}
          initialValues={settings}
        >
          <Form.Item
            name="brand_tone"
            label="Brand Tone & Voice"
            tooltip="Define the desired tone for the AI-generated content (e.g., professional, witty, conversational)."
          >
            <TextArea rows={4} placeholder="e.g., Professional and authoritative, but accessible to a general audience." />
          </Form.Item>

          <Form.Item
            name="target_audience"
            label="Target Audience"
            tooltip="Describe the primary audience for the content."
          >
            <TextArea rows={4} placeholder="e.g., Marketing managers at mid-sized tech companies." />
          </Form.Item>

          <Form.Item
            name="terms_to_avoid"
            label="Terms to Avoid"
            tooltip="List any specific words or phrases the AI should not use. Separate with commas."
          >
            <Input placeholder="e.g., synergy, disruptive, unicorn" />
          </Form.Item>

          <Form.Item
            name="client_knowledge_base"
            label="Client Knowledge Base"
            tooltip="Provide key facts, product names, and unique selling propositions for your brand. This will be injected into every AI content generation prompt."
          >
            <TextArea rows={6} placeholder="e.g., Our flagship product is 'ProfitPilot', an AI-powered SEO tool designed for small businesses to automate keyword research and content generation. We focus on ROI and actionable insights, not just vanity metrics." />
          </Form.Item>

          <Form.Item>
            <Button type="primary" htmlType="submit" loading={isUpdating}>
              Save Settings
            </Button>
          </Form.Item>
        </Form>
      </Content>
    </Layout>
  );
};

export default ClientSettingsPage;
```

## File: my-content-app/src/pages/Dashboard/DashboardPage.jsx
```javascript
import React from 'react';
import { useQuery } from 'react-query';
import { Layout, Typography, Spin, Alert, Row, Col, Card, Statistic, Table, Tag } from 'antd';
import { 
  FileTextOutlined, 
  CheckCircleOutlined, 
  DollarCircleOutlined, 
  ExperimentOutlined,
  ClockCircleOutlined,
  WarningOutlined,
  ArrowRightOutlined,
  ReadOutlined
} from '@ant-design/icons';
import { useClient } from '../../hooks/useClient';
import { getDashboardData } from '../../services/clientService';
import { format } from 'date-fns';
import { useNavigate } from 'react-router-dom';
import FunnelChart from '../DiscoveryPage/components/FunnelChart';

const { Content } = Layout;
const { Title, Text, Link } = Typography;

const KpiCard = ({ icon, title, value, prefix, precision = 0 }) => (
  <Card>
    <Statistic 
      title={title} 
      value={value} 
      precision={precision} 
      prefix={icon ? React.createElement(icon, { style: { marginRight: 8 } }) : prefix} 
    />
  </Card>
);

const DashboardPage = () => {
  const { clientId } = useClient();
  const navigate = useNavigate();

  const { data, isLoading, isError, error } = useQuery(
    ['dashboardData', clientId],
    () => getDashboardData(clientId),
    {
      enabled: !!clientId,
    }
  );

  if (isLoading) {
    return <Spin tip="Loading dashboard..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (isError) {
    return <Alert message="Error" description={error.message} type="error" showIcon />;
  }

  const { kpis, funnelData, actionItems, recent_items } = data;

  const actionColumns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
      render: (text, record) => <Link onClick={() => navigate(`/opportunities/${record.id}`)}>{text}</Link>,
    },
    {
      title: 'Score',
      dataIndex: 'strategic_score',
      key: 'strategic_score',
      render: (score) => (
        <Tag color="blue">
          {typeof score === 'number' ? score.toFixed(1) : 'N/A'}
        </Tag>
      ),
    },
    {
      title: 'Updated',
      dataIndex: 'updated_at',
      key: 'updated_at',
      render: (ts) => ts ? format(new Date(ts), 'yyyy-MM-dd') : 'N/A',
    },
    {
      title: 'Action',
      key: 'action',
      render: (_, record) => <Link onClick={() => navigate(`/opportunities/${record.id}`)}>Review <ArrowRightOutlined /></Link>,
    },
  ];

  const failedColumns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
      render: (text, record) => <Link onClick={() => navigate(`/opportunities/${record.id}`)}>{text}</Link>,
    },
    {
      title: 'Error',
      dataIndex: 'error_message',
      key: 'error_message',
      render: (msg) => <Text type="danger" ellipsis={{ tooltip: msg }}>{msg || 'No details'}</Text>,
    },
    {
      title: 'Action',
      key: 'action',
      render: (_, record) => <Link onClick={() => navigate(`/opportunities/${record.id}`)}>Details <ArrowRightOutlined /></Link>,
    },
  ];
  
  const recentActivityColumns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
      render: (text, record) => <Link onClick={() => navigate(`/opportunities/${record.id}`)}>{text}</Link>,
    },
    {
      title: 'Generated On',
      dataIndex: 'date_processed',
      key: 'date_processed',
      render: (ts) => ts ? format(new Date(ts), 'yyyy-MM-dd HH:mm') : 'N/A',
    },
    {
        title: 'Action',
        key: 'action',
        render: (_, record) => <Link onClick={() => navigate(`/opportunities/${record.id}?tab=Article`)}>View Article <ReadOutlined /></Link>,
    },
  ];

  // Safely extract funnel data
  const getFunnelCount = (stage) => funnelData?.find(d => d.stage === stage)?.count || 0;

  return (
    <Layout>
      <Content style={{ padding: '24px' }}>
        <Title level={2}>Dashboard for {clientId}</Title>
        
        <Row gutter={[24, 24]} style={{ marginBottom: '24px' }}>
          <Col xs={24} sm={12} md={6}><KpiCard icon={FileTextOutlined} title="Total Opportunities" value={kpis.totalOpportunities} /></Col>
          <Col xs={24} sm={12} md={6}><KpiCard icon={CheckCircleOutlined} title="Content Generated" value={kpis.contentGenerated} /></Col>
          <Col xs={24} sm={12} md={6}><KpiCard icon={DollarCircleOutlined} title="Est. Monthly Traffic Value" value={kpis.totalTrafficValue} prefix="$" precision={2} /></Col>
          <Col xs={24} sm={12} md={6}><KpiCard icon={ExperimentOutlined} title="Total API Cost" value={kpis.totalApiCost} prefix="$" precision={2} /></Col>
        </Row>

        <Row gutter={[24, 24]}>
          {/* Left Column */}
          <Col xs={24} lg={16}>
            <div style={{ display: 'flex', flexDirection: 'column', gap: '24px' }}>
              {actionItems?.awaitingApproval?.length > 0 && (
                <Card title={<><ClockCircleOutlined style={{marginRight: 8}} /> Awaiting Your Approval</>}>
                  <Table
                    dataSource={actionItems.awaitingApproval}
                    columns={actionColumns}
                    rowKey="id"
                    pagination={{ pageSize: 5 }}
                    size="small"
                  />
                </Card>
              )}
              {actionItems?.failed?.length > 0 && (
                <Card title={<><WarningOutlined style={{marginRight: 8}} /> Failed Workflows</>}>
                  <Table
                    dataSource={actionItems.failed}
                    columns={failedColumns}
                    rowKey="id"
                    pagination={{ pageSize: 5 }}
                    size="small"
                  />
                </Card>
              )}
            </div>
          </Col>

          {/* Right Column */}
          <Col xs={24} lg={8}>
             <div style={{ display: 'flex', flexDirection: 'column', gap: '24px' }}>
                <Card title="Recent Activity">
                    <Table
                        dataSource={recent_items}
                        columns={recentActivityColumns}
                        rowKey="id"
                        pagination={{ pageSize: 10 }}
                        size="small"
                        locale={{ emptyText: 'No recent activity' }}
                    />
                </Card>
                <Card title="Content Pipeline Funnel">
                  <FunnelChart 
                    totalRaw={getFunnelCount('Total')}
                    unique={getFunnelCount('Validated')}
                    qualified={getFunnelCount('Analyzed')}
                    disqualified={getFunnelCount('Disqualified')}
                    addedToDB={getFunnelCount('Generated')}
                  />
                </Card>
             </div>
          </Col>
        </Row>
      </Content>
    </Layout>
  );
};

export default DashboardPage;
```

## File: my-content-app/src/pages/DiscoveryPage/components/DiscoveryForm.jsx
```javascript
import React from 'react';
import { Input, Button, Typography, Form, Row, Col, InputNumber, Select, Switch, Modal } from 'antd';
import { RocketOutlined } from '@ant-design/icons';
import { useDiscoveryFilters } from '../hooks/useDiscoveryFilters';

const { Title } = Typography;
const { Option } = Select;

const DiscoveryForm = ({ isSubmitting, onSubmit }) => {
  const [form] = Form.useForm();
  useDiscoveryFilters();

      const onFinish = (values) => {
        const currentFilters = form.getFieldValue('filters'); // Assuming 'filters' is the form item holding the filter array
        if (currentFilters && currentFilters.length > 8) {
            Modal.error({
                title: 'Too Many Filters',
                content: 'You can specify a maximum of 8 filter conditions. Please remove some filters.',
            });
            return;
        }
        const { keyword, limit, search_volume_value, difficulty_value, competition_level, search_intent, include_clickstream_data, closely_variants, ignore_synonyms, exact_match } = values;

// ADDITION: Read discovery mode and max pages from form values
const discovery_modes = form.getFieldValue('discovery_modes') || ['ideas'];
const discovery_max_pages = form.getFieldValue('discovery_max_pages') || 1;

        // ... (rest of filtering logic) ...
        const filters = [];
        const filterPathPrefix = 'keyword_data.'; // Defaulting to a common prefix, backend will handle specifics
        if (search_volume_value !== undefined && search_volume_value !== null) {
          filters.push({ field: `${filterPathPrefix}keyword_info.search_volume`, operator: '>', value: search_volume_value });
        }
        if (difficulty_value !== undefined && difficulty_value !== null) {
          filters.push({ field: `${filterPathPrefix}keyword_properties.keyword_difficulty`, operator: '<', value: difficulty_value });
        }
        if (competition_level && competition_level.length > 0) {
          filters.push({ field: `${filterPathPrefix}keyword_info.competition_level`, operator: 'in', value: competition_level });
        }
        if (search_intent && search_intent.length > 0) {
          filters.push({ field: `${filterPathPrefix}search_intent_info.main_intent`, operator: 'in', value: search_intent });
        }

        const runData = {
          seed_keywords: [keyword],
          limit: limit,
          filters: filters.length > 0 ? filters : null,
          include_clickstream_data,
          closely_variants,
          ignore_synonyms,
          exact_match,
// ADD these fields to runData:
discovery_modes,
discovery_max_pages,
        };
        
        onSubmit({ runData });
      };

  

      return (

  

        <Form form={form} layout="vertical" onFinish={onFinish} initialValues={{

  

          limit: 1000,

  

          search_volume_value: 500,

  

          difficulty_value: 20,

  

          competition_level: ['LOW'],

  

          search_intent: ['informational'],

  

        }}>

  

          <Title level={4}>1. Enter a Seed Keyword</Title>

  

          <Form.Item name="keyword" rules={[{ required: true, message: 'Please enter a seed keyword.' }]}>

  

            <Input placeholder="e.g., content marketing" />

  

          </Form.Item>

  

          

  

          <Title level={4}>2. Add Filters (Optional)</Title>

  

          <Row gutter={16}>

  

            <Col span={12}>

  

              <Form.Item name="limit" label="Limit (Number of keywords to find)">

  

                <InputNumber style={{ width: '100%' }} placeholder="e.g., 1000" />

  

              </Form.Item>

  

            </Col>

  

            <Col span={12}>

  

              <Form.Item name="search_volume_value" label="Monthly Search Volume (Greater than)">

  

                <InputNumber style={{ width: '100%' }} placeholder="e.g., 500" />

  

              </Form.Item>

  

            </Col>

  

            <Col span={12}>

  

              <Form.Item name="difficulty_value" label="SEO Difficulty (Less than)">

  

                <InputNumber style={{ width: '100%' }} placeholder="e.g., 20" min={0} max={100} />

  

              </Form.Item>

  

            </Col>

  

            <Col span={12}>

  

              <Form.Item name="competition_level" label="Competition Level">

  

                <Select mode="multiple" placeholder="Any" allowClear>

  

                  <Option value="LOW">Low</Option>

  

                  <Option value="MEDIUM">Medium</Option>

  

                  <Option value="HIGH">High</Option>

  

                </Select>

  

              </Form.Item>

  

            </Col>

  

            <Col span={12}>

  

              <Form.Item name="search_intent" label="Search Intent">

  

                <Select mode="multiple" placeholder="Any" allowClear>

  

                  <Option value="informational">Informational</Option>

  

                  <Option value="commercial">Commercial</Option>

  

                  <Option value="transactional">Transactional</Option>

  

                  <Option value="navigational">Navigational</Option>

  

                </Select>

  

              </Form.Item>

  

            </Col>

  

          </Row>

          <Row gutter={16}>
            <Col span={24}>
              <Form.Item name="include_clickstream_data" label="Include Clickstream Demographics" valuePropName="checked" tooltip="Provides audience demographic data but doubles the API cost of the discovery run.">
                <Switch />
              </Form.Item>
            </Col>
            <Col span={12}>
              <Form.Item name="closely_variants" label="Use Phrase Match" valuePropName="checked" tooltip="Limits results to keywords that are close variants of the seed keyword (more targeted).">
                <Switch />
              </Form.Item>
            </Col>
            <Col span={12}>
              <Form.Item name="exact_match" label="Use Exact Match" valuePropName="checked" tooltip="Returns only keywords that exactly match the seed keyword's phrasing.">
                <Switch />
              </Form.Item>
            </Col>
            <Col span={12}>
              <Form.Item name="ignore_synonyms" label="Ignore Synonyms" valuePropName="checked" tooltip="Returns only core keywords, excluding highly similar variations.">
                <Switch />
              </Form.Item>
            </Col>
          </Row>

      <Row justify="end" align="middle" style={{ marginTop: '24px' }}>
        <Col>
          <Button type="primary" htmlType="submit" icon={<RocketOutlined />} loading={isSubmitting} size="large">
            Find Opportunities
          </Button>
        </Col>
      </Row>
    </Form>
  );
};

export default DiscoveryForm;
```

## File: my-content-app/src/pages/DiscoveryPage/components/DiscoveryHistory.jsx
```javascript
import React, { useState, useMemo } from 'react';
import { Table, Tag, Tooltip, Button, Modal, Progress, Empty, Typography, Input, Row, Col, DatePicker, Alert } from 'antd';
import { ReloadOutlined, ProfileOutlined, ClockCircleOutlined, CheckCircleOutlined, CloseCircleOutlined, LoadingOutlined, EyeOutlined } from '@ant-design/icons';
import { useNavigate } from 'react-router-dom';
import { formatDistanceToNow } from 'date-fns';
import RunDetailsModal from './RunDetailsModal';
import DiscoveryStatsBreakdown from './DiscoveryStatsBreakdown'; // NEW: For expandable row

const { Title, Text } = Typography;
const { Search } = Input;
const { RangePicker } = DatePicker;

const STATUS_CONFIG = {
  running: { color: 'processing', text: 'Running', icon: <LoadingOutlined spin /> },
  completed: { color: 'success', text: 'Completed', icon: <CheckCircleOutlined /> },
  failed: { color: 'error', text: 'Failed', icon: <CloseCircleOutlined /> },
  pending: { color: 'default', text: 'Pending', icon: <ClockCircleOutlined /> },
};

const DiscoveryHistory = ({ runs, totalRuns, page, setPage, isLoading, onRerun, isRerunning }) => {
  const navigate = useNavigate();
  const [filterText, setFilterText] = useState('');
  const [dateRange, setDateRange] = useState(null);
  const [errorModal, setErrorModal] = useState({ open: false, content: '' });
  const [detailsModal, setDetailsModal] = useState({ open: false, run: null });

  const handleShowError = (errorMsg) => {
    setErrorModal({ open: true, content: errorMsg || 'No error details provided.' });
  };

  const handleShowDetails = (run) => {
    setDetailsModal({ open: true, run: run });
  };

  const filteredRuns = useMemo(() => {
    let filtered = runs;

    if (filterText) {
      const lowerCaseFilter = filterText.toLowerCase();
      filtered = filtered.filter(run => 
        run.parameters?.seed_keywords?.some(kw => kw.toLowerCase().includes(lowerCaseFilter)) || 
        run.status.toLowerCase().includes(lowerCaseFilter)
      );
    }

    if (dateRange) {
      const [start, end] = dateRange;
      filtered = filtered.filter(run => {
        const runDate = new Date(run.start_time);
        return runDate >= start && runDate <= end;
      });
    }

    return filtered;
  }, [runs, filterText, dateRange]);

  const expandedRowRender = (record) => {
    if (!record.results_summary) {
      return <Text type="secondary">No detailed summary available for this run.</Text>;
    }
    return <DiscoveryStatsBreakdown summary={record.results_summary} runId={record.id} />;
  };

  const columns = [
    {
      title: 'Start Time',
      dataIndex: 'start_time',
      key: 'start_time',
      render: (text) => text ? `${formatDistanceToNow(new Date(text))} ago` : 'N/A',
      sorter: (a, b) => new Date(a.start_time) - new Date(b.start_time),
      defaultSortOrder: 'descend',
    },
    {
      title: 'Status',
      dataIndex: 'status',
      key: 'status',
      render: (status, record) => {
        const config = STATUS_CONFIG[status] || STATUS_CONFIG.pending;
        const progress = record.results_summary?.progress || (status === 'running' ? record.progress || 0 : 0);
        return (
          <div style={{ display: 'flex', alignItems: 'center', flexDirection: 'column' }}>
            <Tag icon={config.icon} color={config.color} onClick={() => status === 'failed' && handleShowError(record.error_message)} style={{ cursor: status === 'failed' ? 'pointer' : 'default', marginBottom: 4 }}>
              {config.text}
            </Tag>
            {status === 'running' && <Progress percent={progress} size="small" status="active" showInfo={false} style={{ width: 100 }} />}
          </div>
        );
      },
      sorter: (a, b) => a.status.localeCompare(b.status),
    },
    {
      title: 'Seed Keywords',
      dataIndex: 'parameters',
      key: 'seed_keywords',
      responsive: ['md'],
      render: (params) => {
        const keywords = params?.seed_keywords || [];
        if (keywords.length === 0) return <Text type="secondary">N/A</Text>;
        const displayedKeywords = keywords.slice(0, 3);
        const remainingCount = keywords.length - displayedKeywords.length;
        return (
          <>
            {displayedKeywords.map(kw => <Tag key={kw}>{kw}</Tag>)}
            {remainingCount > 0 && <Tooltip title={keywords.slice(3).join(', ')}><Tag>+{remainingCount} more</Tag></Tooltip>}
          </>
        );
      },
    },
    {
      title: 'Discovery Mode',
      dataIndex: 'parameters',
      key: 'discovery_mode',
      responsive: ['lg'],
      render: (params) => {
        const modes = params?.discovery_modes || ['ideas'];
        const filters = params?.filters;
        const orderBy = params?.order_by;
        const tooltipContent = (
          <pre style={{ maxWidth: 500, whiteSpace: 'pre-wrap' }}>
            {JSON.stringify({ filters, order_by: orderBy }, null, 2)}
          </pre>
        );
        return (
          <Tooltip title={tooltipContent}>
            {modes.map(mode => <Tag key={mode}>{mode.replace('_', ' ').toUpperCase()}</Tag>)}
          </Tooltip>
        );
      },
    },
    {
      title: 'Actions',
      key: 'actions',
      align: 'right',
      fixed: 'right',
      render: (_, record) => (
        <div style={{ display: 'flex', gap: '8px', justifyContent: 'flex-end' }}>
          <Tooltip title="Re-run this discovery with the same settings">
            <Button icon={<ReloadOutlined />} onClick={() => onRerun(record.id)} disabled={isRerunning} />
          </Tooltip>
          {record.status === 'completed' && (
            <>
              <Tooltip title="View Run Details">
                <Button icon={<EyeOutlined />} onClick={() => handleShowDetails(record)} />
              </Tooltip>
              <Tooltip title="View Keywords">
                <Button type="primary" icon={<ProfileOutlined />} onClick={() => navigate(`/discovery-run/${record.id}`)} />
              </Tooltip>
            </>
          )}
        </div>
      ),
    },
  ];

  return (
    <div style={{ marginTop: '32px' }}>
        <Row justify="space-between" align="middle" style={{ marginBottom: '16px' }} gutter={[16, 16]}>
            <Col><Title level={3} style={{margin: 0}}>Discovery History</Title></Col>
            <Col flex="auto" style={{textAlign: 'right'}}>
              <Search 
                placeholder="Filter by keyword or status..." 
                allowClear 
                value={filterText} 
                onChange={e => setFilterText(e.target.value)} 
                style={{ width: 250, marginRight: '8px' }} 
              />
              <RangePicker onChange={(dates) => setDateRange(dates)} />
            </Col>
        </Row>
      <Table
        loading={isLoading}
        columns={columns}
        dataSource={filteredRuns}
        rowKey="id"
        scroll={{ x: 800 }}
        locale={{ emptyText: <Empty description="No discovery runs found. Start one above to see your history." /> }}
        pagination={{
          current: page,
          pageSize: 10,
          total: totalRuns,
          onChange: setPage,
        }}
        expandable={{ expandedRowRender }} // NEW: Enable expandable rows
      />
      <Modal title="Run Failed" open={errorModal.open} onOk={() => setErrorModal({ open: false, content: '' })} onCancel={() => setErrorModal({ open: false, content: '' })} footer={[<Button key="back" onClick={() => setErrorModal({ open: false, content: '' })}>Close</Button>]}>
<Text strong>Error Message:</Text>
<pre style={{ marginTop: '8px', background: '#f5f5f5', padding: '12px', borderRadius: '4px', overflowX: 'auto', whiteSpace: 'pre-wrap', wordBreak: 'break-word' }}>{errorModal.content}</pre>
</Modal>
      
      <RunDetailsModal 
        run={detailsModal.run}
        open={detailsModal.open}
        onCancel={() => setDetailsModal({ open: false, run: null })}
      />
    </div>
  );
};

export default DiscoveryHistory;
```

## File: my-content-app/src/pages/DiscoveryPage/components/DiscoveryStatsBreakdown.jsx
```javascript
import React, { useState } from 'react';
import { Row, Col, Typography, Empty, Modal, Table } from 'antd';
import PieChartCard from './PieChartCard';
import FunnelChart from './FunnelChart'; // NEW: From Task 35
import { getDisqualifiedKeywords } from '../../../services/discoveryService';

const { Title, Text } = Typography;

const DiscoveryStatsBreakdown = ({ summary, runId }) => {
  const [modalVisible, setModalVisible] = useState(false);
  const [modalTitle, setModalTitle] = useState('');
  const [modalData, setModalData] = useState([]);
  const [modalLoading, setModalLoading] = useState(false);

  if (!summary) {
    return <Empty description="No summary data available for this run." />;
  }

  const handleReasonClick = async (reason) => {
    setModalTitle(`Disqualified Keywords: ${reason}`);
    setModalVisible(true);
    setModalLoading(true);
    try {
      const data = await getDisqualifiedKeywords(runId, reason);
      setModalData(data);
    } catch (error) {
      console.error('Failed to fetch disqualified keywords:', error);
    } finally {
      setModalLoading(false);
    }
  };

  // Destructure with defaults to prevent errors if fields are missing
  const {
    source_counts = {},
    disqualification_reasons = {},
    total_raw_count = 0,
    total_unique_count = 0,
    disqualified_count = 0,
    final_added_to_db = 0,
    final_qualified_count = 0
  } = summary;

  const modalColumns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
    },
    {
      title: 'Search Volume',
      dataIndex: ['keyword_info', 'search_volume'],
      key: 'search_volume',
    },
  ];

  return (
    <div style={{ padding: '16px', backgroundColor: '#fafafa' }}>
      <Row gutter={[32, 32]}>
        {/* Section 1: Keyword Sources */}
        <Col xs={24} md={12} lg={8}>
          <PieChartCard title="Keyword Sources" data={source_counts} />
        </Col>

        {/* Section 2: Processing Funnel (Visualized) */}
        <Col xs={24} lg={16}>
            <FunnelChart 
                totalRaw={total_raw_count}
                unique={total_unique_count}
                qualified={final_qualified_count}
                disqualified={disqualified_count}
                addedToDB={final_added_to_db}
            />
        </Col>

        {/* Section 3: Disqualification Reasons */}
        <Col xs={24} md={12} lg={8}>
          <PieChartCard title="Disqualification Reasons" data={disqualification_reasons} onSliceClick={handleReasonClick} />
        </Col>
      </Row>
      <Modal
        title={modalTitle}
        open={modalVisible}
        onCancel={() => setModalVisible(false)}
        footer={null}
        width={800}
      >
        <Table
          loading={modalLoading}
          dataSource={modalData}
          columns={modalColumns}
          rowKey="id"
          pagination={{ pageSize: 10 }}
        />
      </Modal>
    </div>
  );
};

export default DiscoveryStatsBreakdown;
```

## File: my-content-app/src/pages/DiscoveryPage/components/FilterBuilder.jsx
```javascript
import React, { useState } from 'react';
import { Select, Button, InputNumber, Row, Col } from 'antd';
import { PlusOutlined, DeleteOutlined } from '@ant-design/icons';

const { Option } = Select;

const FilterBuilder = ({ availableFilters, onChange }) => {
  const [filters, setFilters] = useState([{ field: null, operator: null, value: null }]);

  const handleFilterChange = (index, field, value) => {
    const newFilters = [...filters];
    newFilters[index][field] = value;

    // Reset operator and value if field changes
    if (field === 'field') {
      newFilters[index]['operator'] = null;
      newFilters[index]['value'] = null;
    }

    setFilters(newFilters);
    onChange(newFilters);
  };

  const addFilter = () => {
    const newFilters = [...filters, { field: null, operator: null, value: null }];
    setFilters(newFilters);
    onChange(newFilters);
  };

  const removeFilter = (index) => {
    const newFilters = filters.filter((_, i) => i !== index);
    setFilters(newFilters);
    onChange(newFilters);
  };

  const getOperatorsForField = (fieldName) => {
    const modeFilters = availableFilters?.filtersData?.modes.find(m => m.id === 'keyword_ideas')?.filters;
    const field = modeFilters?.find(f => f.name === fieldName);
    return field ? field.operators : [];
  };

  const getInputForField = (fieldName, index) => {
    const modeFilters = availableFilters?.filtersData?.modes.find(m => m.id === 'keyword_ideas')?.filters;
    const field = modeFilters?.find(f => f.name === fieldName);
    if (!field) return null;

    switch (field.type) {
      case 'number':
        return <InputNumber value={filters[index].value} onChange={(val) => handleFilterChange(index, 'value', val)} />;
      case 'select':
        return (
          <Select
            style={{ width: 120 }}
            value={filters[index].value}
            onChange={(val) => handleFilterChange(index, 'value', val)}
          >
            {field.options.map(opt => <Option key={opt} value={opt}>{opt}</Option>)}
          </Select>
        );
      default:
        return null;
    }
  };

  return (
    <div>
      {filters.map((filter, index) => (
        <Row key={index} gutter={8} style={{ marginBottom: 8 }}>
          <Col>
            <Select
              style={{ width: 200 }}
              placeholder="Select field"
              value={filter.field}
              onChange={(val) => handleFilterChange(index, 'field', val)}
            >
              {availableFilters?.filters.map(f => <Option key={f.name} value={f.name}>{f.label}</Option>)}
            </Select>
          </Col>
          <Col>
            <Select
              style={{ width: 80 }}
              placeholder="Op"
              value={filter.operator}
              onChange={(val) => handleFilterChange(index, 'operator', val)}
              disabled={!filter.field}
            >
              {getOperatorsForField(filter.field).map(op => <Option key={op} value={op}>{op}</Option>)}
            </Select>
          </Col>
          <Col>
            {getInputForField(filter.field, index)}
          </Col>
          <Col>
            <Button icon={<DeleteOutlined />} onClick={() => removeFilter(index)} danger />
          </Col>
        </Row>
      ))}
      <Button type="dashed" onClick={addFilter} icon={<PlusOutlined />} disabled={filters.length >= 8}>
        Add Filter ({filters.length}/8)
      </Button>
    </div>
  );
};

export default FilterBuilder;
```

## File: my-content-app/src/pages/DiscoveryPage/components/FunnelChart.css
```css
/* Styling for the FunnelChart component */
.funnel-chart-container {
  background-color: #f9f9f9; /* Light background for the expanded row */
  padding: 24px;
  border-radius: 8px;
  border: 1px solid #e8e8e8;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.09);
  margin-bottom: 24px;
}

.funnel-steps {
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 100%;
  position: relative;
  min-height: 150px; /* Ensure some height for visualization */
}

.funnel-step {
  height: 30px; /* Fixed height for each step */
  margin-bottom: 5px; /* Spacing between steps */
  border-radius: 4px;
  display: flex;
  justify-content: center;
  align-items: center;
  color: white;
  font-size: 14px;
  font-weight: bold;
  transition: all 0.3s ease-in-out;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
}

.funnel-step:hover {
  transform: scale(1.02);
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
}

.funnel-value {
  color: white !important;
}
```

## File: my-content-app/src/pages/DiscoveryPage/components/FunnelChart.jsx
```javascript
import React from 'react';
import { Typography, Row, Col, Statistic, Tooltip } from 'antd';
import './FunnelChart.css'; // Specific styles for the funnel chart

const { Text, Title } = Typography;

const FunnelChart = ({ totalRaw = 0, unique = 0, qualified = 0, disqualified = 0, addedToDB = 0 }) => {
  const data = [
    { label: "Total Found", value: totalRaw },
    { label: "Unique Keywords", value: unique },
    { label: "Qualified Keywords", value: qualified },
    { label: "Disqualified", value: disqualified },
    { label: "Added to Pipeline", value: addedToDB },
  ];

  const getColor = (label) => {
    switch(label) {
      case "Total Found": return "#1890ff";
      case "Unique Keywords": return "#2db7f5";
      case "Qualified Keywords": return "#52c41a";
      case "Disqualified": return "#f5222d";
      case "Added to Pipeline": return "#722ed1";
      default: return "#d9d9d9";
    }
  };

  return (
    <div className="funnel-chart-container">
      <Title level={5} style={{ marginBottom: '24px', textAlign: 'center' }}>Discovery Funnel</Title>
      <div className="funnel-steps">
        {data.map((item, index) => (
          <Tooltip title={item.label} key={index}>
            <div className="funnel-step" style={{ 
                backgroundColor: getColor(item.label),
                width: `${Math.max(20, (item.value / totalRaw) * 100)}%`, // Scale width
                zIndex: data.length - index // Ensure larger bars are behind
            }}>
              <Text strong className="funnel-value">{item.value.toLocaleString()}</Text>
            </div>
          </Tooltip>
        ))}
      </div>
      <Row gutter={[16, 16]} justify="space-around" style={{ marginTop: '24px' }}>
        {data.map((item, index) => (
          <Col key={index} span={Math.floor(24 / data.length)}>
            <Statistic 
              title={item.label} 
              value={item.value.toLocaleString()} 
              valueStyle={{ color: getColor(item.label) }} 
            />
          </Col>
        ))}
      </Row>
    </div>
  );
};

export default FunnelChart;
```

## File: my-content-app/src/pages/DiscoveryPage/components/PieChartCard.jsx
```javascript
import React from 'react';
import { Card, Typography, Empty, Tooltip } from 'antd';
import { PieChart, Pie, Cell, Tooltip as RechartsTooltip, Legend, ResponsiveContainer } from 'recharts';

const { Title } = Typography;

const COLORS = ['#0088FE', '#00C49F', '#FFBB28', '#FF8042', '#AF19FF', '#FF19AF'];

const PieChartCard = ({ title, data, onSliceClick }) => {
  const chartData = Object.entries(data).map(([name, value]) => ({ name, value }));

  if (chartData.length === 0) {
    return (
      <Card>
        <Title level={5} style={{ marginBottom: '16px' }}>{title}</Title>
        <Empty description={`No ${title.toLowerCase()} data`} />
      </Card>
    );
  }

  const renderLegend = (props) => {
    const { payload } = props;
    return (
      <ul style={{ listStyle: 'none', padding: '0', margin: '0', maxHeight: '300px', overflowY: 'auto' }}>
        {payload.map((entry, index) => {
          const { value, color } = entry;
          const maxLength = 40;
          const isTruncated = value.length > maxLength;
          const truncatedValue = isTruncated ? `${value.substring(0, maxLength)}...` : value;

          return (
            <li key={`item-${index}`} style={{ marginBottom: '4px', display: 'flex', alignItems: 'center' }}>
              <span style={{ width: '10px', height: '10px', backgroundColor: color, marginRight: '10px', display: 'inline-block' }}></span>
              {isTruncated ? (
                <Tooltip title={value}>
                  <span style={{ cursor: 'default' }}>{truncatedValue}</span>
                </Tooltip>
              ) : (
                <span>{value}</span>
              )}
            </li>
          );
        })}
      </ul>
    );
  };


  return (
    <Card>
      <Title level={5} style={{ marginBottom: '16px' }}>{title}</Title>
      <ResponsiveContainer width="100%" height={300}>
        <PieChart>
          <Pie
            data={chartData}
            cx="50%"
            cy="50%"
            labelLine={false}
            outerRadius={80}
            fill="#8884d8"
            dataKey="value"
            nameKey="name"
            onClick={(data) => onSliceClick && onSliceClick(data.name)}
          >
            {chartData.map((entry, index) => (
              <Cell key={`cell-${index}`} fill={COLORS[index % COLORS.length]} />
            ))}
          </Pie>
          <RechartsTooltip />
          <Legend content={renderLegend} />
        </PieChart>
      </ResponsiveContainer>
    </Card>
  );
};

export default PieChartCard;
```

## File: my-content-app/src/pages/DiscoveryPage/components/RunDetailsModal.jsx
```javascript
import React from 'react';
import { Modal, Tag, Row, Col, Descriptions, Statistic, Steps, Card, Typography } from 'antd';
import { formatDistanceStrict } from 'date-fns';
import PieChartCard from './PieChartCard';

const { Title, Text } = Typography;
const { Step } = Steps;

const STATUS_CONFIG = {
  completed: { color: 'success', text: 'Completed' },
  failed: { color: 'error', text: 'Failed' },
  running: { color: 'processing', text: 'Running' },
  pending: { color: 'default', text: 'Pending' },
};

const RunDetailsModal = ({ run, open, onCancel }) => {
  if (!run) return null;

  const {
    id,
    start_time,
    end_time,
    status,
    parameters = {},
    results_summary = {},
  } = run;

  const {
    total_cost = 0,
    source_counts = {},
    total_raw_count = 0,
    total_unique_count = 0,
    final_qualified_count = 0,
    duplicates_removed = 0,
    final_added_to_db = 0,
    disqualification_reasons = {},
  } = results_summary;

  const statusInfo = STATUS_CONFIG[status] || STATUS_CONFIG.pending;

  return (
    <Modal
      title={
        <div style={{ display: 'flex', alignItems: 'center' }}>
          <Title level={4} style={{ margin: 0 }}>Discovery Run #{id}</Title>
          <Tag color={statusInfo.color} style={{ marginLeft: '12px' }}>{statusInfo.text}</Tag>
        </div>
      }
      open={open}
      onCancel={onCancel}
      footer={null}
      width="80vw"
      style={{ top: 20 }}
    >
      {/* Key Metrics */}
      <Row gutter={[32, 16]} style={{ marginBottom: '24px' }}>
        <Col><Statistic title="Total Cost" prefix="$" value={total_cost.toFixed(2)} /></Col>
        <Col><Statistic title="Run Duration" value={end_time ? formatDistanceStrict(new Date(end_time), new Date(start_time)) : 'N/A'} /></Col>
        <Col><Statistic title="Keywords Found" value={total_unique_count} /></Col>
        <Col><Statistic title="Added to Pipeline" value={final_added_to_db} valueStyle={{ color: '#3f8600' }} /></Col>
      </Row>

      {/* Processing Funnel */}
      <Card title="Processing Funnel" style={{ marginBottom: '24px' }}>
        <Steps current={5} size="small">
          <Step title="Total Found" description={`${total_raw_count.toLocaleString()}`} />
          <Step title="Unique" description={`${total_unique_count.toLocaleString()}`} />
          <Step title="Qualified" description={`${final_qualified_count.toLocaleString()}`} />
          <Step title="Duplicates Removed" description={`${duplicates_removed.toLocaleString()}`} />
          <Step title="Added to DB" description={<Text strong style={{color: '#3f8600'}}>{final_added_to_db.toLocaleString()}</Text>} />
        </Steps>
      </Card>

      <Row gutter={[24, 24]}>
        {/* Parameters */}
        <Col xs={24} lg={8}>
          <Card title="Run Parameters">
            <Descriptions bordered column={1} size="small">
              <Descriptions.Item label="Seed Keywords">
                {(parameters.seed_keywords || []).map(kw => <Tag key={kw}>{kw}</Tag>)}
              </Descriptions.Item>
              {Object.entries(parameters.filters_override || {}).map(([key, value]) => (
                <Descriptions.Item key={key} label={key.replace(/_/g, ' ')}>{String(value)}</Descriptions.Item>
              ))}
            </Descriptions>
          </Card>
        </Col>

        {/* Visualizations */}
        <Col xs={24} lg={16}>
          <Row gutter={[24, 24]}>
            <Col xs={24} md={12}>
              <PieChartCard title="Keyword Sources" data={source_counts} />
            </Col>
            <Col xs={24} md={12}>
              <PieChartCard title="Disqualification Reasons" data={disqualification_reasons} />
            </Col>
          </Row>
        </Col>
      </Row>
    </Modal>
  );
};

export default RunDetailsModal;
```

## File: my-content-app/src/pages/DiscoveryPage/components/RunDetailsPage.jsx
```javascript
import React, { useState, useEffect } from 'react';
import { useParams, useNavigate } from 'react-router-dom';
import { Table, Tag, Spin, Alert, Typography, Button, Row, Col } from 'antd';
import { ArrowLeftOutlined, CheckCircleOutlined } from '@ant-design/icons';
import { getKeywordsForRun } from '../../../services/discoveryService';
import { overrideDisqualification } from '../../../services/opportunitiesService';
import { useNotifications } from '../../../hooks/useNotifications';

const { Title, Text } = Typography;

const RunDetailsPage = () => {
  const { runId } = useParams();
  const navigate = useNavigate();
  const [keywords, setKeywords] = useState([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);
  const [updatingIds, setUpdatingIds] = useState(new Set());
  const { showNotification } = useNotifications();

  useEffect(() => {
    const fetchKeywords = async () => {
      try {
        setLoading(true);
        const response = await getKeywordsForRun(runId);
        setKeywords(response || []);
      } catch (err) {
        setError('Failed to fetch keyword details for this run.');
        console.error(err);
      } finally {
        setLoading(false);
      }
    };

    fetchKeywords();
  }, [runId]);

  const handleOverride = async (opportunityId) => {
    setUpdatingIds(prev => new Set(prev).add(opportunityId));
    try {
      await overrideDisqualification(opportunityId);
      showNotification('success', 'Keyword Re-qualified', 'The keyword has been moved to the pending queue.');
      // Optimistically update the UI
      setKeywords(prevKeywords => 
        prevKeywords.map(kw => 
          kw.id === opportunityId 
            ? { ...kw, blog_qualification_status: 'passed_manual_override', blog_qualification_reason: 'Manually overridden by user.' }
            : kw
        )
      );
    } catch (err) {
      showNotification('error', 'Override Failed', err.message || 'Could not re-qualify the keyword.');
      console.error(err);
    } finally {
      setUpdatingIds(prev => {
        const newSet = new Set(prev);
        newSet.delete(opportunityId);
        return newSet;
      });
    }
  };

  const columns = [
    {
      title: 'Keyword',
      dataIndex: 'keyword',
      key: 'keyword',
      sorter: (a, b) => a.keyword.localeCompare(b.keyword),
    },
    {
      title: 'Qualification Status',
      dataIndex: 'blog_qualification_status',
      key: 'blog_qualification_status',
      render: (status) => {
        let color = 'default';
        if (status === 'passed') color = 'success';
        else if (status === 'failed' || status === 'rejected') color = 'error';
        else if (status === 'passed_manual_override') color = 'processing';
        return (
          <Tag color={color}>
            {status ? status.replace(/_/g, ' ').toUpperCase() : 'N/A'}
          </Tag>
        );
      },
      filters: [
        { text: 'Passed', value: 'passed' },
        { text: 'Failed', value: 'failed' },
        { text: 'Override', value: 'passed_manual_override' },
      ],
      onFilter: (value, record) => record.blog_qualification_status === value,
    },
    {
      title: 'Reason',
      dataIndex: 'blog_qualification_reason',
      key: 'blog_qualification_reason',
      render: (reason) => reason || <Text type="secondary">N/A</Text>,
    },
    {
        title: 'Search Volume',
        dataIndex: ['keyword_info', 'search_volume'],
        key: 'search_volume',
        sorter: (a, b) => (a.keyword_info?.search_volume || 0) - (b.keyword_info?.search_volume || 0),
        render: (sv) => sv ? sv.toLocaleString() : 'N/A',
    },
    {
        title: 'Keyword Difficulty',
        dataIndex: ['keyword_properties', 'keyword_difficulty'],
        key: 'keyword_difficulty',
        sorter: (a, b) => (a.keyword_properties?.keyword_difficulty || 0) - (b.keyword_properties?.keyword_difficulty || 0),
        render: (kd) => kd || 'N/A',
    },
    {
      title: 'Actions',
      key: 'actions',
      render: (_, record) => {
        const isFailed = record.blog_qualification_status === 'failed' || record.blog_qualification_status === 'rejected';
        if (isFailed) {
          return (
            <Button 
              type="primary" 
              ghost 
              size="small"
              icon={<CheckCircleOutlined />}
              onClick={() => handleOverride(record.id)}
              loading={updatingIds.has(record.id)}
            >
              Re-qualify
            </Button>
          );
        }
        return null;
      },
    },
  ];

  if (loading) {
    return <Spin tip="Loading keywords..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (error) {
    return <Alert message="Error" description={error} type="error" showIcon />;
  }

  return (
    <>
      <div style={{ padding: '16px 24px', backgroundColor: '#fff', borderBottom: '1px solid #f0f0f0' }}>
        <Row align="middle" gutter={16}>
          <Col>
            <Button icon={<ArrowLeftOutlined />} onClick={() => navigate(-1)} />
          </Col>
          <Col>
            <Title level={4} style={{ margin: 0 }}>
              Keywords for Discovery Run #{runId}
            </Title>
            <Text type="secondary">{keywords.length.toLocaleString()} keywords found</Text>
          </Col>
        </Row>
      </div>
      <div style={{ padding: '24px' }}>
        <Table
          columns={columns}
          dataSource={keywords}
          rowKey="id"
          loading={loading}
          pagination={{ pageSize: 50 }}
        />
      </div>
    </>
  );
};

export default RunDetailsPage;
```

## File: my-content-app/src/pages/DiscoveryPage/hooks/useDiscoveryFilters.js
```javascript
import { useQuery } from 'react-query';
import apiClient from '../../../services/apiClient';

const fetchDiscoveryFilters = async () => {
  const data = await apiClient.get('/api/discovery/available-filters');
  return data;
};

export const useDiscoveryFilters = () => {
  const { data, isLoading, isError } = useQuery('discoveryFilters', fetchDiscoveryFilters, {
    staleTime: Infinity, // This data is static, so we can cache it indefinitely
    cacheTime: Infinity,
  });

  return {
    filtersData: data,
    isLoading,
    isError,
  };
};
```

## File: my-content-app/src/pages/DiscoveryPage/hooks/useDiscoveryRuns.js
```javascript
import { useState, useEffect } from 'react';
import { useQuery, useMutation, useQueryClient } from 'react-query';
import {
  getDiscoveryRuns,
  startDiscoveryRun,
  rerunDiscoveryRun,
  getJobStatus,
} from '../../../services/discoveryService';
import { useClient } from '../../../hooks/useClient';
import { useNotifications } from '../../../context/NotificationContext';

export const useDiscoveryRuns = () => {
  const queryClient = useQueryClient();
  const { clientId } = useClient();
  const { showNotification } = useNotifications();
  const [page, setPage] = useState(1);

  // Query to fetch discovery run history
  const {
    data,
    isLoading,
    isError,
    error,
    refetch, // Get the refetch function
  } = useQuery(
    ['discoveryRuns', clientId, page],
    () => getDiscoveryRuns(clientId, page),
    {
      enabled: !!clientId,
      keepPreviousData: true,
    }
  );

  // Poll for updates on running jobs
  useEffect(() => {
    let intervalId;

    const checkRunningJobs = async () => {
      const currentRuns = queryClient.getQueryData(['discoveryRuns', clientId, page]);
      const runningRuns = currentRuns?.items?.filter(run => run.status === 'running' && run.job_id);

      if (!runningRuns || runningRuns.length === 0) {
        clearInterval(intervalId);
        intervalId = null;
        return;
      }

      let shouldRefetch = false;
      const statusChecks = runningRuns.map(run => getJobStatus(run.job_id));

      try {
        const jobStatuses = await Promise.all(statusChecks);
        if (jobStatuses.some(job => job.status === 'completed' || job.status === 'failed')) {
          shouldRefetch = true;
        }
      } catch (error) {
        console.error("Error polling job statuses:", error);
      }

      if (shouldRefetch) {
        queryClient.invalidateQueries(['discoveryRuns', clientId]);
      }
    };

    // Start polling only if there are running jobs initially
    const initialRunningRuns = data?.items?.filter(run => run.status === 'running' && run.job_id);
    if (initialRunningRuns && initialRunningRuns.length > 0 && !intervalId) {
      intervalId = setInterval(checkRunningJobs, 8000); // Poll every 8 seconds
    }

    return () => {
      if (intervalId) {
        clearInterval(intervalId);
      }
    };
  }, [data, clientId, queryClient, page]); // Add 'page' to dependencies

  // Mutation for starting a new discovery run
  const startRunMutation = useMutation(startDiscoveryRun, {
    // Optimistic update logic
    onMutate: async (newRunRequest) => {
      await queryClient.cancelQueries(['discoveryRuns', clientId]); // Cancel any outgoing refetches

      const previousRuns = queryClient.getQueryData(['discoveryRuns', clientId, page]); // Snapshot previous state

      // Optimistically add a temporary 'running' job to the cache
      queryClient.setQueryData(['discoveryRuns', clientId, 1], (old) => ({
        ...old,
        items: [
          {
            id: `temp-${Date.now()}`, // Temporary ID for optimistic update
            start_time: new Date().toISOString(),
            status: 'running',
            parameters: { seed_keywords: newRunRequest.runData.seed_keywords },
            results_summary: { progress: 0 }, // Initial progress
            error_message: null,
          },
          ...(old?.items || []),
        ]
      }));
      setPage(1); // Go to the first page to see the new run

      return { previousRuns }; // Return context for rollback
    },
    onError: (err, newRun, context) => {
      queryClient.setQueryData(['discoveryRuns', clientId, page], context.previousRuns); // Rollback on error
      showNotification('error', 'Failed to start discovery run', err.message);
    },
    onSuccess: (data) => {
       showNotification('success', 'Discovery run started successfully!', `Job ID: ${data.job_id}`);
    },
    onSettled: () => {
      queryClient.invalidateQueries(['discoveryRuns', clientId]); // Refetch to get actual server state
    },
  });
  
  // Mutation for re-running an existing discovery job
  const rerunMutation = useMutation(rerunDiscoveryRun, {
    onSuccess: (data) => {
      showNotification('success', 'Re-run started successfully!', `Job ID: ${data.job_id}`);
      queryClient.invalidateQueries(['discoveryRuns', clientId]); // Refetch history to show new job
    },
    onError: (err) => {
      showNotification('error', 'Failed to start re-run', err.message);
    },
  });

  return {
    runs: data?.items || [],
    totalRuns: data?.total_items || 0,
    page,
    setPage,
    isLoading,
    isError,
    error,
    startRunMutation,
    rerunMutation,
  };
};
```

## File: my-content-app/src/pages/DiscoveryPage/DiscoveryPage.jsx
```javascript
import React, { useState } from 'react';
import { Layout, Typography, Spin, Alert, Card, Divider, message } from 'antd';
import { useNavigate } from 'react-router-dom'; // Import useNavigate
import { useDiscoveryRuns } from './hooks/useDiscoveryRuns';
import DiscoveryForm from './components/DiscoveryForm';
import DiscoveryHistory from './components/DiscoveryHistory';
import { useClient } from '../../hooks/useClient';
import CostConfirmationModal from '../../components/CostConfirmationModal';
import { estimateActionCost } from '../../services/orchestratorService';

const { Content } = Layout;
const { Title } = Typography;

const DiscoveryPage = () => {
  const { runs, isLoading, isError, error, startRunMutation, rerunMutation } = useDiscoveryRuns();
  const { clientId } = useClient();
  const navigate = useNavigate(); // Initialize navigate

  const handleRerun = (runId) => {
      rerunMutation.mutate(runId);
  }

  if (isLoading) {
    return (
      <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: '100vh' }}>
        <Spin size="large" tip="Loading Discovery Hub..." />
      </div>
    );
  }

  if (isError) {
    return (
        <Alert
            message="Error"
            description={error.message || "Failed to load discovery run history. Please try again."}
            type="error"
            showIcon
            style={{ margin: '16px' }}
        />
    );
  }

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Spin spinning={startRunMutation.isLoading} tip="Starting discovery run..." size="large">
          <Title level={2}>Keyword Discovery</Title>
          <Card>
            <DiscoveryForm
              isSubmitting={startRunMutation.isLoading}
              onSubmit={({ runData }) => {
                startRunMutation.mutate({ clientId, runData });
              }}
            />
          </Card>

          <Divider />

          <DiscoveryHistory
              runs={runs}
              isLoading={startRunMutation.isLoading || rerunMutation.isLoading}
              onRerun={handleRerun}
              isRerunning={rerunMutation.isLoading}
          />
        </Spin>
      </Content>
    </Layout>
  );
};

export default DiscoveryPage;
```

## File: my-content-app/src/pages/NotFoundPage/NotFoundPage.jsx
```javascript
import React from 'react';
import { Result, Button } from 'antd';
import { Link } from 'react-router-dom';

const NotFoundPage = () => (
  <Result
    status="404"
    title="404"
    subTitle="Sorry, the page you visited does not exist."
    extra={<Button type="primary"><Link to="/">Back Home</Link></Button>}
  />
);

export default NotFoundPage;
```

## File: my-content-app/src/pages/OpportunitiesPage/components/ScoreBreakdownModal.jsx
```javascript
import React, { useState, useEffect } from 'react';
import { Modal, Typography, Descriptions, Tag, Row, Col, Alert, Space, Spin, Popover, Progress } from 'antd';
import {
  BarChartOutlined, FireOutlined, ThunderboltOutlined, CalendarOutlined, GlobalOutlined, SmileOutlined,
  BuildOutlined, StarOutlined, UsergroupAddOutlined, ApartmentOutlined, WarningOutlined, RiseOutlined,
  DashboardOutlined, BulbOutlined, InfoCircleOutlined
} from '@ant-design/icons';
import { getScoreNarrative } from '../../../services/orchestratorService';

const { Title, Text, Paragraph } = Typography;

const factorExplanations = {
  ease_of_ranking: "How hard it is to rank on the first page of Google for this keyword. It looks at competitor strength and keyword difficulty.",
  traffic_potential: "An estimate of the traffic you could get if you rank for this keyword, based on search volume and click-through rates.",
  commercial_intent: "How likely a user searching for this keyword is to make a purchase or take a commercial action.",
  growth_trend: "Whether this keyword is becoming more or less popular over time.",
  serp_features: "Measures the presence of special search results like Featured Snippets or People Also Ask boxes, which can affect click-through rates.",
  serp_volatility: "How often the search results for this keyword change. High volatility can mean it's easier to break in, but also harder to hold a position.",
  competitor_weakness: "Analyzes the weaknesses of the top-ranking competitors, such as their backlink profiles and domain authority.",
  serp_crowding: "How many non-traditional results (like ads, images, videos) are on the page, which can push organic results down.",
  keyword_structure: "Analyzes the keyword itself, like its length. Longer keywords are often more specific and easier to rank for.",
  serp_threat: "Identifies major, authoritative domains (like Wikipedia, government sites) that are very difficult to outrank.",
  volume_volatility: "How much the search volume fluctuates month-to-month. High volatility can indicate seasonality.",
  serp_freshness: "How recently the search results have been updated. Older results can be easier to displace.",
  competitor_performance: "A technical analysis of competitor websites, looking at things like page load speed and mobile-friendliness.",
};


const ScoreBreakdownModal = ({ open, onCancel, opportunity }) => {
  const [narrative, setNarrative] = useState('');
  const [isLoadingNarrative, setIsLoadingNarrative] = useState(false);

  useEffect(() => {
    if (open && opportunity?.id) {
      setIsLoadingNarrative(true);
      getScoreNarrative(opportunity.id)
        .then(response => {
          setNarrative(response.data.narrative);
        })
        .catch(err => {
          console.error("Failed to fetch score narrative:", err);
          setNarrative("Could not load the strategic summary.");
        })
        .finally(() => {
          setIsLoadingNarrative(false);
        });
    } else {
      setNarrative('');
    }
  }, [open, opportunity]);

  const keyword = opportunity?.keyword;
  const scoreBreakdown = opportunity?.score_breakdown || opportunity?.full_data?.score_breakdown;

  if (!scoreBreakdown) {
    return <Modal title="Score Breakdown" open={open} onCancel={onCancel} footer={null}><Alert message="No score breakdown available for this opportunity." type="info" showIcon /></Modal>;
  }

  const getScoreColor = (score) => {
    if (score >= 80) return 'success';
    if (score >= 60) return 'processing';
    if (score >= 40) return 'warning';
    return 'error';
  };

  const renderFactor = (factorKey, icon) => {
    const factor = scoreBreakdown[factorKey];
    if (!factor) return null;

    const mainExplanation = factorExplanations[factorKey] || "No explanation available.";

    return (
      <Col span={24} style={{ marginBottom: 16 }}>
        <div style={{ background: '#fafafa', padding: '12px 16px', borderRadius: '8px' }}>
          <Row align="middle" justify="space-between">
            <Col>
              <Space align="center">
                {icon}
                <Title level={5} style={{ margin: 0 }}>{factor.name}</Title>
                <Popover content={<Paragraph style={{ maxWidth: 300 }}>{mainExplanation}</Paragraph>} trigger="hover">
                  <InfoCircleOutlined style={{ color: 'rgba(0, 0, 0, 0.45)', cursor: 'pointer' }} />
                </Popover>
              </Space>
            </Col>
            <Col>
              <Space>
                <Text type="secondary" style={{ fontSize: '0.9em' }}>Weight: {factor.weight || 0}%</Text>
                <Progress type="circle" percent={factor.score} width={40} format={percent => `${percent}`} status={getScoreColor(factor.score)} />
              </Space>
            </Col>
          </Row>
          <Descriptions column={1} size="small" style={{ marginTop: 12 }}>
            {Object.entries(factor.breakdown || {}).map(([subFactorKey, subFactor]) => (
              <Descriptions.Item key={subFactorKey} label={<Text strong>{subFactorKey}</Text>}>
                <Row justify="space-between" align="top">
                  <Col span={16}>
                    <Space direction="vertical" size={0}>
                      <Text>{subFactor.value}</Text>
                      {subFactor.explanation && <Paragraph type="secondary" style={{ margin: 0, fontSize: '0.85em' }}>{subFactor.explanation}</Paragraph>}
                    </Space>
                  </Col>
                  <Col span={4} style={{ textAlign: 'right' }}>
                    {subFactor.score !== undefined && <Tag color={getScoreColor(subFactor.score)}>{subFactor.score?.toFixed(0)}</Tag>}
                  </Col>
                </Row>
              </Descriptions.Item>
            ))}
             {factor.breakdown?.message && <Alert message={factor.breakdown.message} type="warning" showIcon style={{width: '100%'}}/>}
          </Descriptions>
        </div>
      </Col>
    );
  };

  return (
    <Modal
      title={<Title level={4} style={{ margin: 0 }}>Strategic Score Breakdown: &quot;{keyword}&quot;</Title>}
      open={open}
      onCancel={onCancel}
      footer={null}
      width={800}
    >
      {isLoadingNarrative ? (
        <Spin tip="Loading AI-powered summary..." />
      ) : (
        narrative && <Alert message="Strategic Summary" description={<Paragraph style={{ whiteSpace: 'pre-wrap' }}>{narrative}</Paragraph>} type="info" showIcon icon={<BulbOutlined />} style={{ marginBottom: '24px' }} />
      )}
      <Row gutter={[16, 16]}>
        {renderFactor('ease_of_ranking', <BarChartOutlined />)}
        {renderFactor('traffic_potential', <FireOutlined />)}
        {renderFactor('commercial_intent', <ThunderboltOutlined />)}
        {renderFactor('growth_trend', <RiseOutlined />)}
        {renderFactor('serp_features', <StarOutlined />)}
        {renderFactor('competitor_weakness', <BuildOutlined />)}
        {renderFactor('serp_volatility', <SmileOutlined />)}
        {renderFactor('serp_crowding', <UsergroupAddOutlined />)}
        {renderFactor('keyword_structure', <ApartmentOutlined />)}
        {renderFactor('serp_threat', <WarningOutlined />)}
        {renderFactor('volume_volatility', <CalendarOutlined />)}
        {renderFactor('serp_freshness', <GlobalOutlined />)}
        {renderFactor('competitor_performance', <DashboardOutlined />)}
      </Row>
    </Modal>
  );
};

export default ScoreBreakdownModal;
```

## File: my-content-app/src/pages/OpportunitiesPage/hooks/useOpportunities.js
```javascript
import { useQuery } from 'react-query';
import { useState, useMemo, useEffect } from 'react';
import { getOpportunities, getDashboardStats } from '../../../services/opportunitiesService';
import { useClient } from '../../../hooks/useClient';

export const useOpportunities = () => {
  const { clientId } = useClient();
  const [pagination, setPagination] = useState({ current: 1, pageSize: 20, total: 0 });
  const [activeStatus, setActiveStatus] = useState('review');
  const [sorter, setSorter] = useState({ field: 'strategic_score', order: 'descend' });
  const [statusCounts, setStatusCounts] = useState({});

  const { data: statsData, isLoading: isLoadingStats } = useQuery(
    ['dashboardStats', clientId],
    () => getDashboardStats(clientId),
    {
      enabled: !!clientId,
      onSuccess: (data) => {
        if (data.status_counts) {
          setStatusCounts(data.status_counts);
          if (!activeStatus && Object.keys(data.status_counts).length > 0) {
            setActiveStatus(Object.keys(data.status_counts)[0]);
          }
        }
      },
    }
  );
  
  const { data: opportunitiesData, isLoading, isError, error, refetch } = useQuery(
    ['opportunities', clientId, pagination.current, pagination.pageSize, sorter, activeStatus],
    () => getOpportunities(clientId, { 
      page: pagination.current, 
      limit: pagination.pageSize, 
      sort_by: sorter.field, 
      sort_direction: sorter.order === 'ascend' ? 'asc' : 'desc',
      status: activeStatus 
    }),
    {
      enabled: !!clientId,
      staleTime: 60 * 1000,
      onSuccess: (data) => {
        setPagination(prev => ({ ...prev, total: data.total_items || 0 }));
      }
    }
  );

  const opportunities = useMemo(() => opportunitiesData?.items || [], [opportunitiesData]);

  const handleTableChange = (newPagination, newFilters, newSorter) => {
    setPagination(prev => ({ ...prev, current: newPagination.current, pageSize: newPagination.pageSize }));
    
    const effectiveSorter = Array.isArray(newSorter) ? newSorter[0] : newSorter;
    if (effectiveSorter?.field) {
        setSorter({ field: effectiveSorter.field, order: effectiveSorter.order });
    } else {
        setSorter({ field: 'strategic_score', order: 'descend' });
    }
    refetch();
  };

  return {
    opportunities,
    isLoading: isLoading || isLoadingStats,
    isError, error,
    pagination,
    handleTableChange,
    activeStatus, setActiveStatus,
    statusCounts,
    refetchOpportunities: refetch
  };
};
```

## File: my-content-app/src/pages/OpportunitiesPage/hooks/useOpportunities.refactored.js
```javascript
import { useQuery } from 'react-query';
import { useState } from 'react';
import { getOpportunities } from '../../../services/opportunitiesService';
import { useClient } from '../../../hooks/useClient';

export const useOpportunities = () => {
  const { clientId } = useClient();
  const [pagination, setPagination] = useState({ current: 1, pageSize: 20, total: 0 });
  const [activeStatus, setActiveStatus] = useState('review');
  const [sorter, setSorter] = useState({ field: 'strategic_score', order: 'descend' });
  const [keyword, setKeyword] = useState('');

  const { data, isLoading, isError, error, refetch } = useQuery(
    ['opportunities', clientId, pagination.current, pagination.pageSize, activeStatus, sorter, keyword],
    () => getOpportunities(clientId, {
      page: pagination.current,
      limit: pagination.pageSize,
      status: activeStatus,
      sort_by: sorter.field,
      sort_direction: sorter.order === 'ascend' ? 'asc' : 'desc',
      keyword: keyword,
    }),
    {
      enabled: !!clientId,
      staleTime: 60 * 1000, // Keep data fresh for 1 minute
      onSuccess: (response) => {
        setPagination(prev => ({ ...prev, total: response.total_items || 0 }));
      }
    }
  );

  const handleTableChange = (newPagination, newFilters, newSorter) => {
    setPagination(prev => ({ ...prev, current: newPagination.current, pageSize: newPagination.pageSize }));

    const effectiveSorter = Array.isArray(newSorter) ? newSorter[0] : newSorter;
    if (effectiveSorter?.field) {
        setSorter({ field: effectiveSorter.field, order: effectiveSorter.order });
    } else {
        // Reset to default sort if no specific column is sorted
        setSorter({ field: 'strategic_score', order: 'descend' });
    }
  };

  const handleSearch = (newKeyword) => {
    setKeyword(newKeyword);
  };

  return {
    opportunities: data?.items || [],
    isLoading,
    isError,
    error,
    pagination,
    handleTableChange,
    activeStatus,
    setActiveStatus,
    handleSearch,
    refetchOpportunities: refetch,
  };
};
```

## File: my-content-app/src/pages/OpportunitiesPage/OpportunitiesPage.css
```css
/* OpportunitiesPage.css */

.custom-tabs .ant-tabs-nav {
  background-color: #f0f2f5;
  padding: 0 16px;
  border-radius: 8px;
  margin-bottom: 16px;
}

.custom-tabs .ant-tabs-tab {
  padding: 12px 16px;
  font-size: 14px;
}

.custom-tabs .ant-tabs-tab-active {
  background-color: #ffffff;
  border-top-left-radius: 8px;
  border-top-right-radius: 8px;
}

.custom-tabs .ant-tabs-ink-bar {
  background-color: #1890ff;
}

.custom-tabs .ant-tag {
  font-weight: 900;
  font-size: 14px;
}
```

## File: my-content-app/src/pages/OpportunitiesPage/OpportunitiesPage.jsx
```javascript
import React from 'react';
import { Layout, Typography, Table, Tag, Button, Space, Tooltip, Modal, Card, Tabs } from 'antd';
import { RocketOutlined, EditOutlined, DeleteOutlined, ExclamationCircleOutlined } from '@ant-design/icons';
import { useOpportunities } from './hooks/useOpportunities';
import { useNotifications } from '../../context/NotificationContext';
import { useMutation, useQueryClient } from 'react-query';
import { startFullWorkflow, rejectOpportunity } from '../../services/orchestratorService';
import JobStatusIndicator from '../../components/JobStatusIndicator';
import { useJobs } from '../../context/JobContext';
import { getJobStatus } from '../../services/jobsService';
import { useNavigate } from 'react-router-dom';
import './OpportunitiesPage.css';

const { Content } = Layout;
const { Title, Text } = Typography;
const { confirm } = Modal;
const { TabPane } = Tabs;

const MAIN_STATUSES = [
  'review', 
  'paused_for_approval', 
  'generated', 
  'rejected', 
  'failed'
];

const statusColors = {
  review: 'blue',
  paused_for_approval: 'orange',
  generated: 'green',
  rejected: 'default',
  failed: 'red',
};

const OpportunitiesPage = () => {
  const { 
    opportunities, isLoading, pagination, 
    handleTableChange, activeStatus, setActiveStatus, statusCounts
  } = useOpportunities();
  const { showNotification } = useNotifications();
  const queryClient = useQueryClient();
  const navigate = useNavigate();
  const { startJob, updateJob, completeJob } = useJobs();

  const { mutate: startWorkflowMutation, isLoading: isStartingWorkflow } = useMutation(
    ({ opportunityId, override, opportunityKeyword }) => startFullWorkflow(opportunityId, override),
    {
      onSuccess: (data, variables) => {
        const { job_id } = data;
        const { opportunityKeyword } = variables;
        startJob(job_id, `Workflow started for "${opportunityKeyword}".`);

        const poll = setInterval(async () => {
          try {
            const statusData = await getJobStatus(job_id);
            if (statusData.status === 'completed' || statusData.status === 'failed') {
              updateJob(job_id, statusData.status, statusData.error || `Workflow for "${opportunityKeyword}" finished.`);
              completeJob(job_id);
              clearInterval(poll);
              queryClient.invalidateQueries('opportunities');
            } else {
              const lastLog = statusData.progress_log?.[statusData.progress_log.length - 1];
              updateJob(job_id, 'running', lastLog?.message || 'Processing...');
            }
          } catch (error) {
            updateJob(job_id, 'failed', 'Failed to get job status.');
            completeJob(job_id);
            clearInterval(poll);
          }
        }, 5000);
      },
      onError: (err, variables) => {
        const { opportunityKeyword } = variables;
        showNotification('error', `Workflow Failed for "${opportunityKeyword}"`, err.message)
      },
    }
  );

  const { mutate: rejectOpportunityMutation, isLoading: isRejecting } = useMutation(
    (opportunityId) => rejectOpportunity(opportunityId),
    {
      onSuccess: () => {
        showNotification('success', 'Opportunity Rejected', 'The opportunity has been marked as rejected.');
        queryClient.invalidateQueries('opportunities');
      },
      onError: (err) => showNotification('error', 'Rejection Failed', err.message),
    }
  );

  const showRejectConfirm = (opportunityId) => {
    confirm({
      title: 'Are you sure you want to reject this opportunity?',
      icon: <ExclamationCircleOutlined />,
      content: 'This action cannot be undone.',
      okText: 'Yes, Reject',
      okType: 'danger',
      cancelText: 'No',
      onOk() {
        rejectOpportunityMutation(opportunityId);
      },
    });
  };

  const renderActions = (_, record) => {
    const isFailed = ['failed', 'rejected'].includes(record.status);
    const isLoading = isStartingWorkflow || isRejecting;

    const buttons = [];

    switch (activeStatus) {
      case 'review':
        buttons.push(
          <Tooltip title="Run Workflow" key="run">
            <Button
              type="primary"
              icon={<RocketOutlined />}
              onClick={(e) => { 
                e.stopPropagation(); 
                console.log('Starting workflow for opportunity:', record.id, 'with override:', isFailed);
                startWorkflowMutation({ opportunityId: record.id, override: isFailed, opportunityKeyword: record.keyword }); 
              }}
              loading={isStartingWorkflow}
              disabled={isLoading}
            />
          </Tooltip>,
          <Tooltip title="Reject Opportunity" key="reject">
            <Button
              danger
              icon={<DeleteOutlined />}
              onClick={(e) => { e.stopPropagation(); showRejectConfirm(record.id); }}
              loading={isRejecting}
              disabled={isLoading}
            />
          </Tooltip>
        );
        break;
      case 'rejected':
      case 'failed':
        buttons.push(
          <Tooltip title="Run Workflow" key="run-failed">
            <Button
              type="primary"
              icon={<RocketOutlined />}
              onClick={(e) => { e.stopPropagation(); startWorkflowMutation({ opportunityId: record.id, override: true }); }}
              loading={isStartingWorkflow}
              disabled={isLoading}
            />
          </Tooltip>
        );
        break;
      default:
        break;
    }

    buttons.push(
      <Tooltip title="View Details" key="view">
        <Button 
          icon={<EditOutlined />} 
          onClick={(e) => { e.stopPropagation(); navigate(`/opportunities/${record.id}`)}} 
        />
      </Tooltip>
    );

    return <Space>{buttons}</Space>;
  };

  const baseColumns = [
    { title: 'Keyword', dataIndex: 'keyword', key: 'keyword', sorter: true, render: (text, record) => <a onClick={(e) => { e.stopPropagation(); navigate(`/opportunities/${record.id}`)}}>{text}</a> },
    { title: 'Search Volume', dataIndex: 'search_volume', key: 'search_volume', sorter: true, render: (sv) => sv ? sv.toLocaleString() : 'N/A' },
    { title: 'KD', dataIndex: 'keyword_difficulty', key: 'keyword_difficulty', sorter: true, render: (kd) => kd != null ? kd : 'N/A' },
  ];

  const rejectedColumns = [
    ...baseColumns,
    { 
      title: 'Rejection Reason', 
      dataIndex: 'blog_qualification_reason', 
      key: 'blog_qualification_reason',
      render: (reason) => reason || <Text type="secondary">No reason provided</Text>
    },
    { title: 'Actions', key: 'actions', fixed: 'right', render: renderActions },
  ];

  const defaultColumns = [
    ...baseColumns,
    { title: 'Strategic Score', dataIndex: 'strategic_score', key: 'strategic_score', sorter: true, render: (score) => score ? <strong>{score.toFixed(1)}</strong> : 'N/A' },
    { title: 'CPC', dataIndex: 'cpc', key: 'cpc', sorter: true, render: (cpc) => cpc ? `$${cpc.toFixed(2)}` : 'N/A' },
    { title: 'Actions', key: 'actions', fixed: 'right', render: renderActions },
  ];

  const columns = activeStatus === 'rejected' ? rejectedColumns : defaultColumns;

  return (
    <Layout style={{ padding: '24px' }}><Content>
      <Title level={2}>Content Opportunities</Title>
      <Card>
        <div className="custom-tabs">
          <Tabs activeKey={activeStatus} onChange={setActiveStatus}>
            {MAIN_STATUSES.map(status => (
              <TabPane 
                tab={
                  <span>
                    {status.replace(/_/g, ' ').toUpperCase()}
                    <Tag color={statusColors[status]} style={{ marginLeft: 8 }}>
                      {statusCounts[status] || 0}
                    </Tag>
                  </span>
                }
                key={status} 
              />
            ))}
          </Tabs>
        </div>
        <Table
          columns={columns}
          dataSource={opportunities}
          rowKey="id"
          loading={isLoading}
          pagination={pagination}
          onChange={handleTableChange}
          onRow={(record) => ({
            onClick: () => navigate(`/opportunities/${record.id}`),
            style: { cursor: 'pointer' },
          })}
          rowClassName="table-row-hover"
        />
      </Card>
    </Content></Layout>
  );
};

export default OpportunitiesPage;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/ActionCenter.jsx
```javascript
import React from 'react';
import { Card, Button, Space, Alert, Modal } from 'antd';
import { CheckOutlined, ExperimentOutlined, RocketOutlined, DeleteOutlined, ExclamationCircleOutlined } from '@ant-design/icons';
import { useNotifications } from '../../../context/NotificationContext';
import { useMutation } from 'react-query';
import { approveAnalysis, startFullContentGeneration, startFullWorkflow, rejectOpportunity } from '../../../services/orchestratorService';

const { confirm } = Modal;

const ActionCenter = ({ status, opportunityId, overrides, refetch }) => {
  const { showNotification } = useNotifications();

  const { mutate: approveAnalysisMutation, isLoading: isApproving } = useMutation(
    () => approveAnalysis(opportunityId, overrides),
    {
      onSuccess: () => {
        showNotification('success', 'Analysis Approved', 'The content generation process has been initiated.');
        refetch();
      },
      onError: (error) => showNotification('error', 'Approval Failed', error.message),
    }
  );

  const { mutate: generateContentMutation, isLoading: isGenerating } = useMutation(
    (variables) => startFullContentGeneration(variables.opportunityId, variables.modelOverride, variables.temperature),
    {
      onSuccess: () => {
        showNotification('success', 'Content Generation Started', 'The AI is now generating the content.');
        refetch();
      },
      onError: (error) => showNotification('error', 'Generation Failed', error.message),
    }
  );

  const { mutate: startWorkflowMutation, isLoading: isStartingWorkflow } = useMutation(
    () => startFullWorkflow(opportunityId, ['failed', 'rejected'].includes(status)),
    {
      onSuccess: (data) => {
        showNotification('success', 'Workflow Started', `Job has been queued. Job ID: ${data.job_id}`);
        refetch();
      },
      onError: (err) => showNotification('error', 'Workflow Failed', err.message),
    }
  );

  const { mutate: rejectOpportunityMutation, isLoading: isRejecting } = useMutation(
    () => rejectOpportunity(opportunityId),
    {
      onSuccess: () => {
        showNotification('success', 'Opportunity Rejected', 'The opportunity has been marked as rejected.');
        refetch();
      },
      onError: (err) => showNotification('error', 'Rejection Failed', err.message),
    }
  );

  const showRejectConfirm = () => {
    confirm({
      title: 'Are you sure you want to reject this opportunity?',
      icon: <ExclamationCircleOutlined />,
      content: 'This action cannot be undone.',
      okText: 'Yes, Reject',
      okType: 'danger',
      cancelText: 'No',
      onOk: () => rejectOpportunityMutation(),
    });
  };

  const renderActions = () => {
    const isLoading = isApproving || isGenerating || isStartingWorkflow || isRejecting;

    switch (status) {
      case 'review':
        return (
          <Space>
            <Button type="primary" icon={<RocketOutlined />} onClick={() => startWorkflowMutation()} loading={isStartingWorkflow} disabled={isLoading}>
              Run Workflow
            </Button>
            <Button type="danger" icon={<DeleteOutlined />} onClick={showRejectConfirm} loading={isRejecting} disabled={isLoading}>
              Reject
            </Button>
          </Space>
        );
      case 'paused_for_approval':
        return (
          <Button type="primary" icon={<CheckOutlined />} onClick={() => approveAnalysisMutation()} loading={isApproving} disabled={isLoading}>
            Approve Analysis & Proceed to Content Generation
          </Button>
        );
      case 'validated':
        return (
          <Button type="primary" icon={<ExperimentOutlined />} onClick={() => generateContentMutation({ opportunityId, modelOverride: null, temperature: null })} loading={isGenerating} disabled={isLoading}>
            Generate Content
          </Button>
        );
      case 'failed':
      case 'rejected':
        return (
          <Button type="primary" icon={<RocketOutlined />} onClick={() => startWorkflowMutation()} loading={isStartingWorkflow} disabled={isLoading}>
            Rerun Workflow
          </Button>
        );
      default:
        return <Alert message="No actions available for the current status." type="info" showIcon />;
    }
  };

  return (
    <Card>
      <Space direction="vertical" style={{ width: '100%' }}>
        <Alert
          message="Next Step"
          description="This is the primary action to move this opportunity forward in the workflow."
          type="info"
          showIcon
        />
        {renderActions()}
      </Space>
    </Card>
  );
};

export default ActionCenter;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/AdditionalInsights.jsx
```javascript
import React from 'react';
import { Card, Typography, List, Tag } from 'antd';

const { Title, Paragraph } = Typography;

const AdditionalInsights = ({ serpOverview }) => {
  return (
    <Card title="Additional Insights">
      {serpOverview?.discussion_snippets?.length > 0 && (
        <>
          <Title level={5}>Discussion Snippets</Title>
          <List
            dataSource={serpOverview.discussion_snippets}
            renderItem={(item) => <List.Item>{item}</List.Item>}
            size="small"
            bordered
            style={{ marginBottom: '24px' }}
          />
        </>
      )}
    </Card>
  );
};

export default AdditionalInsights;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/ArticlePreview.jsx
```javascript
import React from 'react';
import { Card, Typography, Image, Alert } from 'antd';
import NoData from './NoData';

const { Title, Paragraph } = Typography;

const ArticlePreview = ({ aiContent, featuredImagePath }) => {
  if (!aiContent) {
    return <NoData description="No article content has been generated yet." />;
  }

  const { article_title, article_body_html } = aiContent;

  return (
    <Card>
      <Title level={2}>{article_title}</Title>
      {featuredImagePath ? (
        <div style={{ textAlign: 'center', marginBottom: '24px' }}>
          <Image
            width="50%"
            src={`/api/images/${featuredImagePath.split('/').pop()}`}
            alt={article_title}
          />
        </div>
      ) : (
        <NoData description="No featured image has been generated yet." />
      )}
      <div dangerouslySetInnerHTML={{ __html: article_body_html }} />
    </Card>
  );
};

export default ArticlePreview;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/CompetitorBacklinks.jsx
```javascript
import React from 'react';
import { Card, Statistic, Row, Col, Tooltip } from 'antd';
import { LinkOutlined, TeamOutlined, RiseOutlined } from '@ant-design/icons';

const CompetitorBacklinks = ({ avgBacklinksInfo }) => {
  if (!avgBacklinksInfo) {
    return <Card title="Competitor Backlink Analysis">No data available.</Card>;
  }

  const { backlinks, referring_domains, main_domain_rank } = avgBacklinksInfo;

  return (
    <Card title="Competitor Backlink Analysis">
      <Row gutter={16}>
        <Col span={8}>
          <Tooltip title="The average number of backlinks for the top-ranking pages.">
            <Statistic title="Avg. Backlinks" value={backlinks} prefix={<LinkOutlined />} />
          </Tooltip>
        </Col>
        <Col span={8}>
          <Tooltip title="The average number of unique domains linking to the top-ranking pages.">
            <Statistic title="Avg. Referring Domains" value={referring_domains} prefix={<TeamOutlined />} />
          </Tooltip>
        </Col>
        <Col span={8}>
          <Tooltip title="The average Domain Rank (a measure of a website's authority) of the top-ranking pages.">
            <Statistic title="Avg. Domain Rank" value={main_domain_rank} prefix={<RiseOutlined />} />
          </Tooltip>
        </Col>
      </Row>
    </Card>
  );
};

export default CompetitorBacklinks;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/ContentAuditCard.jsx
```javascript
import React from 'react';
import { Card, Typography, List, Tag, Progress } from 'antd';
import { CheckCircleOutlined, CloseCircleOutlined } from '@ant-design/icons';

const { Title, Text } = Typography;

const ContentAuditCard = ({ auditResults }) => {
  if (!auditResults) {
    return null;
  }

  const {
    flesch_kincaid_grade,
    readability_assessment,
    entity_coverage_score,
    missing_entities,
    publish_readiness_issues,
  } = auditResults;

  return (
    <Card title="Content Audit" style={{ marginTop: 24 }}>
      <Title level={5}>Readability</Title>
      <Text>{readability_assessment}</Text>
      <Text strong style={{ display: 'block', marginTop: 8 }}>
        Flesch-Kincaid Grade Level: {flesch_kincaid_grade.toFixed(1)}
      </Text>

      <Title level={5} style={{ marginTop: 16 }}>Entity Coverage</Title>
      <Progress percent={entity_coverage_score} />
      {missing_entities && missing_entities.length > 0 && (
        <>
          <Text strong style={{ display: 'block', marginTop: 8 }}>Missing Entities:</Text>
          <List
            dataSource={missing_entities}
            renderItem={(item) => (
              <List.Item>
                <CloseCircleOutlined style={{ color: 'red', marginRight: 8 }} />
                {item}
              </List.Item>
            )}
            size="small"
          />
        </>
      )}

      <Title level={5} style={{ marginTop: 16 }}>Publishing Readiness</Title>
      {publish_readiness_issues && publish_readiness_issues.length > 0 ? (
        <List
          dataSource={publish_readiness_issues}
          renderItem={(item) => (
            <List.Item>
              <CloseCircleOutlined style={{ color: 'red', marginRight: 8 }} />
              <Text>{item.issue}: {item.context}</Text>
            </List.Item>
          )}
          size="small"
        />
      ) : (
        <Space>
          <CheckCircleOutlined style={{ color: 'green' }} />
          <Text>No publishing readiness issues found.</Text>
        </Space>
      )}
    </Card>
  );
};

export default ContentAuditCard;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/ContentBlueprint.jsx
```javascript
import React, { useState, useEffect } from 'react';
import { Card, Typography, List, Tag, Descriptions, Button, Tooltip, Select } from 'antd';
import { CopyOutlined, LinkOutlined, BulbOutlined, PlusOutlined } from '@ant-design/icons';
import { useNotifications } from '../../../context/NotificationContext';
import NoData from './NoData';

const { Title, Paragraph, Text } = Typography;
const { Option } = Select;

const ContentBlueprint = ({ blueprint, overrides, setOverrides }) => {
  const { showNotification } = useNotifications();
  const [paaToAdd, setPaaToAdd] = useState(null);

  if (!blueprint) {
    return <Card><Paragraph type="secondary">No content blueprint available.</Paragraph></Card>;
  }

  const { ai_content_brief, content_intelligence, recommended_strategy } = blueprint;
  const people_also_ask = blueprint.serp_overview?.people_also_ask || [];

  const handleCopyOutline = () => {
    const outline = overrides
      .map((item) => {
        const h3s = item.h3s.map((h3) => `  - ${h3}`).join('\n');
        return `${item.h2}\n${h3s}`;
      })
      .join('\n\n');
    navigator.clipboard.writeText(outline);
    showNotification('success', 'Copied to Clipboard', 'The article outline has been copied.');
  };

  const handleAddPaa = () => {
    if (!paaToAdd) return;

    const faqSectionIndex = overrides.findIndex(sec => sec.h2.toLowerCase().includes('frequently asked questions'));
    
    if (faqSectionIndex > -1) {
      const newStructure = [...overrides];
      newStructure[faqSectionIndex].h3s.push(paaToAdd);
      setOverrides(newStructure);
      showNotification('success', 'Question Added', `"${paaToAdd}" was added to the outline.`);
    } else {
      showNotification('warning', 'Section Not Found', 'Could not find a "Frequently Asked Questions" section to add this to.');
    }
    setPaaToAdd(null);
  };
  
  return (
    <Card title="AI Content Blueprint">
      <Descriptions bordered column={1} size="small" style={{ marginBottom: '24px' }}>
        <Descriptions.Item label="Target Audience">{ai_content_brief?.target_audience_persona || 'Not available'}</Descriptions.Item>
        <Descriptions.Item label="Primary Goal">{ai_content_brief?.primary_goal || 'Not available'}</Descriptions.Item>
        <Descriptions.Item label="Target Word Count">{ai_content_brief?.target_word_count || 'Not available'}</Descriptions.Item>
      </Descriptions>

      <Title level={5}>Dynamic SERP Instructions</Title>
      <List
        dataSource={ai_content_brief.dynamic_serp_instructions}
        renderItem={(item) => <List.Item><BulbOutlined style={{ marginRight: 8 }} />{item}</List.Item>}
        style={{ marginBottom: '24px' }}
        size="small"
      />

      <Title level={5}>Content Gaps & Unique Angles</Title>
      <List
        dataSource={content_intelligence.identified_content_gaps}
        renderItem={(item) => <List.Item>{item}</List.Item>}
        style={{ marginBottom: '24px' }}
      />

      <Title level={5}>Recommended Article Structure</Title>
      <div style={{ marginBottom: 16 }}>
        <Select
          showSearch
          placeholder="Select a 'People Also Ask' question to add to your outline"
          style={{ width: 'calc(100% - 120px)', marginRight: 8 }}
          onChange={value => setPaaToAdd(value)}
          value={paaToAdd}
        >
          {people_also_ask.map(q => <Option key={q} value={q}>{q}</Option>)}
        </Select>
        <Button icon={<PlusOutlined />} onClick={handleAddPaa} disabled={!paaToAdd}>Add</Button>
        <Button
          icon={<CopyOutlined />}
          onClick={handleCopyOutline}
          style={{ float: 'right' }}
        >
          Copy Outline
        </Button>
      </div>
      <List
        dataSource={overrides}
        renderItem={(item) => (
          <List.Item>
            <List.Item.Meta
              title={item.h2}
              description={
                <div style={{ paddingLeft: '20px' }}>
                  {item.h3s.map(h3 => <p key={h3} style={{ margin: '4px 0' }}>- {h3}</p>)}
                </div>
              }
            />
          </List.Item>
        )}
        style={{ marginBottom: '24px' }}
      />

      <Title level={5}>Key Entities to Mention</Title>
      <div style={{ marginBottom: '24px' }}>
        {content_intelligence.key_entities_from_competitors.map((entity) => (
          <Tag key={entity} style={{ margin: '4px' }}>{entity}</Tag>
        ))}
      </div>

      <Title level={5}>Focus Competitors</Title>
      <List
        dataSource={recommended_strategy.focus_competitors}
        renderItem={(item) => (
          <List.Item>
            <a href={item.url} target="_blank" rel="noopener noreferrer">
              <LinkOutlined style={{ marginRight: 8 }} />
              {item.title}
            </a>
          </List.Item>
        )}
        style={{ marginBottom: '24px' }}
        bordered
        size="small"
      />

      <Title level={5}>Internal Linking Suggestions</Title>
      {blueprint.internal_linking_suggestions && blueprint.internal_linking_suggestions.length > 0 ? (
        <List
          dataSource={blueprint.internal_linking_suggestions}
          renderItem={(item) => (
            <List.Item>
              <Tooltip title={`Link to: ${item.url}`}>
                <Text>Anchor Text: "{item.anchor_text}"</Text>
              </Tooltip>
            </List.Item>
          )}
          bordered
          size="small"
        />
      ) : (
        <NoData description="No internal linking suggestions were generated." />
      )}
    </Card>
  );
};

export default ContentBlueprint;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/ErrorMessage.jsx
```javascript
import React from 'react';
import { Alert } from 'antd';

const ErrorMessage = ({ message }) => {
  if (!message || !(message.toLowerCase().includes('error') || message.toLowerCase().includes('failed'))) {
    return null;
  }

  return (
    <Alert
      message="Workflow Error"
      description={message}
      type="error"
      showIcon
      closable
    />
  );
};

export default ErrorMessage;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/ExecutiveSummary.jsx
```javascript
import React from 'react';
import { Card, Typography } from 'antd';

const { Title, Paragraph } = Typography;

const ExecutiveSummary = ({ summary }) => {
  if (!summary) {
    return null;
  }

  return (
    <Card style={{ marginTop: 24 }}>
      <Title level={4}>Executive Summary</Title>
      <Paragraph>{summary}</Paragraph>
    </Card>
  );
};

export default ExecutiveSummary;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/FactorsCard.jsx
```javascript
import React from 'react';
import { Card, Typography, List, Row, Col } from 'antd';
import { CheckCircleOutlined, CloseCircleOutlined } from '@ant-design/icons';
import NoData from './NoData';

const { Title } = Typography;

const FactorsCard = ({ positiveFactors, negativeFactors }) => {
  return (
    <Card style={{ marginTop: 24 }}>
      <Row gutter={16}>
        <Col span={12}>
          <Title level={5}>Positive Factors</Title>
          <List
            dataSource={positiveFactors}
            renderItem={(item) => (
              <List.Item>
                <CheckCircleOutlined style={{ color: 'green', marginRight: '8px' }} /> {item}
              </List.Item>
            )}
          />
        </Col>
        <Col span={12}>
          <Title level={5}>Negative Factors</Title>
          {negativeFactors && negativeFactors.length > 0 ? (
            <List
              dataSource={negativeFactors}
              renderItem={(item) => (
                <List.Item>
                  <CloseCircleOutlined style={{ color: 'red', marginRight: '8px' }} /> {item}
                </List.Item>
              )}
            />
          ) : (
            <NoData description="No negative factors identified." />
          )}
        </Col>
      </Row>
    </Card>
  );
};

export default FactorsCard;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/FeaturedSnippetCard.jsx
```javascript
import React from 'react';
import { Card, Typography } from 'antd';
import { TrophyOutlined } from '@ant-design/icons';

const { Title, Paragraph } = Typography;

const FeaturedSnippetCard = ({ blueprint }) => {
  const featuredSnippet = blueprint?.serp_overview?.featured_snippet_content;

  if (!featuredSnippet) {
    return null; // Don't render the card if there's no snippet
  }

  return (
    <Card
      title={<span><TrophyOutlined style={{ marginRight: 8 }} /> Featured Snippet Opportunity</span>}
      bordered={false}
      style={{ backgroundColor: '#e6f7ff' }}
    >
      <Paragraph>
        The following content currently holds the featured snippet position. Your goal is to provide a better, more direct answer.
      </Paragraph>
      <Paragraph blockquote="true" style={{ fontStyle: 'italic' }}>
        {featuredSnippet}
      </Paragraph>
    </Card>
  );
};

export default FeaturedSnippetCard;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/GrowthTrend.jsx
```javascript
import React from 'react';
import { Card, Statistic, Tooltip } from 'antd';
import { RiseOutlined } from '@ant-design/icons';

const GrowthTrend = ({ scoreBreakdown }) => {
  const growthTrend = scoreBreakdown?.growth_trend;

  if (!growthTrend) {
    return null;
  }

  return (
    <Card>
      <Tooltip title={growthTrend.breakdown['Growth Trend'].explanation}>
        <Statistic
          title="Year-over-Year Growth"
          value={growthTrend.breakdown['Growth Trend'].value}
          prefix={<RiseOutlined />}
          valueStyle={{ color: '#3f8600' }}
        />
      </Tooltip>
    </Card>
  );
};

export default GrowthTrend;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/IntentAnalysis.jsx
```javascript
import React from 'react';
import { Card, Typography, Tag, Tooltip } from 'antd';
import { AimOutlined, DollarOutlined } from '@ant-design/icons';

const { Title, Text } = Typography;

const IntentAnalysis = ({ searchIntentInfo }) => {
  if (!searchIntentInfo) {
    return null;
  }

  const { main_intent, foreign_intent } = searchIntentInfo;

  const intentColor = {
    informational: 'blue',
    commercial: 'gold',
    transactional: 'green',
    navigational: 'purple',
  };

  return (
    <Card title="Search Intent Analysis">
      <Tooltip title="The primary reason a user is searching for this keyword.">
        <div>
          <Text strong>Main Intent: </Text>
          <Tag color={intentColor[main_intent] || 'default'} icon={<AimOutlined />}>
            {main_intent?.toUpperCase()}
          </Tag>
        </div>
      </Tooltip>
      {foreign_intent?.length > 0 && (
        <Tooltip title="Other potential intents this keyword might satisfy.">
          <div style={{ marginTop: '16px' }}>
            <Text strong>Secondary Intents: </Text>
            {foreign_intent.map(intent => (
              <Tag key={intent} color={intentColor[intent] || 'default'} icon={<DollarOutlined />}>
                {intent.toUpperCase()}
              </Tag>
            ))}
          </div>
        </Tooltip>
      )}
    </Card>
  );
};

export default IntentAnalysis;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/KeywordMetrics.jsx
```javascript
import React from 'react';
import { Card, Statistic, Row, Col, Tooltip, Tag } from 'antd';
import { BarChartOutlined, DollarCircleOutlined, ThunderboltOutlined, InfoCircleOutlined } from '@ant-design/icons';
import { Line } from '@ant-design/plots';

const KeywordMetrics = ({ keywordInfo, keywordProperties }) => {
  if (!keywordInfo || !keywordProperties) {
    return <Card title="Keyword Metrics">No data available.</Card>;
  }

  const { search_volume, cpc, competition, monthly_searches, competition_level, low_top_of_page_bid, high_top_of_page_bid } = keywordInfo;
  const { keyword_difficulty } = keywordProperties;

  const chartData = monthly_searches?.map(item => ({
    date: `${item.year}-${item.month}`,
    volume: item.search_volume,
  }));

  const chartConfig = {
    data: chartData,
    xField: 'date',
    yField: 'volume',
    height: 200,
    point: {
      size: 5,
      shape: 'diamond',
    },
    tooltip: {
      formatter: (datum) => {
        return { name: 'Search Volume', value: datum.volume.toLocaleString() };
      },
    },
  };

  return (
    <Card title="Keyword Metrics">
      <Row gutter={[16, 24]}>
        <Col span={12}>
          <Tooltip title="The average number of times this keyword is searched for per month.">
            <Statistic title="Search Volume" value={search_volume} prefix={<BarChartOutlined />} />
          </Tooltip>
        </Col>
        <Col span={12}>
          <Tooltip title="The average cost per click for this keyword in paid search campaigns.">
            <Statistic title="CPC" value={cpc} prefix={<DollarCircleOutlined />} precision={2} />
          </Tooltip>
        </Col>
        <Col span={12}>
          <Tooltip title="The level of competition for this keyword in paid search campaigns, on a scale of 0 to 1.">
            <div>
              <Statistic title="Competition" value={competition} precision={2} />
              <Tag color={competition_level === 'LOW' ? 'green' : competition_level === 'MEDIUM' ? 'orange' : 'red'}>{competition_level}</Tag>
            </div>
          </Tooltip>
        </Col>
        <Col span={12}>
          <Tooltip title="An estimate of how difficult it would be to rank organically for this keyword, on a scale of 0 to 100.">
            <Statistic title="Keyword Difficulty" value={keyword_difficulty} prefix={<ThunderboltOutlined />} />
          </Tooltip>
        </Col>
        <Col span={12}>
          <Tooltip title="The lower range of what advertisers have historically paid for a top-of-page bid.">
            <Statistic title="Low Top of Page Bid" value={low_top_of_page_bid} precision={2} prefix="$" />
          </Tooltip>
        </Col>
        <Col span={12}>
          <Tooltip title="The higher range of what advertisers have historically paid for a top-of-page bid.">
            <Statistic title="High Top of Page Bid" value={high_top_of_page_bid} precision={2} prefix="$" />
          </Tooltip>
        </Col>
      </Row>
      <div style={{ marginTop: '24px' }}>
        {chartData && chartData.length > 0 ? (
          <Line {...chartConfig} />
        ) : (
          <div style={{ textAlign: 'center', padding: '20px' }}>
            <InfoCircleOutlined style={{ marginRight: '8px' }} />
            No monthly search volume data available to display a chart.
          </div>
        )}
      </div>
    </Card>
  );
};

export default KeywordMetrics;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/MetaInfo.jsx
```javascript
import React from 'react';
import { Card, Descriptions, Tag } from 'antd';
import { InfoCircleOutlined, DollarCircleOutlined, HistoryOutlined } from '@ant-design/icons';
import { format } from 'date-fns';

const MetaInfo = ({ blueprint, lastWorkflowStep, dateProcessed }) => {
  const metadata = blueprint?.metadata;

  return (
    <Card title="Process Metadata" icon={<InfoCircleOutlined />}>
      <Descriptions column={1} size="small" variant="outlined">
        {metadata?.blueprint_version && (
          <Descriptions.Item label="Blueprint Version">
            <Tag>{metadata.blueprint_version}</Tag>
          </Descriptions.Item>
        )}
        {lastWorkflowStep && (
          <Descriptions.Item label="Last Workflow Step">
            <Tag color="blue">{lastWorkflowStep.replace(/_/g, ' ')}</Tag>
          </Descriptions.Item>
        )}
        {dateProcessed && (
          <Descriptions.Item label="Date Processed">
            <HistoryOutlined /> {format(new Date(dateProcessed), 'MMM d, yyyy HH:mm')}
          </Descriptions.Item>
        )}
        {metadata?.total_api_cost && (
          <Descriptions.Item label="Analysis API Cost">
            <DollarCircleOutlined /> ${metadata.total_api_cost.toFixed(4)}
          </Descriptions.Item>
        )}
      </Descriptions>
    </Card>
  );
};

export default MetaInfo;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/NoData.jsx
```javascript
import React from 'react';
import { Empty } from 'antd';

const NoData = ({ description }) => {
  return (
    <div style={{ padding: '24px', textAlign: 'center' }}>
      <Empty
        image={Empty.PRESENTED_IMAGE_SIMPLE}
        description={description || 'No data available for this section.'}
      />
    </div>
  );
};

export default NoData;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/OpportunityHeader.jsx
```javascript
import React from 'react';
import { Card, Tag, Statistic, Row, Col, Typography, Progress } from 'antd';
import { format } from 'date-fns';

const { Title, Text } = Typography;

const getStatusColor = (status) => {
  if (status.includes('approved') || status === 'validated') return 'success';
  if (status.includes('paused')) return 'warning';
  if (status.includes('failed')) return 'error';
  return 'processing';
};

const OpportunityHeader = ({ keyword, strategicScore, status, dateAdded, recommendation }) => {
  return (
    <Card style={{ borderRadius: '8px' }}>
      <Row align="middle" justify="space-between">
        <Col>
          <Title level={2} style={{ margin: 0 }}>{keyword}</Title>
          <Tag color={getStatusColor(status)} style={{ marginTop: 8 }}>
            {status.replace(/_/g, ' ').toUpperCase()}
          </Tag>
        </Col>
        <Col>
          <Row align="middle" gutter={32}>
            <Col>
              <Statistic title="Date Added" value={format(new Date(dateAdded), 'MMM d, yyyy')} />
            </Col>
            {recommendation && (
              <Col>
                <Tag color={recommendation === 'Proceed' ? 'success' : 'error'} style={{ fontSize: '1.2rem', padding: '10px' }}>
                  {recommendation}
                </Tag>
              </Col>
            )}
            <Col style={{ textAlign: 'center' }}>
              {typeof strategicScore === 'number' && (
                <>
                  <Progress
                    type="circle"
                    percent={strategicScore}
                    format={(percent) => `${percent.toFixed(1)}`}
                    strokeColor={{
                      '0%': '#B8E1FF',
                      '100%': '#3D76DD',
                    }}
                    size={80}
                  />
                  <Text style={{ display: 'block', marginTop: 8 }}>Strategic Score</Text>
                </>
              )}
            </Col>
          </Row>
        </Col>
      </Row>
    </Card>
  );
};

export default OpportunityHeader;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/QualificationInfo.jsx
```javascript
import React from 'react';
import { Card, Typography, Tag } from 'antd';
import { CheckSquareOutlined } from '@ant-design/icons';

const { Text } = Typography;

const QualificationInfo = ({ status, reason }) => {
  if (!status) {
    return null;
  }

  return (
    <Card title="Initial Qualification">
      <Text strong>Status: </Text>
      <Tag color={status === 'review' ? 'orange' : 'default'}>{status.toUpperCase()}</Tag>
      <Text type="secondary" style={{ marginTop: '16px', display: 'block' }}>
        {reason}
      </Text>
    </Card>
  );
};

export default QualificationInfo;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/RecommendedStrategyCard.jsx
```javascript
import React from 'react';
import { Card, Typography, Tag } from 'antd';
import { BulbOutlined } from '@ant-design/icons';

const { Title, Paragraph } = Typography;

const RecommendedStrategyCard = ({ strategy }) => {
  if (!strategy) {
    return null;
  }

  return (
    <Card style={{ marginTop: 24 }}>
      <Title level={5}><BulbOutlined /> Recommended Strategy</Title>
      <Paragraph strong>Content Format: <Tag color="purple">{strategy.content_format}</Tag></Paragraph>
      <Paragraph>{strategy.strategic_goal}</Paragraph>
    </Card>
  );
};

export default RecommendedStrategyCard;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/SerpAnalysis.jsx
```javascript
import React from 'react';
import { Card, Table, Tag, List, Typography, Button, Statistic, Row, Col } from 'antd';
import {
  FileTextOutlined,
  VideoCameraOutlined,
  QuestionCircleOutlined,
  CommentOutlined,
  SearchOutlined,
  CopyOutlined,
  RobotOutlined,
  FileDoneOutlined,
  LinkOutlined,
  StarFilled
} from '@ant-design/icons';
import { useNotifications } from '../../../context/NotificationContext';

const { Title, Paragraph, Text } = Typography;

const SerpAnalysis = ({ blueprint }) => {
  const { showNotification } = useNotifications();

  if (!blueprint || !blueprint.serp_overview) {
    return <Card><Paragraph type="secondary">No SERP analysis available.</Paragraph></Card>;
  }

  const { top_organic_results, people_also_ask, related_searches, top_organic_sitelinks, serp_has_ai_overview, dominant_content_format } = blueprint.serp_overview;
  const { serp_item_types, se_results_count } = blueprint.winning_keyword.serp_info;

  const getSerpFeatureIcon = (item) => {
    if (item.includes('video')) return <VideoCameraOutlined />;
    if (item.includes('people_also_ask')) return <QuestionCircleOutlined />;
    if (item.includes('perspectives')) return <CommentOutlined />;
    if (item.includes('related_searches')) return <SearchOutlined />;
    return <FileTextOutlined />;
  };

  const handleCopyPaA = () => {
    navigator.clipboard.writeText(people_also_ask.join('\n'));
    showNotification('success', 'Copied to Clipboard', 'People Also Ask questions have been copied.');
  };

  const columns = [
    { title: 'Rank', dataIndex: 'rank', key: 'rank' },
    { title: 'Title', dataIndex: 'title', key: 'title', render: (text, record) => <a href={record.url} target="_blank" rel="noopener noreferrer">{text}</a> },
    { title: 'Domain', dataIndex: 'domain', key: 'domain' },
    { title: 'Page Type', dataIndex: 'page_type', key: 'page_type', render: (type) => <Tag>{type}</Tag> },
    { 
      title: 'Rating', 
      dataIndex: 'rating', 
      key: 'rating', 
      render: (rating) => rating ? <span><StarFilled style={{ color: '#fadb14' }} /> {rating.value} ({rating.votes_count})</span> : null 
    },
  ];

  const handleRelatedSearchClick = (term) => {
    // In a real app, you would likely navigate to the discovery page with this term
    console.log(`Starting a new discovery run for: ${term}`);
    showNotification('info', 'Discovery Run Started', `A new discovery run has been initiated for "${term}".`);
  };

  return (
    <Card title="SERP Analysis">
      <Row gutter={16} style={{ marginBottom: 24 }}>
        <Col span={8}>
          <Statistic title="Total Search Results" value={se_results_count} />
        </Col>
        <Col span={8}>
          <Statistic title="AI Overview in SERP" value={serp_has_ai_overview ? 'Yes' : 'No'} prefix={<RobotOutlined />} />
        </Col>
        <Col span={8}>
           <Statistic title="Dominant Content Format" value={dominant_content_format} prefix={<FileDoneOutlined />} />
        </Col>
      </Row>

      <Title level={5}>SERP Features Present</Title>
      <List
        dataSource={serp_item_types}
        renderItem={(item) => <List.Item><Tag icon={getSerpFeatureIcon(item)}>{item.replace(/_/g, ' ')}</Tag></List.Item>}
        grid={{ gutter: 16, column: 4 }}
        style={{ marginBottom: 24 }}
      />

      <Title level={5}>People Also Ask</Title>
      <Button icon={<CopyOutlined />} onClick={handleCopyPaA} style={{ float: 'right' }}>Copy</Button>
      <List dataSource={people_also_ask} renderItem={(item) => <List.Item>{item}</List.Item>} style={{ marginBottom: 24 }} />

      <Title level={5}>Related Searches</Title>
      <div style={{ marginBottom: 24 }}>
        {related_searches.map(term => (
          <Tag 
            icon={<SearchOutlined />} 
            key={term} 
            style={{ margin: 4, cursor: 'pointer' }}
            onClick={() => handleRelatedSearchClick(term)}
          >
            {term}
          </Tag>
        ))}
      </div>
      
      {top_organic_sitelinks?.length > 0 && <>
        <Title level={5}>Top Organic Sitelinks</Title>
        <List dataSource={top_organic_sitelinks} renderItem={(item) => <List.Item><LinkOutlined /> {item}</List.Item>} style={{ marginBottom: 24 }} />
      </>}

      <Title level={5}>Top 10 Organic Results</Title>
      <Table columns={columns} dataSource={top_organic_results} rowKey="url" pagination={false} size="small" />
    </Card>
  );
};

export default SerpAnalysis;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/SerpVitals.jsx
```javascript
import React from 'react';
import { Card, Row, Col, Statistic, Tooltip } from 'antd';
import { CloudSyncOutlined, AlertOutlined } from '@ant-design/icons';

const SerpVitals = ({ scoreBreakdown }) => {
  const volatility = scoreBreakdown?.serp_volatility;
  const crowding = scoreBreakdown?.serp_crowding;

  if (!volatility && !crowding) {
    return null;
  }

  return (
    <Card title="SERP Vitals">
      <Row gutter={16}>
        {volatility && (
          <Col span={12}>
            <Tooltip title={volatility.breakdown['SERP Stability'].explanation}>
              <Statistic 
                title="SERP Stability" 
                value={volatility.breakdown['SERP Stability'].value} 
                prefix={<CloudSyncOutlined />} 
              />
            </Tooltip>
          </Col>
        )}
        {crowding && (
          <Col span={12}>
            <Tooltip title={crowding.breakdown['SERP Crowding'].explanation}>
              <Statistic 
                title="SERP Crowding" 
                value={`${crowding.breakdown['SERP Crowding'].value} Features`}
                prefix={<AlertOutlined />} 
              />
            </Tooltip>
          </Col>
        )}
      </Row>
    </Card>
  );
};

export default SerpVitals;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/SocialMediaTab.jsx
```javascript
import React from 'react';
import { Card, Typography, List, Button, Tooltip } from 'antd';
import { CopyOutlined } from '@ant-design/icons';
import { useNotifications } from '../../../context/NotificationContext';

const { Title, Paragraph } = Typography;

const SocialMediaTab = ({ socialMediaPosts }) => {
  const { showNotification } = useNotifications();

  if (!socialMediaPosts || socialMediaPosts.length === 0) {
    return <Card><Paragraph>No social media posts have been generated yet.</Paragraph></Card>;
  }

  const handleCopy = (text) => {
    navigator.clipboard.writeText(text);
    showNotification('success', 'Copied to Clipboard', 'The post content has been copied.');
  };

  return (
    <List
      grid={{ gutter: 16, column: 2 }}
      dataSource={socialMediaPosts}
      renderItem={(post) => (
        <List.Item>
          <Card title={post.platform}>
            <Paragraph>{post.content}</Paragraph>
            <Tooltip title="Copy Post">
              <Button
                icon={<CopyOutlined />}
                onClick={() => handleCopy(post.content)}
              />
            </Tooltip>
          </Card>
        </List.Item>
      )}
    />
  );
};

export default SocialMediaTab;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/StrategicNotes.jsx
```javascript
import React from 'react';
import { Alert } from 'antd';
import { BulbOutlined } from '@ant-design/icons';

const StrategicNotes = ({ notes }) => {
  if (!notes) {
    return null;
  }

  return (
    <Alert
      message="Strategic Note from AI Analysis"
      description={notes}
      type="warning"
      showIcon
      icon={<BulbOutlined />}
    />
  );
};

export default StrategicNotes;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/StrategicScoreBreakdown.jsx
```javascript
import React from 'react';
import { Card, Typography, Tooltip, Progress, List, Tag } from 'antd';
import { InfoCircleOutlined } from '@ant-design/icons';

const { Title, Text, Paragraph } = Typography;

const scoreCategoryMapping = {
  'Ranking & Competition': ['ease_of_ranking', 'competitor_weakness', 'competitor_performance'],
  'Traffic & Commercial Value': ['traffic_potential', 'commercial_intent', 'growth_trend', 'volume_volatility'],
  'SERP Environment': ['serp_features', 'serp_volatility', 'serp_crowding', 'serp_threat', 'serp_freshness'],
  'Keyword Profile': ['keyword_structure'],
};

const friendlyExplanations = {
  ease_of_ranking: "This score assesses the overall difficulty of ranking on the first page of Google for this keyword. It's a composite metric that considers the authority of competing websites, the keyword's inherent difficulty, and the number of competing pages. A higher score means we've identified a path of lower resistance.",
  competitor_weakness: "We analyze the top-ranking pages to find their weak spots. This score is higher if competitors have low domain authority, few backlinks, or other vulnerabilities that we can strategically exploit to outrank them.",
  competitor_performance: "This score evaluates the technical performance of competing websites, such as their loading speed and mobile-friendliness. Slower or poorly optimized competitor sites present a clear opportunity, as search engines penalize them, making it easier for us to rank higher with a technically superior page.",
  traffic_potential: "This score estimates the potential traffic this keyword could generate. It's not just about raw search volume; it also considers the keyword's cost-per-click (CPC) value, indicating its commercial worth. A high score suggests the keyword can attract a valuable audience.",
  commercial_intent: "We analyze the language of the keyword and the types of ads on the search results page to determine the user's intent. A high score indicates that the user is likely looking to make a purchase or engage a service, making the traffic more valuable.",
  growth_trend: "This score reflects the keyword's popularity over time. We analyze search data from the past year to identify upward trends. A high score means the keyword is becoming more popular, representing a growing area of interest and a sustainable source of future traffic.",
  volume_volatility: "This score measures the stability of the keyword's search volume. A low score indicates high volatility (e.g., a seasonal trend), while a high score suggests a stable, consistent search volume, making it a more reliable target for long-term content strategy.",
  serp_features: "This score identifies opportunities within the Search Engine Results Page (SERP) itself. We look for features like 'Featured Snippets,' 'People Also Ask' boxes, and video carousels. A high score means there are multiple ways to appear on the first page beyond the standard blue links.",
  serp_volatility: "This score measures how frequently the rankings for this keyword change. A highly volatile SERP, where rankings fluctuate often, can be an opportunity to quickly gain a foothold, as it indicates that search engines are still trying to determine the best results.",
  serp_crowding: "This score assesses how 'crowded' the search results page is with non-organic results like ads, image packs, and shopping results. A lower score means the page is very crowded, which can push organic results further down the page and reduce their visibility.",
  serp_threat: "This score identifies the presence of dominant, high-authority domains (like Wikipedia, government sites, or major news outlets) that are extremely difficult to outrank. A high score indicates the absence of such threats, making it a more level playing field.",
  serp_freshness: "This score evaluates the age of the content currently ranking on the first page. If the top results are several years old, it signals a 'freshness' opportunity, where new, up-to-date content is likely to be favored by search engines.",
  keyword_structure: "This score analyzes the composition of the keyword itself. Longer, more specific keywords (long-tail keywords) are often less competitive and signal a more specific user intent, making them easier to rank for. A high score is awarded to these types of keywords.",
};

const StrategicScoreBreakdown = ({ scoreBreakdown }) => {
  if (!scoreBreakdown) {
    return null;
  }

  const getScoreColor = (score) => {
    if (score > 70) return '#52c41a'; // green
    if (score > 40) return '#faad14'; // orange
    return '#f5222d'; // red
  };

  return (
    <Card title="Strategic Score Analysis" style={{ marginTop: 24 }}>
      <Paragraph type="secondary">
        This analysis breaks down the main factors contributing to the overall Strategic Score. Each factor is scored from 0-100, where a higher score indicates a better opportunity.
      </Paragraph>
      {Object.entries(scoreCategoryMapping).map(([category, keys]) => (
        <div key={category}>
          <Title level={4} style={{ marginTop: 24, marginBottom: 16 }}>{category}</Title>
          <List
            itemLayout="vertical"
            dataSource={keys.map(key => ({ key, ...scoreBreakdown[key] })).filter(item => item.name)}
            renderItem={(factor) => (
              <List.Item key={factor.key}>
                <List.Item.Meta
                  title={
                    <div style={{ display: 'flex', alignItems: 'center', justifyContent: 'space-between' }}>
                      <span>
                        {factor.name}
                        <Tooltip title={friendlyExplanations[factor.key]}>
                          <InfoCircleOutlined style={{ marginLeft: 8, color: '#888' }} />
                        </Tooltip>
                      </span>
                      <Tag color={getScoreColor(factor.score)} style={{ fontSize: '1rem', padding: '4px 8px' }}>
                        {factor.score.toFixed(1)}
                      </Tag>
                    </div>
                  }
                  description={
                    <div>
                      <Progress
                        percent={factor.score}
                        showInfo={false}
                        strokeColor={getScoreColor(factor.score)}
                        style={{ marginBottom: 8 }}
                      />
                      {factor.breakdown.message ? (
                        <Text type="secondary">{factor.breakdown.message}</Text>
                      ) : (
                        <ul style={{ paddingLeft: 20, margin: 0 }}>
                          {Object.entries(factor.breakdown).map(([key, value]) => (
                            <li key={key}>
                              <Text strong>{key}:</Text> {value.value} - <Text type="secondary">{value.explanation}</Text>
                            </li>
                          ))}
                        </ul>
                      )}
                    </div>
                  }
                />
              </List.Item>
            )}
          />
        </div>
      ))}
    </Card>
  );
};

export default StrategicScoreBreakdown;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/VerdictCard.jsx
```javascript
import React from 'react';
import { Card, Typography, Tag } from 'antd';

const { Title, Paragraph } = Typography;

const VerdictCard = ({ recommendation, confidenceScore }) => {
  if (!recommendation) {
    return null;
  }

  return (
    <Card style={{ marginTop: 24 }}>
      <Title level={5}>The Verdict</Title>
      <Tag color={recommendation === 'Proceed' ? 'success' : 'error'} style={{ fontSize: '1.2rem', padding: '10px' }}>
        {recommendation}
      </Tag>
      <Paragraph style={{ marginTop: '10px' }}>Confidence: {confidenceScore.toFixed(1)}%</Paragraph>
    </Card>
  );
};

export default VerdictCard;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/WorkflowStatusAlert.jsx
```javascript
import React from 'react';
import { Alert } from 'antd';
import { InfoCircleOutlined } from '@ant-design/icons';

const WorkflowStatusAlert = ({ status, message }) => {
  // Only show for specific, non-error statuses that have a message
  if (status !== 'paused_for_approval' || !message) {
    return null;
  }

  // Don't show if the message looks like an error
  if (message.toLowerCase().includes('error') || message.toLowerCase().includes('failed')) {
      return null;
  }

  return (
    <Alert
      message="Current Status"
      description={message}
      type="info"
      showIcon
      icon={<InfoCircleOutlined />}
    />
  );
};

export default WorkflowStatusAlert;
```

## File: my-content-app/src/pages/opportunity-detail-page/components/WorkflowTracker.jsx
```javascript
import React, { useEffect, useState } from 'react';
import { Card, Steps, Spin, Alert, Button } from 'antd';
import { useQuery, useQueryClient } from 'react-query';
import { useNavigate } from 'react-router-dom';
import { getJobStatus } from '../../../services/jobsService';
import { CheckCircleOutlined } from '@ant-design/icons';

const { Step } = Steps;

const IN_PROGRESS_STATUSES = ['processing', 'running', 'in_progress', 'pending', 'refresh_started'];

const WorkflowTracker = ({ opportunity }) => {
  const { latest_job_id, status, error_message } = opportunity;
  const navigate = useNavigate();
  const queryClient = useQueryClient();

  const { data: jobStatus, isLoading: isLoadingStatus } = useQuery(
    ['jobStatus', latest_job_id],
    () => getJobStatus(latest_job_id),
    {
      enabled: !!latest_job_id && (!jobStatus || (jobStatus.status !== 'completed' && jobStatus.status !== 'failed')),
      refetchInterval: 3000, // Poll every 3 seconds for faster updates
      onSuccess: (data) => {
        if (data?.status === 'completed' || data?.status === 'failed' || data?.status === 'paused') {
          // Invalidate queries to refetch the main opportunity data for the page
          queryClient.invalidateQueries(['opportunity', opportunity.id]);
        }
        
        if (data?.status === 'completed' && data.result?.redirect_url) {
          setTimeout(() => {
            navigate(data.result.redirect_url);
          }, 1500); // Delay for user to see the final success state
        }
      },
    }
  );

  const progressLog = jobStatus?.progress_log || [];
  const currentStepIndex = progressLog.length > 0 ? progressLog.length - 1 : 0;

  // Don't render anything if there's no job or the workflow is in a non-terminal, non-processing state
  if (!latest_job_id || (!IN_PROGRESS_STATUSES.includes(status) && status !== 'failed' && jobStatus?.status !== 'completed')) {
    return null;
  }

  const isJobRunning = jobStatus?.status === 'running' || jobStatus?.status === 'pending';

  return (
    <Card title="Workflow Status" style={{ marginTop: 24 }}>
      {isLoadingStatus && !jobStatus && <Spin tip="Initializing workflow status..." />}
      
      {progressLog.length > 0 && (
        <Steps direction="vertical" current={currentStepIndex}>
          {progressLog.map((log, index) => (
            <Step 
              key={index} 
              title={log.step} 
              description={log.message} 
              icon={isJobRunning && index === currentStepIndex ? <Spin /> : null}
            />
          ))}
        </Steps>
      )}

      {jobStatus?.status === 'completed' && (
        <Alert
          message="Workflow Completed"
          description={jobStatus.result?.message || 'The workflow finished successfully.'}
          type="success"
          showIcon
          icon={<CheckCircleOutlined />}
          action={
            jobStatus.result?.redirect_url && (
              <Button size="small" type="primary" onClick={() => navigate(jobStatus.result.redirect_url)}>
                Go to Results
              </Button>
            )
          }
        />
      )}

      {jobStatus?.status === 'failed' && (
        <Alert
          message="Workflow Failed"
          description={jobStatus.error || error_message || 'An unknown error occurred.'}
          type="error"
          showIcon
        />
      )}
    </Card>
  );
};

export default WorkflowTracker;
```

## File: my-content-app/src/pages/opportunity-detail-page/hooks/useOpportunityData.js
```javascript
import { useQuery } from 'react-query';
import { getOpportunityById } from '../../../services/opportunitiesService';

export const useOpportunityData = (opportunityId) => {
  const id = parseInt(opportunityId);
  const { data, isLoading, isError, error, refetch } = useQuery(
    ['opportunity', id],
    () => getOpportunityById(id),
    {
      enabled: !!id,
      staleTime: 5 * 60 * 1000, // 5 minutes
      cacheTime: 10 * 60 * 1000, // 10 minutes
    }
  );

  return { opportunity: data, isLoading, isError, error, refetch };
};
```

## File: my-content-app/src/pages/opportunity-detail-page/index.jsx
```javascript
import React, { useState, useEffect } from 'react';
import { useParams } from 'react-router-dom';
import { Layout, Spin, Alert, Row, Col, Tabs } from 'antd';
import { useOpportunityData } from './hooks/useOpportunityData';
import OpportunityHeader from './components/OpportunityHeader';
import ActionCenter from './components/ActionCenter';
import ExecutiveSummary from './components/ExecutiveSummary';
import KeywordMetrics from './components/KeywordMetrics';
import StrategicScoreBreakdown from './components/StrategicScoreBreakdown';
import SerpAnalysis from './components/SerpAnalysis';
import ContentBlueprint from './components/ContentBlueprint';
import ArticlePreview from './components/ArticlePreview';
import ContentAuditCard from './components/ContentAuditCard';
import SocialMediaTab from './components/SocialMediaTab';
import VerdictCard from './components/VerdictCard';
import FactorsCard from './components/FactorsCard';
import RecommendedStrategyCard from './components/RecommendedStrategyCard';
import StrategicNotes from './components/StrategicNotes';
import CompetitorBacklinks from './components/CompetitorBacklinks';
import IntentAnalysis from './components/IntentAnalysis';
import SerpVitals from './components/SerpVitals';
import GrowthTrend from './components/GrowthTrend';

import WorkflowTracker from './components/WorkflowTracker';

const { TabPane } = Tabs;

const OpportunityDetailPageV2 = () => {
  const { opportunityId } = useParams();
  const { opportunity, isLoading, isError, error, refetch } = useOpportunityData(opportunityId);
  const [blueprintOverrides, setBlueprintOverrides] = useState(null);

  useEffect(() => {
    if (opportunity?.blueprint?.content_intelligence?.article_structure) {
      setBlueprintOverrides(opportunity.blueprint.content_intelligence.article_structure);
    }
  }, [opportunity]);

  if (isLoading) {
    return (
      <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: '100vh' }}>
        <Spin tip="Loading opportunity..." size="large" />
      </div>
    );
  }

  if (isError) {
    return <Alert message="Error" description={error.message} type="error" showIcon />;
  }

  if (!opportunity) {
    return null;
  }

  const { blueprint, ai_content, social_media_posts_json, score_breakdown, full_data } = opportunity;

  return (
    <Layout style={{ padding: '24px', background: '#f0f2f5' }}>
      <OpportunityHeader
        keyword={opportunity.keyword}
        strategicScore={opportunity.strategic_score}
        status={opportunity.status}
        dateAdded={opportunity.date_added}
        recommendation={blueprint?.final_qualification_assessment?.recommendation}
      />
      <ActionCenter 
        status={opportunity.status} 
        opportunityId={opportunity.id} 
        overrides={blueprintOverrides} 
        refetch={refetch}
        style={{ marginTop: 24, marginBottom: 24 }}
      />
      <WorkflowTracker opportunity={opportunity} />
      <Tabs defaultActiveKey="1">
        <TabPane tab="Overview" key="1">
          <Row gutter={[24, 24]}>
            <Col xs={24} lg={8}>
              <VerdictCard 
                recommendation={blueprint?.final_qualification_assessment?.recommendation}
                confidenceScore={blueprint?.final_qualification_assessment?.confidence_score}
              />
              <RecommendedStrategyCard strategy={blueprint?.recommended_strategy} />
              <KeywordMetrics 
                keywordInfo={opportunity.keyword_info} 
                keywordProperties={opportunity.keyword_properties}
              />
              <IntentAnalysis searchIntentInfo={opportunity.search_intent_info} />
              <GrowthTrend scoreBreakdown={score_breakdown} />
            </Col>
            <Col xs={24} lg={16}>
              <StrategicNotes notes={blueprint?.analysis_notes} />
              <FactorsCard 
                positiveFactors={blueprint?.final_qualification_assessment?.positive_factors}
                negativeFactors={blueprint?.final_qualification_assessment?.negative_factors}
              />
              <ExecutiveSummary summary={blueprint?.executive_summary} />
              <StrategicScoreBreakdown scoreBreakdown={score_breakdown} />
              <CompetitorBacklinks avgBacklinksInfo={full_data?.avg_backlinks_info} />
              <SerpVitals scoreBreakdown={score_breakdown} />
            </Col>
          </Row>
        </TabPane>
        <TabPane tab="SERP Analysis" key="2">
          <SerpAnalysis blueprint={blueprint} />
        </TabPane>
        <TabPane tab="Content Blueprint" key="3">
          <ContentBlueprint
            blueprint={blueprint}
            overrides={blueprintOverrides}
            setOverrides={setBlueprintOverrides}
          />
        </TabPane>
        <TabPane tab="Publishing" key="4">
          <Tabs defaultActiveKey="article">
            <TabPane tab="Article" key="article">
              <ArticlePreview
                aiContent={ai_content}
                featuredImagePath={opportunity.featured_image_local_path}
              />
            </TabPane>
            <TabPane tab="Social Media" key="social">
              <SocialMediaTab socialMediaPosts={social_media_posts_json} />
            </TabPane>
          </Tabs>
        </TabPane>
        <TabPane tab="Audit" key="5">
          <ContentAuditCard auditResults={ai_content?.audit_results} />
        </TabPane>
      </Tabs>
    </Layout>
  );
};

export default OpportunityDetailPageV2;
```

## File: my-content-app/src/pages/RunDetailsPage/RunDetailsPage.jsx
```javascript
import React from 'react';
import { useParams, Link } from 'react-router-dom';
import { useQuery } from 'react-query';
import { Layout, Spin, Alert, Typography, Progress, Card, Descriptions, Tag, Button } from 'antd';
import { ArrowLeftOutlined } from '@ant-design/icons';
import { getDiscoveryRunById, getKeywordsForRun } from '../../services/discoveryService';
import OpportunityTable from '../../components/OpportunityTable';

const { Content } = Layout;
const { Title } = Typography;

const RunDetailsPage = () => {
  const { runId } = useParams();

  const { data: run, isLoading: isLoadingRun, isError, error } = useQuery(
    ['discoveryRun', runId],
    () => getDiscoveryRunById(runId),
    {
      refetchInterval: (data) => data?.status === 'running' ? 5000 : false,
      refetchOnWindowFocus: false,
    }
  );

  const { data: opportunities, isLoading: isLoadingOpportunities } = useQuery(
    ['discoveryRunOpportunities', runId],
    () => getKeywordsForRun(runId),
    {
      enabled: !!run && run.status === 'completed',
    }
  );

  if (isLoadingRun) {
    return <Spin tip="Loading run details..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (isError) {
    return <Alert message="Error" description={error.message} type="error" showIcon />;
  }

  const getStatusTag = (status) => {
    switch (status) {
      case 'completed': return <Tag color="success">Completed</Tag>;
      case 'failed': return <Tag color="error">Failed</Tag>;
      case 'running': return <Tag color="processing">In Progress</Tag>;
      default: return <Tag>{status}</Tag>;
    }
  };

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Button type="link" icon={<ArrowLeftOutlined />} style={{ marginBottom: '16px', paddingLeft: 0 }}>
          <Link to="/discovery">Back to Discovery Hub</Link>
        </Button>
        <Title level={2}>Discovery Run Details</Title>
        <Card style={{ marginBottom: '24px' }}>
          <Descriptions title="Run Summary" bordered>
            <Descriptions.Item label="Run ID">{run.id}</Descriptions.Item>
            <Descriptions.Item label="Status">{getStatusTag(run.status)}</Descriptions.Item>
            <Descriptions.Item label="Seed Keyword">{run.parameters?.seed_keywords?.join(', ')}</Descriptions.Item>
            <Descriptions.Item label="Created At">{new Date(run.start_time).toLocaleString()}</Descriptions.Item>
          </Descriptions>
        </Card>

        {run.status === 'completed' && (
          <Card title="Discovered Opportunities">
            <OpportunityTable opportunities={opportunities} isLoading={isLoadingOpportunities} />
          </Card>
        )}

        {run.status === 'failed' && (
          <Alert message="Run Failed" description={run.error_message || 'An unknown error occurred.'} type="error" showIcon />
        )}
      </Content>
    </Layout>
  );
};

export default RunDetailsPage;
```

## File: my-content-app/src/pages/Settings/tabs/AiContentSettingsTab.jsx
```javascript
// This is a new file. Create it with the following content:
import React from 'react';
import { Form, Input, InputNumber, Select, Switch, Slider, Typography, Row, Col, Divider, Tooltip, Alert, Space } from 'antd';
import { InfoCircleOutlined } from '@ant-design/icons';
import PromptTemplateEditor from '../../../components/PromptTemplateEditor'; // NEW

const { Title, Text } = Typography;
const { Option } = Select;

const AiContentSettingsTab = ({ settings, form }) => {
  const contentModel = Form.useWatch('ai_content_model', form); // Watch for changes in the AI content model

  return (
    <>
      <Title level={5}>AI Model & Generation</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="ai_content_model" label="AI Content Generation Model">
            <Select style={{ width: '100%' }}>
              <Option value="gpt-4o">GPT-4o (Recommended)</Option>
              <Option value="gpt-4-turbo">GPT-4 Turbo</Option>
              <Option value="gpt-3.5-turbo">GPT-3.5 Turbo (Cost-Effective)</Option>
            </Select>
          </Form.Item>
          {contentModel === 'gpt-3.5-turbo' && (
            <Alert
              message="Cost-Effective Model Selected"
              description="GPT-3.5 Turbo is cheaper but may require more prompt engineering for quality."
              type="info"
              showIcon
              style={{ marginBottom: '16px' }}
            />
          )}
        </Col>
        <Col span={12}>
          <Form.Item name="ai_generation_temperature" label="AI Creativity (Temperature)">
            <Slider min={0.0} max={1.0} step={0.01} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="expert_persona" label="AI Writer Persona">
            <Input placeholder="e.g., a certified financial planner with 15 years of experience" />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="recommended_word_count_multiplier" label="Word Count Multiplier">
            <InputNumber min={0.5} max={3.0} step={0.1} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_completion_tokens_for_generation" label="Max Output Tokens">
            <InputNumber min={1000} max={32000} step={1000} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_words_for_ai_analysis" label="Max Words for AI Analysis">
            <InputNumber min={500} max={5000} step={100} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
      </Row>

      <Divider />

      <Title level={5}>Image Generation</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="use_pexels_first" label="Use Pexels for Images" valuePropName="checked">
            <Switch checkedChildren="Enabled" unCheckedChildren="Disabled" />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="num_in_article_images" label="Number of In-Article Images">
            <InputNumber min={0} max={5} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="overlay_text_enabled" label="Add Text Overlay to Images" valuePropName="checked">
            <Switch checkedChildren="Enabled" unCheckedChildren="Disabled" />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="overlay_text_color" label="Overlay Text Color">
            <Input type="color" style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="overlay_background_color" label="Overlay Background Color">
            <Input type="color" style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="overlay_font_size" label="Overlay Font Size">
            <InputNumber min={10} max={60} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="overlay_position" label="Overlay Position">
            <Select style={{ width: '100%' }}>
              <Option value="top_left">Top Left</Option>
              <Option value="top_center">Top Center</Option>
              <Option value="top_right">Top Right</Option>
              <Option value="bottom_left">Bottom Left</Option>
              <Option value="bottom_center">Bottom Center</Option>
              <Option value="bottom_right">Bottom Right</Option>
              <Option value="center">Center</Option>
            </Select>
          </Form.Item>
        </Col>
      </Row>

      <Divider />

      <Title level={5}>Custom AI Prompt Template</Title>
      <Form.Item 
        name="custom_prompt_template" 
        label={
          <Space>
            Edit your base prompt for the AI content generator.
            <Tooltip title="This template guides the AI's writing. Use available placeholders for dynamic data.">
              <InfoCircleOutlined />
            </Tooltip>
          </Space>
        }
        style={{ marginBottom: 0 }}
      >
        <PromptTemplateEditor disabled={false} />
      </Form.Item>
    </>
  );
};

export default AiContentSettingsTab;
```

## File: my-content-app/src/pages/Settings/tabs/DiscoverySettingsTab.jsx
```javascript
// This is a new file. Create it with the following content:
import React from 'react';
import { Form, Input, InputNumber, Select, Switch, Checkbox, Slider, Typography, Row, Col, Divider, Tooltip, Space } from 'antd';
import { InfoCircleOutlined } from '@ant-design/icons';

const { Title, Text } = Typography;
const { Option } = Select;

const DiscoverySettingsTab = ({ settings, form }) => {
  return (
    <>
      <Title level={5}>General Discovery Parameters</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="min_search_volume" label="Minimum Search Volume">
            <InputNumber min={0} max={100000} step={10} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_keyword_difficulty" label="Maximum Keyword Difficulty">
            <InputNumber min={0} max={100} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="min_keyword_word_count" label="Minimum Keyword Word Count">
            <InputNumber min={1} max={20} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_keyword_word_count" label="Maximum Keyword Word Count">
            <InputNumber min={1} max={20} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_max_pages" label="Max API Pages to Fetch">
            <InputNumber min={1} max={100} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_related_depth" label="Related Keywords Depth">
            <Slider min={0} max={4} />
          </Form.Item>
        </Col>
        <Col span={24}>
          <Form.Item name="discovery_strategies" label="Discovery Strategies">
            <Checkbox.Group
              options={[
                { label: 'Keyword Ideas (Category-based)', value: 'Keyword Ideas' },
                { label: 'Keyword Suggestions (Phrase-based)', value: 'Keyword Suggestions' },
                { label: 'Related Keywords (SERP-based)', value: 'Related Keywords' },
              ]}
            />
          </Form.Item>
        </Col>
      </Row>

      <Divider />

      <Title level={5}>Advanced Filtering & Ordering</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="closely_variants" label="Search Mode (Keyword Ideas)" valuePropName="checked">
            <Switch checkedChildren="Phrase Match" unCheckedChildren="Broad Match" />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_ignore_synonyms" label="Ignore Synonyms" valuePropName="checked">
            <Switch />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_replace_with_core_keyword" label="Replace with Core Keyword" valuePropName="checked">
            <Switch />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="min_cpc_filter" label="Minimum CPC ($)">
            <InputNumber min={0.0} step={0.1} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_cpc_filter" label="Maximum CPC ($)">
            <InputNumber min={0.0} step={0.1} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="min_competition" label="Minimum Competition (0-1)">
            <InputNumber min={0.0} max={1.0} step={0.01} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_competition" label="Maximum Competition (0-1)">
            <InputNumber min={0.0} max={1.0} step={0.01} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_competition_level" label="Max Competition Level">
            <Select style={{ width: '100%' }}>
              <Option value="LOW">LOW</Option>
              <Option value="MEDIUM">MEDIUM</Option>
              <Option value="HIGH">HIGH</Option>
            </Select>
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_order_by_field" label="Order Results By">
            <Select style={{ width: '100%' }}>
              <Option value="keyword_info.search_volume">Search Volume</Option>
              <Option value="keyword_properties.keyword_difficulty">Keyword Difficulty</Option>
              <Option value="keyword_info.cpc">CPC</Option>
              <Option value="keyword_info.competition">Competition</Option>
            </Select>
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="discovery_order_by_direction" label="Order Direction">
            <Select style={{ width: '100%' }}>
              <Option value="desc">Descending</Option>
              <Option value="asc">Ascending</Option>
            </Select>
          </Form.Item>
        </Col>
        <Col span={24}>
          <Form.Item 
            name="search_phrase_regex" 
            label={
              <Space>
                Keyword Regex Filter
                <Tooltip title="Use regular expressions to filter keywords (e.g., ^how to.*$)">
                  <InfoCircleOutlined />
                </Tooltip>
              </Space>
            }
          >
            <Input placeholder="e.g., ^best.*reviews$" />
          </Form.Item>
        </Col>
      </Row>

      <Divider />

      <Title level={5}>Intent & Qualification</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="enforce_intent_filter" label="Enforce Intent Filter" valuePropName="checked">
            <Switch />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="allowed_intents" label="Allowed Intents">
            <Select mode="multiple" style={{ width: '100%' }}>
              <Option value="informational">Informational</Option>
              <Option value="commercial">Commercial</Option>
              <Option value="transactional">Transactional</Option>
              <Option value="navigational">Navigational</Option>
            </Select>
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="prohibited_intents" label="Prohibited Secondary Intents">
            <Select mode="multiple" style={{ width: '100%' }}>
              <Option value="informational">Informational</Option>
              <Option value="commercial">Commercial</Option>
              <Option value="transactional">Transactional</Option>
              <Option value="navigational">Navigational</Option>
            </Select>
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="require_question_keywords" label="Require Question Keywords" valuePropName="checked">
            <Switch />
          </Form.Item>
        </Col>
        <Col span={24}>
          <Form.Item name="negative_keywords" label="Negative Keywords" extra="Comma-separated list of keywords to exclude (e.g., free, login)">
            <Input.TextArea rows={2} />
          </Form.Item>
        </Col>
      </Row>

      <Divider /><br/>
      <Title level={5}>SERP & Competitor Analysis Cost Controls</Title><br/>
      <Row gutter={16}><br/>
        <Col span={12}><br/>
          <Form.Item name="load_async_ai_overview" label="Load Async AI Overview" valuePropName="checked" tooltip="Set to true to obtain AI Overview items in SERPs even if they are loaded asynchronously. Costs $0.002 extra per SERP call."><br/>
            <Switch checkedChildren="Enabled" unCheckedChildren="Disabled" /><br/>
          </Form.Item><br/>
        </Col><br/>
        <Col span={12}><br/>
          <Form.Item name="people_also_ask_click_depth" label="PAA Click Depth"><br/>
            <InputNumber min={0} max={4} style={{ width: '100%' }} tooltip="Specify the depth of clicks (1 to 4) on People Also Ask elements. Costs $0.00015 extra per click per level." /><br/>
          </Form.Item><br/>
        </Col><br/>
        <Col span={12}><br/>
          <Form.Item name="onpage_enable_browser_rendering" label="Enable Browser Rendering (High Cost)" valuePropName="checked" tooltip="If true, emulates a full browser load for Core Web Vitals (LCP/CLS) and loads JavaScript/Resources automatically. Primary cost factor for Analysis."><br/>
            <Switch checkedChildren="Enabled" unCheckedChildren="Disabled" /><br/>
          </Form.Item><br/>
        </Col><br/>
        <Col span={12}><br/>
          <Form.Item name="onpage_enable_custom_js" label="Enable Custom JavaScript" valuePropName="checked" tooltip="Allows execution of custom JavaScript during OnPage crawl. Costs $0.00025 extra per page analyzed."><br/>
            <Switch checkedChildren="Enabled" unCheckedChildren="Disabled" /><br/>
          </Form.Item><br/>
        </Col><br/>
      </Row><br/><br/>
    </>
  );
};

export default DiscoverySettingsTab;
```

## File: my-content-app/src/pages/Settings/tabs/ScoringWeightsTab.jsx
```javascript
// This is a new file. Create it with the following content:
import React from 'react';
import { Form, Slider, InputNumber, Typography, Row, Col, Divider, Tooltip, Space, Alert } from 'antd';
import { InfoCircleOutlined } from '@ant-design/icons';

const { Title, Text } = Typography;

const ScoringWeightsTab = ({ settings, form }) => {
  const allWeights = Form.useWatch([
    'ease_of_ranking_weight',
    'traffic_potential_weight',
    'commercial_intent_weight',
    'serp_features_weight',
    'growth_trend_weight',
    'serp_freshness_weight',
    'serp_volatility_weight',
    'competitor_weakness_weight'
  ], form);

  const totalWeight = Object.values(allWeights || {}).reduce((sum, current) => sum + (current || 0), 0);

  const renderWeightInput = (name, label, tooltip) => (
    <Col span={24}>
      <Form.Item 
        name={name} 
        label={
          <Space>
            {label}
            {tooltip && <Tooltip title={tooltip}><InfoCircleOutlined /></Tooltip>}
          </Space>
        }
        rules={[{ required: true, message: 'Weight is required' }]}
        style={{ marginBottom: 0 }}
      >
        <Row>
          <Col span={18}>
            <Slider min={0} max={100} step={1} style={{ margin: '0 8px' }} />
          </Col>
          <Col span={4}>
            <InputNumber min={0} max={100} step={1} style={{ width: '100%' }} />
          </Col>
        </Row>
      </Form.Item>
    </Col>
  );

  return (
    <>
      <Title level={5}>Strategic Scoring Weights (Sum to {totalWeight}%)</Title>
      {totalWeight !== 100 && (
        <Alert
          message="Warning: Total weight is not 100%"
          description="The sum of all weights should ideally be 100% for proper normalization. Consider adjusting your weights."
          type="warning"
          showIcon
          style={{ marginBottom: '16px' }}
        />
      )}
      <Row gutter={16}>
        {renderWeightInput('ease_of_ranking_weight', 'Ease of Ranking', 'How easy it is to rank (based on KD, backlinks).')}
        {renderWeightInput('traffic_potential_weight', 'Traffic Potential', 'How much traffic the keyword can bring (based on Search Volume).')}
        {renderWeightInput('commercial_intent_weight', 'Commercial Intent', 'How likely the keyword is to lead to a conversion (based on CPC, intent type).')}
        {renderWeightInput('serp_features_weight', 'SERP Features', 'Impact of rich SERP features (Featured Snippets, AI Overviews).')}
        {renderWeightInput('growth_trend_weight', 'Growth Trend', 'How quickly the search volume is growing or declining.')}
        {renderWeightInput('serp_freshness_weight', 'SERP Freshness', 'How recently the SERP was updated (opportunity if stale).')}
        {renderWeightInput('serp_volatility_weight', 'SERP Volatility', 'How often the SERP changes (opportunity if stable).')}
        {renderWeightInput('competitor_weakness_weight', 'Competitor Weakness', 'Exploitable technical or content flaws of top competitors.')}
      </Row>

      <Divider />

      <Title level={5}>Scoring Normalization Values</Title>
      <Row gutter={16}>
        <Col span={12}>
          <Form.Item name="max_cpc_for_scoring" label="Max CPC for Scoring">
            <InputNumber min={0.0} step={1.0} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_sv_for_scoring" label="Max Search Volume for Scoring">
            <InputNumber min={0} step={1000} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_domain_rank_for_scoring" label="Max Domain Rank for Scoring">
            <InputNumber min={0} step={100} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
        <Col span={12}>
          <Form.Item name="max_referring_domains_for_scoring" label="Max Referring Domains for Scoring">
            <InputNumber min={0} step={50} style={{ width: '100%' }} />
          </Form.Item>
        </Col>
      </Row>
    </>
  );
};

export default ScoringWeightsTab;
```

## File: my-content-app/src/pages/Settings/SettingsPage.jsx
```javascript
// This is a new file. Create it with the following content:
import React, { useState, useEffect } from 'react';
import { Layout, Typography, Tabs, Spin, Alert, Button, Space, Form } from 'antd';
import { useQuery, useMutation, useQueryClient } from 'react-query';
import { getClientSettings, updateClientSettings } from '../../services/clientSettingsService'; // Corrected
import { useClient } from '../../hooks/useClient'; // NEW
import { useNotifications } from '../../context/NotificationContext'; // NEW
import { SaveOutlined, ReloadOutlined } from '@ant-design/icons';
import DiscoverySettingsTab from './tabs/DiscoverySettingsTab'; // NEW
import ScoringWeightsTab from './tabs/ScoringWeightsTab'; // NEW
import AiContentSettingsTab from './tabs/AiContentSettingsTab'; // NEW

const { Content } = Layout;
const { Title, Text } = Typography;

// Placeholder tab components
const ApiKeysSettingsTab = ({ settings, form }) => <Alert message="API Keys" description="API key management will go here." type="info" />; // NEW for task 3.3.1 (API Key Management)

const SettingsPage = () => {
  console.log('Rendering SettingsPage');
  const { clientId } = useClient();
  const { showNotification } = useNotifications();
  const queryClient = useQueryClient();
  const [form] = Form.useForm();
  const [isDirty, setIsDirty] = useState(false); // To track if form has unsaved changes

  const { data: currentSettings, isLoading, isError, error, refetch } = useQuery(
    ['clientSettings', clientId],
    () => getClientSettings(clientId),
    {
      enabled: !!clientId,
      onSuccess: (data) => {
        form.setFieldsValue(data); // Populate form with fetched settings
        setIsDirty(false); // Reset dirty state on successful fetch/load
      },
      onError: (err) => {
        showNotification('error', 'Failed to Load Settings', err.message || 'An error occurred while loading settings.');
      },
      staleTime: 5 * 60 * 1000, // Consider settings stale after 5 minutes
    }
  );

  const { mutate: saveSettingsMutation, isLoading: isSavingSettings } = useMutation(
    (updatedSettings) => updateClientSettings(clientId, updatedSettings),
    {
      onSuccess: () => {
        showNotification('success', 'Settings Saved', 'Client settings updated successfully.');
        setIsDirty(false); // Mark as clean after saving
        queryClient.invalidateQueries(['clientSettings', clientId]); // Invalidate to ensure fresh data if re-fetched elsewhere
      },
      onError: (err) => {
        showNotification('error', 'Save Failed', err.message || 'An error occurred while saving settings.');
      },
    }
  );

  const handleFormChange = () => {
    setIsDirty(true); // Mark form as dirty on any change
  };

  const handleSave = () => {
    form.validateFields()
      .then(values => {
        saveSettingsMutation(values);
      })
      .catch(info => {
        showNotification('error', 'Validation Error', 'Please correct the highlighted fields.');
        console.log('Validate Failed:', info);
      });
  };

  const handleResetToCurrent = () => {
    if (currentSettings) {
      form.setFieldsValue(currentSettings);
      setIsDirty(false);
      showNotification('info', 'Reset', 'Form reset to current saved settings.');
    }
  };

  if (isLoading) {
    return <Spin tip="Loading settings..." style={{ display: 'block', marginTop: '50px' }} />;
  }

  if (isError) {
    return <Alert message="Error" description={error?.message || "Failed to load settings."} type="error" showIcon />;
  }

  const tabItems = [
    {
      label: 'Discovery & Filtering',
      key: 'discovery',
      children: <DiscoverySettingsTab settings={currentSettings} form={form} />,
    },
    {
      label: 'Scoring & Weights',
      key: 'scoring',
      children: <ScoringWeightsTab settings={currentSettings} form={form} />,
    },
    {
      label: 'AI & Content Generation',
      key: 'ai-content',
      children: <AiContentSettingsTab settings={currentSettings} form={form} />,
    },
    {
      label: 'API Keys', // NEW tab
      key: 'api-keys',
      children: <ApiKeysSettingsTab settings={currentSettings} form={form} />,
    }
  ];

  return (
    <Layout style={{ padding: '24px' }}>
      <Content>
        <Title level={2}>Client Settings: {clientId}</Title>
        <Form
          form={form}
          layout="vertical"
          onValuesChange={handleFormChange}
          onFinish={handleSave}
          initialValues={currentSettings} // Set initial values from fetched data
        >
          <Tabs defaultActiveKey="discovery" items={tabItems} style={{ marginBottom: '24px' }} />

          <Space>
            <Button 
              type="primary" 
              htmlType="submit" 
              icon={<SaveOutlined />} 
              loading={isSavingSettings} 
              disabled={!isDirty || isSavingSettings}
            >
              Save Changes
            </Button>
            <Button 
              icon={<ReloadOutlined />} 
              onClick={handleResetToCurrent} 
              disabled={!isDirty || isSavingSettings}
            >
              Reset to Current
            </Button>
          </Space>
        </Form>
      </Content>
    </Layout>
  );
};

export default SettingsPage;
```

## File: my-content-app/src/services/apiClient.js
```javascript
import axios from 'axios';
import { ClientContext } from '../context/ClientContext'; // Import ClientContext

// Create an Axios instance for API communication
const apiClient = axios.create({
  // The base URL is handled by the Vite proxy, so we can use relative paths like /api
  headers: {
    'Content-Type': 'application/json', // Default content type for requests
  },
  // You could add a timeout here if desired
  // timeout: 10000, 
});

// Below the `axios.create` block, add the request interceptor:
apiClient.interceptors.request.use(
  (config) => {
    // Dynamically get the current client ID from localStorage
    const currentClientId = localStorage.getItem('currentClientId');
    if (currentClientId) {
      config.headers['X-Client-ID'] = currentClientId;
    }
    const token = localStorage.getItem('authToken');
    if (token) {
        config.headers.Authorization = `Bearer ${token}`;
    }
    return config;
  },
  (error) => {
    return Promise.reject(error);
  }
);

// Interceptor to handle successful responses
apiClient.interceptors.response.use(
  (response) => {
    // Axios wraps the actual data in a `data` property. We return just the data.
    return response.data;
  },
  (error) => {
    // Centralized error handling for all API calls
    console.error('API Error:', error.response || error.message);

    // Optionally, you could check for specific status codes (e.g., 401 for unauthorized)
    // and trigger global actions like showing a notification or redirecting to login.
    // if (error.response && error.response.status === 401) {
    //   // handle unauthorized
    // }

    // If the error is an AbortError from AbortController, do not treat it as a critical failure
    if (axios.isCancel(error) || error.code === 'ERR_CANCELED') {
        // Propagate as a cancelled error for specific handling in components
        const cancelledError = new Error('Request was cancelled');
        cancelledError.name = 'CanceledError';
        return Promise.reject(cancelledError);
    }

    return Promise.reject(error); // Re-throw the error for component-specific handling
  }
);

export default apiClient;
```

## File: my-content-app/src/services/authService.js
```javascript
// This is a new file. Create it with the following content:
import apiClient from './apiClient';

/**
 * Sends login request to the backend.
 * @param {string} password - The user's password.
 * @returns {Promise<object>} A promise that resolves to the login response (user and token).
 */
export const login = (password) => {
  return apiClient.post('/api/auth/login', { password });
};

/**
 * Sends logout request to the backend.
 * @returns {Promise<object>} A promise that resolves to the logout message.
 */
export const logout = () => {
  return apiClient.post('/api/auth/logout');
};
```

## File: my-content-app/src/services/clientService.js
```javascript
// This is a new file. Create it with the following content:
import apiClient from './apiClient';

/**
 * Fetches a list of all clients.
 * @returns {Promise<Array>} A promise that resolves to an array of client objects.
 */
export const getClients = () => {
  return apiClient.get('/api/clients');
};

/**
 * Fetches dashboard statistics for a specific client.
 * @param {string} clientId - The ID of the client.
 * @returns {Promise<Object>} A promise that resolves to the dashboard stats object.
 */
export const getDashboardStats = (clientId) => {
  return apiClient.get(`/api/clients/${clientId}/dashboard-stats`);
};

/**
 * Adds a new client to the system.
 * @param {Object} clientData - The data for the new client (client_id, client_name).
 * @returns {Promise<Object>} A promise that resolves to the new client's data.
 */
export const addClient = (clientData) => {
  return apiClient.post('/api/clients', clientData);
};


/**
 * Searches across all client-specific assets (opportunities, runs, etc.).
 * @param {string} clientId - The ID of the client.
 * @param {string} query - The search query.
 * @returns {Promise<Array>} A promise that resolves to an array of search results.
 */
export const searchAllAssets = (clientId, query) => {
  return apiClient.get(`/api/clients/${clientId}/search-all-assets?query=${encodeURIComponent(query)}`);
};

/**
 * Fetches aggregated data for the main dashboard.
 * @param {string} clientId - The ID of the client.
 * @returns {Promise<Object>} A promise that resolves to the dashboard data object.
 */
export const getDashboardData = (clientId) => {
  return apiClient.get(`/api/clients/${clientId}/dashboard`);
};
```

## File: my-content-app/src/services/clientSettingsService.js
```javascript
// my-content-app/src/services/clientSettingsService.js
// NEW FILE
import apiClient from './apiClient';

export const getClientSettings = (clientId) => {
  return apiClient.get(`/api/settings/${clientId}`);
};

export const updateClientSettings = (clientId, settings) => {
  return apiClient.put(`/api/settings/${clientId}`, settings);
};
```

## File: my-content-app/src/services/discoveryService.js
```javascript
import apiClient from './apiClient';

export const startDiscoveryRun = ({ clientId, runData }) => {
  return apiClient.post(`/api/clients/${clientId}/discovery-runs-async`, runData);
};

export const getDiscoveryRuns = (clientId, page = 1) => {
  return apiClient.get(`/api/clients/${clientId}/discovery-runs`, { params: { page } });
};

export const estimateCost = ({ clientId, seed_keywords, signal }) => {
  return apiClient.post(`/api/discovery/estimate-cost`, { seed_keywords }, { signal });
};

export const preCheckKeywords = ({ clientId, seed_keywords, signal }) => {
  return apiClient.post(`/api/discovery/pre-check`, { seed_keywords }, { signal });
};

export const rerunDiscoveryRun = (runId) => {
  return apiClient.post(`/api/discovery-runs/rerun/${runId}`);
};

export const getKeywordsForRun = (runId) => {
  return apiClient.get(`/api/discovery-runs/${runId}/keywords`);
};

export const getDisqualifiedKeywords = (runId, reason) => {
    return apiClient.get(`/api/discovery-runs/${runId}/keywords/${reason}`);
};

export const getDisqualificationReasons = (runId) => {
  return apiClient.get(`/api/discovery-runs/${runId}/disqualification-reasons`);
};

export const getJobStatus = (jobId) => {
  return apiClient.get(`/api/jobs/${jobId}`);
};

export const getOpportunities = (clientId, { page = 1, limit = 50, status = 'qualified', sort_by = 'strategic_score', sort_direction = 'desc' }) => {
  return apiClient.get(`/api/clients/${clientId}/opportunities`, { 
    params: { page, limit, status, sort_by, sort_direction } 
  });
};

export const getDiscoveryRunById = async (runId) => {
  const response = await apiClient.get(`/api/discovery-runs/${runId}`);
  return response;
};
```

## File: my-content-app/src/services/jobsService.js
```javascript
import apiClient from './apiClient';

export const getJobStatus = (jobId) => {
  return apiClient.get(`/api/jobs/${jobId}`);
};
```

## File: my-content-app/src/services/opportunitiesService.js
```javascript
import apiClient from './apiClient';

export const getOpportunities = (clientId, params) => {
  return apiClient.get(`/api/clients/${clientId}/opportunities`, { params });
};

export const getDashboardStats = (clientId) => {
  return apiClient.get(`/api/clients/${clientId}/dashboard`);
};

export const getOpportunityById = (id) => {
  return apiClient.get(`/api/opportunities/${id}`);
};

export const updateOpportunityStatus = (opportunityId, status) => {
  return apiClient.put(`/api/opportunities/${opportunityId}/status?status=${status}`);
};

export const bulkAction = (action, opportunityIds) => {
  return apiClient.post('/api/opportunities/bulk-action', { action, opportunity_ids: opportunityIds });
};

export const compareOpportunities = (opportunityIds) => {
  return apiClient.post('/api/opportunities/compare', { opportunity_ids: opportunityIds });
};

export const updateOpportunityAiContent = (id, updatedContent) => {
  return apiClient.put(`/api/opportunities/${id}/ai-content`, updatedContent);
};

export const generateImage = (id, prompt) => {
  return apiClient.post(`/api/opportunities/${id}/generate-image`, { prompt });
};

export const updateOpportunityImages = (id, images) => {
  return apiClient.put(`/api/opportunities/${id}/images`, images);
};

export const generateSocialPosts = (id, platforms) => {
  return apiClient.post(`/api/opportunities/${id}/generate-social-posts`, { platforms });
};

export const updateOpportunitySocialPosts = (id, posts) => {
  return apiClient.put(`/api/opportunities/${id}/social-posts`, { social_media_posts: posts });
};

export const getContentHistory = (opportunityId) => {
  return apiClient.get(`/api/opportunities/${opportunityId}/content-history`);
};

export const restoreContentVersion = (opportunityId, versionTimestamp) => {
  return apiClient.post(`/api/opportunities/${opportunityId}/restore-content`, { version_timestamp: versionTimestamp });
};

export const submitContentFeedback = (opportunityId, feedbackData) => {
  return apiClient.post(`/api/opportunities/${opportunityId}/feedback`, feedbackData);
};

export const overrideDisqualification = (opportunityId) => {
  return apiClient.post(`/api/opportunities/${opportunityId}/override-disqualification`);
};

export const updateOpportunityContent = (opportunityId, newContentPayload) => {
  return apiClient.put(`/api/opportunities/${opportunityId}/content`, newContentPayload);
};

export const approveAnalysis = (opportunityId, overrides) => {
  return apiClient.post(`/api/orchestrator/approve-analysis/${opportunityId}`, overrides);
};
```

## File: my-content-app/src/services/orchestratorService.js
```javascript
import apiClient from './apiClient';

export const runAnalysis = (opportunityId, selectedUrls) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/run-analysis-async`, { selected_competitor_urls: selectedUrls });
};

export const startFullContentGeneration = (opportunityId, modelOverride = null, temperature = null) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/run-generation-async`, { model_override: modelOverride, temperature: temperature });
};

export const approveAnalysis = (opportunityId, overrides = null) => {
  const payload = {
    overrides: {
      additional_instructions: JSON.stringify(overrides),
    },
  };
  return apiClient.post(`/api/orchestrator/approve-analysis/${opportunityId}`, payload);
};

export const refreshContentWorkflow = (opportunityId) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/refresh-content-async`);
};

export const startFullAnalysis = (opportunityId, modelOverride) => {
    return apiClient.post(`/api/orchestrator/run-full-analysis/${opportunityId}`, { model_override: modelOverride });
};

export const getJobStatus = (jobId) => {
    return apiClient.get(`/api/jobs/${jobId}/status`);
};

export const estimateActionCost = (actionType, opportunityId = null, discoveryParams = null) => {
  const url = opportunityId 
    ? `/api/orchestrator/estimate-cost/${opportunityId}` 
    : '/api/orchestrator/estimate-cost';
  
  const payload = {
    action_type: actionType,
    discovery_params: discoveryParams,
  };
  
  return apiClient.post(url, payload);
};

export const getSerpDataLive = (opportunityId) => {
    return apiClient.get(`/api/orchestrator/${opportunityId}/serp-preview`);
};

export const refreshSerpData = (opportunityId) => {
            return apiClient.post(`/api/orchestrator/${opportunityId}/rerun-analysis-async`);
        };
export const getAllJobs = () => {
    return apiClient.get('/api/jobs');
};

export const cancelJob = (jobId) => {
    return apiClient.post(`/api/jobs/${jobId}/cancel`);
};

// Update startFullWorkflow to accept overrideValidation
export const startFullWorkflow = (opportunityId, overrideValidation = false) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/run-full-auto-async`, {
    override_validation: overrideValidation
  });
};

// Add refineContent to call the backend refinement endpoint
export const refineContent = (opportunityId, htmlContent, command) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/refine-content`, {
    html_content: htmlContent,
    command: command
  });
};

export const generateContentOverride = (opportunityId) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/generate-content-override`);
};

// New service function to reject an opportunity
export const rejectOpportunity = (opportunityId) => {
  return apiClient.post(`/api/orchestrator/reject-opportunity/${opportunityId}`);
};

export const updateSocialMediaPostsStatus = (opportunityId, newStatus) => {
  return apiClient.post(`/api/orchestrator/${opportunityId}/social-media-status`, { new_status: newStatus });
};

 // Add this new function

 export const startFullAutomationWorkflow = (opportunityId, overrideValidation = false) => {

   return apiClient.post(`/api/orchestrator/${opportunityId}/run-full-automation-async`, {

     override_validation: overrideValidation

   });

 };



 export const getFullPrompt = (opportunityId) => {



   return apiClient.get(`/api/orchestrator/${opportunityId}/full-prompt`);



 };



 



 export const getScoreNarrative = (opportunityId) => {



   return apiClient.get(`/api/orchestrator/${opportunityId}/score-narrative`);



 };
```

## File: my-content-app/src/services/settingsService.js
```javascript
import apiClient from './apiClient';

export const getDiscoveryStrategies = async () => {
  // This is a placeholder. In a real app, you might fetch this from the backend.
  return ["Keyword Ideas", "Keyword Suggestions", "Related Keywords"];
};

export const getAvailableDiscoveryFilters = async () => {
  return apiClient.get('/api/discovery/available-filters');
};
```

## File: my-content-app/src/App.jsx
```javascript
import React from 'react';
import { Routes, Route } from 'react-router-dom';
import MainLayout from './components/layout/MainLayout';
import DiscoveryPage from './pages/DiscoveryPage/DiscoveryPage';
import RunDetailsPage from './pages/RunDetailsPage/RunDetailsPage';
import OpportunitiesPage from './pages/OpportunitiesPage/OpportunitiesPage';
import { useAuth } from './context/AuthContext';
import LoginPage from './pages/Auth/LoginPage';
import { Spin } from 'antd'; // For loading state
import DashboardPage from './pages/Dashboard/DashboardPage';
import ClientDashboardPage from './pages/ClientDashboard/ClientDashboardPage';
import OpportunityDetailPage from './pages/opportunity-detail-page/index.jsx';
import ActivityLogPage from './pages/ActivityLog/ActivityLogPage';
import SettingsPage from './pages/Settings/SettingsPage';
import NotFoundPage from './pages/NotFoundPage/NotFoundPage';

import { JobProvider } from './context/JobContext';
import BlogPage from './pages/BlogPage/BlogPage';

// REPLACE the existing `function App() { ... }` block with this:
function App() {
  const { isAuthenticated, loading } = useAuth();

  if (loading) {
    return (
      <div style={{ display: 'flex', justifyContent: 'center', alignItems: 'center', height: '100vh' }}>
        <Spin size="large" tip="Loading authentication..." />
      </div>
    );
  }

  return (
    <Routes>
      {isAuthenticated ? (
        <>
          <Route path="/" element={<MainLayout />}>
            <Route index element={<DashboardPage />} />
            <Route path="/dashboard" element={<DashboardPage />} />
            <Route path="/clients" element={<ClientDashboardPage />} />
            <Route path="/opportunities" element={<OpportunitiesPage />} />
            <Route path="/opportunities/:opportunityId" element={<OpportunityDetailPage />} />
            <Route path="/discovery-run/:runId" element={<RunDetailsPage />} />
            <Route path="/activity-log" element={<ActivityLogPage />} />
            <Route path="/settings" element={<SettingsPage />} />
            <Route path="/discovery" element={<DiscoveryPage />} />
            <Route path="*" element={<NotFoundPage />} />
          </Route>
          <Route path="/blog/:opportunityId" element={<BlogPage />} />
        </>
      ) : (
        <>
          <Route path="/login" element={<LoginPage />} />
          <Route path="*" element={<LoginPage />} />
        </>
      )}
    </Routes>
  );
}

export default App;
```

## File: my-content-app/src/index.css
```css
/* Global styles for the application */
body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  background-color: #f0f2f5; /* Light grey background for the app */
}

/* Add specific Ant Design overrides if needed */
/* For example, to make tags slightly smaller globally */
.ant-tag {
  font-size: 12px;
  height: auto;
  line-height: 1.8;
  padding: 0 7px;
}
```

## File: my-content-app/src/main.jsx
```javascript
import React from 'react';
import ReactDOM from 'react-dom/client';
import { BrowserRouter } from 'react-router-dom';
import { QueryClient, QueryClientProvider } from 'react-query';
import { ClientProvider } from './context/ClientContext';
import { NotificationProvider } from './context/NotificationContext';
import { AuthProvider } from './context/AuthContext';
import App from './App';
import 'antd/dist/reset.css'; // Ant Design's base styles
import './index.css'; // Your global custom styles

import { JobProvider } from './context/JobContext';
import GlobalJobTracker from './components/GlobalJobTracker';

// ... (other imports)

const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      refetchOnWindowFocus: false, // Don't refetch automatically on window focus
      retry: 1, // Retry failed queries once
    },
  },
});

ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <BrowserRouter>
      <QueryClientProvider client={queryClient}>
        <NotificationProvider>
          <AuthProvider>
            <ClientProvider>
              <JobProvider>
                <App />
                <GlobalJobTracker />
              </JobProvider>
            </ClientProvider>
          </AuthProvider>
        </NotificationProvider>
      </QueryClientProvider>
    </BrowserRouter>
  </React.StrictMode>
);
```

## File: my-content-app/.dockerignore
```
# Ignore dependencies, as they are installed inside the container
node_modules

# Ignore build output
dist
build

# Ignore miscellaneous files
.env
.eslintignore
.eslintrc.json
npm-debug.log
README.md
```

## File: my-content-app/.eslintignore
```
/node_modules
/dist
/vite.config.js
/.eslintrc.json
```

## File: my-content-app/.eslintrc.json
```json
{
    "env": {
        "browser": true,
        "es2021": true
    },
    "extends": [
        "eslint:recommended",
        "plugin:react/recommended",
        "plugin:react-hooks/recommended"
    ],
    "parserOptions": {
        "ecmaFeatures": {
            "jsx": true
        },
        "ecmaVersion": 12,
        "sourceType": "module"
    },
    "plugins": [
        "react",
        "react-hooks"
    ],
    "rules": {
        "react/prop-types": "off"
    },
    "settings": {
        "react": {
            "version": "detect"
        }
    }
}
```

## File: my-content-app/Dockerfile
```
# Stage 1: Build the React application
FROM node:18-alpine as build

WORKDIR /app

COPY package.json ./

RUN npm install --legacy-peer-deps

COPY . .

RUN npm run build

# Stage 2: Serve the application with Nginx
FROM nginx:stable-alpine

# Copy the built files from the build stage
COPY --from=build /app/dist /usr/share/nginx/html

# Copy the custom Nginx configuration
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Expose port 80
EXPOSE 80

# Start Nginx
CMD ["nginx", "-g", "daemon off;"]
```

## File: my-content-app/index.html
```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + React</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>
```

## File: my-content-app/nginx.conf
```
server {
  listen 80;

  location / {
    root   /usr/share/nginx/html;
    index  index.html index.htm;
    try_files $uri $uri/ /index.html;
  }

  location /api {
    proxy_pass http://backend:8000;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
  }

  error_page   500 502 503 504  /50x.html;
  location = /50x.html {
    root   /usr/share/nginx/html;
  }
}
```

## File: my-content-app/package.json
```json
{
  "name": "my-content-app",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint . --ext js,jsx --report-unused-disable-directives --max-warnings 0",
    "preview": "vite preview"
  },
  "dependencies": {
    "@ant-design/charts": "^2.6.5",
    "@ant-design/icons": "^5.3.7",
    "antd": "^5.17.4",
    "axios": "^1.7.2",
    "date-fns": "^3.6.0",
    "file-saver": "^2.0.5",
    "jszip": "^3.10.1",
    "react": "^18.2.0",
    "react-diff-viewer": "^3.1.1",
    "react-dom": "^18.2.0",
    "react-query": "^3.39.3",
    "react-quill": "^2.0.0",
    "react-router-dom": "^6.23.1",
    "recharts": "^2.12.7"
  },
  "devDependencies": {
    "@types/react": "^18.2.66",
    "@types/react-dom": "^18.2.22",
    "@vitejs/plugin-react": "^4.2.1",
    "eslint": "^8.57.0",
    "eslint-plugin-react": "^7.34.1",
    "eslint-plugin-react-hooks": "^4.6.0",
    "eslint-plugin-react-refresh": "^0.4.6",
    "vite": "^5.2.0"
  }
}
```

## File: my-content-app/vite.config.js
```javascript
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react';

// https://vitejs.dev/config/
export default defineConfig({
  plugins: [react()],
  server: {
    port: 3000, // Frontend will run on port 3000
    proxy: {
      // Proxy API requests to the FastAPI backend
      '/api': {
        target: 'http://localhost:8000', // Backend address
        changeOrigin: true,
        rewrite: (path) => path.replace(/^\/api/, ''), // Strip /api prefix
      },
    },
  },
});
```




BACKEND:

This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-10-30T20:52:16.589Z

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Info

# Directory Structure
```
agents/
  __init__.py
  article_generator.py
  brief_assembler.py
  content_auditor.py
  html_formatter.py
  image_generator.py
  internal_linking_suggester.py
  prompt_assembler.py
  social_media_crafter.py
  summary_generator.py
api/
  routers/
    auth.py
    client_settings.py
    clients.py
    discovery.py
    jobs.py
    opportunities.py
    orchestrator.py
    qualification_settings.py
    qualification_strategies.py
    settings.py
  dependencies.py
  globals.py
  main.py
  models.py
app_config/
  __init__.py
  manager.py
  settings.ini
core/
  serp_analyzers/
    disqualification_analyzer.py
    featured_snippet_analyzer.py
    pixel_ranking_analyzer.py
    video_analyzer.py
  __init__.py
  blueprint_factory.py
  page_classifier.py
  serp_analyzer.py
  utils.py
data_access/
  migrations/
    001_add_new_tables.sql
    001_initial_schema.sql
    002_add_keywords_table.sql
    002_remove_json_columns.sql
    003_add_indexes.sql
    003_add_total_api_cost.sql
    004_add_qualification_settings_table.sql
    005_add_qualification_columns.sql
    006_add_intent_weights.sql
    007_add_competitor_strength_weight.sql
    008_add_serp_features_weight.sql
    009_add_trend_weight.sql
    010_add_history_columns.sql
    012_add_seasonality_weight.sql
    013_add_serp_volatility_weight.sql
    014_add_disqualification_rules.sql
    015_add_brand_keywords.sql
    016_add_review_threshold.sql
    017_add_strategies_table.sql
    018_add_job_id_and_cluster_name.sql
    019_add_core_keyword_and_competitor_metrics_to_opportunities.sql
    020_backfill_core_keyword_metrics.sql
    021_add_unique_keyword_constraint.sql
    023_add_run_id_to_opportunities.sql
    024_add_total_api_cost_to_opportunities.sql
    025_add_cost_to_discovery_runs.sql
  __init__.py
  database_manager.py
  initialize.py
  models.py
  queries.py
data_mappers/
  dataforseo_mapper.py
  keyword_data_mapper.py
  serp_overview_mapper.py
external_apis/
  dataforseo_client_v2.py
  on-page-api.py
  openai_client.py
  pexels_client.py
pipeline/
  orchestrator/
    __init__.py
    analysis_orchestrator.py
    content_orchestrator.py
    cost_estimator.py
    discovery_orchestrator.py
    image_orchestrator.py
    main.py
    social_orchestrator.py
    validation_orchestrator.py
    workflow_orchestrator.py
  step_01_discovery/
    keyword_discovery/
      expander.py
      filters.py
    __init__.py
    blog_content_qualifier.py
    cannibalization_checker.py
    disqualification_rules.py
    keyword_expander.py
    run_discovery.py
  step_02_qualification/
    __init__.py
    competitor_analyzer.py
    serp_analyzer.py
  step_03_prioritization/
    scoring_components/
      __init__.py
      commercial_intent.py
      competitor_performance.py
      competitor_weakness.py
      ease_of_ranking.py
      growth_trend.py
      keyword_structure.py
      serp_crowding.py
      serp_features.py
      serp_freshness.py
      serp_threat.py
      serp_volatility.py
      traffic_potential.py
      volume_volatility.py
    __init__.py
    run_prioritization.py
    scoring_engine.py
  step_04_analysis/
    content_analysis_modules/
      ai_intelligence_caller.py
      heading_analyzer.py
      metric_analyzer.py
    __init__.py
    competitor_analyzer.py
    content_analyzer.py
    run_analysis.py
  step_05_strategy/
    decision_engine.py
  step_06_content_creation/
    __init__.py
  __init__.py
  orchestrator.py
services/
  discovery_service.py
  disqualification_service.py
  keyword_data_aggregator.py
  opportunities_service.py
  qualification_service.py
  scoring_service.py
  serp_analysis_service.py
tests/
  test_content_generation.py
  test_filter_transformation.py
  test_onpage_instant_pages.py
Dockerfile
export_db.py
jobs.py
requirements.txt
```

# Files

## File: agents/__init__.py
```python
# agents/__init__.py
```

## File: agents/article_generator.py
```python
import logging
from typing import Dict, Any, Tuple, Optional, List
from concurrent.futures import ThreadPoolExecutor, as_completed

from external_apis.openai_client import OpenAIClientWrapper
from agents.prompt_assembler import DynamicPromptAssembler


class SectionalArticleGenerator:
    """
    An agentic generator that creates content for specific sections of an article
    based on a highly contextual prompt.
    """

    def __init__(
        self, openai_client: OpenAIClientWrapper, config: Dict[str, Any], db_manager
    ):
        self.openai_client = openai_client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.prompt_assembler = DynamicPromptAssembler(db_manager)

    def _generate_component(
        self, messages: List[Dict[str, str]], model: str, temperature: float
    ) -> Tuple[Optional[str], float]:
        """Internal helper to call the LLM and get a raw HTML string."""
        schema = {
            "name": "generate_html_content",
            "type": "object",
            "properties": {
                "content_html": {
                    "type": "string",
                    "description": "The generated content as a clean, well-structured HTML block. Do not include the main heading tag itself.",
                }
            },
            "required": ["content_html"],
            "additionalProperties": False,
        }
        response, error = self.openai_client.call_chat_completion(
            messages=messages,
            schema=schema,
            model=model,
            max_completion_tokens=self.config.get(
                "max_completion_tokens_for_generation", 4096
            ),
        )
        cost = self.openai_client.latest_cost
        if error or not response:
            self.logger.error(f"Failed to generate content component: {error}")
            return None, cost
        return response.get("content_html"), cost

    def generate_full_article(self, opportunity: Dict[str, Any]) -> Tuple[Dict[str, Any], float]:
        """
        Generates a full article by generating its sections in parallel and then assembling them.
        """
        total_cost = 0.0
        blueprint = opportunity.get("blueprint", {})
        brief = blueprint.get("ai_content_brief", {})
        outline_h2s = brief.get("mandatory_sections", [])
        
        generated_content = {}

        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_section = {}

            # 1. Submit Introduction task
            intro_future = executor.submit(self.generate_introduction, opportunity)
            future_to_section[intro_future] = "introduction"

            # 2. Submit Body Section tasks
            for i, section_title in enumerate(outline_h2s):
                previous_title = outline_h2s[i-1] if i > 0 else "Introduction"
                next_title = outline_h2s[i+1] if i < len(outline_h2s) - 1 else "Conclusion"
                
                # Assuming sub_points are not explicitly defined per section in the brief for now
                section_future = executor.submit(
                    self._generate_parallel_section,
                    opportunity,
                    section_title,
                    [], 
                    previous_title,
                    next_title
                )
                future_to_section[section_future] = section_title

            # 3. Collect results as they complete
            for future in as_completed(future_to_section):
                section_identifier = future_to_section[future]
                try:
                    content, cost = future.result()
                    total_cost += cost
                    generated_content[section_identifier] = content or f"<!-- Error generating section: {section_identifier} -->"
                except Exception as exc:
                    self.logger.error(f"Section '{section_identifier}' generated an exception: {exc}", exc_info=True)
                    generated_content[section_identifier] = f"<!-- Exception generating section: {section_identifier}. Error: {exc} -->"

        # 4. Assemble the article body in the correct order
        article_body_html = generated_content.get("introduction", "")
        for section_title in outline_h2s:
            article_body_html += f"\n<h2>{section_title}</h2>\n"
            article_body_html += generated_content.get(section_title, "")

        # 5. Generate Conclusion sequentially after the main body is assembled
        conclusion_html, cost = self.generate_conclusion(opportunity, article_body_html)
        total_cost += cost
        article_body_html += f"\n<h2>Conclusion</h2>\n"
        article_body_html += conclusion_html or "<!-- Error generating conclusion -->"

        # The method returns the assembled HTML content and the total cost.
        # The orchestrator will be responsible for packaging this into the final ai_content_json blob.
        return {"article_body_html": article_body_html}, total_cost

    def _generate_parallel_section(
        self,
        opportunity: Dict[str, Any],
        section_title: str,
        section_sub_points: List[str],
        previous_section_title: str,
        next_section_title: str,
    ) -> Tuple[Optional[str], float]:
        """Generates a single article section with context about surrounding sections."""
        brief = opportunity.get("blueprint", {}).get("ai_content_brief", {})
        prompt = f"""
        You are an expert SEO content writer and subject matter expert. Your task is to write a single, detailed section for a blog post about "{opportunity["keyword"]}".

        **Current Section to Write:** "{section_title}"
        **Key Sub-points to cover in this section:** {", ".join(section_sub_points) if section_sub_points else "N/A"}
        
        **Context of surrounding sections:**
        - The previous section was about: "{previous_section_title}"
        - The next section will be about: "{next_section_title}"

        **Instructions:**
        - Write a comprehensive, in-depth section covering the topic "{section_title}".
        - If provided, elaborate on all key sub-points, using them to structure the section's content.
        - Ensure your writing is aware of the surrounding topics to maintain a logical flow.
        - Incorporate relevant entities and demonstrate expertise by using practical examples or insights.
        - Persona: {brief.get("target_audience_persona")}
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}
        
        Return a JSON object with a single key "content_html" containing the HTML for this section (e.g., using <p>, <ul>, <h3> tags). Do NOT include the main <h2> tag for "{section_title}" itself.
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.7,
        )

    def generate_introduction(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[Optional[str], float]:
        brief = opportunity.get("blueprint", {}).get("ai_content_brief", {})
        prompt = f"""
        You are an expert copywriter. Write a compelling and hook-driven introduction for a blog post titled "{opportunity["keyword"]}".
        The introduction should be 2-3 paragraphs.
        - Immediately grab the reader's attention with a relatable problem or surprising statistic.
        - Briefly state the core problem or question the article will solve and why it matters.
        - End with a transition that clearly outlines what the reader will learn.
        - Target Audience: {brief.get("target_audience_persona")}
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}
        Return a JSON object with a single key "content_html" containing the HTML for the introduction (e.g., {{"content_html": "<p>...</p><p>...</p>"}}).
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.75,
        )

    def generate_conclusion(
        self, opportunity: Dict[str, Any], full_article_context: str
    ) -> Tuple[Optional[str], float]:
        cta_url = opportunity.get("client_cfg", {}).get("default_cta_url", "#")
        prompt = f"""
        You are an expert copywriter. Write a powerful conclusion for the following blog post.
        The conclusion should be 2 paragraphs.
        - Briefly summarize the most important takeaways from the article.
        - Provide a final, actionable thought or encouragement for the reader.
        - End with a compelling call-to-action that encourages the reader to visit {cta_url}.
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}

        **Full Article Context (for summary):**
        {full_article_context}

        Return a JSON object with a single key "content_html" containing the HTML for the conclusion (e.g., {{"content_html": "<p>...</p><p>...</p>"}}).
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.7,
        )

    def generate_section(
        self,
        opportunity: Dict[str, Any],
        section_title: str,
        section_sub_points: List[str],
        previous_section_content: str,
    ) -> Tuple[Optional[str], float]:
        brief = opportunity.get("blueprint", {}).get("ai_content_brief", {})
        prompt = f"""
        You are an expert SEO content writer and subject matter expert. Your task is to write a single, detailed section for a blog post about "{opportunity["keyword"]}".

        **Current Section to Write:** "{section_title}"
        **Key Sub-points to cover in this section:** {", ".join(section_sub_points) if section_sub_points else "N/A"}
        **Content from the Previous Section (for transition and context):**
        ...{previous_section_content[-1000:]}...

        **Instructions:**
        - Write a comprehensive, in-depth section covering the topic "{section_title}".
        - If provided, elaborate on all key sub-points, using them to structure the section's content.
        - Ensure a smooth, logical transition from the previous section's content.
        - Incorporate relevant entities and demonstrate expertise by using practical examples or insights.
        - Persona: {brief.get("target_audience_persona")}
        - Tone: {opportunity.get("client_cfg", {}).get("brand_tone")}
        
        Return a JSON object with a single key "content_html" containing the HTML for this section (e.g., using <p>, <ul>, <h3> tags). Do NOT include the main <h2> tag for "{section_title}" itself.
        """
        return self._generate_component(
            [{"role": "user", "content": prompt}],
            self.config.get("default_model", "gpt-5-nano"),
            0.7,
        )
```

## File: agents/brief_assembler.py
```python
import logging
from typing import Dict, Any


class BriefAssembler:
    """
    Assembles the final AI content brief from the blueprint data.
    This agent acts as a transformation layer between the analysis blueprint
    and the actionable brief for the content generation AI.
    """

    def __init__(self, openai_client):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.openai_client = openai_client

    def _generate_dynamic_brief_attributes(self, blueprint: Dict[str, Any], client_cfg: Dict[str, Any]) -> Dict[str, Any]:
        """Uses an AI call to generate dynamic persona and goal."""
        try:
            target_keyword = blueprint.get("winning_keyword", {}).get("keyword", "the target topic")
            serp_overview = blueprint.get("serp_overview", {})
            
            prompt = f"""
            Based on the following SERP data for the keyword '{target_keyword}', define a target audience persona and a primary goal for a new piece of content.

            SERP Data:
            - Dominant Content Format: {serp_overview.get('dominant_content_format', 'N/A')}
            - People Also Ask: {', '.join(serp_overview.get('people_also_ask', []))}
            - Related Searches: {', '.join(serp_overview.get('related_searches', []))}

            Client's Brand Voice: "{client_cfg.get('brand_voice', 'expert and informative')}"

            Return a JSON object with two keys: 'target_audience_persona' and 'primary_goal'.
            The persona should be a brief, descriptive summary of the ideal reader.
            The goal should be a concise statement about what the content aims to achieve for that reader.
            """
            
            messages = [{"role": "user", "content": prompt}]
            schema = {
                "name": "generate_brief_attributes",
                "description": "Generates a target audience persona and primary goal for a piece of content.",
                "type": "object",
                "properties": {
                    "target_audience_persona": {
                        "type": "string",
                        "description": "A brief, descriptive summary of the ideal reader."
                    },
                    "primary_goal": {
                        "type": "string",
                        "description": "A concise statement about what the content aims to achieve for that reader."
                    }
                },
                "required": ["target_audience_persona", "primary_goal"],
                "additionalProperties": False
            }

            response_json, error = self.openai_client.call_chat_completion(
                messages=messages,
                model="gpt-5-nano",
                schema=schema
            )

            if error:
                raise Exception(f"AI call failed: {error}")

            return response_json

        except Exception as e:
            self.logger.error(f"Failed to generate dynamic brief attributes: {e}")
            # Fallback to static values
            return {
                "target_audience_persona": self._determine_persona("Blog Post", client_cfg),
                "primary_goal": f"To provide a comprehensive and helpful resource that ranks for '{target_keyword}'.",
            }

    def assemble_brief(
        self, blueprint: Dict[str, Any], client_id: str, client_cfg: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Constructs the AI content brief by extracting and structuring
        information from the blueprint.
        """
        winning_keyword_data = blueprint.get("winning_keyword", {})
        serp_overview = blueprint.get("serp_overview", {})
        content_intelligence = blueprint.get("content_intelligence", {})
        content_type = blueprint.get("recommended_strategy", {}).get(
            "content_format", "Blog Post"
        )
        
        dynamic_attrs = self._generate_dynamic_brief_attributes(blueprint, client_cfg)
        
        word_count_multiplier = self._get_word_count_multiplier(
            content_type, client_cfg
        )
        base_word_count = content_intelligence.get("recommended_word_count", 1500)
        target_word_count = int(base_word_count * word_count_multiplier)

        brief = {
            "target_keyword": winning_keyword_data.get("keyword"),
            "content_type": content_type,
            "target_audience_persona": dynamic_attrs.get("target_audience_persona"),
            "primary_goal": dynamic_attrs.get("primary_goal"),
            "target_word_count": target_word_count,
            "mandatory_sections": content_intelligence.get(
                "common_headings_to_cover", []
            ),
            "unique_angles_to_cover": content_intelligence.get(
                "unique_angles_to_include", []
            ),
            "questions_to_answer_directly": serp_overview.get("paa_questions", []),
            "key_entities_to_mention": content_intelligence.get(
                "key_entities_from_competitors", []
            ),
            "compelling_arguments_to_integrate": content_intelligence.get(
                "unique_arguments_from_competitors", []
            ),
            "core_questions_competitors_answer": content_intelligence.get(
                "core_questions_answered_by_competitors", []
            ),
            "related_topics_to_include": serp_overview.get("related_searches", []),
            "google_preferred_answers": serp_overview.get(
                "extracted_serp_features", []
            ),
            "dynamic_serp_instructions": self._get_dynamic_serp_instructions(
                serp_overview, content_intelligence, blueprint
            ),
            "source_and_inspiration_content": {
                "competitors_urls": [
                    comp["url"]
                    for comp in blueprint.get("competitor_analysis", [])
                    if "url" in comp
                ]
            },
            "client_id": client_id,
        }

        # --- START MODIFICATION ---
        # NEW: Add all the rich SERP data to the brief
        if serp_overview.get("knowledge_graph_facts"):
            brief["knowledge_graph_facts"] = serp_overview["knowledge_graph_facts"]
        if serp_overview.get("paid_ad_copy"):
            brief["paid_ad_copy"] = serp_overview["paid_ad_copy"]
        if serp_overview.get("ai_overview_sources"):
            brief["ai_overview_sources"] = serp_overview["ai_overview_sources"]
        if serp_overview.get("top_organic_faqs"):
            brief["top_organic_faqs"] = serp_overview["top_organic_faqs"]
        if serp_overview.get("top_organic_sitelinks"):
            brief["top_organic_sitelinks"] = serp_overview["top_organic_sitelinks"]
        if serp_overview.get("discussion_snippets"):
            brief["discussion_snippets"] = serp_overview["discussion_snippets"]

        all_about_search_terms = []
        all_about_related_terms = []
        for res in blueprint.get("serp_overview", {}).get("top_organic_results", []):
            if res.get("about_this_result_search_terms"):
                all_about_search_terms.extend(res["about_this_result_search_terms"])
            if res.get("about_this_result_related_terms"):
                all_about_related_terms.extend(res["about_this_result_related_terms"])

        if all_about_search_terms:
            brief["google_understanding_search_terms"] = list(
                set(all_about_search_terms)
            )
        if all_about_related_terms:
            brief["google_understanding_related_terms"] = list(
                set(all_about_related_terms)
            )
        # --- END MODIFICATION ---

        self.logger.info(f"Assembled AI content brief for '{brief['target_keyword']}'.")
        return brief

    def _get_dynamic_serp_instructions(
        self,
        serp_overview: Dict[str, Any],
        content_intelligence: Dict[str, Any],
        blueprint: Dict[str, Any],
    ) -> list[str]:
        """Generates specific instructions for the AI based on SERP features and content intelligence."""
        instructions = []
        if serp_overview.get("extracted_serp_features"):
            instructions.append(
                "Pay close attention to the 'google_preferred_answers' section. This contains content that Google has already featured in rich snippets, so it's a strong indication of what Google considers a good answer. Use it as a primary source of inspiration."
            )
        if serp_overview.get("serp_has_featured_snippet"):
            instructions.append(
                "Create a concise, clear paragraph early in the article that directly answers the main query to target the featured snippet."
            )
        if serp_overview.get("serp_has_ai_overview"):
            instructions.append(
                "The content must be exceptionally high-quality, original, and provide unique insights to stand out against Google's AI Overview."
            )
        if serp_overview.get("has_video_carousel"):
            instructions.append(
                "Structure content in a way that is easily adaptable into a video script, as video is a key format for this topic."
            )
        if serp_overview.get("knowledge_graph_facts"):
            instructions.append(
                f"Incorporate the key facts from the Knowledge Graph: {', '.join(serp_overview['knowledge_graph_facts'])}."
            )

        top_results_with_context = [
            r
            for r in serp_overview.get("top_organic_results", [])
            if r.get("about_this_result_source_info")
        ]
        if top_results_with_context:
            context_info = top_results_with_context[0]["about_this_result_source_info"]
            instructions.append(
                f"Google's 'About this Result' panel says the top result is relevant because: '{context_info}'. Use this to understand the core topic."
            )

        if serp_overview.get("discussions_and_forums_snippets"):
            instructions.append(
                f"Address user pain points from forum discussions: {' | '.join(serp_overview['discussions_and_forums_snippets'])}."
            )

        days_ago = serp_overview.get("serp_last_updated_days_ago")
        if days_ago is not None:
            if days_ago <= 30:
                instructions.append(
                    f"The SERP is fresh ({days_ago} days old). Ensure content reflects the latest information."
                )
            elif days_ago > 180:
                instructions.append(
                    f"The SERP is outdated ({days_ago} days old). This is an opportunity to publish a more current and comprehensive article."
                )

        # Instructions from content intelligence (including competitor weaknesses)
        if content_intelligence.get("key_entities_from_competitors"):
            instructions.append(
                f"Mention key entities identified from competitors: {', '.join(content_intelligence['key_entities_from_competitors'])}."
            )

            # Summarize competitor weaknesses based on new granular scores
            competitor_analysis = blueprint.get("competitor_analysis", [])
            if competitor_analysis and all(
                "overall_strength_score" in c
                for c in competitor_analysis
                if c.get("url")
            ):
                tech_scores = [
                    c.get("technical_strength_score", 100)
                    for c in competitor_analysis
                    if c.get("url")
                ]
                content_scores = [
                    c.get("content_quality_score", 100)
                    for c in competitor_analysis
                    if c.get("url")
                ]

                # NEW: Add specific technical weaknesses if identified
                all_tech_warnings = []
                for comp in competitor_analysis:
                    all_tech_warnings.extend(comp.get("technical_warnings", []))

                unique_issues_to_highlight = list(set(all_tech_warnings))[
                    :3
                ]  # Limit to top 3
                if unique_issues_to_highlight:
                    formatted_warnings = ", ".join(
                        [w.replace("_", " ") for w in unique_issues_to_highlight]
                    )
                    instructions.append(
                        f"EXPLOIT WEAKNESS: Top competitors exhibit specific technical flaws such as: {formatted_warnings}. Your content must be technically superior."
                    )
                elif tech_scores:  # Fallback to general if no specific warnings
                    avg_tech_score = sum(tech_scores) / len(tech_scores)
                    if avg_tech_score < 65:
                        instructions.append(
                            "EXPLOIT WEAKNESS: Top competitors are generally technically poor (e.g., slow page speed, render-blocking resources). A fast, well-built page has a strong advantage."
                        )

                if content_scores:
                    avg_content_score = sum(content_scores) / len(content_scores)
                    if avg_content_score < 65:
                        instructions.append(
                            "EXPLOIT WEAKNESS: Ranking content is low-quality, thin, or outdated. Win by providing significantly more depth and fresh information."
                        )

        return instructions

    def _determine_persona(self, content_type: str, client_cfg: Dict[str, Any]) -> str:
        """Determines the persona to use based on client config."""
        base_persona = client_cfg.get("expert_persona", "an expert writer")
        return f"{base_persona} who writes like a human"

    def _get_word_count_multiplier(
        self, content_type: str, client_cfg: Dict[str, Any]
    ) -> float:
        """
        Gets the word count multiplier for a given content type from the client config,
        falling back to a default if not found.
        """
        # Sanitize the content_type to match the keys in settings.ini
        # e.g., "Comprehensive Article" -> "comprehensive_article"
        sanitized_format = content_type.lower().replace(" ", "_")

        # Look up the specific multiplier, or use the default multiplier if not found
        return client_cfg.get(
            sanitized_format, client_cfg.get("default_multiplier", 1.2)
        )
```

## File: agents/content_auditor.py
```python
import logging
import textstat
from typing import Dict, Any, List, Optional  # ADD List
from bs4 import BeautifulSoup  # ADD this for HTML parsing
import re  # ADD this for regex checks
import requests


class ContentAuditor:
    """
    Audits the generated content for SEO and readability metrics,
    and checks for "publish-readiness" issues.
    """

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def _check_for_broken_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """Checks all <a> tags for 4xx or 5xx status codes."""
        issues = []
        links = soup.find_all("a", href=True)
        for link in links:
            href = link["href"]
            # Skip internal/anchor links and javascript links
            if (
                not href
                or href.startswith("#")
                or href.startswith("/")
                or href.startswith("javascript:")
            ):
                continue
            try:
                # Use a HEAD request for efficiency with a timeout
                response = requests.head(href, timeout=5, allow_redirects=True)
                if response.status_code >= 400:
                    issues.append(
                        {
                            "issue": "broken_link",
                            "context": f"URL '{href}' returned status {response.status_code}.",
                        }
                    )
            except requests.exceptions.Timeout:
                issues.append(
                    {
                        "issue": "link_timeout",
                        "context": f"Could not get response from '{href}' within 5 seconds.",
                    }
                )
            except requests.RequestException:
                issues.append(
                    {
                        "issue": "unreachable_link",
                        "context": f"Could not connect to URL '{href}'.",
                    }
                )
        return issues

    def audit_content(
        self,
        article_html: str,
        primary_keyword: str,
        blueprint: Dict[str, Any],
        client_cfg: Dict[str, Any],
        avg_competitor_readability: Optional[float] = None,
    ) -> Dict[str, Any]:
        """
        Audits the HTML content and returns a dictionary of metrics.
        """
        # Extract plain text for text-based analysis
        soup = BeautifulSoup(article_html, "html.parser")
        plain_text = soup.get_text(separator=" ", strip=True)
        html_issues = self._check_html_publish_readiness(article_html)
        if html_issues is None:
            html_issues = []

        # Add broken link check results
        broken_link_issues = self._check_for_broken_links(soup)
        html_issues.extend(broken_link_issues)

        word_count = len(plain_text.split())
        target_word_count = blueprint.get("ai_content_brief", {}).get(
            "target_word_count", 0
        )
        if target_word_count > 0:
            deviation = abs(word_count - target_word_count) / target_word_count
            if deviation > 0.20:
                html_issues.append(
                    {
                        "issue": "word_count_deviation",
                        "context": f"Actual count ({word_count}) deviates from target ({target_word_count}) by more than 20%.",
                    }
                )

        readability_score = textstat.flesch_kincaid_grade(plain_text)
        # Calculate additional metrics for comprehensive audit (Task 9.1)
        smog_score = textstat.smog_index(plain_text)
        coleman_liau_score = textstat.coleman_liau_index(plain_text)

        persona = blueprint.get("ai_content_brief", {}).get(
            "target_audience_persona", "General audience"
        )

        # W13 FIX: Determine Readability Mismatch and Required Refinement Command
        refinement_command = None
        readability_assessment = f"Flesch-Kincaid Grade Level: {readability_score:.1f}."

        if avg_competitor_readability is not None:
            readability_assessment += (
                f" (Avg. Competitor F-K: {avg_competitor_readability:.1f})."
            )
            if (
                abs(readability_score - avg_competitor_readability) > 3.0
            ):  # If our score is more than 3 grades off
                if readability_score < avg_competitor_readability:
                    readability_assessment += " Assessment: CRITICAL: Content is significantly simpler than competitors. Consider increasing complexity."
                    refinement_command = f"Increase the complexity to match competitor average of Flesch-Kincaid Grade Level {avg_competitor_readability:.1f}."
                else:
                    readability_assessment += " Assessment: CRITICAL: Content is significantly more complex than competitors. Consider simplifying."
                    refinement_command = f"Simplify the content to match competitor average of Flesch-Kincaid Grade Level {avg_competitor_readability:.1f}."
            else:
                readability_assessment += (
                    " Assessment: Readability is consistent with top competitors."
                )
        else:
            # Fallback to persona-based assessment if no competitor average is provided
            if "expert" in persona.lower() or "planner" in persona.lower():
                if readability_score < 9.5:
                    readability_assessment += " Assessment: CRITICAL: Content is likely oversimplified (Grade < 9.5)."
                    refinement_command = "Increase the complexity and authoritative tone of the writing to target a Flesch-Kincaid Grade Level of 10 or higher."
                else:
                    readability_assessment += (
                        " Assessment: Appropriate complexity for an expert audience."
                    )
            else:
                if readability_score > 12 or smog_score > 10.0:
                    readability_assessment += " Assessment: CRITICAL: Content is too academic or complex (Grade > 12 or SMOG > 10.0)."
                    refinement_command = "Simplify the complexity and reduce sentence length to target a Flesch-Kincaid Grade Level between 7 and 9, and a SMOG Index under 8."
                else:
                    readability_assessment += (
                        " Assessment: Appropriate complexity for a general audience."
                    )

        entity_metrics = self._check_entity_coverage(plain_text, blueprint)
        if entity_metrics.get("score", 100) < 75.0 and entity_metrics.get(
            "missing"
        ):  # Only flag if there are actually missing entities
            missing_entities_str = ", ".join(entity_metrics.get("missing", []))
            html_issues.append(
                {
                    "issue": "critical_entity_gap",
                    "context": f"Entity coverage is below 75%. Missing: {missing_entities_str}",
                }
            )

        # Ensure the final return object from audit_content includes all new data:
        return {
            "flesch_kincaid_grade": readability_score,
            "smog_index": smog_score,
            "coleman_liau_index": coleman_liau_score,
            "readability_assessment": readability_assessment,
            "refinement_command": refinement_command,
            "entity_coverage_score": entity_metrics.get("score", 0),
            "missing_entities": entity_metrics.get("missing", []),
            "covered_sections": None,
            "publish_readiness_issues": html_issues,  # This now includes broken links and critical entity gaps
        }

    def _check_entity_coverage(
        self, article_text: str, blueprint: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Checks if the key entities from the blueprint are present in the article text.
        """
        # ... (existing logic)
        entities = blueprint.get("ai_content_brief", {}).get(
            "key_entities_to_mention", []
        )
        if not entities:
            return {"score": 100, "missing": []}

        missing_entities = []
        for entity in entities:
            # Heuristic check: Look for exact match or simple pluralization (Task 11.1)
            if entity.endswith("s"):
                # If entity is already plural (e.g., 'tools'), check for exact match only
                pattern = r"\b" + re.escape(entity) + r"\b"
            else:
                # Check for singular or plural form (e.g., 'tool' or 'tools')
                pattern = r"\b" + re.escape(entity) + r"s?\b"
            if not re.search(pattern, article_text, re.IGNORECASE):
                missing_entities.append(entity)

        coverage_score = (
            100 - (len(missing_entities) / len(entities) * 100) if entities else 100
        )
        return {
            "score": coverage_score,
            "missing": missing_entities,
            "found_count": len(entities) - len(missing_entities),
            "total_expected": len(entities),
        }

    def _check_html_publish_readiness(self, article_html: str) -> List[Dict[str, Any]]:
        """
        Performs specific checks on the final HTML for publish-readiness, returning structured issues.
        """
        issues = []
        soup = BeautifulSoup(article_html, "html.parser")

        # Check for unresolved image placeholders
        placeholder_pattern = r"\[\[IMAGE_ID:\s*(.*?)\s*PROMPT:\s*(.*?)\s*\]\]"
        placeholders_found = re.findall(placeholder_pattern, article_html)
        if placeholders_found:
            issues.append(
                {
                    "issue": "unresolved_placeholder",
                    "context": f"{len(placeholders_found)} image placeholders remain in the text.",
                }
            )

        # Check for empty headings
        for h_tag in soup.find_all(re.compile(r"^h[1-6]$")):
            if not h_tag.get_text(strip=True):
                issues.append({"issue": "empty_heading", "context": str(h_tag)})

        # Check for extremely short paragraphs
        for p_tag in soup.find_all("p"):
            text = p_tag.get_text(strip=True)
            if 0 < len(text.split()) < 5:
                issues.append({"issue": "short_paragraph", "context": str(p_tag)})

        return issues
```

## File: agents/html_formatter.py
```python
import logging
from typing import Dict, Any, List, Optional
import os
import re
import markdown
from bs4 import BeautifulSoup
from datetime import datetime
from backend.core import utils


class HtmlFormatter:
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def _convert_markdown_tables_to_html(self, html_body: str) -> str:
        """Converts Markdown tables to HTML using markdown library directly."""
        return markdown.markdown(html_body, extensions=["tables", "fenced_code"])

    def _insert_internal_links(
        self, soup: BeautifulSoup, internal_links: List[Dict[str, str]]
    ) -> None:
        """
        Inserts internal links into the BeautifulSoup object based on specific contextual suggestions from an AI agent.
        """
        if not internal_links:
            return

        linked_anchors = set()

        for link_data in internal_links:
            anchor_text = link_data.get("anchor_text")
            url = link_data.get("url")
            context_paragraph = link_data.get("context_paragraph_text")

            if (
                not all([anchor_text, url, context_paragraph])
                or anchor_text.lower() in linked_anchors
            ):
                continue

            # Find all paragraphs that contain the exact context text
            potential_paragraphs = soup.find_all(
                "p", string=re.compile(re.escape(context_paragraph))
            )

            for p_tag in potential_paragraphs:
                # Find the text node within this paragraph that contains the anchor
                text_node = p_tag.find(
                    string=re.compile(re.escape(anchor_text), re.IGNORECASE)
                )

                if text_node and not text_node.find_parent(
                    "a"
                ):  # Ensure it's not already linked
                    match = re.search(
                        re.escape(anchor_text), str(text_node), re.IGNORECASE
                    )
                    if match:
                        before_text = str(text_node)[: match.start()]
                        matched_text = match.group(0)
                        after_text = str(text_node)[match.end() :]

                        link_tag = soup.new_tag("a", href=url)
                        link_tag.string = matched_text

                        new_content = []
                        if before_text:
                            new_content.append(before_text)
                        new_content.append(link_tag)
                        if after_text:
                            new_content.append(after_text)

                        text_node.replace_with(*new_content)
                        linked_anchors.add(anchor_text.lower())
                        break  # Move to the next link suggestion once placed

    def _generate_toc(self, soup: BeautifulSoup) -> None:
        """Generates and inserts a Table of Contents from H2 tags into the BeautifulSoup object."""
        toc_list = soup.new_tag("ul", **{"class": "toc-list"})
        h2_tags = soup.find_all("h2")

        if len(h2_tags) < 2:
            return  # No TOC needed for less than 2 headings

        # Add unique IDs to H2 tags and build TOC
        for i, h2 in enumerate(h2_tags):
            slug = utils.slugify(h2.text)
            if not slug:  # Fallback for empty/unsluggable H2s
                slug = f"section-{i + 1}"
            h2["id"] = slug  # Add ID to H2 for linking

            toc_item = soup.new_tag("li")
            toc_link = soup.new_tag("a", href=f"#{slug}")
            toc_link.string = h2.text
            toc_item.append(toc_link)
            toc_list.append(toc_item)

        toc_header = soup.new_tag("h2")
        toc_header.string = "Table of Contents"
        toc_header["id"] = "table-of-contents"  # Give TOC its own ID

        first_h2 = soup.find("h2")
        if first_h2:
            first_h2.insert_before(toc_list)
            first_h2.insert_before(toc_header)
        else:
            # Fallback if no H2s exist, place it after the first paragraph or at the start
            first_p = soup.find("p")
            if first_p:
                first_p.insert_after(toc_header)
                first_p.insert_after(toc_list)
            else:
                soup.insert(0, toc_list)
                soup.insert(0, toc_header)

    def _generate_schema_org(
        self, soup: BeautifulSoup, opportunity: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Dynamically generates Schema.org JSON-LD from the final HTML soup.
        """
        schema_graph: List[Dict[str, Any]] = []
        client_cfg = opportunity.get("client_cfg", {})
        slug = opportunity.get("blueprint", {}).get("slug", "default-slug")

        domain = client_cfg.get("target_domain", "")
        article_url = f"https://{domain}/{slug}" if domain else slug
        publisher_name = domain or "Publisher Name"

        h1_tag = soup.find("h1")
        article_headline = (
            h1_tag.get_text(strip=True)
            if h1_tag
            else opportunity.get("keyword", "Article")
        )

        article_schema = {
            "@type": "BlogPosting",
            "@id": f"{article_url}#article",
            "mainEntityOfPage": {"@id": article_url},
            "headline": article_headline,
            "author": {
                "@type": client_cfg.get("schema_author_type", "Organization"),
                "name": client_cfg.get("default_author_name", "Author"),
            },
            "publisher": {"@type": "Organization", "name": publisher_name},
            "datePublished": datetime.now().isoformat(),
            "dateModified": datetime.now().isoformat(),
            "additionalProperties": False,
        }
        schema_graph.append(article_schema)

        # Dynamic HowTo Schema
        how_to_headings = soup.find_all(
            ["h2", "h3"], string=re.compile(r"how to", re.IGNORECASE)
        )
        for heading in how_to_headings:
            ol = heading.find_next("ol")
            if ol:
                steps = [
                    {"@type": "HowToStep", "text": li.get_text(strip=True)}
                    for li in ol.find_all("li")
                ]
                if steps:
                    schema_graph.append(
                        {
                            "@type": "HowTo",
                            "name": heading.get_text(strip=True),
                            "step": steps,
                        }
                    )

        return {"@context": "https://schema.org", "@graph": schema_graph}

    def format_final_package(
        self,
        opportunity: Dict[str, Any],
        internal_linking_suggestions: Optional[List[Dict[str, str]]] = None,
        in_article_images_data: Optional[List[Dict[str, Any]]] = None,
    ) -> Dict[str, Any]:
        """
        Constructs the final content package, now including schema generation.
        """
        # ... (existing code for html_body_str, soup creation, internal linking, ToC, etc.) ...
        ai_content = opportunity.get("ai_content", {})
        client_cfg = opportunity.get("client_cfg", {})
        html_body_str = ai_content.get("article_body_html", "")
        soup = BeautifulSoup(f"<div>{html_body_str}</div>", "html.parser")

        if internal_linking_suggestions:
            self._insert_internal_links(soup, internal_linking_suggestions)

        if client_cfg.get("generate_toc", True):
            self._generate_toc(soup)

        # ... (image replacement logic) ...

        article_html_final = (
            str(soup.body.decode_contents()) if soup.body else str(soup)
        )

        # --- NEW: Call the schema generator ---
        schema_org_json = self._generate_schema_org(soup, opportunity)

        featured_image = opportunity.get("featured_image_data", {})
        featured_image_relative_path = (
            f"/api/images/{os.path.basename(featured_image.get('local_path'))}"
            if featured_image and featured_image.get("local_path")
            else None
        )

        return {
            "meta_title": ai_content.get("meta_title", "No Title"),
            "meta_description": ai_content.get("meta_description", ""),
            "article_html_final": article_html_final,
            "schema_org_json": schema_org_json,  # Add the generated schema to the final package
            "featured_image_path": featured_image.get("local_path"),
            "featured_image_relative_path": featured_image_relative_path,
            "social_media_posts": opportunity.get("social_media_posts_json", []),
        }
```

## File: agents/image_generator.py
```python
import logging
import os
from typing import Dict, Any, Tuple, Optional, List

from backend.external_apis.pexels_client import PexelsClient, download_image_from_url
from backend.core import utils

from PIL import Image, ImageDraw, ImageFont, ImageColor


class ImageGenerator:
    """
    Agent for finding featured and in-article images from Pexels.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

        self.pexels_client = None
        if self.config.get("pexels_api_key"):
            try:
                self.pexels_client = PexelsClient(self.config["pexels_api_key"])
            except ValueError as e:
                self.logger.warning(
                    f"Pexels client could not be initialized: {e}. Image generation will be skipped."
                )

    def _add_text_overlay(self, image_path: str, text: str) -> str:
        """Adds a text overlay to the image based on configured settings."""
        if not self.config.get("overlay_text_enabled", False):
            return image_path  # If disabled, return original path

        try:
            image = Image.open(image_path).convert(
                "RGBA"
            )  # Convert to RGBA for alpha channel in overlay background
            draw = ImageDraw.Draw(image)

            text_color = ImageColor.getrgb(
                self.config.get("overlay_text_color", "#FFFFFF")
            )
            bg_color_hex = self.config.get("overlay_background_color", "#00000080")
            # Extract RGB and alpha from RGBA hex
            bg_color = ImageColor.getrgb(
                bg_color_hex
            )  # This returns (R,G,B) for #RRGGBB or (R,G,B,A) for #RRGGBBAA
            # Ensure bg_color is (R,G,B,A) if alpha is specified
            if len(bg_color) == 3 and len(bg_color_hex) == 9:  # #RRGGBBAA format
                bg_alpha = int(bg_color_hex[7:9], 16)
                bg_color = bg_color + (bg_alpha,)
            elif len(bg_color) == 3:  # default to some alpha if only RGB is given
                bg_color = bg_color + (128,)  # Default 50% opacity

            font_size = self.config.get("overlay_font_size", 40)

            try:
                # Use a reliable path to a bundled font file.
                # Assumes a `resources/fonts` directory exists relative to the project root.
                font_path = os.path.join(
                    os.path.dirname(__file__),
                    "..",
                    "..",
                    "resources",
                    "fonts",
                    "DejaVuSans-Bold.ttf",
                )
                if not os.path.exists(font_path):
                    raise IOError("Bundled font file not found.")
                font = ImageFont.truetype(font_path, font_size)
            except IOError:
                self.logger.warning(
                    f"Could not load the bundled font at {font_path}. "
                    "Falling back to default bitmap font. Text quality will be poor. "
                    "Ensure the font file exists."
                )
                font = ImageFont.load_default()

            # Use textbbox (or textsize for older PIL versions)
            # draw.textbbox is preferred
            try:
                text_bbox = draw.textbbox((0, 0), text, font=font)
                text_width = text_bbox[2] - text_bbox[0]
                text_height = text_bbox[3] - text_bbox[1]
            except (
                AttributeError
            ):  # Fallback for older PIL where textbbox might not exist
                text_width, text_height = draw.textsize(text, font=font)

            # Position the text based on configuration
            position = self.config.get("overlay_position", "bottom_center")
            padding = 20  # Padding around text

            x, y = 0, 0
            if "center" in position:
                x = (image.width - text_width) / 2
            elif "left" in position:
                x = padding
            elif "right" in position:
                x = image.width - text_width - padding

            if "bottom" in position:
                y = image.height - text_height - padding
            elif "top" in position:
                y = padding
            elif "center" in position:  # Vertical center
                y = (image.height - text_height) / 2

            # Create a transparent layer for the background
            overlay = Image.new("RGBA", image.size, (255, 255, 255, 0))
            draw_overlay = ImageDraw.Draw(overlay)

            # Draw semi-transparent background
            draw_overlay.rectangle(
                (
                    x - padding / 2,
                    y - padding / 2,
                    x + text_width + padding / 2,
                    y + text_height + padding / 2,
                ),
                fill=bg_color,
            )

            image = Image.alpha_composite(image, overlay)  # Composite the background
            draw = ImageDraw.Draw(image)  # Re-get draw object for updated image

            draw.text((x, y), text, font=font, fill=text_color)

            # Save the modified image
            new_image_path = image_path.replace(".jpeg", "-overlay.jpeg")
            image.convert("RGB").save(new_image_path)
            return new_image_path
        except Exception as e:
            self.logger.error(f"Failed to add text overlay to image: {e}")
            return image_path

    def generate_featured_image(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Finds and saves the featured image for the article from Pexels.
        """
        if not self.pexels_client:
            self.logger.warning(
                "Pexels client not initialized. Cannot generate featured image."
            )
            return None, 0.0

        search_query = opportunity["keyword"]
        self.logger.info(f"Searching Pexels for featured image for '{search_query}'...")

        pexels_photos, cost = self.pexels_client.search_photos(
            query=search_query, orientation="landscape", size="large", per_page=1
        )

        if not pexels_photos:
            self.logger.warning(
                f"No suitable featured image found on Pexels for '{search_query}'."
            )
            return None, cost

        best_photo = pexels_photos[0]
        best_photo_url = (
            best_photo["src"].get("landscape")
            or best_photo["src"].get("large2x")
            or best_photo["src"].get("original")
        )

        if not best_photo_url:
            self.logger.warning(
                f"Pexels photo found, but no usable URL for '{search_query}'."
            )
            return None, cost

        image_dir = "generated_images"
        os.makedirs(image_dir, exist_ok=True)
        file_path = os.path.join(
            image_dir,
            f"pexels-featured-{utils.slugify(search_query)}-{best_photo['id']}.jpeg",
        )

        local_path = download_image_from_url(best_photo_url, file_path)

        if not local_path:
            self.logger.error(
                f"Failed to download featured image from Pexels: {best_photo_url}"
            )
            return None, cost

        # Add text overlay
        meta_title = opportunity.get("ai_content", {}).get(
            "meta_title", opportunity["keyword"]
        )
        local_path = self._add_text_overlay(local_path, meta_title)

        self.logger.info(
            f"Successfully sourced featured image from Pexels: {local_path}"
        )
        return {
            "type": "featured",
            "search_query": search_query,
            "local_path": local_path,
            "remote_url": best_photo_url,  # Store Pexels URL directly
            "alt_text": best_photo["alt"],
            "source_id": best_photo["id"],
            "source": "Pexels",
        }, cost

    def _simplify_prompt_for_pexels(self, descriptive_prompt: str) -> str:
        """
        Uses an LLM to extract 3-5 high-impact keywords suitable for a stock photo search
        from a more descriptive AI image prompt.
        """
        if not descriptive_prompt or not self.openai_client:
            return descriptive_prompt  # Fallback to original if no client or prompt

        self.logger.info(
            f"Refining image prompt for Pexels search: '{descriptive_prompt}'"
        )

        prompt_messages = [
            {
                "role": "system",
                "content": "You are a concise keyword extractor for stock photo sites. Extract 3-5 key nouns, adjectives, or short phrases from the user's descriptive image prompt that would be most effective for searching a stock photo library like Pexels. Return only a comma-separated list of keywords.",
            },
            {"role": "user", "content": f"Descriptive prompt: '{descriptive_prompt}'"},
        ]

        # Use a low temperature for predictable, factual output
        extracted_keywords_str, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
                            model=self.config.get("default_model", "gpt-5-nano"),  # Use a cost-effective model for this            temperature=0.1,
            max_completion_tokens=50,  # Keep output very short
            schema={
                "name": "extract_keywords",
                "type": "object",
                "properties": {
                    "keywords": {
                        "type": "string",
                        "description": "Comma-separated keywords.",
                    }
                },
                "required": ["keywords"],
                "additionalProperties": False
            },
        )

        if error or not extracted_keywords_str:
            self.logger.warning(
                f"Failed to extract keywords for Pexels. Falling back to original prompt. Error: {error}"
            )
            return descriptive_prompt  # Fallback to original prompt

        # The AI should return a dictionary with a 'keywords' key
        if (
            isinstance(extracted_keywords_str, dict)
            and "keywords" in extracted_keywords_str
        ):
            return extracted_keywords_str["keywords"]
        elif isinstance(
            extracted_keywords_str, str
        ):  # Fallback if AI doesn't follow schema perfectly
            return extracted_keywords_str

        return descriptive_prompt  # Final fallback

    # In class ImageGenerator, replace the generate_images_from_prompts method
    def generate_images_from_prompts(
        self, prompts: List[str]
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Finds and saves in-article images from Pexels based on a list of specific prompts.
        """
        if not self.pexels_client:
            self.logger.warning(
                "Pexels client not initialized. Cannot generate images from prompts."
            )
            return [], 0.0

        images_data = []
        total_cost = 0.0

        for i, prompt in enumerate(prompts):
            search_query = self._simplify_prompt_for_pexels(prompt)
            self.logger.info(
                f"Searching Pexels for in-article image with simplified query: '{search_query}' (from prompt: '{prompt}')..."
            )

            pexels_photos, cost = self.pexels_client.search_photos(
                query=search_query, orientation="landscape", size="large", per_page=1
            )
            total_cost += cost

            if pexels_photos:
                photo = pexels_photos[0]
                photo_url = photo["src"].get("large") or photo["src"].get("original")

                if photo_url:
                    image_dir = "generated_images"
                    os.makedirs(image_dir, exist_ok=True)
                    file_path = os.path.join(
                        image_dir,
                        f"pexels-in-article-{utils.slugify(search_query)}-{photo['id']}.jpeg",
                    )
                    local_path = download_image_from_url(photo_url, file_path)

                    if local_path:
                        images_data.append(
                            {
                                "type": f"in_article_{i + 1}",
                                "search_query": search_query,
                                "original_prompt": prompt,
                                "local_path": local_path,
                                "remote_url": photo_url,
                                "alt_text": photo.get("alt") or prompt,
                                "source_id": photo["id"],
                                "source": "Pexels",
                            }
                        )
                        self.logger.info(
                            f"Successfully sourced in-article image from Pexels: {local_path}"
                        )
                        continue

            self.logger.warning(
                f"Could not find a suitable Pexels image for prompt: '{prompt}'."
            )

        return images_data, total_cost
```

## File: agents/internal_linking_suggester.py
```python
import logging
from typing import Dict, Any, List, Tuple

from backend.external_apis.openai_client import OpenAIClientWrapper
from backend.data_access.database_manager import DatabaseManager


class InternalLinkingSuggester:
    def __init__(
        self,
        openai_client: OpenAIClientWrapper,
        config: Dict[str, Any],
        db_manager: DatabaseManager,
    ):
        self.openai_client = openai_client
        self.config = config
        self.db_manager = db_manager
        self.logger = logging.getLogger(self.__class__.__name__)

    def suggest_links(
        self,
        article_text: str,
        key_entities: List[str],
        target_domain: str,
        client_id: str,
    ) -> Tuple[List[Dict[str, str]], float]:
        existing_articles = self._fetch_existing_articles(client_id)
        if not existing_articles:
            return [], 0.0

        prompt_messages = self._build_suggestion_prompt(
            article_text, key_entities, existing_articles
        )

        schema = {
            "name": "suggest_contextual_internal_links",
            "type": "object",
            "properties": {
                "internal_links": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "anchor_text": {
                                "type": "string",
                                "description": "The exact, natural phrase from the paragraph to be used as anchor text.",
                            },
                            "url": {
                                "type": "string",
                                "description": "The corresponding relative URL from the available articles list.",
                            },
                            "context_paragraph_text": {
                                "type": "string",
                                "description": "The full, exact text of the paragraph where the anchor text was found. This will be used to pinpoint the link location.",
                            },
                        },
                        "required": ["anchor_text", "url", "context_paragraph_text"],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["internal_links"],
            "additionalProperties": False,
        }

        response, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
        )

        if error or not response:
            self.logger.error(
                f"Failed to get contextual internal linking suggestions from AI: {error}"
            )
            return [], self.openai_client.latest_cost

        return response.get("internal_links", []), self.openai_client.latest_cost

    def _fetch_existing_articles(self, client_id: str) -> List[Dict[str, str]]:
        """
        Fetches a list of already 'generated' articles for the given client from the local DB.
        """
        self.logger.info(
            f"Fetching existing published articles for internal linking for client: {client_id}."
        )

        if not self.config.get("enable_automated_internal_linking", False):
            self.logger.info(
                "Automated internal linking is disabled by client configuration."
            )
            return []

        existing_articles = self.db_manager.get_published_articles_for_linking(
            client_id
        )

        if not existing_articles:
            self.logger.warning(
                f"No existing 'published' articles found for client '{client_id}' to use for internal linking suggestions."
            )
            return []

        self.logger.info(
            f"Found {len(existing_articles)} published articles for internal linking."
        )
        return existing_articles

    def _build_suggestion_prompt(
        self,
        linking_context: str,
        key_entities: List[str],
        existing_articles: List[Dict[str, str]],
    ) -> List[Dict[str, str]]:
        """Builds the prompt for context-aware AI internal linking suggestion."""

        existing_articles_text = "\n".join(
            [
                f'- Title: "{article["title"]}", Relative URL: {article["url"]}'
                for article in existing_articles
            ]
        )

        prompt = f"""
        You are an expert SEO strategist. Your task is to analyze a new blog post and identify the most semantically relevant opportunities to link to existing articles on the site.

        **Main Article Text (as HTML):**
        ```html
        {linking_context}
        ```

        **Available Published Articles to Link To:**
        {existing_articles_text}

        **Instructions:**
        1. Read the main article HTML thoroughly to understand its context and structure.
        2. For each available published article, identify the single BEST paragraph in the main article to place a link. The best location is one that is highly contextually and semantically related to the title of the published article.
        3. From that best location, extract a natural, compelling phrase of 3-7 words to use as the anchor text.
        4. Suggest a maximum of 3-5 of the most relevant internal links.
        5. Return your suggestions in the required JSON format. For 'context_paragraph_text', provide the full, clean text content of the paragraph where the link should be placed.
        """
        return [{"role": "user", "content": prompt}]
```

## File: agents/prompt_assembler.py
```python
import logging
from typing import Dict, Any, List

from backend.data_access.database_manager import DatabaseManager


class DynamicPromptAssembler:
    def __init__(self, db_manager: DatabaseManager):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.db_manager = db_manager

    def build_prompt(self, opportunity: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        Constructs the detailed prompt for the article generation AI using a safe,
        user-configurable template, now with rich SERP data.
        """
        blueprint = opportunity.get("blueprint", {})
        brief = blueprint.get("ai_content_brief", {})
        strategy = blueprint.get("recommended_strategy", {})
        client_cfg = opportunity.get("client_cfg", {})
        num_images = client_cfg.get("num_in_article_images", 2)

        template = client_cfg.get("custom_prompt_template")
        if not template or not template.strip():
            template = """Write a comprehensive, helpful, and expert-level blog post on [TOPIC]. The article must demonstrate first-hand experience and deep expertise. Structure the content for maximum readability and SEO impact. The post must:

        - Be approximately [WORD_COUNT] words, providing authoritative depth on the topic.
        - Target the primary keyword "[PRIMARY KEYWORD]" and naturally incorporate related LSI keywords: [LSI/secondary keywords], along with relevant entities, synonyms, and contextually related concepts to ensure topical completeness.
        - **Demonstrate E-E-A-T (Experience, Expertise, Authoritativeness, Trustworthiness):**
            - Start with a clear, direct 1-2 sentence summary that immediately answers the user's core question.
            - Write from a first-person or expert perspective. Include at least one hypothetical scenario, relatable anecdote, or personal insight to signal direct experience.
            - Cite specific data or statistics and attribute them to a source (e.g., 'According to a 2023 study by...').
            - Include multiple answer formats: short direct responses, step-by-step instructions, and quick takeaway lists (bullet points) so AI models and users can easily extract information.
        - Structure the article with a logical flow using clear subheadings (H2s and H3s).
        - Include a "Frequently Asked Questions" (FAQ) section at the end using real-world questions users search for, written in a conversational Q&A style.
        - Naturally promote [CTA_URL] with a relevant call-to-action at the end of the post.
        - **AVOID:** Do not use generic filler, over-optimization, or unnatural keyword stuffing. Focus on topical relevance, not keyword density. Avoid making unsubstantiated claims.
        """

        default_cta_url = client_cfg.get(
            "default_cta_url", "https://profitparrot.com/contact/"
        )
        recommended_word_count = brief.get("target_word_count", 1000)
        expert_persona_from_brief = brief.get(
            "target_audience_persona",
            client_cfg.get("expert_persona", "an expert writer"),
        )
        replacements = {
            "[TOPIC]": brief.get("target_keyword", "the topic"),
            "[PRIMARY KEYWORD]": brief.get("target_keyword", "the topic"),
            "[LSI/secondary keywords]": ", ".join(brief.get("lsi_keywords", [])),
            "[WORD_COUNT]": str(recommended_word_count),
            "[CTA_URL]": default_cta_url,
            "%%NUM_IMAGES%%": str(num_images),
            "[PERSONA]": expert_persona_from_brief,
        }

        base_instructions = template
        for placeholder, value in replacements.items():
            base_instructions = base_instructions.replace(placeholder, str(value))

        persona = brief.get("target_audience_persona", "General audience")
        if "expert" in persona.lower() or "planner" in persona.lower():
            readability_instruction = "The tone must be highly sophisticated and authoritative. Maintain a Flesch-Kincaid Grade level of 10 or higher."
        elif "general" in persona.lower() or "beginner" in persona.lower():
            readability_instruction = "The tone must be clear and accessible. Maintain a Flesch-Kincaid Grade level between 7 and 9."
        else:
            readability_instruction = ""

        if readability_instruction:
            base_instructions += (
                f"\n- **Readability Target:** {readability_instruction}"
            )

        base_instructions += f"\n- Adopt the persona of {expert_persona_from_brief}."
        base_instructions += "\n- Include 1-2 'Expert Tips' in blockquotes."

        client_knowledge_base = client_cfg.get("client_knowledge_base")
        if client_knowledge_base and client_knowledge_base.strip():
            base_instructions += "\n\n**CLIENT KNOWLEDGE BASE (CRITICAL CONTEXT):**\n"
            base_instructions += "The following information is crucial for content accuracy and brand alignment. Incorporate relevant details naturally and factually:\n"
            base_instructions += f"{client_knowledge_base}\n"
            base_instructions += "Prioritize accuracy based on this knowledge base.\n"

        base_instructions += "\n- If the content contains data suitable for a table (e.g., comparisons, specifications, statistics), format it as a Markdown table."
        base_instructions += "\n- Include one link to a non-competing, high-authority external resource to back up a key statistic or claim."
        base_instructions += "\n- For in-article images, use a placeholder with the exact format `[[IMAGE: <A descriptive prompt for the image>]]`. For example: `[[IMAGE: A bar chart showing SEO growth over time]]`."

        # --- START MODIFICATION ---
        # NEW: Add instructions based on rich SERP data
        dynamic_serp_data_instructions = []

        if brief.get("knowledge_graph_facts"):
            facts_str = "\n- ".join(brief["knowledge_graph_facts"])
            dynamic_serp_data_instructions.append(
                f"**CRITICAL:** Incorporate these verified facts from Google's Knowledge Graph directly into the article to ensure factual accuracy and boost E-A-T:\n- {facts_str}"
            )

        if brief.get("paid_ad_copy"):
            paid_titles = [ad["title"] for ad in brief["paid_ad_copy"]]
            paid_descriptions = [ad["description"] for ad in brief["paid_ad_copy"]]
            dynamic_serp_data_instructions.append(
                f"**HIGH PRIORITY:** Analyze the following top paid ad copy to understand high-conversion language, primary value propositions, and key pain points. Use these insights to craft compelling article headlines, the introduction, and calls-to-action:\n- Titles: {paid_titles}\n- Descriptions: {paid_descriptions}"
            )

        if brief.get("top_organic_sitelinks"):
            sitelinks_str = "\n- ".join(brief["top_organic_sitelinks"])
            dynamic_serp_data_instructions.append(
                f"**MANDATORY:** Include dedicated sections (H2s or H3s) covering the following high-priority subtopics identified from competitor sitelinks:\n- {sitelinks_str}"
            )

        if brief.get("top_organic_faqs"):
            faqs_str = "\n- ".join(brief["top_organic_faqs"])
            dynamic_serp_data_instructions.append(
                f"**MANDATORY:** Create a dedicated 'Frequently Asked Questions' section (as an H2) at the end of the article, using these exact questions from competitor FAQ snippets:\n- {faqs_str}"
            )

        if brief.get("ai_overview_sources"):
            sources_str = "\n- ".join(brief["ai_overview_sources"])
            dynamic_serp_data_instructions.append(
                f"**STRATEGIC:** Give analytical priority to concepts and insights derived from these authoritative sources used by Google's own AI Overview:\n- {sources_str}"
            )

        if brief.get("discussion_snippets"):
            snippets_str = "\n- ".join(brief["discussion_snippets"])
            dynamic_serp_data_instructions.append(
                f"**TONE & EXPERIENCE:** Analyze the tone, specific pain points, and real-world language from these discussion snippets. Infuse the article with a personal, experience-driven, and authentic voice to directly address user concerns:\n- {snippets_str}"
            )

        # Append to dynamic instructions list
        dynamic_instructions = [
            f"**Primary Content Format:** Your output should be a '{strategy.get('content_format', 'Comprehensive Article')}'.",
            f"**Strategic Goal:** {strategy.get('strategic_goal', '')}",
        ]
        if brief.get("dynamic_serp_instructions"):
            dynamic_instructions.append(
                "**Tactical Guidance:**\n"
                + "\n".join(
                    [f"- {inst}" for inst in brief["dynamic_serp_instructions"]]
                )
            )

        # Combine all dynamic instructions
        all_dynamic_instructions = dynamic_instructions + dynamic_serp_data_instructions
        dynamic_instructions_str = "\n- ".join(all_dynamic_instructions)

        final_prompt_content = f"""You are an expert SEO writer. Generate a complete blog post package in JSON format based on the brief below.

        **Topic:** "{brief.get("target_keyword")}"
        **Core Instructions:**
        {base_instructions}
        **Dynamic Strategic Instructions:**
        - {dynamic_instructions_str}
        **Mandatory Information & Structure:**
        - **WORD COUNT: The final article body MUST be AT LEAST {recommended_word_count} words.** This is a strict requirement.
        - To meet the word count, elaborate on each of the following sections, providing detailed explanations, examples, and insights.
        - Must include sections on: {", ".join(blueprint.get("content_intelligence", {}).get("common_headings_to_cover", ["N/A"]))}
        - Must explore these unique angles: {", ".join(brief.get("unique_angles_to_cover", ["N/A"]))}
        - Mention these key entities: {", ".join(brief.get("key_entities_to_mention", ["N/A"]))}

        Generate a single, valid JSON object with three keys: "article_body_html", "meta_title", and "meta_description". The "article_body_html" must be well-structured HTML."""
        # --- END MODIFICATION ---

        feedback_examples_text = ""
        feedback_data = self.db_manager.get_content_feedback_examples(
            opportunity.get("client_id")
        )
        if feedback_data.get("good_examples") or feedback_data.get("bad_examples"):
            feedback_examples_text = "\n\n**Style Guide based on Past Feedback:**\n"
            if feedback_data.get("good_examples"):
                feedback_examples_text += (
                    "- **DO:** Emulate the style of these highly-rated articles:\n"
                )
                for ex in feedback_data["good_examples"]:
                    feedback_examples_text += f"  - Title: '{ex['keyword']}'. User Feedback: '{ex['comments']}' (Rated {ex['rating']}/5)\n"
            if feedback_data.get("bad_examples"):
                feedback_examples_text += "- **AVOID:** Avoid the issues found in these poorly-rated articles:\n"
                for ex in feedback_data["bad_examples"]:
                    feedback_examples_text += f"  - Title: '{ex['keyword']}'. User Feedback: '{ex['comments']}' (Rated {ex['rating']}/5)\n"

        system_message = "You are an expert SEO content strategist. Your output must be a single, valid JSON object."
        final_prompt_content = final_prompt_content + feedback_examples_text

        return [
            {"role": "system", "content": system_message},
            {"role": "user", "content": final_prompt_content},
        ]

    def flatten_prompt_for_display(self, messages: List[Dict[str, str]]) -> str:
        """Flattens the structured prompt messages into a single string for UI preview."""
        # ... (existing method, no change needed) ...
```

## File: agents/social_media_crafter.py
```python
# agents/social_media_crafter.py
import logging
from typing import Dict, Any, Tuple, Optional, List

from bs4 import BeautifulSoup

from backend.external_apis.openai_client import OpenAIClientWrapper


class SocialMediaCrafter:
    """
    AI agent for crafting social media posts based on the generated article.
    """

    def __init__(self, openai_client: OpenAIClientWrapper, config: Dict[str, Any]):
        self.openai_client = openai_client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def craft_posts(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[Optional[List[Dict[str, Any]]], float]:
        """
        Generates social media blurbs for different platforms.
        """
        prompt_messages = self._build_crafting_prompt(opportunity)

        schema = {
            "type": "object",
            "properties": {
                "social_media_posts": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "platform": {
                                "type": "string",
                                "description": "The social media platform (e.g., 'Twitter', 'LinkedIn').",
                            },
                            "content": {
                                "type": "string",
                                "description": "The content of the social media post.",
                            },
                        },
                        "required": ["platform", "content"],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["social_media_posts"],
            "additionalProperties": False,
        }

        response, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
        )

        # Get the actual cost from the client after the API call
        cost = self.openai_client.latest_cost

        if error or not response:
            self.logger.error(f"Failed to craft social media posts: {error}")
            return None, cost  # Return the actual cost even on failure

        return response.get("social_media_posts"), cost

    def _build_crafting_prompt(
        self, opportunity: Dict[str, Any]
    ) -> list[Dict[str, str]]:
        """Constructs the prompt for the social media crafting AI."""
        ai_content = opportunity.get("ai_content", {})
        article_title = ai_content.get("meta_title", "Untitled")
        article_summary = ai_content.get("meta_description", "No summary available.")
        article_body_html = ai_content.get(
            "article_body_html", ""
        )  # NEW: Get full HTML for parsing

        client_cfg = opportunity.get("client_cfg", {})
        platforms = ", ".join(client_cfg.get("platforms", ["Twitter", "LinkedIn"]))

        # --- Extract key information from the generated article's HTML ---
        soup = BeautifulSoup(article_body_html, "html.parser")

        h1_tag = soup.find("h1")
        # W17 FIX: Ensure h1 extraction is guarded, falling back to a modified title if necessary
        h1_text = (
            h1_tag.get_text(strip=True)
            if h1_tag
            else f"The Article on {article_title.replace(':', ' - ')}"
        )

        # W17 FIX: Extract H2s robustly, filtering out any empty tags
        h2_texts = [
            h.get_text(strip=True)
            for h in soup.find_all("h2")
            if h.get_text(strip=True)
        ]

        key_entities = (
            opportunity.get("blueprint", {})
            .get("ai_content_brief", {})
            .get("key_entities_to_mention", [])
        )

        # --- Social Media Tag Analysis (existing logic) ---
        competitor_social_tags = []
        for competitor in opportunity.get("blueprint", {}).get(
            "competitor_analysis", []
        )[:3]:
            if competitor.get("social_media_tags"):
                tags = competitor["social_media_tags"]
                key_tags = {
                    "og:title": tags.get("og:title"),
                    "og:description": tags.get("og:description"),
                    "twitter:title": tags.get("twitter:title"),
                    "twitter:description": tags.get("twitter:description"),
                }
                competitor_social_tags.append(
                    {
                        "url": competitor["url"],
                        "tags": {k: v for k, v in key_tags.items() if v},
                    }
                )

        competitor_examples = ""
        if competitor_social_tags:
            competitor_examples = (
                "\n**Competitor Social Media Examples (for inspiration):**\n"
            )
            for item in competitor_social_tags:
                competitor_examples += f"- For {item['url']}:\n"
                for tag, value in item["tags"].items():
                    competitor_examples += f"  - {tag}: {value}\n"

        prompt = f"""
        You are a social media marketing expert. Your task is to create engaging social media posts to promote a new blog article.

        **Article Details:**
        - **Primary Headline (H1):** {h1_text} # Now guaranteed to be populated
        - **Key Sections (H2s):** {", ".join(h2_texts[:5]) if h2_texts else "N/A (No subheadings found)"} # Now guaranteed text if available
        - **Summary:** {article_summary}
        - **Key Entities/Topics:** {", ".join(key_entities) if key_entities else "N/A"}
        - **Link (use placeholder):** [LINK]
        {competitor_examples}
        **Instructions:**
        1.  Create a unique post for each of the following platforms: {platforms}.
        2.  Analyze the competitor examples to understand how they frame their content on social media.
        3.  Tailor the tone and length of each post to the specific platform.
        4.  Include relevant hashtags.
        5.  End each post with a call to action and the [LINK] placeholder.
        6.  Ensure posts directly reference information present in the article's headlines and key entities.

        Provide the output in the required JSON format.
        """
        return [{"role": "user", "content": prompt}]
```

## File: agents/summary_generator.py
```python
import logging
from typing import Dict, Any


class SummaryGenerator:
    """
    Generates a human-readable strategic summary of a keyword opportunity.
    """

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def generate_summary(self, opportunity: Dict[str, Any]) -> str:
        """Builds a narrative summary based on the opportunity's data."""
        full_data = opportunity.get("full_data", {})
        score_breakdown = full_data.get("score_breakdown", {})

        # Check qualification status first
        quality_status = full_data.get("quality_status", "passed")
        cannibal_status = full_data.get("cannibalization_status", "passed")
        if quality_status == "failed" or cannibal_status == "failed":
            reasons = ", ".join(full_data.get("reasons", ["Unknown reason"]))
            return f"**Disqualified:** This keyword was filtered out. Reason(s): {reasons}."

        # Build summary for qualified keywords using the new breakdown
        intent = full_data.get("search_intent_info", {}).get("main_intent", "unknown")
        summary_parts = [f"This is a promising **{intent.upper()}** opportunity."]

        ease_of_ranking_score = score_breakdown.get("ease_of_ranking", {}).get(
            "score", 0
        )
        if ease_of_ranking_score < 60:
            summary_parts.append(
                "However, the SERP shows some competitive challenges that will require a strong article."
            )
        elif ease_of_ranking_score < 85:
            summary_parts.append(
                "The competitive landscape appears favorable, with weaknesses to exploit."
            )
        else:  # 85+
            summary_parts.append(
                "The competitive landscape appears **extremely favorable**, with clear technical and content weaknesses among top competitors."
            )

        traffic_score = score_breakdown.get("traffic_potential", {}).get("score", 0)
        if traffic_score > 70:
            summary_parts.append("It has **excellent traffic potential**.")
        elif traffic_score > 40:
            summary_parts.append("It has solid traffic potential.")

        commercial_score = score_breakdown.get("commercial_intent", {}).get("score", 0)
        if commercial_score > 85:
            summary_parts.append(
                "The keyword shows **strong commercial value**, making it a high-priority target."
            )
        elif commercial_score > 60:
            summary_parts.append("It has good commercial value.")

        return " ".join(summary_parts)

    def generate_score_narrative(self, score_breakdown: Dict[str, Any]) -> str:
        narrative_parts = []
        if not score_breakdown:
            return "No score breakdown available to generate a narrative."

        # Ease of Ranking
        ease_score = score_breakdown.get("ease_of_ranking", {}).get("score", 0)
        if ease_score >= 80:
            narrative_parts.append(
                "Ranks highly due to a **very weak competitive landscape**."
            )
        elif ease_score >= 60:
            narrative_parts.append(
                "Has a good chance to rank because of a **favorable competitive landscape**."
            )
        else:
            narrative_parts.append(
                "Faces **strong competition**, making it a challenging keyword to rank for."
            )

        # Traffic Potential
        traffic_score = score_breakdown.get("traffic_potential", {}).get("score", 0)
        if traffic_score >= 75:
            narrative_parts.append("It has **excellent traffic potential**.")
        elif traffic_score >= 50:
            narrative_parts.append("It has **solid traffic potential**.")
        else:
            narrative_parts.append("Its traffic potential is moderate.")

        # Commercial Intent
        commercial_score = score_breakdown.get("commercial_intent", {}).get("score", 0)
        if commercial_score >= 80:
            narrative_parts.append("The keyword shows **strong commercial value**.")
        elif commercial_score >= 50:
            narrative_parts.append("It has **good commercial value**.")

        return "Overall: " + " ".join(narrative_parts)
```

## File: api/routers/auth.py
```python
from fastapi import APIRouter, HTTPException
from ..models import LoginRequest

router = APIRouter()


@router.post("/auth/login")
async def login(request: LoginRequest):
    """
    Dummy login endpoint for development.
    In a real app, verify a hashed password against a user database.
    """
    DUMMY_PASSWORD = "password123"
    if request.password == DUMMY_PASSWORD:
        dummy_user = {"username": "admin", "email": "admin@example.com"}
        dummy_token = "dummy-secret-token"
        return {"user": dummy_user, "token": dummy_token}
    else:
        raise HTTPException(status_code=401, detail="Incorrect password")


@router.post("/auth/logout")
async def logout():
    """Dummy logout endpoint."""
    return {"message": "Logged out successfully"}
```

## File: api/routers/client_settings.py
```python
# api/routers/client_settings.py
# NEW FILE
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from ..models import ClientSettings  # Assuming a Pydantic model exists
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()


@router.get("/settings/{client_id}", response_model=ClientSettings)
async def get_client_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    settings = db.get_client_settings(client_id)
    if not settings:
        raise HTTPException(
            status_code=404, detail="Settings not found for this client."
        )
    return settings


@router.put("/settings/{client_id}", response_model=Dict[str, str])
async def update_client_settings_endpoint(
    client_id: str,
    settings: ClientSettings,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        db.update_client_settings(client_id, settings.dict())
        return {"message": "Settings updated successfully."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## File: api/routers/clients.py
```python
import logging
from pydantic import BaseModel
from typing import List
from fastapi import APIRouter, Depends, HTTPException
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from .. import globals as api_globals
from backend.pipeline import WorkflowOrchestrator


class NewClientRequest(BaseModel):
    client_id: str
    client_name: str


router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/clients")
async def get_all_clients(db: DatabaseManager = Depends(get_db)):
    logger.info("Received request for /clients")
    clients = db.get_clients()
    logger.info(f"Found clients: {clients}")
    if not clients:
        return []
    return clients


@router.get("/clients/{client_id}/settings")
async def get_client_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    settings = db.get_client_settings(client_id)
    if not settings:
        raise HTTPException(
            status_code=404, detail=f"Settings not found for client '{client_id}'"
        )
    return settings


@router.get("/clients/{client_id}/dashboard-stats")
async def get_dashboard_stats_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    stats = db.get_dashboard_stats(client_id)
    if not stats:
        raise HTTPException(
            status_code=404, detail=f"Stats not found for client '{client_id}'"
        )
    return stats


@router.get("/clients/{client_id}/dashboard")
async def get_dashboard_data_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to fetch aggregated data for the main dashboard."""
    logger.info(f"Dashboard endpoint called for client: {client_id}")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        logger.info("Fetching dashboard data from database...")
        dashboard_data = db.get_dashboard_data(client_id)
        logger.info("Successfully fetched dashboard data.")
        if not dashboard_data:
            logger.warning(f"No dashboard data found for client {client_id}")
            raise HTTPException(
                status_code=404, detail=f"Dashboard data not found for client '{client_id}'"
            )
        logger.info("Returning dashboard data.")
        return dashboard_data
    except Exception as e:
        logger.error(f"Error fetching dashboard data: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal Server Error")


@router.get("/clients/{client_id}/processed-keywords")
async def get_processed_keywords_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Retrieves all processed keywords for a client to prevent duplicates."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    keywords = db.get_all_processed_keywords_for_client(client_id)
    return keywords


@router.post("/clients/{client_id}/check-keywords")
async def check_existing_keywords_endpoint(
    client_id: str,
    keywords: List[str],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Checks a batch of keywords and returns which ones already exist."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    existing = db.check_existing_keywords(client_id, keywords)
    return {"existing_keywords": existing}


# ADD the new endpoint to the router:
@router.get("/clients/{client_id}/search-all-assets")
async def search_all_assets_endpoint(
    client_id: str,
    query: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    logger.info(
        f"Received search-all-assets request for client {client_id} with query: '{query}'"
    )
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    if len(query) < 3:
        return []

    results = []

    # Search Opportunities
    opportunities = db.search_opportunities(client_id, query)
    for opp in opportunities:
        results.append({"id": opp["id"], "name": opp["keyword"], "type": "opportunity"})

    # Search Discovery Runs (by seed keywords or run status/error)
    discovery_runs = db.search_discovery_runs(client_id, query)
    for run in discovery_runs:
        seed_keywords_str = ", ".join(
            run.get("parameters", {}).get("seed_keywords", [])
        )
        results.append(
            {
                "id": run["id"],
                "name": f"Discovery Run #{run['id']}: {seed_keywords_str[:50]}...",
                "type": "discovery_run",
            }
        )

    # Deduplicate results by type-id if necessary (optional, but good practice)
    unique_results = {}
    for item in results:
        key = f"{item['type']}-{item['id']}"
        if key not in unique_results:
            unique_results[key] = item

    return list(unique_results.values())


@router.get("/clients/{client_id}/opportunities/high-priority")
async def get_high_priority_opportunities_endpoint(
    client_id: str,
    limit: int = 5,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Retrieves a short list of the highest-scored, validated opportunities."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    opportunities = db.get_high_priority_opportunities(client_id, limit)
    return opportunities


@router.post("/clients")
async def add_new_client(
    request: NewClientRequest, db: DatabaseManager = Depends(get_db)
):
    """Adds a new client to the database and initializes their settings."""
    try:
        # Ensure the config manager is available
        if not api_globals.config_manager:
            raise HTTPException(
                status_code=500, detail="Configuration manager not initialized."
            )

        default_settings = (
            api_globals.config_manager.get_default_client_settings_template()
        )

        success = db.add_client(
            client_id=request.client_id,
            client_name=request.client_name,
            default_settings=default_settings,
        )

        if success:
            return {"message": "Client added successfully."}
        else:
            # This could happen if the client_id already exists (IntegrityError)
            raise HTTPException(
                status_code=409,
                detail=f"Client with ID '{request.client_id}' already exists.",
            )
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.error(
            f"Error adding new client '{request.client_name}': {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail="Failed to add new client.")
```

## File: api/routers/discovery.py
```python
import logging
from fastapi import APIRouter, Depends, HTTPException
from data_access.database_manager import DatabaseManager
from backend.pipeline import WorkflowOrchestrator
from services.discovery_service import DiscoveryService
from ..dependencies import get_db, get_orchestrator, get_discovery_service
from ..models import (
    JobResponse,
    DiscoveryRunRequest,
)  # Ensure DiscoveryRunRequest is imported

# --- NEW IMPORTS AND MODELS FOR FRONTEND FEATURES ---
from typing import Dict


# --- END NEW IMPORTS AND MODELS ---

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/discovery/available-filters")
async def get_available_filters():
    """
    Returns a curated list of available discovery modes, filters, and sorting options,
    structured to be easily consumable by the frontend.
    """
    base_filters = [
        {
            "name": "search_volume",
            "label": "Search Volume",
            "type": "number",
            "operators": [">", "<", "="],
        },
        {
            "name": "keyword_difficulty",
            "label": "Keyword Difficulty",
            "type": "number",
            "operators": [">", "<", "="],
        },
        {
            "name": "main_intent",
            "label": "Search Intent",
            "type": "select",
            "options": ["informational", "navigational", "commercial", "transactional"],
            "operators": ["="],
        },
        {
            "name": "competition_level",
            "label": "Competition Level",
            "type": "select",
            "options": ["LOW", "MEDIUM", "HIGH"],
            "operators": ["="],
        },
        {"name": "cpc", "label": "CPC", "type": "number", "operators": [">", "<", "="]},
    ]

    base_sorting = [
        {"name": "search_volume", "label": "Search Volume"},
        {"name": "keyword_difficulty", "label": "Keyword Difficulty"},
        {"name": "cpc", "label": "CPC"},
        {"name": "competition", "label": "Competition"},
    ]

    def construct_paths(prefix, items):
        new_items = []
        for item in items:
            new_item = item.copy()
            if "search_volume" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.search_volume"
            elif "keyword_difficulty" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_properties.keyword_difficulty"
            elif "main_intent" in new_item["name"]:
                new_item["name"] = f"{prefix}search_intent_info.main_intent"
            elif "competition_level" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.competition_level"
            elif "cpc" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.cpc"
            elif "competition" in new_item["name"]:
                new_item["name"] = f"{prefix}keyword_info.competition"
            new_items.append(new_item)
        return new_items

    return [
        {
            "id": "keyword_ideas",
            "name": "Broad Market Exploration",
            "description": "Discover a wide range of foundational keywords related to your core topics. Ideal for initial research and uncovering new content pillars.",
            "filters": construct_paths("", base_filters),
            "sorting": [{"name": "relevance", "label": "Relevance"}]
            + construct_paths("", base_sorting),
        },
        {
            "id": "keyword_suggestions",
            "name": "Targeted Query Expansion",
            "description": "Generate specific, long-tail variations of your seed keywords. Perfect for finding niche opportunities and targeted article ideas.",
            "filters": construct_paths("", base_filters),
            "sorting": construct_paths("", base_sorting),
        },
        {
            "id": "related_keywords",
            "name": "Semantic & Competitor Analysis",
            "description": "Find semantically related terms and phrases that your competitors may be ranking for. Excellent for expanding content depth and authority.",
            "filters": construct_paths("keyword_data.", base_filters),
            "sorting": construct_paths("keyword_data.", base_sorting),
        },
        {
            "id": "find_questions",
            "name": "Find Questions",
            "description": "Discover question-based keywords (e.g., 'how to...', 'what is...') related to your core topics.",
            "filters": construct_paths("", base_filters),
            "sorting": construct_paths("", base_sorting),
        },
    ]


@router.post("/clients/{client_id}/discovery-runs-async", response_model=JobResponse)
async def start_discovery_run_async(
    client_id: str,
    request: DiscoveryRunRequest,
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
    discovery_service: DiscoveryService = Depends(get_discovery_service),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        filters = request.filters
        limit = request.limit or 1000
        discovery_modes = ["keyword_ideas", "keyword_suggestions", "related_keywords"]

        if limit <= 500:
            depth = 2
        elif limit <= 2000:
            depth = 3
        else:
            depth = 4

        parameters = {
            "seed_keywords": request.seed_keywords,
            "discovery_modes": discovery_modes,
            "filters": filters,
            "order_by": request.order_by,
            "filters_override": request.filters_override,
            "limit": limit,
            "depth": depth,
            "include_clickstream_data": request.include_clickstream_data,  # NEW
            "closely_variants": request.closely_variants,  # NEW
            "ignore_synonyms": request.ignore_synonyms,  # NEW
        }
        run_id = discovery_service.create_discovery_run(
            client_id=client_id, parameters=parameters
        )

        job_id = orchestrator.run_discovery_and_save(
            run_id,
            request.seed_keywords,
            discovery_modes,
            filters,
            request.order_by,
            request.filters_override,
            limit,
            depth,
            request.ignore_synonyms,
            request.include_clickstream_data,  # NEW
            request.closely_variants,  # NEW
        )
        return {"job_id": job_id, "message": f"Discovery run job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start discovery run for client {client_id}: {e}", exc_info=True
        )
        raise HTTPException(
            status_code=500, detail=f"Failed to start discovery run: {e}"
        )


@router.get("/clients/{client_id}/discovery-runs")
async def get_discovery_runs(
    client_id: str,
    page: int = 1,
    limit: int = 10,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    runs, total_count = db.get_all_discovery_runs_paginated(client_id, page, limit)
    if not runs:
        return {"items": [], "total_items": 0, "page": page, "limit": limit}
    return {"items": runs, "total_items": total_count, "page": page, "limit": limit}


# def calculate_discovery_cost(request: KeywordListRequest) -> Dict[str, Any]:

# # ... (existing setup) ...

#     # ... lines 141-143 unchanged

#     # item_cost = num_items * cost_per_item

#     # estimated_cost = task_cost + item_cost


#     # explanation = [

#     #     f"{num_tasks} tasks @ ${cost_per_task:.4f} each: ${task_cost:.4f}",

#     #     f"{num_items} items @ ${cost_per_item:.4f} each: ${item_cost:.4f}"

#     # ]

#     pass

# if request.include_clickstream_data:

#     estimated_cost *= 2

#     explanation.append("Cost multiplied by 2x due to 'include_clickstream_data' flag.")

# return {"estimated_cost": round(estimated_cost, 2), "explanation": explanation}


@router.post("/discovery-runs/rerun/{run_id}")
async def rerun_discovery_run(
    run_id: int,
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
    db: DatabaseManager = Depends(get_db),  # current_client_id dependency removed here
):
    """
    Initiates a new discovery run using the parameters from a previous run.
    """
    previous_run = db.get_discovery_run_by_id(run_id)
    if not previous_run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")

    # Authorization check within the function (using orchestrator's client_id)
    if (
        previous_run["client_id"] != orchestrator.client_id
    ):  # Use orchestrator's client_id
        raise HTTPException(
            status_code=403, detail="You do not have permission to re-run this job."
        )

    try:
        parameters = previous_run.get("parameters", {})
        seed_keywords = parameters.get("seed_keywords", [])
        filters = parameters.get("filters")
        order_by = parameters.get("order_by")
        filters_override = parameters.get("filters_override", {})
        limit = parameters.get("limit")
        depth = parameters.get("depth")

        if not seed_keywords:
            raise HTTPException(
                status_code=400, detail="No seed keywords found in the original run."
            )

        # Dynamic discovery logic based on limit
        limit = limit or 1000
        discovery_modes = ["keyword_ideas", "keyword_suggestions", "related_keywords"]

        if depth is None:
            if limit <= 500:
                depth = 2
            elif limit <= 2000:
                depth = 3
            else:
                depth = 4

        # Reconstruct parameters for the new run to be created
        new_run_parameters = {
            "seed_keywords": seed_keywords,
            "discovery_modes": discovery_modes,
            "filters": filters,
            "order_by": order_by,
            "filters_override": filters_override,
            "limit": limit,
            "depth": depth,
        }

        new_run_id = orchestrator.db_manager.create_discovery_run(
            client_id=previous_run["client_id"], parameters=new_run_parameters
        )
        job_id = orchestrator.run_discovery_and_save(
            new_run_id,
            seed_keywords,
            discovery_modes,
            filters,
            order_by,
            filters_override,
            limit,
            depth,
        )

        return {
            "job_id": job_id,
            "message": f"Re-run of job {run_id} started as new job {job_id}.",
        }
    except Exception as e:
        logger.error(f"Failed to re-run discovery run {run_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to start re-run: {e}")


@router.get("/discovery-runs/{run_id}")
async def get_discovery_run_by_id(
    run_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves a single discovery run by its ID.
    """
    run = db.get_discovery_run_by_id(run_id)
    if not run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")
    if run["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this discovery run.",
        )
    return run


@router.get("/discovery-runs/{run_id}/keywords")
async def get_run_keywords(
    run_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all keywords that were added to the database as part of a specific discovery run.
    """
    run = db.get_discovery_run_by_id(run_id)
    if not run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")
    if (
        run["client_id"] != orchestrator.client_id
    ):  # Use orchestrator's client_id for auth
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access these keywords.",
        )

    try:
        keywords = db.get_keywords_for_run(run_id)
        return keywords
    except Exception as e:
        logger.error(
            f"Failed to retrieve keywords for run {run_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=f"Failed to retrieve keywords: {e}")


@router.get("/discovery-runs/{run_id}/keywords/{reason}")
async def get_run_keywords_by_reason(
    run_id: int,
    reason: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all keywords that were added to the database as part of a specific discovery run
    that were disqualified for a specific reason.
    """
    run = db.get_discovery_run_by_id(run_id)
    if not run:
        raise HTTPException(status_code=404, detail="Discovery run not found.")
    if (
        run["client_id"] != orchestrator.client_id
    ):  # Use orchestrator's client_id for auth
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access these keywords.",
        )

    try:
        keywords = db.get_keywords_for_run_by_reason(run_id, reason)
        return keywords
    except Exception as e:
        logger.error(
            f"Failed to retrieve keywords for run {run_id} and reason {reason}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=f"Failed to retrieve keywords: {e}")


@router.get(
    "/discovery-runs/{run_id}/disqualification-reasons", response_model=Dict[str, int]
)
async def get_disqualification_reasons_endpoint(
    run_id: int, discovery_service: DiscoveryService = Depends(get_discovery_service)
):
    """
    Retrieves a summary of disqualification reasons for a specific discovery run.
    """
    logger.info(f"Received request for disqualification reasons for run {run_id}")
    reasons = discovery_service.get_disqualification_reasons(run_id)
    return reasons
```

## File: api/routers/jobs.py
```python
# api/routers/jobs.py

import logging
from fastapi import APIRouter, Depends, HTTPException
from jobs import JobManager
from ..dependencies import get_job_manager
from ..models import JobResponse

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/jobs/{job_id}", response_model=JobResponse)
async def get_job_status(
    job_id: str, job_manager: JobManager = Depends(get_job_manager)
):
    """
    Retrieves the status of a background job.
    """
    logger.info(f"Received request for job status for job_id: {job_id}")
    job = job_manager.get_job_status(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    return {
        "job_id": job["id"],
        "message": f"Status: {job['status']}",
        "status": job["status"],
        "progress": job["progress"],
        "result": job.get("result"),
        "error": job.get("error"),
        "progress_log": job.get("progress_log"),
    }
```

## File: api/routers/opportunities.py
```python
import logging
import bleach
import json
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException
from data_access.database_manager import DatabaseManager
from fastapi.concurrency import run_in_threadpool
from services.opportunities_service import OpportunitiesService
from ..dependencies import get_db, get_opportunities_service, get_orchestrator
from ..models import (
    OpportunityListResponse,
    ContentHistoryItem,
    RestoreRequest,
    SocialMediaPostsUpdate,
    ContentUpdatePayload,
)
from pydantic import BaseModel
from .. import globals as api_globals
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/settings/discovery-strategies", response_model=List[str])
async def get_discovery_strategies():
    """Returns the available discovery strategies from the global config."""
    strategies = api_globals.config_manager.get_global_config().get(
        "discovery_strategies", []
    )
    return strategies


class ContentFeedbackRequest(BaseModel):
    rating: int
    comments: Optional[str] = None


@router.get(
    "/clients/{client_id}/opportunities", response_model=OpportunityListResponse
)
async def get_all_opportunities_summary_endpoint(
    client_id: str,
    status: Optional[str] = None,
    keyword: Optional[str] = None,
    page: int = 1,
    limit: int = 20,
    sort_by: str = "date_added",
    sort_direction: str = "desc",
    opportunities_service: OpportunitiesService = Depends(get_opportunities_service),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint for fetching a paginated summary of opportunities for the main table view."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    params = {
        "status": status,
        "keyword": keyword,
        "page": page,
        "limit": limit,
        "sort_by": sort_by,
        "sort_direction": sort_direction,
    }
    opportunities, total_count = await run_in_threadpool(
        opportunities_service.get_all_opportunities_summary,
        client_id,
        params,
        select_columns="id, keyword, status, date_added, strategic_score, cpc, competition, main_intent, blog_qualification_status, blog_qualification_reason, latest_job_id, cluster_name, full_data",
    )
    return {
        "items": opportunities,
        "total_items": total_count,
        "page": page,
        "limit": limit,
    }


@router.get(
    "/clients/{client_id}/opportunities/by-cluster",
    response_model=Dict[str, List[Dict[str, Any]]],
)
async def get_opportunities_by_cluster_endpoint(
    client_id: str,
    opportunities_service: OpportunitiesService = Depends(get_opportunities_service),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all opportunities for a client, grouped by cluster.
    """
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    opportunities_by_cluster = await run_in_threadpool(
        opportunities_service.get_opportunities_by_cluster, client_id
    )
    return opportunities_by_cluster


@router.put("/opportunities/{opportunity_id}/status", response_model=Dict[str, str])
async def update_opportunity_status_endpoint(
    opportunity_id: int, status: str, db: DatabaseManager = Depends(get_db)
):
    """
    Manually updates the status of an opportunity.
    """
    logger.info(
        f"Received request to update status for opportunity {opportunity_id} to {status}"
    )
    db.update_opportunity_status(opportunity_id, status)
    return {"message": "Opportunity status updated successfully."}


@router.post("/opportunities/bulk-action", response_model=Dict[str, str])
async def bulk_action_endpoint(
    action: str, opportunity_ids: List[int], db: DatabaseManager = Depends(get_db)
):
    """
    Performs a bulk action on a list of opportunities.
    """
    logger.info(
        f"Received request to perform bulk action '{action}' on {len(opportunity_ids)} opportunities"
    )
    for opportunity_id in opportunity_ids:
        if action == "reject":
            db.update_opportunity_status(opportunity_id, "rejected")
        elif action == "approve":
            db.update_opportunity_status(opportunity_id, "qualified")

    return {"message": "Bulk action completed successfully."}


@router.post("/opportunities/compare", response_model=List[Dict[str, Any]])
async def compare_opportunities_endpoint(
    opportunity_ids: List[int], db: DatabaseManager = Depends(get_db)
):
    """
    Retrieves a list of opportunities for comparison.
    """
    logger.info(f"Received request to compare {len(opportunity_ids)} opportunities")
    opportunities = []
    for opportunity_id in opportunity_ids:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if opportunity:
            opportunities.append(opportunity)

    return opportunities


@router.get(
    "/clients/{client_id}/opportunities/search", response_model=List[Dict[str, Any]]
)
async def search_opportunities_endpoint(
    client_id: str,
    query: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    logger.info(f"Received search request for client {client_id} with query: '{query}'")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    if len(query) < 3:
        return []

    opportunities = db.search_opportunities(client_id, query)
    return opportunities


@router.get("/opportunities/{opportunity_id}", response_model=Dict[str, Any])
async def get_opportunity_by_id_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),  # Add this
):
    logger.info(f"Received request for opportunity {opportunity_id}")
    opportunity = db.get_opportunity_by_id(opportunity_id)
    if not opportunity:
        raise HTTPException(status_code=404, detail="Opportunity not found")
    # Add authorization check
    if opportunity["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this opportunity.",
        )

    # W23 FIX: Manually parse the blueprint from full_data if it exists
    if opportunity.get("full_data") and isinstance(opportunity["full_data"], str):
        try:
            full_data_json = json.loads(opportunity["full_data"])
            if "blueprint" in full_data_json:
                opportunity["blueprint"] = full_data_json["blueprint"]
            if "serp_overview" in full_data_json:
                opportunity["serp_overview"] = full_data_json["serp_overview"]
        except json.JSONDecodeError:
            logger.warning(
                f"Could not decode full_data JSON for opportunity {opportunity_id}."
            )

    logger.info(f"Retrieved opportunity from DB: {opportunity}")
    return opportunity


@router.get(
    "/opportunities/{opportunity_id}/content-history",
    response_model=List[ContentHistoryItem],
)
async def get_content_history_endpoint(
    opportunity_id: int, db: DatabaseManager = Depends(get_db)
):
    logger.info(f"Fetching content history for opportunity {opportunity_id}")
    history = db.get_content_history(opportunity_id)
    if not history:
        return []
    return history


@router.post(
    "/opportunities/{opportunity_id}/restore-content", response_model=Dict[str, Any]
)
async def restore_content_version_endpoint(
    opportunity_id: int, request: RestoreRequest, db: DatabaseManager = Depends(get_db)
):
    logger.info(
        f"Restoring content version from {request.version_timestamp} for opportunity {opportunity_id}"
    )
    try:
        restored_content = db.restore_content_version(
            opportunity_id, request.version_timestamp
        )
        if restored_content:
            return {
                "message": "Content version restored successfully.",
                "restored_content": restored_content,
            }
        else:
            raise HTTPException(
                status_code=404, detail="Failed to restore content version."
            )
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(
            f"Error restoring content for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500,
            detail="An internal error occurred during content restoration.",
        )


@router.put(
    "/opportunities/{opportunity_id}/social-media-posts", response_model=Dict[str, str]
)
async def update_social_media_posts_endpoint(
    opportunity_id: int,
    payload: SocialMediaPostsUpdate,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),  # Add this
):
    logger.info(f"Updating social media posts for opportunity {opportunity_id}")
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        db.update_opportunity_social_posts(opportunity_id, payload.social_media_posts)
        return {"message": "Social media posts updated successfully."}
    except Exception as e:
        logger.error(
            f"Error updating social media posts for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Failed to update social media posts."
        )


@router.put("/opportunities/{opportunity_id}/content", response_model=Dict[str, str])
async def update_opportunity_content_endpoint(
    opportunity_id: int,
    payload: ContentUpdatePayload,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),  # Add this
):
    """Updates the main HTML content of an opportunity's ai_content blob with server-side sanitization."""
    logger.info(f"Received manual content update for opportunity {opportunity_id}")
    from datetime import datetime

    try:
        current_opp = db.get_opportunity_by_id(opportunity_id)
        if not current_opp:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if current_opp["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        # 1. SERVER-SIDE SANITIZATION (CRITICAL SECURITY FIX)
        ALLOWED_TAGS = bleach.sanitizer.ALLOWED_TAGS + [
            "h1",
            "h2",
            "h3",
            "h4",
            "h5",
            "h6",
            "p",
            "br",
            "a",
            "i",
            "u",
            "em",
            "strong",
            "blockquote",
            "li",
            "ul",
            "ol",
            "img",
            "div",
            "span",
            "table",
            "thead",
            "tbody",
            "tr",
            "td",
            "th",
            "code",
            "pre",
        ]
        ALLOWED_ATTRIBUTES_SAFE = {
            "*": ["id", "class"],
            "a": ["href", "title"],
            "img": ["src", "alt", "width", "height"],
        }

        clean_html = bleach.clean(
            payload.article_body_html,
            tags=ALLOWED_TAGS,
            attributes=ALLOWED_ATTRIBUTES_SAFE,
        )

        # 2. Save the current version to history before overwriting
        current_ai_content = current_opp.get("ai_content", {})
        if current_ai_content:
            db.save_content_version_to_history(
                opportunity_id,
                current_ai_content,
                timestamp=f"{datetime.now().isoformat()} (Before Manual Edit)",
            )

        # 3. Update the content with the new payload
        updated_ai_content = current_ai_content.copy()
        updated_ai_content["article_body_html"] = clean_html

        # Use the query that also updates status and timestamp
        db.update_opportunity_ai_content_and_status(
            opportunity_id,
            updated_ai_content,
            current_opp.get("ai_content_model"),
            "generated",  # Reset status to 'generated' to reflect it's ready
        )
        return {"message": "Content updated and previous version saved to history."}
    except Exception as e:
        logger.error(
            f"Error updating content for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail="Failed to update content due to a server error."
        )


@router.post(
    "/opportunities/{opportunity_id}/override-disqualification",
    response_model=Dict[str, str],
)
async def override_disqualification_endpoint(
    opportunity_id: int, db: DatabaseManager = Depends(get_db)
):
    """Manually overrides a 'failed' or 'rejected' qualification status."""
    success = db.override_disqualification(opportunity_id)
    if not success:
        raise HTTPException(
            status_code=404,
            detail="Opportunity not found or its status did not permit an override.",
        )
    return {
        "message": "Opportunity has been re-qualified and moved to the pending queue."
    }


@router.post("/opportunities/{opportunity_id}/feedback", response_model=Dict[str, str])
async def submit_content_feedback_endpoint(
    opportunity_id: int,
    request: ContentFeedbackRequest,
    db: DatabaseManager = Depends(get_db),
):
    """Submits user feedback for the generated content."""
    if not (1 <= request.rating <= 5):
        raise HTTPException(status_code=400, detail="Rating must be between 1 and 5.")
    try:
        db.save_content_feedback(opportunity_id, request.rating, request.comments)
        return {"message": "Feedback submitted successfully."}
    except Exception as e:
        logger.error(
            f"Error submitting feedback for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail="Failed to submit feedback.")
```

## File: api/routers/orchestrator.py
```python
import logging
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from typing import Optional, Dict, List
from data_access.database_manager import DatabaseManager
from jobs import JobManager
from ..dependencies import get_db, get_job_manager, get_orchestrator
from ..models import (
    JobResponse,
    AnalysisRequest,
    AutoWorkflowRequest,
    RefineContentRequest,
    ApproveAnalysisRequest,
)  # Add ApproveAnalysisRequest
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


class GenerationRequest(BaseModel):
    model_override: Optional[str] = None
    temperature: Optional[float] = None


@router.post(
    "/orchestrator/{opportunity_id}/run-generation-async", response_model=JobResponse
)
async def run_generation_async_endpoint(
    opportunity_id: int,
    request: GenerationRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        # Add authorization check
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        # orchestrator is already initialized with the correct client_id from the header
        job_id = orchestrator.run_full_content_generation(
            opportunity_id, request.model_override, request.temperature
        )
        return {
            "job_id": job_id,
            "message": f"Content generation job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start generation job for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/run-validation-async", response_model=JobResponse
)
async def run_validation_async_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        job_id = orchestrator.run_validation(opportunity_id)
        return {"job_id": job_id, "message": f"Validation job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start validation job for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/run-analysis-async", response_model=JobResponse
)
async def run_analysis_async_endpoint(
    opportunity_id: int,
    request: AnalysisRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        # --- START MODIFICATION ---
        # Pass selected_competitor_urls from the request
        job_id = orchestrator.run_full_analysis(
            opportunity_id, request.selected_competitor_urls
        )
        # --- END MODIFICATION ---
        return {"job_id": job_id, "message": f"Full analysis job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start analysis job for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/orchestrator/{opportunity_id}/full-prompt", response_model=str)
async def get_full_prompt_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to get the full, flattened prompt for an opportunity."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        full_prompt = orchestrator.get_full_prompt_for_display(opportunity_id)
        if not full_prompt:
            raise HTTPException(
                status_code=404,
                detail="Prompt has not been generated yet or opportunity data is missing.",
            )
        return full_prompt
    except Exception as e:
        logger.error(
            f"Failed to get full prompt for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/regenerate-social-async", response_model=JobResponse
)
async def regenerate_social_async_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to start a job for regenerating only the social media posts."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.regenerate_social_posts(opportunity_id)
        return {
            "job_id": job_id,
            "message": f"Social media post regeneration job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start social post regeneration for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


class DiscoveryCostParams(BaseModel):
    seed_keywords: List[str]
    discovery_modes: List[str]
    # We only need fields that affect cost calculation
    discovery_max_pages: Optional[int] = 1


class CostEstimationRequest(BaseModel):
    action_type: str
    discovery_params: Optional[DiscoveryCostParams] = None


@router.post("/orchestrator/estimate-cost")
async def estimate_cost_endpoint(
    request: CostEstimationRequest,
    opportunity_id: Optional[int] = None,  # Make opportunity_id optional
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Endpoint to estimate the cost of a workflow action.
    For 'discovery', it uses discovery_params.
    For other actions, it uses opportunity_id.
    """
    action = request.action_type.lower()
    ALLOWED_ACTIONS = {"analyze", "generate", "validate", "discovery"}

    if action not in ALLOWED_ACTIONS:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid action '{action}'. Must be one of: {', '.join(ALLOWED_ACTIONS)}.",
        )

    try:
        if action == "discovery":
            if not request.discovery_params:
                raise HTTPException(
                    status_code=400,
                    detail="discovery_params are required for 'discovery' action.",
                )
            # For discovery, we don't need to check for an opportunity
            cost_estimation = orchestrator.estimate_action_cost(
                action=action, discovery_params=request.discovery_params.dict()
            )
        else:
            if not opportunity_id:
                raise HTTPException(
                    status_code=400,
                    detail="opportunity_id is required for this action.",
                )

            opportunity = db.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                raise HTTPException(status_code=404, detail="Opportunity not found")

            if opportunity["client_id"] != orchestrator.client_id:
                raise HTTPException(
                    status_code=403,
                    detail="You do not have permission to access this opportunity.",
                )

            cost_estimation = orchestrator.estimate_action_cost(
                action=action, opportunity_id=opportunity_id
            )

        return cost_estimation
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to estimate cost for action {action}: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, detail="Failed to estimate cost due to a server error."
        )


@router.get("/jobs/{job_id}/status")
async def get_job_status_endpoint(
    job_id: str, jm: JobManager = Depends(get_job_manager)
):
    """Endpoint to get the status of a background job."""
    logger.info(f"API: Received request for job status: {job_id}")
    job_status = jm.get_job_status(job_id)
    if not job_status:
        logger.warning(f"API: Job with ID {job_id} not found in JobManager.")
        raise HTTPException(status_code=404, detail="Job not found")
    logger.info(f"API: Found job {job_id}, status: {job_status.get('status')}")
    return {"job_id": job_status["id"], "message": f"Status: {job_status['status']}", **job_status}


@router.get("/jobs")
async def get_all_jobs_endpoint(db: DatabaseManager = Depends(get_db)):
    """Endpoint to get all jobs for the activity log."""
    try:
        jobs = db.get_all_jobs()
        return jobs
    except Exception as e:
        logger.error(f"Failed to retrieve all jobs: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to retrieve jobs.")


@router.post("/jobs/{job_id}/cancel")
async def cancel_job_endpoint(job_id: str, jm: JobManager = Depends(get_job_manager)):
    """Endpoint to cancel a running job."""
    try:
        success = jm.cancel_job(job_id)
        if success:
            return {"message": "Job cancellation request sent."}
        else:
            raise HTTPException(
                status_code=404, detail="Job not found or already completed."
            )
    except Exception as e:
        logger.error(f"Failed to cancel job {job_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to cancel job.")


@router.post(
    "/orchestrator/{opportunity_id}/run-full-auto-async", response_model=JobResponse
)
async def run_full_auto_async_endpoint(
    opportunity_id: int,
    request: AutoWorkflowRequest,  # ADD request body
    db: DatabaseManager = Depends(
        get_db
    ),  # Ensure DatabaseManager is correctly imported and injected
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to start the full 'auto' workflow from validation to generation."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.run_full_auto_workflow(
            opportunity_id, request.override_validation
        )  # Pass override flag
        return {
            "job_id": job_id,
            "message": f"Full auto workflow job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start full auto workflow for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/rerun-analysis-async", response_model=JobResponse
)
async def clear_cache_and_analyze_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Clears API cache for the opportunity's keyword and starts a new analysis job."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.run_full_analysis(opportunity_id)
        return {
            "job_id": job_id,
            "message": f"Cache cleared and analysis job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to clear cache and analyze for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.get(
    "/orchestrator/{opportunity_id}/score-narrative", response_model=Dict[str, str]
)
async def get_score_narrative_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to generate a human-readable narrative for the score breakdown."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        narrative = orchestrator.summary_generator.generate_score_narrative(
            opportunity.get("full_data", {}).get("score_breakdown", {})
        )
        return {"narrative": narrative}
    except Exception as e:
        logger.error(
            f"Failed to generate score narrative for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/refresh-content-async", response_model=JobResponse
)
async def refresh_content_async_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to trigger a refresh of an existing opportunity's content."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.run_content_refresh_workflow(opportunity_id)
        return {"job_id": job_id, "message": f"Content refresh job {job_id} started."}
    except Exception as e:
        logger.error(
            f"Failed to start content refresh for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


class FeaturedImageRequest(BaseModel):
    prompt: str


@router.post(
    "/orchestrator/{opportunity_id}/generate-featured-image-async",
    response_model=JobResponse,
)
async def generate_featured_image_async_endpoint(
    opportunity_id: int,
    request: FeaturedImageRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Starts a job to generate a new featured image for an opportunity."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        job_id = orchestrator.regenerate_featured_image(opportunity_id, request.prompt)
        return {
            "job_id": job_id,
            "message": f"Featured image generation job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start featured image generation for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/{opportunity_id}/refine-content", response_model=Dict[str, str]
)
async def refine_content_endpoint(
    opportunity_id: int,
    request: RefineContentRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to refine a snippet of HTML content using an AI command."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        prompt_messages = [
            {
                "role": "system",
                "content": "You are an expert web content editor. You will receive a block of HTML and a specific command. Your task is to apply the command to the provided HTML segment and return ONLY the modified HTML block. You MUST preserve all original HTML tags, attributes (like IDs, classes, styles), and structure, only changing the text content or adding/removing tags as absolutely necessary to fulfill the command. Do not add any introductory or concluding remarks, just the refined HTML.",
            },
            {
                "role": "user",
                "content": f"Command: '{request.command}'\n\nHTML to modify:\n```html\n{request.html_content}\n```\n\nReturn ONLY the refined HTML:",
            },
        ]

        refined_html, error = orchestrator.openai_client.call_chat_completion(
            messages=prompt_messages,
            model=orchestrator.client_cfg.get("default_model", "gpt-5-nano"),
            temperature=0.4,
        )

        if error or not refined_html:
            raise HTTPException(
                status_code=500, detail=f"AI content refinement failed: {error}"
            )

        # Clean up potential markdown code block fences from the AI response
        refined_html = refined_html.strip()
        if refined_html.startswith("```html"):
            refined_html = refined_html[len("```html") :]
        if refined_html.endswith("```"):
            refined_html = refined_html[: -len("```")]

        return {"refined_html": refined_html.strip()}
    except Exception as e:
        logger.error(
            f"Failed to refine content for opp {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(
            status_code=500, detail="Failed to refine content due to a server error."
        )


@router.post(
    "/orchestrator/{opportunity_id}/generate-content-override",
    response_model=JobResponse,
)
async def generate_content_override(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint to manually trigger content generation override"""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )
        job_id = orchestrator.run_full_auto_workflow(
            opportunity_id, True
        )  # Run with override = True
        return {
            "job_id": job_id,
            "message": f"Content generation override job {job_id} started.",
        }
    except Exception as e:
        logger.error(
            f"Failed to start content generation override for {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/approve-analysis/{opportunity_id}", response_model=JobResponse
)
async def approve_analysis_endpoint(
    opportunity_id: int,
    request: ApproveAnalysisRequest,  # Use the new request body model
    db: DatabaseManager = Depends(get_db),
    jm: JobManager = Depends(get_job_manager),
    orchestrator: WorkflowOrchestrator = Depends(
        get_orchestrator
    ),  # Add orchestrator to get client_id
):
    """Endpoint to approve the analysis and continue the workflow by starting content generation with optional overrides."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["status"] in [
            "running",
            "in_progress",
            "pending",
            "refresh_started",
        ]:
            raise HTTPException(
                status_code=409,
                detail=f"A workflow is already active for this opportunity (Status: {opportunity['status']}).",
            )

        # Add authorization check
        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        # Convert Pydantic model to dict if it exists, else pass None
        overrides_dict = request.overrides.dict() if request.overrides else None

        job_id = orchestrator.run_full_content_generation(
            opportunity_id, overrides=overrides_dict
        )

        return {
            "job_id": job_id,
            "message": f"Analysis approved. Started content generation job {job_id}.",
        }
    except ValueError as ve:
        logger.error(
            f"State mismatch trying to approve analysis for {opportunity_id}: {ve}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=409, detail=str(ve)
        )  # 409 Conflict for state issues
    except Exception as e:
        logger.error(
            f"Failed to approve analysis for {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/orchestrator/reject-opportunity/{opportunity_id}", response_model=Dict[str, str]
)
async def reject_opportunity_endpoint(
    opportunity_id: int,
    db: DatabaseManager = Depends(get_db),
):
    """Endpoint to reject the opportunity and set status to 'rejected'"""
    try:
        # This is a direct status update, no job manager needed for rejection itself
        db.update_opportunity_workflow_state(
            opportunity_id,
            "rejected_by_user",
            "rejected",
            error_message="Opportunity rejected by user.",
        )
        return {"message": "Opportunity rejected."}
    except Exception as e:
        logger.error(
            f"Failed to reject opportunity {opportunity_id}: {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=str(e))


class SocialMediaStatusUpdateRequest(BaseModel):
    new_status: str


# Add this new endpoint at the end of the file:
@router.post(
    "/orchestrator/{opportunity_id}/social-media-status", response_model=Dict[str, str]
)
async def update_social_media_status_endpoint(
    opportunity_id: int,
    request: SocialMediaStatusUpdateRequest,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(
        get_orchestrator
    ),  # For client_id auth
):
    """Endpoint to update the status of social media posts (e.g., 'approved', 'rejected')."""
    try:
        opportunity = db.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise HTTPException(status_code=404, detail="Opportunity not found.")

        if opportunity["client_id"] != orchestrator.client_id:
            raise HTTPException(
                status_code=403,
                detail="You do not have permission to access this opportunity.",
            )

        valid_statuses = ["draft", "approved", "rejected", "scheduled", "published"]
        if request.new_status not in valid_statuses:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid status: {request.new_status}. Must be one of {valid_statuses}.",
            )

        db.update_social_media_posts_status(opportunity_id, request.new_status)
        return {
            "message": "Social media posts status updated successfully.",
            "new_status": request.new_status,
        }
    except Exception as e:
        logger.error(
            f"Failed to update social media status for opportunity {opportunity_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(status_code=500, detail=str(e))
```

## File: api/routers/qualification_settings.py
```python
# api/routers/qualification_settings.py

import logging
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get(
    "/clients/{client_id}/qualification-settings", response_model=Dict[str, Any]
)
async def get_qualification_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves the qualification settings for a specific client.
    """
    logger.info(f"Received request for qualification settings for client {client_id}")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    settings = db.get_qualification_settings(client_id)
    if not settings:
        raise HTTPException(status_code=404, detail="Qualification settings not found")
    return settings


@router.put(
    "/clients/{client_id}/qualification-settings", response_model=Dict[str, Any]
)
async def update_qualification_settings_endpoint(
    client_id: str,
    settings: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Updates the qualification settings for a specific client.
    """
    logger.info(
        f"Received request to update qualification settings for client {client_id}"
    )
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    db.update_qualification_settings(client_id, settings)
    return {"message": "Qualification settings updated successfully."}
```

## File: api/routers/qualification_strategies.py
```python
# api/routers/qualification_strategies.py

import logging
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any, List
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get(
    "/clients/{client_id}/qualification-strategies", response_model=List[Dict[str, Any]]
)
async def get_qualification_strategies_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Retrieves all qualification strategies for a specific client.
    """
    logger.info(f"Received request for qualification strategies for client {client_id}")
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    strategies = db.get_qualification_strategies(client_id)
    return strategies


@router.post(
    "/clients/{client_id}/qualification-strategies", response_model=Dict[str, Any]
)
async def create_qualification_strategy_endpoint(
    client_id: str,
    strategy: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Creates a new qualification strategy for a specific client.
    """
    logger.info(
        f"Received request to create qualification strategy for client {client_id}"
    )
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    strategy_id = db.create_qualification_strategy(client_id, strategy)
    return {"id": strategy_id}


@router.put("/qualification-strategies/{strategy_id}", response_model=Dict[str, str])
async def update_qualification_strategy_endpoint(
    strategy_id: int,
    strategy: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Updates a qualification strategy.
    """
    logger.info(f"Received request to update qualification strategy {strategy_id}")
    # Auth check
    strat_to_update = db.get_qualification_strategy_by_id(strategy_id)
    if not strat_to_update:
        raise HTTPException(status_code=404, detail="Strategy not found.")
    if strat_to_update["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to modify this resource.",
        )

    db.update_qualification_strategy(strategy_id, strategy)
    return {"message": "Qualification strategy updated successfully."}


@router.delete("/qualification-strategies/{strategy_id}", response_model=Dict[str, str])
async def delete_qualification_strategy_endpoint(
    strategy_id: int,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """
    Deletes a qualification strategy.
    """
    logger.info(f"Received request to delete qualification strategy {strategy_id}")
    # Auth check
    strat_to_delete = db.get_qualification_strategy_by_id(strategy_id)
    if not strat_to_delete:
        raise HTTPException(status_code=404, detail="Strategy not found.")
    if strat_to_delete["client_id"] != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to delete this resource.",
        )

    db.delete_qualification_strategy(strategy_id)
    return {"message": "Qualification strategy deleted successfully."}
```

## File: api/routers/settings.py
```python
# api/routers/settings.py
import logging
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Any
from data_access.database_manager import DatabaseManager
from ..dependencies import get_db, get_orchestrator
from backend.pipeline import WorkflowOrchestrator

router = APIRouter()
logger = logging.getLogger(__name__)

@router.get("/settings/{client_id}", response_model=Dict[str, Any])
async def get_settings_endpoint(
    client_id: str,
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint for fetching all client-specific settings."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        settings = db.get_client_settings(client_id)
        if not settings:
            raise HTTPException(status_code=404, detail="Settings not found for this client.")
        return settings
    except Exception as e:
        logger.error(f"Failed to retrieve settings for client {client_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to retrieve settings.")

@router.put("/settings/{client_id}", response_model=Dict[str, str])
async def update_settings_endpoint(
    client_id: str,
    settings: Dict[str, Any],
    db: DatabaseManager = Depends(get_db),
    orchestrator: WorkflowOrchestrator = Depends(get_orchestrator),
):
    """Endpoint for updating client-specific settings."""
    if client_id != orchestrator.client_id:
        raise HTTPException(
            status_code=403,
            detail="You do not have permission to access this client's resources.",
        )
    try:
        db.update_client_settings(client_id, settings)
        return {"message": "Settings updated successfully."}
    except Exception as e:
        logger.error(f"Failed to update settings for client {client_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to update settings.")
```

## File: api/dependencies.py
```python
# api/dependencies.py
import os
from fastapi import Depends, HTTPException, Security, Request
from fastapi.security import APIKeyHeader
from data_access.database_manager import DatabaseManager
from backend.pipeline import WorkflowOrchestrator
from jobs import JobManager
from services.opportunities_service import OpportunitiesService
from services.discovery_service import DiscoveryService
from . import globals as api_globals


def get_db() -> DatabaseManager:
    """Dependency injector for DatabaseManager."""
    return api_globals.db_manager


def get_opportunities_service(
    db: DatabaseManager = Depends(get_db),
) -> OpportunitiesService:
    """Dependency injector for OpportunitiesService."""
    return OpportunitiesService(db)


def get_discovery_service(db: DatabaseManager = Depends(get_db)) -> DiscoveryService:
    """Dependency injector for DiscoveryService."""
    return DiscoveryService(db)


def get_job_manager() -> JobManager:
    """Dependency injector for JobManager."""
    return api_globals.job_manager


# Replace the entire `get_current_client_id` function with this:
def get_current_client_id(request: Request) -> str:
    """
    Dependency to get the current client_id from the X-Client-ID header.
    In a real multi-tenant application, this would also be validated against user's permissions.
    """
    client_id = request.headers.get("X-Client-ID")
    if not client_id:
        # Fallback to default if header is missing, or raise HTTPException
        # For development, we might fallback. For production, raising is safer.
        # raise HTTPException(status_code=400, detail="X-Client-ID header is required")
        return "Lark_Main_Site"  # Fallback for local dev/testing
    return client_id


# Update the `get_orchestrator` dependency to *not* directly use `get_current_client_id` within its signature
# because it will be passed explicitly to the endpoint if needed.
# Modify `get_orchestrator` signature from `get_orchestrator(client_id: str, ...)` to:
def get_orchestrator(
    request: Request,  # Add Request to get client_id
    db: DatabaseManager = Depends(get_db),
    jm: JobManager = Depends(get_job_manager),
) -> WorkflowOrchestrator:
    """
    Dependency injector for WorkflowOrchestrator.
    Creates a new instance for each request, configured for the specific client_id.
    """
    client_id = request.headers.get("X-Client-ID")  # Get client_id from request headers
    if not client_id:
        # Fallback to default for orchestrator initialization if header is missing
        client_id = "Lark_Main_Site"

    if not db.get_client_settings(client_id):
        raise HTTPException(
            status_code=404, detail=f"Client with ID '{client_id}' not found."
        )

    return WorkflowOrchestrator(api_globals.config_manager, db, client_id, jm)


API_KEY = os.getenv("INTERNAL_API_KEY")
API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)


async def get_api_key(api_key: str = Security(api_key_header)):
    if api_key == API_KEY:
        return api_key
    else:
        raise HTTPException(status_code=403, detail="Could not validate credentials")
```

## File: api/globals.py
```python
# api/globals.py
from typing import Optional
from app_config.manager import ConfigManager
from data_access.database_manager import DatabaseManager
from jobs import JobManager

config_manager: Optional[ConfigManager] = None
db_manager: Optional[DatabaseManager] = None
job_manager: Optional[JobManager] = None
```

## File: api/main.py
```python
# api/main.py
# api/main.py (New File, or existing FastAPI entry point)
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles  # ADD THIS for Task 3
import logging
import os
import sys

# Add project root to sys.path to resolve imports from agents, pipeline, etc.
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# Import from your existing project structure
from app_config.manager import ConfigManager
from data_access.database_manager import DatabaseManager
from jobs import JobManager  # Import the class

from . import globals as api_globals


logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI()

# Mount the static directory for generated images
# Images will be accessible at /api/images/{filename}
static_images_path = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "generated_images")
)
app.mount(
    "/api/images", StaticFiles(directory=static_images_path), name="static_images"
)


# --- Global Dependency Initialization (simplified for example) ---
# In a real app, use @lru_cache or proper dependency injection
@app.on_event("startup")
async def startup_event():
    api_globals.config_manager = ConfigManager()
    api_globals.db_manager = DatabaseManager(cfg_manager=api_globals.config_manager)
    api_globals.db_manager.initialize()  # Ensure DB tables are created/migrated
    api_globals.job_manager = JobManager(
        db_manager=api_globals.db_manager
    )  # Initialize JobManager with db_manager

    logger.info("FastAPI application startup complete. Dependencies initialized.")

    from .routers import (
        auth,
        clients,
        opportunities,
        discovery,
        orchestrator,
        jobs,
        qualification_settings,
        qualification_strategies,
        settings,
    )

    app.include_router(auth.router)
    app.include_router(clients.router)
    app.include_router(opportunities.router)
    app.include_router(discovery.router)
    app.include_router(orchestrator.router)
    app.include_router(jobs.router)
    app.include_router(qualification_settings.router)
    app.include_router(qualification_strategies.router)
    app.include_router(settings.router)
```

## File: api/models.py
```python
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any


class DiscoveryCostParams(BaseModel):
    seed_keywords: List[str]
    discovery_modes: Optional[List[str]] = ["ideas"]
    limit: Optional[int] = 1000
    include_clickstream_data: Optional[bool] = False
    people_also_ask_click_depth: Optional[int] = 0


# Define ContentUpdateRequest (W18 FIX)
class ContentUpdatePayload(BaseModel):
    article_body_html: str = Field(
        ..., description="The new HTML content for the article body."
    )


# Define ImageUpdatePayload (W18 FIX)
class ImageRegenRequest(BaseModel):
    original_prompt: str
    new_prompt: str


class DiscoveryRunRequest(BaseModel):
    seed_keywords: List[str]
    discovery_modes: Optional[List[str]] = ["ideas"]
    filters: Optional[List[Any]] = None
    order_by: Optional[List[str]] = None
    filters_override: Optional[Dict[str, Any]] = {}
    depth: Optional[int] = None
    limit: Optional[int] = None
    ignore_synonyms: Optional[bool] = False
    # NEW: Parameters for user flexibility
    include_clickstream_data: Optional[bool] = None
    closely_variants: Optional[bool] = None


class KeywordListRequest(BaseModel):
    seed_keywords: List[str]
    # NEW: Parameter for dynamic cost estimation
    include_clickstream_data: Optional[bool] = False


class JobResponse(BaseModel):
    job_id: str
    message: str
    status: Optional[str] = None
    progress: Optional[int] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    progress_log: Optional[List[Dict[str, Any]]] = None


class LoginRequest(BaseModel):
    password: str


class TemplateContent(BaseModel):
    name: str
    content: str
    description: Optional[str] = None


class TemplateResponse(BaseModel):
    name: str
    content: str
    description: Optional[str] = None
    last_updated: str


class PromptPreviewRequest(BaseModel):
    custom_template_content: Optional[str] = None


class PromptPreviewResponse(BaseModel):
    prompt: str


class ContentHistoryItem(BaseModel):
    id: int
    opportunity_id: int
    timestamp: str
    ai_content_json: Dict[str, Any]


class RestoreRequest(BaseModel):
    version_timestamp: str


class SingleImageRegenRequest(BaseModel):
    opportunity_id: int
    original_prompt: str
    new_prompt: str


class AutoWorkflowRequest(BaseModel):
    override_validation: bool = False


class SocialMediaPostsUpdate(BaseModel):
    social_media_posts: List[Dict[str, Any]]


class GlobalSettingsUpdate(BaseModel):
    settings: Dict[str, Any]


class OpportunityListResponse(BaseModel):
    items: List[Dict[str, Any]]
    total_items: int
    page: int
    limit: int


class AnalysisRequest(BaseModel):
    selected_competitor_urls: Optional[List[str]] = None


class RefineContentRequest(BaseModel):
    html_content: str
    command: str


class ClientSettings(BaseModel):
    brand_tone: Optional[str] = None
    target_audience: Optional[str] = None
    terms_to_avoid: Optional[str] = None


class GenerationOverrides(BaseModel):
    target_word_count: Optional[int] = None
    expert_persona: Optional[str] = None
    additional_instructions: Optional[str] = None


class ApproveAnalysisRequest(BaseModel):
    overrides: Optional[GenerationOverrides] = None
```

## File: app_config/__init__.py
```python
# app_config/__init__.py
# This file marks the directory as a Python package.
```

## File: app_config/manager.py
```python
import os
import configparser
from dotenv import load_dotenv
from typing import Dict, Any, List, Optional
import logging


class ConfigManager:
    """
    Manages loading, validating, and providing configuration settings.
    Handles global defaults from settings.ini and client-specific overrides.
    """

    _setting_types = {
        # Integers
        "location_code": int,
        "max_sv_for_scoring": int,
        "max_domain_rank_for_scoring": int,
        "max_referring_domains_for_scoring": int,
        "max_avg_referring_domains_filter": int,
        "serp_freshness_old_threshold_days": int,
        "serp_volatility_stable_threshold_days": int,
        "min_competitor_word_count": int,
        "max_competitor_technical_warnings": int,
        "num_competitors_to_analyze": int,
        "num_common_headings": int,
        "num_unique_angles": int,
        "max_initial_serp_urls_to_analyze": int,
        "people_also_ask_click_depth": int,
        "min_search_volume": int,
        "max_keyword_difficulty": int,
        "num_in_article_images": int,
        "onpage_max_domains_per_request": int,
        "onpage_max_tasks_per_request": int,
        "deep_dive_top_n_keywords": int,
        "max_completion_tokens_for_generation": int,
        "discovery_max_pages": int,
        "min_serp_results": int,
        "max_serp_results": int,
        "min_avg_backlinks": int,
        "max_avg_backlinks": int,
        "discovery_related_depth": int,
        "yearly_trend_decline_threshold": int,
        "quarterly_trend_decline_threshold": int,
        "max_kd_hard_limit": int,
        "max_referring_main_domains_limit": int,
        "max_avg_domain_rank_threshold": int,
        "min_keyword_word_count": int,
        "max_keyword_word_count": int,
        "crowded_serp_features_threshold": int,
        "min_serp_stability_days": int,
        "max_non_blog_results": int,
        "max_ai_overview_words": int,
        "max_first_organic_y_pixel": int,
        "max_words_for_ai_analysis": int,
        "num_competitors_for_ai_analysis": int,
        "max_avg_lcp_time": int,  # NEW
        "high_value_sv_override_threshold": int,
        "overlay_font_size": int,
        # Floats
        "informational_score": float,
        "commercial_score": float,
        "transactional_score": float,
        "navigational_score": float,
        "question_keyword_bonus": float,
        "max_cpc_for_scoring": float,
        "featured_snippet_bonus": float,
        "ai_overview_bonus": float,
        "serp_freshness_bonus_max": float,
        "min_cpc_filter": float,
        "min_yearly_trend_filter": float,
        "min_cpc": float,
        "max_cpc": float,
        "min_competition": float,
        "max_competition": float,
        "min_cpc_filter_api": float,
        "category_intent_bonus": float,
        "search_volume_volatility_threshold": float,
        "max_paid_competition_score": float,
        "max_high_top_of_page_bid": float,
        "max_pages_to_domain_ratio": float,
        "ai_generation_temperature": float,
        "recommended_word_count_multiplier": float,
        "default_multiplier": float,  # ADDED
        "comprehensive_article": float,  # ADDED
        "how_to_guide": float,  # ADDED
        "comparison_post": float,  # ADDED
        "review_article": float,  # ADDED
        "video_led_article": float,  # ADDED
        "forum_summary_post": float,  # ADDED
        "recipe_article": float,
        "scholarly_summary": float,
        "product_comparison": float,
        "high_value_cpc_override_threshold": float,
        # Booleans
        "require_question_keywords": bool,
        "enforce_intent_filter": bool,
        "calculate_rectangles": bool,
        "enable_cache": bool,
        "deep_dive_discovery": bool,
        "use_pexels_first": bool,
        "cleanup_local_images": bool,
        "onpage_enable_javascript": bool,
        "onpage_load_resources": bool,
        "onpage_disable_cookie_popup": bool,
        "onpage_return_despite_timeout": bool,
        "onpage_enable_browser_rendering": bool,
        "onpage_store_raw_html": bool,
        "onpage_validate_micromarkup": bool,
        "discovery_replace_with_core_keyword": bool,
        "discovery_ignore_synonyms": bool,
        "enable_automated_internal_linking": bool,
        "generate_toc": bool,
        "overlay_text_enabled": bool,
        "include_clickstream_data": bool,
        "load_async_ai_overview": bool,  # ADD THIS FOR W3
        "onpage_check_spell": bool,  # ADD THIS LINE (W5 FIX)
        "disable_ai_overview_check": bool,
        "onpage_accept_language": str,  # ADD THIS LINE (W7 FIX)
        "onpage_enable_switch_pool": bool,  # ADD THIS LINE (W13 FIX)
        "onpage_enable_custom_js": bool,  # ADD THIS LINE (W12 FIX)
        "onpage_custom_js": str,  # ADD THIS LINE (W12 FIX)
        "discovery_exact_match": bool,  # ADD THIS LINE (W7 FIX)
        "onpage_browser_screen_resolution_ratio": float,  # ADD THIS LINE (W7 FIX)
        # Lists (comma-separated strings)
        "allowed_intents": list,
        "negative_keywords": list,
        "competitor_blacklist_domains": list,
        "serp_feature_filters": list,
        "serp_features_exclude_filter": list,
        "platforms": list,
        "default_wordpress_categories": list,
        "default_wordpress_tags": list,
        "ugc_and_parasite_domains": list,
        "high_value_categories": list,
        "hostile_serp_features": list,
        "final_validation_non_blog_domains": list,
        # Weights
        "ease_of_ranking_weight": float,
        "traffic_potential_weight": float,
        "commercial_intent_weight": float,
        "serp_features_weight": float,
        "growth_trend_weight": float,
        "serp_freshness_weight": float,
        "serp_volatility_weight": float,
        "competitor_weakness_weight": float,
        "competitor_performance_weight": float,  # ADDED THIS LINE
        # Strings
        "max_competition_level": str,
        "non_evergreen_year_pattern": str,
        "db_file_name": str,  # NEW
        "db_type": str,  # NEW
        "overlay_text_color": str,
        "overlay_background_color": str,
        "overlay_position": str,
        "closely_variants": bool,
        "max_cpc_filter": float,
        "discovery_order_by_field": str,
        "discovery_order_by_direction": str,
        "search_phrase_regex": str,
        "onpage_custom_checks_thresholds": str,  # ADD THIS LINE (W9 FIX)
        "serp_remove_from_url_params": str,
        "schema_author_type": str,
        "client_knowledge_base": str,
        "wordpress_url": str,
        "wordpress_user": str,
        "wordpress_app_password": str,
        "wordpress_seo_plugin": str,
    }

    def __init__(self, settings_path: str = "backend/app_config/settings.ini"):
        load_dotenv()
        self.config_parser = configparser.ConfigParser(inline_comment_prefixes=(";",))
        if not os.path.exists(settings_path):
            raise FileNotFoundError(f"Configuration file not found at: {settings_path}")
        self.config_parser.read(settings_path)
        self.logger = logging.getLogger(self.__class__.__name__)
        self._configure_logging()
        self._global_settings = self._load_and_validate_global()

    def _configure_logging(self):
        """Sets up basic logging for the application."""
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        logging.getLogger("urllib3").setLevel(logging.WARNING)
        self.logger.info("Logging configured.")

    def _get_int_from_config(
        self, section: str, key: str, fallback: Optional[int] = None
    ) -> int:
        try:
            value_str = self.config_parser.get(section, key)
            return int(value_str)
        except (configparser.NoOptionError, configparser.NoSectionError, ValueError):
            if fallback is not None:
                return fallback
            raise ValueError(
                f"Missing or invalid integer configuration for [{section}]{key}"
            )

    def _get_float_from_config(
        self, section: str, key: str, fallback: Optional[float] = None
    ) -> float:
        try:
            value_str = self.config_parser.get(section, key)
            return float(value_str)
        except (configparser.NoOptionError, configparser.NoSectionError, ValueError):
            if fallback is not None:
                return fallback
            raise ValueError(
                f"Missing or invalid float configuration for [{section}]{key}"
            )

    def _get_list_from_config(
        self, section: str, key: str, fallback: str = ""
    ) -> List[str]:
        try:
            value_str = self.config_parser.get(section, key)
            return [item.strip() for item in value_str.split(",") if item.strip()]
        except (configparser.NoOptionError, configparser.NoSectionError):
            return [item.strip() for item in fallback.split(",") if item.strip()]

    def _load_and_validate_global(self) -> Dict[str, Any]:
        """Loads all global settings from settings.ini and .env."""
        settings = {}

        # API Credentials (from .env)
        settings["dataforseo_login"] = os.getenv("DATAFORSEO_LOGIN")
        settings["dataforseo_password"] = os.getenv("DATAFORSEO_PASSWORD")
        if not settings["dataforseo_login"] or not settings["dataforseo_password"]:
            self.logger.critical(
                "DATAFORSEO_LOGIN and DATAFORSEO_PASSWORD must be set in the .env file."
            )
            raise ValueError(
                "DATAFORSEO_LOGIN and DATAFORSEO_PASSWORD must be set in the .env file."
            )

        settings["openai_api_key"] = os.getenv("OPENAI_API_KEY")
        if not settings["openai_api_key"]:
            self.logger.warning(
                "OPENAI_API_KEY is not set in the .env file. AI content generation will likely fail."
            )

        settings["pexels_api_key"] = os.getenv("PEXELS_API_KEY")
        if not settings["pexels_api_key"]:
            self.logger.warning(
                "PEXELS_API_KEY is not set in the .env file. Pexels integration will be unavailable."
            )

        # UI Password (from .env)
        settings["ui_password"] = os.getenv("UI_PASSWORD")
        if not settings["ui_password"]:
            self.logger.critical(
                "UI_PASSWORD must be set in the .env file for Streamlit authentication."
            )
            raise ValueError("UI_PASSWORD must be set in the .env file.")

        # Load all settings from settings.ini
        for section in self.config_parser.sections():
            for key, value in self.config_parser.items(section):
                try:
                    target_type = self._setting_types.get(key)
                    if target_type is bool:
                        settings[key] = self.config_parser.getboolean(section, key)
                    elif target_type is int:
                        settings[key] = self.config_parser.getint(section, key)
                    elif target_type is float:
                        settings[key] = self.config_parser.getfloat(section, key)
                    elif target_type is list:
                        raw_values = self._get_list_from_config(section, key)
                        if key == "serp_feature_filters":
                            parsed_filters = []
                            for f_str in raw_values:
                                if f_str.startswith("no_"):
                                    parsed_filters.append(
                                        {"type": "has_not", "feature": f_str[3:]}
                                    )
                                elif f_str.startswith("has_"):
                                    parsed_filters.append(
                                        {"type": "has", "feature": f_str[4:]}
                                    )
                            settings[key] = parsed_filters
                        else:
                            settings[key] = raw_values
                    else:  # Default to string if no type is mapped
                        settings[key] = value
                except Exception as e:
                    self.logger.critical(
                        f"FATAL CONFIG ERROR: Could not parse key [{section}]{key} with value '{value}' to expected type: {e}"
                    )
                    raise ValueError(
                        f"Configuration key parsing failed for [{section}]{key}. Value: '{value}'."
                    )

        self.logger.info("Global settings loaded.")
        return settings

    def get_global_config(self) -> Dict[str, Any]:
        """Returns the loaded global configuration."""
        return self._global_settings

    def get_default_client_settings_template(self) -> Dict[str, Any]:
        """Returns a template of client settings based on global config, for new client creation."""
        template = self._global_settings.copy()
        template.pop("dataforseo_login", None)
        template.pop("dataforseo_password", None)
        template.pop("openai_api_key", None)
        template.pop("pexels_api_key", None)
        template.pop("ui_password", None)

        for key, value in template.items():
            if isinstance(value, list):
                template[key] = value[:]
        return template

    def load_client_config(self, client_id: str, db_manager: Any) -> Dict[str, Any]:
        """
        Loads client-specific settings from the database and merges them with global settings.
        Scoring weights are always loaded from the global settings to ensure consistency.
        """
        client_settings_from_db = db_manager.get_client_settings(client_id)
        overridden_settings = self._global_settings.copy()

        # Define keys that should NOT be overridden by client-specific settings to ensure they are globally managed.
        globally_managed_keys = set(
            [
                "dataforseo_login",
                "dataforseo_password",
                "openai_api_key",
                "pexels_api_key",
                "ui_password",
                "db_file_name",
                "cache_file_name",
                "default_client_id",
                "db_type",
            ]
        )  # UPDATED

        for key, value in client_settings_from_db.items():
            if key in globally_managed_keys:
                continue  # Skip override for globally managed keys

            if value is not None and value != "":
                overridden_settings[key] = value

        self.logger.info(f"Loaded client-specific configuration for '{client_id}'.")
        return overridden_settings

    def save_client_settings(
        self, client_id: str, new_settings: Dict[str, Any], db_manager: Any
    ):
        """Saves specified client settings to the database."""
        db_manager.update_client_settings(client_id, new_settings)
        self.logger.info(f"Client settings for '{client_id}' saved to database.")

    def save_global_settings_to_file(self, updated_global_settings: Dict[str, Any]):
        """Saves specified settings back to the settings.ini file (for global defaults)."""
        for section in self.config_parser.sections():
            for key in self.config_parser.options(section):
                if (
                    key in updated_global_settings
                    and updated_global_settings[key] is not None
                ):
                    if isinstance(updated_global_settings[key], list):
                        self.config_parser.set(
                            section, key, ",".join(updated_global_settings[key])
                        )
                    elif isinstance(updated_global_settings[key], bool):
                        self.config_parser.set(
                            section, key, str(updated_global_settings[key]).lower()
                        )
                    else:
                        self.config_parser.set(
                            section, key, str(updated_global_settings[key])
                        )

        with open("backend/app_config/settings.ini", "w") as configfile:
            self.config_parser.write(configfile)

        self._global_settings = self._load_and_validate_global()
        self.logger.info("Global settings updated and reloaded from settings.ini.")
```

## File: app_config/settings.ini
```
[SEO_CRITERIA]
location_code = 2840
language_code = en
target_domain = profitparrot.com
device = desktop ; NEW: Global default device for SERP and OnPage calls
os = windows ; NEW: Global default OS for SERP calls

[INTENT_SCORING]
informational_score = 100
commercial_score = 70
transactional_score = 50
navigational_score = 10
question_keyword_bonus = 5

[SCORING_WEIGHTS]
ease_of_ranking_weight = 40
traffic_potential_weight = 15
commercial_intent_weight = 5
serp_features_weight = 5
growth_trend_weight = 5
serp_freshness_weight = 5
serp_volatility_weight = 5
competitor_weakness_weight = 20
competitor_performance_weight = 5 ; NEW: Weight for competitor technical performance

[SCORING_NORMALIZATION]
max_cpc_for_scoring = 20.0
max_sv_for_scoring = 50000
max_domain_rank_for_scoring = 700
max_referring_domains_for_scoring = 200 ; NEW: For ease of ranking calculation
max_avg_referring_domains_filter = 20 ; NEW: Filter for discovery - max avg referring domains for top competitors

[SERP_FEATURE_SCORING]
featured_snippet_bonus = 15
ai_overview_bonus = 10
serp_freshness_bonus_max = 20
serp_freshness_old_threshold_days = 180
serp_volatility_stable_threshold_days = 90 ; NEW: Threshold for a "stable" SERP

[QUALITY_FILTERS]
require_question_keywords = true
enforce_intent_filter = true
allowed_intents = informational
negative_keywords = login, sign in, account, free, cheap
min_search_volume = 100
max_keyword_difficulty = 80
min_cpc = 0.0
max_cpc = 5.0
min_competition = 0.0
max_competition = 1.0
max_competition_level = LOW
min_serp_results = 100000
max_serp_results = 10000000
min_avg_backlinks = 0
max_avg_backlinks = 20
min_keyword_word_count = 2 ; NEW
max_keyword_word_count = 8 ; NEW
high_value_sv_override_threshold = 10000 ; NEW
high_value_cpc_override_threshold = 5.0 ; NEW


[DEFAULT]
enable_cache = true
cache_file_name = data/cache.json
max_completion_tokens_for_generation = 32768
db_file_name = data/opportunities.db
ai_generation_temperature = 0.7
include_clickstream_data = false

[Lark_Main_Site]
target_domain = profitparrot.com
# Other client-specific settings can go here
expert_persona = a certified financial planner with 15 years of experience

[DISCOVERY_SETTINGS]
discovery_strategies = Keyword Ideas, Related Keywords, Keyword Suggestions ; W20 FIX: Centralize strategies
deep_dive_discovery = false
deep_dive_top_n_keywords = 5
serp_feature_filters = no_ai_overview, has_featured_snippet
load_async_ai_overview = false ; ADD THIS (W3 FIX)
discovery_max_pages = 100
people_also_ask_click_depth = 2
serp_features_exclude_filter = popular_products,local_pack,shopping,app,jobs,refine_products
closely_variants = false
discovery_exact_match = false ; ADD THIS (W7 Default: Disable for broad match by default)
min_cpc_filter = 0.0
max_cpc_filter = 999.0
min_competition = 0.0
max_competition = 1.0
max_competition_level = HIGH
discovery_ignore_synonyms = false


search_phrase_regex =

[IMAGE_GENERATION]
num_in_article_images = 2
use_pexels_first = true
cleanup_local_images = true
overlay_text_enabled = true
overlay_text_color = #FFFFFF
overlay_background_color = #00000080 ; RGBA hex for semi-transparent black
overlay_font_size = 40 ; Pixels
overlay_position = bottom_center ; top_left, top_right, bottom_left, bottom_center, bottom_right

[CONTENT]
generate_toc = true
default_author_name = Profit Parrot Marketing Expert
schema_author_type = Organization ; ADD THIS (W12 Default)

[ONPAGE_API_CLIENT_CONFIG] ; NEW: Separate section for OnPage API client parameters
onpage_enable_javascript = true
onpage_load_resources = true ; Set to true if browser rendering is enabled, as it forces loading resources.
onpage_disable_cookie_popup = true
onpage_return_despite_timeout = false
onpage_enable_browser_rendering = true
onpage_store_raw_html = false
onpage_validate_micromarkup = true
onpage_check_spell = true ; ADD THIS (W5 FIX)
onpage_accept_language = en ; ADD THIS (W7 FIX)
onpage_custom_user_agent = Mozilla/5.0 (compatible; RSiteAuditor)
onpage_max_domains_per_request = 5 ; Max 5 identical domains in one batch
onpage_max_tasks_per_request = 20
onpage_custom_checks_thresholds = {"high_loading_time": 2500} ; ADD THIS (W9 Default: Target < 2.5s instead of > 3s)
onpage_enable_switch_pool = true
ip_pool_for_scan = us
onpage_enable_custom_js = false ; ADD THIS (W12 Default)
onpage_custom_js = meta = {}; meta.url = document.URL; meta; ; ADD THIS (W12 Example)
onpage_browser_screen_resolution_ratio = 1.0 ; ADD THIS (W7 Default)

[APP_SETTINGS]
default_client_id = Lark_Main_Site
recommended_word_count_multiplier = 1.2
enable_automated_internal_linking = false

[SOCIAL_MEDIA]
platforms = facebook,linkedin,twitter,google_business_profile

[WORDPRESS_SETTINGS]
wordpress_url = 
wordpress_user = 
wordpress_app_password = 
wordpress_seo_plugin = aioseo
default_wordpress_categories = Blog
default_wordpress_tags = SEO,Content Marketing

[PAGE_CLASSIFICATION]
forum_domains = reddit.com,quora.com,stackoverflow.com,forums.somethingawful.com
ecommerce_domains = amazon.com,ebay.com,walmart.com,target.com,bestbuy.com,etsy.com
news_domains = nytimes.com,bbc.com,cnn.com,theguardian.com,wsj.com
blog_url_patterns = /blog/,/post/,/article/,/\d{4}/\d{2}/
forum_url_patterns = /thread/,/forum/,/discussion/,/q/,/questions/
ugc_and_parasite_domains = linkedin.com, pinterest.com, amazon.com, ebay.com, wikipedia.org, reddit.com, facebook.com, youtube.com, medium.com, quora.com

[DISQUALIFICATION_RULES]
; Tier 1
allowed_intents = informational
negative_keywords = login, sign in, account, free, cheap, porn
min_search_volume = 100

; Tier 2
yearly_trend_decline_threshold = -25
quarterly_trend_decline_threshold = 0
search_volume_volatility_threshold = 1.5

; Tier 3
max_paid_competition_score = 0.8
max_high_top_of_page_bid = 15.00
max_kd_hard_limit = 70
max_referring_main_domains_limit = 100
max_avg_domain_rank_threshold = 500
max_pages_to_domain_ratio = 15

; Tier 4
non_evergreen_year_pattern = 20[12]\d ; e.g., 2010-2029
min_keyword_word_count = 2
max_keyword_word_count = 8
hostile_serp_features = shopping,local_pack,google_flights,google_hotels,popular_products,local_services
crowded_serp_features_threshold = 4
min_serp_stability_days = 14
max_y_pixel_threshold = 800
max_forum_results_in_top_10 = 3
max_ecommerce_results_in_top_10 = 2
disallowed_page_types_in_top_3 = E-commerce,Forum

[FINAL_VALIDATION]
; Thresholds for the final, live SERP validation gate before running a full analysis.
disable_ai_overview_check = true
max_non_blog_results = 3
max_ai_overview_words = 250
max_first_organic_y_pixel = 1500
final_validation_non_blog_domains = amazon.com,walmart.com,ebay.com,reddit.com,quora.com
max_avg_lcp_time = 4000 ; NEW: For Rule 21 in run_final_validation

[ANALYSIS]
enable_deep_competitor_analysis = false
num_competitors_for_ai_analysis = 3
serp_analysis_depth = 100
max_words_for_ai_analysis = 2000
serp_remove_from_url_params = srsltid,utm_source,ref_id ; ADD THIS (W11 Default)

[WORD_COUNT_MULTIPLIERS]
default_multiplier = 1.2
comprehensive_article = 1.3
how_to_guide = 1.5
comparison_post = 1.1
review_article = 1.2
video_led_article = 0.8
forum_summary_post = 1.0
recipe_article = 1.0
scholarly_summary = 1.1
product_comparison = 1.2

[OPENAI_PRICING]
# Prices are per 1 million tokens
gpt-4o_input = 5.00
gpt-4o_output = 15.00
gpt-3.5-turbo_input = 0.50
gpt-3.5-turbo_output = 1.50

[OpenAI]
default_model = gpt-5-nano
default_image_model = dall-e-3
api_key = ${OPENAI_API_KEY}
```

## File: core/serp_analyzers/disqualification_analyzer.py
```python
# core/serp_analyzers/disqualification_analyzer.py
from typing import Dict, Any


class DisqualificationAnalyzer:
    def analyze(
        self, analysis: Dict[str, Any], config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Applies a set of granular rules to determine if a keyword should be disqualified.
        """
        disqualification_rules = config.get("disqualification_rules", {})

        # Rule: SERP is too crowded (pixel ranking)
        max_y_pixel = disqualification_rules.get("max_y_pixel_threshold")
        if max_y_pixel and analysis.get("first_organic_y_pixel") is not None:
            if analysis["first_organic_y_pixel"] > max_y_pixel:
                return {
                    "is_disqualified": True,
                    "disqualification_reason": f"First organic result is pushed down by {analysis['first_organic_y_pixel']} pixels, exceeding the {max_y_pixel}px threshold.",
                }

        # Rules based on page types in top 10
        top_10_results = analysis.get("top_organic_results", [])[:10]
        page_types = [result.get("page_type") for result in top_10_results]

        max_forum_results = disqualification_rules.get("max_forum_results_in_top_10")
        if (
            max_forum_results is not None
            and page_types.count("Forum") > max_forum_results
        ):
            return {
                "is_disqualified": True,
                "disqualification_reason": f"SERP contains {page_types.count('Forum')} forum results in the top 10, exceeding the threshold of {max_forum_results}.",
            }

        max_ecommerce_results = disqualification_rules.get(
            "max_ecommerce_results_in_top_10"
        )
        if (
            max_ecommerce_results is not None
            and page_types.count("E-commerce") > max_ecommerce_results
        ):
            return {
                "is_disqualified": True,
                "disqualification_reason": f"SERP contains {page_types.count('E-commerce')} e-commerce results in the top 10, exceeding the threshold of {max_ecommerce_results}.",
            }

        # Rule: Disallowed page types in top 3
        disallowed_in_top_3 = disqualification_rules.get(
            "disallowed_page_types_in_top_3", []
        )
        if disallowed_in_top_3:
            top_3_page_types = [
                result.get("page_type") for result in top_10_results[:3]
            ]
            for page_type in disallowed_in_top_3:
                if page_type in top_3_page_types:
                    return {
                        "is_disqualified": True,
                        "disqualification_reason": f"A '{page_type}' result was found in the top 3, which is a disallowed page type for high-ranking positions.",
                    }

        return {"is_disqualified": False, "disqualification_reason": None}
```

## File: core/serp_analyzers/featured_snippet_analyzer.py
```python
# core/serp_analyzers/featured_snippet_analyzer.py

from typing import Dict, Any


class FeaturedSnippetAnalyzer:
    def analyze(self, serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyzes the featured snippet in the SERP."""
        analysis = {
            "serp_has_featured_snippet": "featured_snippet"
            in serp_data.get("item_types", []),
            "featured_snippet_content": None,
        }

        for item in serp_data.get("items", []):
            if item.get("type") == "featured_snippet":
                analysis["featured_snippet_content"] = item.get("description")
                break

        return analysis
```

## File: core/serp_analyzers/pixel_ranking_analyzer.py
```python
# core/serp_analyzers/pixel_ranking_analyzer.py

from typing import Dict, Any


class PixelRankingAnalyzer:
    def analyze(self, serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyzes the pixel ranking data in the SERP."""
        analysis = {
            "pixel_ranking_summary": None,
            "raw_pixel_ranking_data": [],
            "first_organic_y_pixel": None,
        }

        for item in serp_data.get("items", []):
            if item.get("rectangle"):
                analysis["raw_pixel_ranking_data"].append(
                    {
                        "type": item.get("type"),
                        "rank_group": item.get("rank_group"),
                        "rank_absolute": item.get("rank_absolute"),
                        "title": item.get("title"),
                        "rectangle": item.get("rectangle"),
                    }
                )

        if analysis["raw_pixel_ranking_data"]:
            top_organic_rects_y_coords = [
                r["rectangle"]["y"]
                for r in analysis["raw_pixel_ranking_data"]
                if r["type"] == "organic"
                and r.get("rank_absolute", 99) <= 3
                and "y" in r.get("rectangle", {})
            ]
            if top_organic_rects_y_coords:
                avg_y = sum(top_organic_rects_y_coords) / len(
                    top_organic_rects_y_coords
                )
                analysis["pixel_ranking_summary"] = (
                    f"Top 3 organic results start an average of {avg_y:.0f} pixels from the top of the page."
                )

        first_organic_result = next(
            (
                r
                for r in analysis["raw_pixel_ranking_data"]
                if r["type"] == "organic" and r.get("rank_absolute") == 1
            ),
            None,
        )
        if first_organic_result and "y" in first_organic_result.get("rectangle", {}):
            analysis["first_organic_y_pixel"] = first_organic_result["rectangle"]["y"]

        return analysis
```

## File: core/serp_analyzers/video_analyzer.py
```python
# core/serp_analyzers/video_analyzer.py

from typing import Dict, Any


class VideoAnalyzer:
    def analyze(self, serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyzes the video results in the SERP."""
        analysis = {
            "serp_has_video_results": "video" in serp_data.get("item_types", [])
        }

        return analysis
```

## File: core/__init__.py
```python
# core/__init__.py
# This file marks the directory as a Python package.
```

## File: core/blueprint_factory.py
```python
import json
from datetime import datetime
from typing import Dict, Any
import logging

from backend.agents.brief_assembler import BriefAssembler
from backend.agents.internal_linking_suggester import InternalLinkingSuggester
from backend.core import utils
from backend.data_access.database_manager import DatabaseManager


class BlueprintFactory:
    def __init__(
        self, openai_client, client_cfg, dataforseo_client, db_manager: DatabaseManager
    ):
        self.brief_assembler = BriefAssembler(openai_client)
        self.internal_linking_suggester = InternalLinkingSuggester(
            openai_client, client_cfg, db_manager
        )
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = client_cfg

    def _create_executive_summary(self, blueprint_data: Dict[str, Any]) -> str:
        # Placeholder until SummaryGenerator is integrated here
        return "The executive summary will be generated by the AI based on the full analysis when implementation is complete."

    def create_blueprint(
        self,
        seed_topic: str,
        winning_keyword_data: Dict[str, Any],
        analysis_data: Dict[str, Any],
        total_api_cost: float,
        client_id: str,
    ) -> Dict[str, Any]:
        """Assembles all data into the final, structured JSON blueprint."""

        analysis_notes = None
        competitor_analysis_data = analysis_data.get("competitor_analysis", [])
        if not competitor_analysis_data or (
            len(competitor_analysis_data) == 1
            and "message" in competitor_analysis_data[0]
        ):
            analysis_notes = "No qualified article-based competitors were found in the top results after rigorous qualification. This SERP may be dominated by social media, video, or other non-article formats, making it a challenging topic to rank for with a standard blog post."
            competitor_analysis_data = []  # Ensure it's always an empty list of competitors

        recommended_strategy_data = analysis_data.get("recommended_strategy", {})
        self.logger.info(
            f"STRATEGY DATA FOR BLUEPRINT: {json.dumps(recommended_strategy_data)}"
        )

        blueprint_data = {
            "metadata": {
                "seed_topic": seed_topic,
                "blueprint_version": "6.0",
                "generated_at": datetime.now().isoformat(),
                "total_api_cost": round(total_api_cost, 4),
                "client_id": client_id,
            },
            "winning_keyword": winning_keyword_data,
            "serp_overview": analysis_data.get("serp_overview", {}),
            "content_intelligence": analysis_data.get("content_intelligence", {}),
            "competitor_analysis": competitor_analysis_data,
            "recommended_strategy": recommended_strategy_data,
            "final_qualification_assessment": recommended_strategy_data.get(
                "final_qualification_assessment", {}
            ),
            "analysis_notes": analysis_notes,
        }

        blueprint_data["executive_summary"] = self._create_executive_summary(
            blueprint_data
        )

        # --- START MODIFICATION ---
        # Pass rich serp_overview data to brief_assembler
        blueprint_data["ai_content_brief"] = self.brief_assembler.assemble_brief(
            blueprint_data, client_id, self.client_cfg
        )

        brief_text_for_linking = json.dumps(blueprint_data["ai_content_brief"])
        target_domain = self.client_cfg.get("target_domain")
        key_entities = blueprint_data.get("ai_content_brief", {}).get(
            "key_entities_to_mention", []
        )

        if brief_text_for_linking and target_domain:
            suggestions, linking_cost = self.internal_linking_suggester.suggest_links(
                brief_text_for_linking, key_entities, target_domain, client_id
            )
            blueprint_data["internal_linking_suggestions"] = suggestions
            blueprint_data["metadata"]["total_api_cost"] = round(
                blueprint_data["metadata"]["total_api_cost"] + linking_cost, 4
            )

        keyword_for_slug = winning_keyword_data.get("keyword", seed_topic)
        # Ensure the slug is part of the blueprint
        opportunity_slug = (
            f"{utils.slugify(keyword_for_slug)}-{int(datetime.now().timestamp())}"
        )
        blueprint_data["slug"] = opportunity_slug
        # --- END MODIFICATION ---

        return blueprint_data
```

## File: core/page_classifier.py
```python
# core/page_classifier.py
import re
from typing import Dict, Any
from urllib.parse import urlparse


class PageClassifier:
    """
    Categorizes a webpage based on its URL, domain, title, and other attributes.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config.get("page_classification", {})
        self.forum_domains = self.config.get("forum_domains", [])
        self.ecommerce_domains = self.config.get("ecommerce_domains", [])
        self.news_domains = self.config.get("news_domains", [])

        # Pre-compile regex for efficiency
        self.blog_patterns = [
            re.compile(p) for p in self.config.get("blog_url_patterns", [])
        ]
        self.forum_patterns = [
            re.compile(p) for p in self.config.get("forum_url_patterns", [])
        ]

    def classify(self, url: str, domain: str, title: str) -> str:
        """
        Classifies the given URL into a specific page type.

        Args:
            url: The full URL of the page.
            domain: The domain of the page.
            title: The title of the page.

        Returns:
            A string representing the classified page type.
        """
        if domain in self.ecommerce_domains:
            return "E-commerce"
        if domain in self.forum_domains:
            return "Forum"
        if domain in self.news_domains:
            return "News"

        # Check URL patterns for more specific types
        parsed_url = urlparse(url)
        path = parsed_url.path

        for pattern in self.forum_patterns:
            if pattern.search(path) or pattern.search(title.lower()):
                return "Forum"

        for pattern in self.blog_patterns:
            if pattern.search(path):
                return "Blog/Article"

        # Check for homepage/landing page (short path)
        if len(path.strip("/").split("/")) <= 1:
            return "Homepage/Landing Page"

        return "Blog/Article"  # Default category
```

## File: core/serp_analyzer.py
```python
import logging
from typing import Dict, Any, Tuple, Optional
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from core import utils
from core.serp_analyzers.featured_snippet_analyzer import FeaturedSnippetAnalyzer
from core.serp_analyzers.video_analyzer import VideoAnalyzer
from core.serp_analyzers.pixel_ranking_analyzer import PixelRankingAnalyzer
from core.page_classifier import PageClassifier
from core.serp_analyzers.disqualification_analyzer import DisqualificationAnalyzer


class FullSerpAnalyzer:
    """
    Performs a comprehensive analysis of the SERP for a given keyword.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.featured_snippet_analyzer = FeaturedSnippetAnalyzer()
        self.video_analyzer = VideoAnalyzer()
        self.pixel_ranking_analyzer = PixelRankingAnalyzer()
        self.page_classifier = PageClassifier(config)
        self.disqualification_analyzer = DisqualificationAnalyzer()

    def analyze_serp(self, keyword: str) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Fetches SERP data and extracts a wide range of insights, including rich SERP elements.
        """
        high_cost_operators = [
            "allinanchor:",
            "allintext:",
            "allintitle:",
            "allinurl:",
            "define:",
            "filetype:",
            "id:",
            "inanchor:",
            "info:",
            "intext:",
            "intitle:",
            "inurl:",
            "link:",
            "site:",
        ]
        keyword_lower = keyword.lower()

        if any(op in keyword_lower for op in high_cost_operators):
            raise ValueError(
                f"Keyword '{keyword}' contains a high-cost search operator. Please remove it and try again."
            )

        location_code = self.config.get("location_code")
        language_code = self.config.get("language_code")
        serp_call_params = {}

        serp_call_params["depth"] = 10  # Default to depth 10 for quick fetch

        paa_click_depth = self.config.get("people_also_ask_click_depth", 0)
        if isinstance(paa_click_depth, int) and 1 <= paa_click_depth <= 4:
            serp_call_params["people_also_ask_click_depth"] = paa_click_depth

        device = self.config.get("device", "desktop")
        os_name = self.config.get("os", "windows")
        if device == "mobile" and os_name not in ["android", "ios"]:
            os_name = "android"
        serp_call_params["device"] = device
        serp_call_params["os"] = os_name

        serp_results, cost = self.client.get_serp_results(
            keyword,
            location_code,
            language_code,
            client_cfg=self.config,
            serp_call_params=serp_call_params,
        )

        if not serp_results:
            return None, cost

        serp_times = utils.calculate_serp_times(
            serp_results.get("datetime"), serp_results.get("previous_updated_time")
        )

        analysis = {
            "serp_has_ai_overview": "ai_overview" in serp_results.get("item_types", []),
            "has_popular_products": "popular_products"
            in serp_results.get("item_types", []),
            "people_also_ask": [],  # Kept for backward compatibility, new field below is preferred
            "paa_questions": [],  # Primary field for cleaned PAA questions
            "top_organic_results": [],
            "related_searches": [],
            "knowledge_graph_data": {},  # Kept for backward compatibility
            "knowledge_graph_facts": [],  # NEW: Structured facts from KG
            "paid_ad_copy": [],  # NEW: Top paid ad titles/descriptions
            "ai_overview_content": None,
            "ai_overview_sources": [],  # NEW: URLs from AI Overview references
            "top_organic_faqs": [],  # NEW: FAQ questions from organic results
            "top_organic_sitelinks": [],  # NEW: Sitelinks from organic results
            "discussion_snippets": [],  # NEW: Snippets from discussions/forums/perspectives
            "product_considerations_summary": None,
            "refinement_chips": [],
            "extracted_serp_features": [],
            "serp_last_updated_days_ago": serp_times.get("days_ago"),
            "serp_update_interval_days": serp_times.get("update_interval_days"),
            "dominant_content_format": "Article",
        }

        analysis.update(self.featured_snippet_analyzer.analyze(serp_results))
        analysis.update(self.video_analyzer.analyze(serp_results))
        analysis.update(self.pixel_ranking_analyzer.analyze(serp_results))

        for item in serp_results.get("items") or []:
            item_type = item.get("type")

            # Organic Results
            if item_type == "organic":
                organic_result = {
                    "rank": item.get("rank_absolute"),
                    "url": item.get("url"),
                    "title": item.get("title"),
                    "domain": item.get("domain"),
                    "description": item.get("description"),
                    "page_type": self.page_classifier.classify(
                        item.get("url"), item.get("domain"), item.get("title")
                    ),
                }
                if item.get("rating"):
                    organic_result["rating"] = {
                        "value": item["rating"].get("value"),
                        "votes_count": item["rating"].get("votes_count"),
                        "rating_max": item["rating"].get("rating_max"),
                    }
                if item.get("about_this_result"):
                    organic_result["about_this_result_source_info"] = item[
                        "about_this_result"
                    ].get("source_info")
                    organic_result["about_this_result_search_terms"] = item[
                        "about_this_result"
                    ].get("search_terms")
                    organic_result["about_this_result_related_terms"] = item[
                        "about_this_result"
                    ].get("related_terms")

                # NEW: Extract FAQ and Sitelinks directly from organic results
                if item.get("faq") and item["faq"].get("items"):
                    analysis["top_organic_faqs"].extend(
                        [
                            faq_item.get("title")
                            for faq_item in item["faq"]["items"]
                            if faq_item.get("title")
                        ]
                    )
                if item.get("links"):
                    analysis["top_organic_sitelinks"].extend(
                        [
                            link.get("title")
                            for link in item["links"]
                            if link.get("title")
                        ]
                    )

                analysis["top_organic_results"].append(organic_result)

            # Paid Ads
            elif item_type == "paid":
                if item.get("title") and item.get("description"):
                    analysis["paid_ad_copy"].append(
                        {
                            "title": item.get("title"),
                            "description": item.get("description"),
                            "url": item.get("url"),
                        }
                    )

            # People Also Ask (PAA)
            elif item_type == "people_also_ask":
                all_paa_questions = []
                for paa_item in item.get("items") or []:
                    if paa_item and paa_item.get("title"):
                        (all_paa_questions.append(paa_item.get("title")),)
                    if paa_item and paa_item.get("expanded_element"):
                        for expanded_item in paa_item.get("expanded_element") or []:
                            if expanded_item and expanded_item.get("title"):
                                (all_paa_questions.append(expanded_item.get("title")),)
                analysis["paa_questions"] = list(set(all_paa_questions))
                analysis["people_also_ask"] = analysis[
                    "paa_questions"
                ]  # For backward compatibility

            # Knowledge Graph
            elif item_type == "knowledge_graph":
                analysis["knowledge_graph_data"] = {  # For backward compatibility
                    "title": item.get("title"),
                    "description": item.get("description"),
                    "url": item.get("url"),
                    "image_url": item.get("image_url"),
                }
                # NEW: Deep parse Knowledge Graph structured items
                if item.get("items"):
                    for kg_sub_item in item["items"]:
                        if (
                            kg_sub_item
                            and kg_sub_item.get("type") == "knowledge_graph_row_item"
                        ):
                            analysis["knowledge_graph_facts"].append(
                                f"{kg_sub_item.get('title')}: {kg_sub_item.get('text')}"
                            )
                        elif (
                            kg_sub_item
                            and kg_sub_item.get("type")
                            == "knowledge_graph_carousel_item"
                            and kg_sub_item.get("items")
                        ):
                            analysis["knowledge_graph_facts"].extend(
                                [
                                    carousel_el.get("title")
                                    for carousel_el in kg_sub_item["items"]
                                    if carousel_el.get("title")
                                ]
                            )
                        elif (
                            kg_sub_item
                            and kg_sub_item.get("type") == "knowledge_graph_list_item"
                            and kg_sub_item.get("items")
                        ):
                            analysis["knowledge_graph_facts"].extend(
                                [
                                    list_el.get("title")
                                    for list_el in kg_sub_item["items"]
                                    if list_el.get("title")
                                ]
                            )

            # AI Overview
            elif item_type == "ai_overview":
                ai_items = item.get("items") or []
                ai_parts = [
                    sub_item.get("markdown")
                    for sub_item in ai_items
                    if sub_item and sub_item.get("markdown")
                ]
                analysis["ai_overview_content"] = "\n".join(ai_parts)
                # NEW: Extract AI Overview References
                for sub_item in ai_items:
                    if sub_item and sub_item.get("references"):
                        analysis["ai_overview_sources"].extend(
                            [
                                ref.get("url")
                                for ref in sub_item["references"]
                                if ref.get("url")
                            ]
                        )

            # Discussions and Forums / Perspectives
            elif item_type in [
                "discussions_and_forums",
                "perspectives",
            ]:  # Combined handling
                if item.get("items"):
                    analysis["discussion_snippets"].extend(
                        [
                            d_item.get("title")
                            for d_item in item["items"]
                            if d_item.get("title")
                        ]
                    )

            # Related Searches
            elif item_type == "related_searches":
                related_items = item.get("items") or []
                for s in related_items:
                    if isinstance(s, str):
                        analysis["related_searches"].append(s)
                    elif isinstance(s, dict) and s.get("title"):
                        (analysis["related_searches"].append(s.get("title")),)

            # Product Considerations (existing)
            elif item_type == "product_considerations":
                title = item.get("title")
                items = item.get("items") or []
                if title and items:
                    considerations = [
                        sub.get("title") for sub in items if sub and sub.get("title")
                    ]
                    analysis["product_considerations_summary"] = (
                        f"{title}: {', '.join(considerations)}"
                    )

        # Deduplicate all lists
        analysis["ai_overview_sources"] = list(set(analysis["ai_overview_sources"]))
        analysis["top_organic_faqs"] = list(set(analysis["top_organic_faqs"]))
        analysis["top_organic_sitelinks"] = list(set(analysis["top_organic_sitelinks"]))
        analysis["discussion_snippets"] = list(set(analysis["discussion_snippets"]))

        # Determine dominant content format (existing logic)
        # ...

        disqualification_results = self.disqualification_analyzer.analyze(
            analysis, self.config
        )
        analysis.update(disqualification_results)

        return analysis, cost
```

## File: core/utils.py
```python
# core/utils.py
import logging
import re
from typing import Optional, Union, Dict
from datetime import datetime


def slugify(text: str) -> str:
    """
    Convert a string to a URL-friendly slug.
    """
    if not text:
        return ""
    text = text.lower()
    # Remove special characters
    text = re.sub(r"[^\w\s-]", "", text)
    # Replace spaces with hyphens
    text = re.sub(r"\s+", "-", text)
    return text


def is_question_keyword(keyword: str) -> bool:
    """
    Checks if a keyword is likely a question.
    Covers common question formats and leading words.
    """
    if not keyword:
        return False

    keyword_lower = keyword.lower().strip()

    # Common question prefixes
    question_starters = [
        "what",
        "when",
        "where",
        "who",
        "why",
        "how",
        "which",
        "whose",
        "is",
        "are",
        "am",
        "was",
        "were",
        "do",
        "does",
        "did",
        "can",
        "could",
        "will",
        "would",
        "should",
        "may",
        "might",
        "have",
        "has",
        "had",
        "are there",
        "is there",
    ]

    # Check if the keyword starts with a question word or ends with a question mark
    if keyword_lower.endswith("?"):
        return True

    for starter in question_starters:
        if keyword_lower.startswith(starter + " "):
            return True

    return False


def safe_compare(
    value: Optional[Union[int, float]],
    threshold: Optional[Union[int, float]],
    operation: str,
) -> bool:
    """
    Safely compares a potentially None value against a potentially None threshold.
    Returns False if either value is None to prevent TypeErrors.

    :param value: The value to check (e.g., from API data).
    :param threshold: The threshold to compare against (e.g., from config).
    :param operation: The comparison to perform ('gt' for >, 'lt' for <).
    :return: Boolean result of the comparison, or False if unsafe.
    """
    if value is None or threshold is None:
        return False

    if operation == "gt":
        return value > threshold
    elif operation == "lt":
        return value < threshold

    return False


def parse_datetime_string(dt_str: Optional[str]) -> Optional[str]:
    """
    Parses a DataForSEO datetime string (e.g., "yyyy-mm-dd hh-mm-ss +00:00")
    into a consistent ISO format string or returns None.
    """
    if not dt_str:
        return None

    # Remove timezone offset for consistent parsing if it's always +00:00
    cleaned_dt_str = dt_str.replace(" +00:00", "").strip()

    formats = [
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%dT%H:%M:%S",  # Added ISO 8601 format
        "%Y-%m-%d %H:%M:%S.%f",  # With microseconds
        "%Y-%m-%d",  # Date only
    ]

    for fmt in formats:
        try:
            return datetime.strptime(cleaned_dt_str, fmt).isoformat()
        except ValueError:
            pass

    logging.getLogger(__name__).warning(
        f"Could not parse datetime string: {dt_str}. Returning None."
    )
    return None


def calculate_serp_times(
    datetime_str: Optional[str], previous_datetime_str: Optional[str]
) -> Dict[str, Optional[int]]:
    """
    Calculates the age of the SERP and the interval between the last two updates.
    """
    days_ago = None
    update_interval_days = None

    if datetime_str:
        parsed_date_iso = parse_datetime_string(datetime_str)
        if parsed_date_iso:
            serp_date = datetime.fromisoformat(parsed_date_iso)
            days_ago = (datetime.utcnow() - serp_date).days
        else:
            logging.getLogger(__name__).warning(
                f"Could not parse SERP datetime for days_ago: {datetime_str}"
            )

    if datetime_str and previous_datetime_str:
        parsed_last_update_iso = parse_datetime_string(datetime_str)
        parsed_prev_update_iso = parse_datetime_string(previous_datetime_str)

        if parsed_last_update_iso and parsed_prev_update_iso:
            last_update_dt = datetime.fromisoformat(parsed_last_update_iso)
            prev_update_dt = datetime.fromisoformat(parsed_prev_update_iso)
            update_interval_days = abs((last_update_dt - prev_update_dt).days)
        else:
            logging.getLogger(__name__).warning(
                f"Could not parse SERP previous update times for interval: {datetime_str}, {previous_datetime_str}"
            )

    return {"days_ago": days_ago, "update_interval_days": update_interval_days}
```

## File: data_access/migrations/001_add_new_tables.sql
```sql
-- data_access/migrations/001_add_new_tables.sql

CREATE TABLE keyword_info (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    search_volume INTEGER,
    keyword_difficulty INTEGER,
    cpc REAL,
    competition REAL,
    search_volume_trend TEXT,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);

CREATE TABLE serp_overview (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    serp_has_featured_snippet BOOLEAN,
    serp_has_video_results BOOLEAN,
    serp_has_ai_overview BOOLEAN,
    people_also_ask TEXT,
    ai_overview_content TEXT,
    featured_snippet_content TEXT,
    avg_referring_domains_top5_organic REAL,
    avg_main_domain_rank_top5_organic REAL,
    serp_last_updated_days_ago INTEGER,
    dominant_content_format TEXT,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);
```

## File: data_access/migrations/001_initial_schema.sql
```sql
-- data_access/migrations/001_initial_schema.sql
-- This script sets up the initial schema for existing tables.
-- It should only be run if the tables do not exist.
-- The database_manager already creates them, so this is mainly for tracking.

-- CREATE TABLE IF NOT EXISTS opportunities (
--     id INTEGER PRIMARY KEY AUTOINCREMENT,
--     keyword TEXT NOT NULL,
--     status TEXT NOT NULL DEFAULT 'pending',
--     client_id TEXT NOT NULL DEFAULT 'default',
--     date_added TEXT NOT NULL,
--     date_processed TEXT,
--     full_data TEXT NOT NULL,
--     blueprint_data TEXT,
--     ai_content_json TEXT,
--     ai_content_model TEXT,
--     featured_image_url TEXT,
--     featured_image_local_path TEXT,
--     in_article_images_data TEXT,
--     social_media_posts_json TEXT,
--     last_workflow_step TEXT,
--     error_message TEXT,
--     wordpress_payload_json TEXT,
--     final_package_json TEXT,
--     UNIQUE(keyword, client_id)
-- );

-- CREATE TABLE IF NOT EXISTS clients (
--     client_id TEXT PRIMARY KEY,
--     client_name TEXT NOT NULL,
--     date_created TEXT NOT NULL
-- );

-- CREATE TABLE IF NOT EXISTS client_settings (
--     client_id TEXT PRIMARY KEY,
--     openai_api_key TEXT,
--     pexels_api_key TEXT,
--     location_code INTEGER,
--     language_code TEXT,
--     target_domain TEXT,
--     device TEXT,
--     os TEXT,
--     informational_score REAL,
--     commercial_score REAL,
--     transactional_score REAL,
--     navigational_score REAL,
--     question_keyword_bonus REAL,
--     ease_of_ranking_weight INTEGER,
--     traffic_potential_weight INTEGER,
--     commercial_intent_weight INTEGER,
--     growth_trend_weight INTEGER,
--     serp_features_weight INTEGER,
--     serp_freshness_weight INTEGER,
--     serp_volatility_weight INTEGER,
--     competitor_weakness_weight INTEGER,
--     max_cpc_for_scoring REAL,
--     max_sv_for_scoring INTEGER,
--     max_domain_rank_for_scoring INTEGER,
--     max_referring_domains_for_scoring INTEGER,
--     max_avg_referring_domains_filter INTEGER,
--     featured_snippet_bonus REAL,
--     ai_overview_bonus REAL,
--     serp_freshness_bonus_max REAL,
--     serp_freshness_old_threshold_days INTEGER,
--     serp_volatility_stable_threshold_days INTEGER,
--     enforce_intent_filter INTEGER,
--     allowed_intents TEXT,
--     require_question_keywords INTEGER,
--     negative_keywords TEXT,
--     min_monthly_trend_percentage REAL,
--     min_competitor_word_count INTEGER,
--     max_competitor_technical_warnings INTEGER,
--     competitor_blacklist_domains TEXT,
--     ugc_and_parasite_domains TEXT,
--     num_competitors_to_analyze INTEGER,
--     num_common_headings INTEGER,
--     num_unique_angles INTEGER,
--     max_initial_serp_urls_to_analyze INTEGER,
--     calculate_rectangles INTEGER,
--     people_also_ask_click_depth INTEGER,
--     min_search_volume INTEGER,
--     max_keyword_difficulty INTEGER,
--     ai_content_model TEXT,
--     num_in_article_images INTEGER,
--     use_pexels_first INTEGER,
--     cleanup_local_images INTEGER,
--     onpage_enable_javascript INTEGER,
--     onpage_load_resources INTEGER,
--     onpage_disable_cookie_popup INTEGER,
--     onpage_return_despite_timeout INTEGER,
--     onpage_enable_browser_rendering INTEGER,
--     onpage_store_raw_html INTEGER,
--     onpage_validate_micromarkup INTEGER,
--     onpage_custom_user_agent TEXT,
--     onpage_max_domains_per_request INTEGER,
--     onpage_max_tasks_per_request INTEGER,
--     platforms TEXT,
--     custom_prompt_template TEXT,
--     wordpress_url TEXT,
--     wordpress_user TEXT,
--     wordpress_app_password TEXT,
--     wordpress_seo_plugin TEXT,
--     default_wordpress_categories TEXT,
--     default_wordpress_tags TEXT,
--     enable_automated_internal_linking INTEGER,
--     db_type TEXT,
--     max_words_for_ai_analysis INTEGER,
--     ai_generation_temperature REAL,
--     recommended_word_count_multiplier REAL,
--     max_avg_lcp_time INTEGER,
--     prohibited_intents TEXT,
--     last_updated TEXT NOT NULL,
--     FOREIGN KEY (client_id) REFERENCES clients (client_id)
-- );

-- CREATE TABLE IF NOT EXISTS discovery_runs (
--     id INTEGER PRIMARY KEY AUTOINCREMENT,
--     client_id TEXT NOT NULL,
--     start_time TEXT NOT NULL,
--     end_time TEXT,
--     status TEXT NOT NULL,
--     parameters TEXT,
--     results_summary TEXT,
--     log_file_path TEXT,
--     error_message TEXT,
--     FOREIGN KEY (client_id) REFERENCES clients (client_id)
-- );

-- These are initially created by `database_manager.py` before migrations start.
-- This file exists for migration tracking purposes.
```

## File: data_access/migrations/002_add_keywords_table.sql
```sql
-- data_access/migrations/011_add_keywords_table.sql

CREATE TABLE keywords (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    keyword TEXT NOT NULL UNIQUE,
    search_volume INTEGER,
    keyword_difficulty INTEGER,
    cpc REAL,
    competition REAL,
    search_volume_trend TEXT,
    main_intent TEXT,
    core_keyword TEXT
);

ALTER TABLE opportunities ADD COLUMN keyword_id INTEGER REFERENCES keywords(id);
CREATE INDEX IF NOT EXISTS idx_opportunities_keyword_id ON opportunities (keyword_id);
```

## File: data_access/migrations/002_remove_json_columns.sql
```sql
-- data_access/migrations/002_remove_json_columns.sql

-- This migration is intentionally left blank.
-- SQLite does not support dropping columns.
-- The data from the JSON columns will be migrated to the new tables in the application logic.
-- The old columns will be ignored by the application.
```

## File: data_access/migrations/003_add_indexes.sql
```sql
-- data_access/migrations/003_add_indexes.sql

CREATE INDEX IF NOT EXISTS idx_opportunities_status ON opportunities (status);
CREATE INDEX IF NOT EXISTS idx_opportunities_client_id ON opportunities (client_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_strategic_score ON opportunities (strategic_score);
CREATE INDEX IF NOT EXISTS idx_opportunities_run_id ON opportunities (run_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_keyword_id ON opportunities (keyword_id);
CREATE INDEX IF NOT EXISTS idx_opportunities_slug ON opportunities (slug);
```

## File: data_access/migrations/003_add_total_api_cost.sql
```sql
-- Add a column to store the total API cost for the entire workflow
ALTER TABLE opportunities ADD COLUMN total_api_cost REAL DEFAULT 0.0;
```

## File: data_access/migrations/004_add_qualification_settings_table.sql
```sql
-- data_access/migrations/004_add_qualification_settings_table.sql

CREATE TABLE qualification_settings (
    client_id TEXT PRIMARY KEY,
    ease_of_ranking_weight REAL,
    traffic_potential_weight REAL,
    commercial_intent_weight REAL,
    serp_features_weight REAL,
    growth_trend_weight REAL,
    serp_freshness_weight REAL,
    serp_volatility_weight REAL,
    competitor_weakness_weight REAL,
    competitor_performance_weight REAL,
    informational_intent_weight REAL,
    navigational_intent_weight REAL,
    transactional_intent_weight REAL,
    competitor_strength_weight REAL,
    trend_weight REAL,
    seasonality_weight REAL,
    review_threshold REAL,
    disqualification_rules TEXT,
    brand_keywords TEXT,
    competitor_brand_keywords TEXT,
    min_search_volume INTEGER,
    max_keyword_difficulty INTEGER,
    negative_keywords TEXT,
    prohibited_intents TEXT,
    max_y_pixel_threshold INTEGER,
    max_forum_results_in_top_10 INTEGER,
    max_ecommerce_results_in_top_10 INTEGER,
    disallowed_page_types_in_top_3 TEXT,
    FOREIGN KEY (client_id) REFERENCES clients (client_id)
);
```

## File: data_access/migrations/005_add_qualification_columns.sql
```sql
-- data_access/migrations/005_add_qualification_columns.sql

ALTER TABLE opportunities ADD COLUMN strategic_score REAL;
ALTER TABLE opportunities ADD COLUMN score_breakdown TEXT;
ALTER TABLE opportunities ADD COLUMN qualification_status TEXT;
ALTER TABLE opportunities ADD COLUMN qualification_reason TEXT;
```

## File: data_access/migrations/006_add_intent_weights.sql
```sql
-- data_access/migrations/006_add_intent_weights.sql

ALTER TABLE qualification_settings ADD COLUMN informational_intent_weight REAL;
ALTER TABLE qualification_settings ADD COLUMN navigational_intent_weight REAL;
ALTER TABLE qualification_settings ADD COLUMN commercial_intent_weight REAL;
ALTER TABLE qualification_settings ADD COLUMN transactional_intent_weight REAL;
```

## File: data_access/migrations/007_add_competitor_strength_weight.sql
```sql
-- data_access/migrations/007_add_competitor_strength_weight.sql

ALTER TABLE qualification_settings ADD COLUMN competitor_strength_weight REAL;
```

## File: data_access/migrations/008_add_serp_features_weight.sql
```sql
-- data_access/migrations/008_add_serp_features_weight.sql

ALTER TABLE qualification_settings ADD COLUMN serp_features_weight REAL;
```

## File: data_access/migrations/009_add_trend_weight.sql
```sql
-- data_access/migrations/009_add_trend_weight.sql

ALTER TABLE qualification_settings ADD COLUMN trend_weight REAL;
```

## File: data_access/migrations/010_add_history_columns.sql
```sql
-- data_access/migrations/010_add_history_columns.sql

ALTER TABLE opportunities ADD COLUMN last_seen_at TEXT;
ALTER TABLE opportunities ADD COLUMN metrics_history TEXT;
```

## File: data_access/migrations/012_add_seasonality_weight.sql
```sql
-- data_access/migrations/012_add_seasonality_weight.sql

ALTER TABLE qualification_settings ADD COLUMN seasonality_weight REAL;
```

## File: data_access/migrations/013_add_serp_volatility_weight.sql
```sql
-- data_access/migrations/013_add_serp_volatility_weight.sql

ALTER TABLE qualification_settings ADD COLUMN serp_volatility_weight REAL;
```

## File: data_access/migrations/014_add_disqualification_rules.sql
```sql
-- data_access/migrations/014_add_disqualification_rules.sql

ALTER TABLE qualification_settings ADD COLUMN disqualification_rules TEXT;
```

## File: data_access/migrations/015_add_brand_keywords.sql
```sql
-- data_access/migrations/015_add_brand_keywords.sql

ALTER TABLE qualification_settings ADD COLUMN brand_keywords TEXT;
ALTER TABLE qualification_settings ADD COLUMN competitor_brand_keywords TEXT;
```

## File: data_access/migrations/016_add_review_threshold.sql
```sql
-- data_access/migrations/016_add_review_threshold.sql

ALTER TABLE qualification_settings ADD COLUMN review_threshold REAL;
```

## File: data_access/migrations/017_add_strategies_table.sql
```sql
-- data_access/migrations/017_add_strategies_table.sql

CREATE TABLE qualification_strategies (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    client_id TEXT NOT NULL,
    name TEXT NOT NULL,
    traffic_potential_weight REAL,
    keyword_difficulty_weight REAL,
    cpc_weight REAL,
    search_intent_weight REAL,
    min_search_volume INTEGER,
    max_keyword_difficulty INTEGER,
    negative_keywords TEXT,
    prohibited_intents TEXT,
    informational_intent_weight REAL,
    navigational_intent_weight REAL,
    commercial_intent_weight REAL,
    transactional_intent_weight REAL,
    competitor_strength_weight REAL,
    serp_features_weight REAL,
    trend_weight REAL,
    seasonality_weight REAL,
    serp_volatility_weight REAL,
    review_threshold REAL,
    disqualification_rules TEXT,
    brand_keywords TEXT,
    competitor_brand_keywords TEXT,
    FOREIGN KEY (client_id) REFERENCES clients (client_id)
);
```

## File: data_access/migrations/018_add_job_id_and_cluster_name.sql
```sql
-- data_access/migrations/018_add_job_id_and_cluster_name.sql

ALTER TABLE opportunities ADD COLUMN latest_job_id TEXT;
ALTER TABLE opportunities ADD COLUMN cluster_name TEXT;
```

## File: data_access/migrations/019_add_core_keyword_and_competitor_metrics_to_opportunities.sql
```sql
-- data_access/migrations/019_add_core_keyword_and_competitor_metrics_to_opportunities.sql

-- Add new columns for direct access to frequently used keyword metrics
ALTER TABLE opportunities ADD COLUMN cpc REAL DEFAULT 0.0;
ALTER TABLE opportunities ADD COLUMN competition REAL DEFAULT 0.0;
ALTER TABLE opportunities ADD COLUMN main_intent TEXT DEFAULT 'informational';
ALTER TABLE opportunities ADD COLUMN search_volume_trend_json TEXT;

-- Add new columns for storing aggregated competitor data for easier access
ALTER TABLE opportunities ADD COLUMN competitor_social_media_tags_json TEXT;
ALTER TABLE opportunities ADD COLUMN competitor_page_timing_json TEXT;

-- Create indexes on these new columns for improved query performance
CREATE INDEX idx_opportunities_cpc ON opportunities (cpc);
CREATE INDEX idx_opportunities_competition ON opportunities (competition);
CREATE INDEX idx_opportunities_main_intent ON opportunities (main_intent);
```

## File: data_access/migrations/020_backfill_core_keyword_metrics.sql
```sql
-- data_access/migrations/020_backfill_core_keyword_metrics.sql

-- Backfill cpc, competition, main_intent, and search_volume_trend_json from existing keyword_info and search_intent_info JSON blobs
UPDATE opportunities
SET
    cpc = CAST(JSON_EXTRACT(keyword_info, '$.cpc') AS REAL),
    competition = CAST(JSON_EXTRACT(keyword_info, '$.competition') AS REAL),
    main_intent = JSON_EXTRACT(search_intent_info, '$.main_intent'),
    search_volume_trend_json = JSON_EXTRACT(keyword_info, '$.search_volume_trend')
WHERE
    keyword_info IS NOT NULL AND search_intent_info IS NOT NULL;

-- Backfill aggregated competitor social media tags and page timing from blueprint_data
-- This requires iterating through the competitor_analysis array within blueprint_data
-- Note: SQLite's JSON functions can be limited for complex array aggregation directly in SQL.
-- This might require application-level backfill for more complex aggregations if `blueprint_data` is large.
-- For a simple direct copy of the *first* competitor's data (as an example), or an empty JSON if none:
UPDATE opportunities
SET
    competitor_social_media_tags_json = (
        SELECT CASE
            WHEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].social_media_tags') IS NOT NULL
            THEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].social_media_tags')
            ELSE '{}'
        END
    ),
    competitor_page_timing_json = (
        SELECT CASE
            WHEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].page_timing') IS NOT NULL
            THEN JSON_EXTRACT(blueprint_data, '$.competitor_analysis[0].page_timing')
            ELSE '{}'
        END
    )
WHERE
    blueprint_data IS NOT NULL;
```

## File: data_access/migrations/021_add_unique_keyword_constraint.sql
```sql
CREATE UNIQUE INDEX IF NOT EXISTS idx_unique_client_keyword ON opportunities (client_id, keyword);
```

## File: data_access/migrations/023_add_run_id_to_opportunities.sql
```sql
ALTER TABLE opportunities ADD COLUMN run_id INTEGER;
CREATE INDEX IF NOT EXISTS idx_opportunities_run_id ON opportunities (run_id);
```

## File: data_access/migrations/024_add_total_api_cost_to_opportunities.sql
```sql
ALTER TABLE opportunities ADD COLUMN total_api_cost REAL DEFAULT 0.0;
```

## File: data_access/migrations/025_add_cost_to_discovery_runs.sql
```sql
-- Add a column to store the total API cost for a discovery run
ALTER TABLE discovery_runs ADD COLUMN total_api_cost REAL DEFAULT 0.0;
```

## File: data_access/__init__.py
```python
# data_access/__init__.py
# This file marks the directory as a Python package.
```

## File: data_access/database_manager.py
```python
import sqlite3
import json
import threading
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
import logging
import bleach  # ADD THIS LINE
import os
from . import queries
from backend.app_config.manager import ConfigManager

ALLOWED_ATTRIBUTES_DB = {
    "*": ["id", "class"],
    "a": ["href", "title"],
    "img": ["src", "alt", "width", "height"],
}

# W20 FIX: Define sanitization constants globally or as module constants (based on router update logic)
ALLOWED_TAGS = list(bleach.sanitizer.ALLOWED_TAGS) + [
    "h1",
    "h2",
    "h3",
    "h4",
    "h5",
    "h6",
    "p",
    "br",
    "a",
    "i",
    "u",
    "em",
    "strong",
    "blockquote",
    "li",
    "ul",
    "ol",
    "img",
    "div",
    "span",
    "table",
    "thead",
    "tbody",
    "tr",
    "td",
    "th",
    "code",
    "pre",
]
ALLOWED_ATTRIBUTES = bleach.sanitizer.ALLOWED_ATTRIBUTES.copy()
ALLOWED_ATTRIBUTES.update(
    {
        "a": ["href", "title"],
        "img": ["src", "alt", "width", "height", "style"],
        "*": ["id", "class", "style"],
    }
)

DB_FILE = "data/opportunities.db"


class DatabaseManager:
    """Handles all interactions with the SQLite opportunity queue database."""

    def __init__(
        self, cfg_manager: Optional[ConfigManager] = None, db_path: Optional[str] = None
    ):  # db_path is now passed via cfg_manager
        self.cfg_manager = cfg_manager

        # Define project root relative to this file's location
        project_root = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "..")
        )

        if db_path:
            self.db_path = db_path
            self.db_type = "sqlite"
        elif cfg_manager:
            global_cfg = cfg_manager.get_global_config()
            db_file_name = global_cfg.get("db_file_name", DB_FILE)
            self.db_path = os.path.join(
                project_root, db_file_name
            )  # Ensure path is relative to project root
            self.db_type = global_cfg.get("db_type", "sqlite")
        else:
            db_file_name = DB_FILE
            self.db_path = os.path.join(
                project_root, db_file_name
            )  # Ensure path is relative to project root
            self.db_type = "sqlite"

        self.logger = logging.getLogger(self.__class__.__name__)
        self._thread_local = threading.local()

    def initialize(self):
        """Connects to the DB, creates tables, applies migrations, and ensures default client exists."""
        conn = self._get_conn()
        try:
            # Combine all CREATE TABLE statements into a single script for atomic execution
            full_schema_script = f"""
                {queries.CREATE_OPPORTUNITIES_TABLE}
                {queries.CREATE_CLIENTS_TABLE}
                {queries.CREATE_CLIENT_SETTINGS_TABLE}
                {queries.CREATE_DISCOVERY_RUNS_TABLE}
                {queries.CREATE_SCHEMA_VERSION_TABLE}
                {queries.CREATE_JOBS_TABLE}
                {queries.CREATE_CONTENT_HISTORY_TABLE}
                {queries.CREATE_CONTENT_FEEDBACK_TABLE}
                {queries.CREATE_API_CACHE_TABLE}
            """
            with conn:  # Ensure transaction for initial setup
                conn.executescript(full_schema_script)

            self._apply_migrations_from_files()
            self._ensure_default_client_exists(conn)  # Add this line
            self.logger.info("Database initialized.")
        finally:
            self._close_conn()

    def _get_conn(self):
        """Gets a connection from the thread-local storage or creates a new one."""
        if not hasattr(self._thread_local, "conn") or self._thread_local.conn is None:
            if self.db_type == "sqlite":
                self._thread_local.conn = sqlite3.connect(
                    self.db_path, check_same_thread=False
                )
                self._thread_local.conn.row_factory = sqlite3.Row
            else:
                raise NotImplementedError(
                    f"External database type '{self.db_type}' is not yet implemented."
                )
        return self._thread_local.conn

    def _close_conn(self):
        """Closes the connection for the current thread."""
        if hasattr(self._thread_local, "conn") and self._thread_local.conn is not None:
            self._thread_local.conn.close()
            self._thread_local.conn = None

    def _ensure_default_client_exists(self, conn):
        """Checks for and creates the default client if it doesn't exist in the database."""
        if not self.cfg_manager:
            return

        global_cfg = self.cfg_manager.get_global_config()
        default_id = global_cfg.get("default_client_id")

        if not default_id:
            self.logger.warning("No default_client_id found in configuration.")
            return

        conn = self._get_conn()
        cursor = conn.execute(
            "SELECT 1 FROM clients WHERE client_id = ?", (default_id,)
        )
        if cursor.fetchone() is None:
            self.logger.warning(
                f"Default client '{default_id}' not found in database. Creating it now."
            )
            default_settings_template = (
                self.cfg_manager.get_default_client_settings_template()
            )
            self.add_client(default_id, default_id, default_settings_template)

    def _get_current_schema_version(self, conn) -> int:
        """Retrieves the current schema version from the database."""
        cursor = conn.execute(queries.GET_SCHEMA_VERSION)
        result = cursor.fetchone()
        return result["version"] if result else 0

    def _apply_migrations_from_files(self):
        """Applies SQL migration scripts from the migrations directory."""
        conn = self._get_conn()
        try:
            current_version = self._get_current_schema_version(conn)
            migrations_dir = os.path.join(os.path.dirname(__file__), "migrations")

            if not os.path.exists(migrations_dir):
                self.logger.warning(
                    f"Migrations directory not found: {migrations_dir}. Skipping migrations."
                )
                return

            migration_files = sorted(
                [f for f in os.listdir(migrations_dir) if f.endswith(".sql")]
            )

            for filename in migration_files:
                version_str = filename.split("_")[0]
                if not version_str.isdigit():
                    self.logger.warning(
                        f"Skipping malformed migration file: {filename}"
                    )
                    continue

                version = int(version_str)

                if version > current_version:
                    self.logger.info(f"Applying migration {filename}...")
                    filepath = os.path.join(migrations_dir, filename)
                    with open(filepath, "r") as f:
                        sql_script = f.read()

                    print(f"Executing migration script: {filename}")
                    print(sql_script)

                    try:
                        with conn:  # Execute in a transaction
                            conn.executescript(sql_script)
                            conn.execute(
                                queries.INSERT_SCHEMA_VERSION,
                                (version, datetime.now().isoformat()),
                            )
                        self.logger.info(f"Migration {filename} applied successfully.")
                        current_version = version
                    except sqlite3.OperationalError as e:
                        if "duplicate column name" in str(e):
                            self.logger.warning(
                                f"Migration {filename} failed because a column already exists. Assuming it was already applied and continuing."
                            )
                            conn.execute(
                                queries.INSERT_SCHEMA_VERSION,
                                (version, datetime.now().isoformat()),
                            )
                            current_version = version
                        else:
                            raise e
                else:
                    self.logger.debug(f"Migration {filename} already applied.")
            self.logger.info("Database migration check complete.")
        except Exception as e:
            self.logger.error(f"Error during database migration: {e}", exc_info=True)
            raise
        finally:
            self._close_conn()  # Ensure connection is closed after migrations

    def _deserialize_rows(self, rows: List[sqlite3.Row]) -> List[Dict[str, Any]]:
        """Deserializes JSON strings from database rows into a clean dictionary."""
        results = []

        json_keys = [
            "blueprint_data",
            "ai_content_json",
            "in_article_images_data",
            "social_media_posts_json",
            "final_package_json",
            "wordpress_payload_json",
            "keyword_info",
            "keyword_properties",
            "search_intent_info",
            "serp_overview",
            "score_breakdown",
            "keyword_info_normalized_with_bing",
            "keyword_info_normalized_with_clickstream",
            "monthly_searches",
            "full_data",
            "search_volume_trend_json",
            "competitor_social_media_tags_json",
            "competitor_page_timing_json",  # ADDED THIS LINE
        ]

        for row in rows:
            final_item = dict(row)

            # Deserialize all JSON fields first
            for key in json_keys:
                if key in final_item and isinstance(final_item[key], str):
                    try:
                        final_item[key] = json.loads(final_item[key])
                    except json.JSONDecodeError:
                        self.logger.warning(
                            f"Failed to parse JSON for key '{key}' on row ID {final_item.get('id')}. Leaving as raw string."
                        )

            # --- Data Unification and Renaming (with added safety checks and handling promoted columns) ---
            # Ensure direct columns are prioritized; if null, try to extract from old JSON blobs for backward compatibility
            if final_item.get("main_intent") is None and isinstance(
                final_item.get("search_intent_info"), dict
            ):
                final_item["main_intent"] = final_item["search_intent_info"].get(
                    "main_intent"
                )
            if final_item.get("cpc") is None and isinstance(
                final_item.get("keyword_info"), dict
            ):
                final_item["cpc"] = float(final_item["keyword_info"].get("cpc") or 0.0)
            if final_item.get("competition") is None and isinstance(
                final_item.get("keyword_info"), dict
            ):
                final_item["competition"] = float(
                    final_item["keyword_info"].get("competition") or 0.0
                )
            if final_item.get("search_volume") is None and isinstance(
                final_item.get("keyword_info"), dict
            ):
                final_item["search_volume"] = int(
                    final_item["keyword_info"].get("search_volume") or 0
                )
            if final_item.get("keyword_difficulty") is None and isinstance(
                final_item.get("keyword_properties"), dict
            ):
                final_item["keyword_difficulty"] = int(
                    final_item["keyword_properties"].get("keyword_difficulty") or 0
                )

            # Deserialize search_volume_trend_json if present in new column
            if isinstance(final_item.get("search_volume_trend_json"), str):
                try:
                    final_item["search_volume_trend"] = json.loads(
                        final_item["search_volume_trend_json"]
                    )
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse search_volume_trend_json for row ID {final_item.get('id')}. Resetting."
                    )
                    final_item["search_volume_trend"] = {}
            # Fallback to old keyword_info if new column is empty
            elif isinstance(final_item.get("keyword_info"), dict):
                final_item["search_volume_trend"] = final_item["keyword_info"].get(
                    "search_volume_trend"
                )

            # Deserialize competitor_social_media_tags_json
            if isinstance(final_item.get("competitor_social_media_tags_json"), str):
                try:
                    final_item["competitor_social_media_tags"] = json.loads(
                        final_item["competitor_social_media_tags_json"]
                    )
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse competitor_social_media_tags_json for row ID {final_item.get('id')}. Resetting."
                    )
                    final_item["competitor_social_media_tags"] = {}

            # Deserialize competitor_page_timing_json
            if isinstance(final_item.get("competitor_page_timing_json"), str):
                try:
                    final_item["competitor_page_timing"] = json.loads(
                        final_item["competitor_page_timing_json"]
                    )
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse competitor_page_timing_json for row ID {final_item.get('id')}. Resetting."
                    )
                    final_item["competitor_page_timing"] = {}

            # Ensure keyword_properties is a dict before assigning to it (for `intent` field that might be manually added)
            if not isinstance(final_item.get("keyword_properties"), dict):
                final_item["keyword_properties"] = {}
            if final_item.get("main_intent") and isinstance(
                final_item.get("keyword_properties"), dict
            ):
                final_item["keyword_properties"]["intent"] = final_item["main_intent"]

            # Simplify monthly_searches if stored as JSON string directly
            if isinstance(
                final_item.get("monthly_searches_json"), str
            ):  # This is from Task 1.2
                try:
                    final_item["monthly_searches"] = json.loads(
                        final_item["monthly_searches_json"]
                    )
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse monthly_searches_json for row ID {final_item.get('id')}. Resetting."
                    )
                    final_item["monthly_searches"] = []
            # Fallback to old keyword_info if new column is empty
            elif isinstance(
                final_item.get("keyword_info"), dict
            ):  # This is the old way, still in place for historical data
                final_item["monthly_searches"] = final_item["keyword_info"].get(
                    "monthly_searches"
                )

            if "blueprint_data" in final_item:
                final_item["blueprint"] = final_item.pop("blueprint_data")
            if "ai_content_json" in final_item:
                final_item["ai_content"] = final_item.pop("ai_content_json")

            results.append(final_item)
        return results

    def add_opportunity(self, client_id: str, opportunity_data: Dict[str, Any]):
        """Adds a new opportunity to the database from a dictionary."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO opportunities (
                    keyword, client_id, status, date_added, date_processed, 
                    strategic_score, keyword_info, keyword_properties, 
                    search_intent_info, serp_overview, score_breakdown, ai_content_json
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    opportunity_data.get("keyword"),
                    client_id,
                    opportunity_data.get("status", "pending"),
                    opportunity_data.get("date_added", datetime.now().isoformat()),
                    opportunity_data.get("date_processed"),
                    opportunity_data.get("strategic_score"),
                    json.dumps(opportunity_data.get("keyword_info")),
                    json.dumps(opportunity_data.get("keyword_properties")),
                    json.dumps(opportunity_data.get("search_intent_info")),
                    json.dumps(opportunity_data.get("serp_overview")),
                    json.dumps(opportunity_data.get("score_breakdown")),
                    json.dumps(opportunity_data.get("ai_content")),
                ),
            )
            return cursor.lastrowid

    def add_opportunities(
        self, opportunities: List[Dict[str, Any]], client_id: str, run_id: int
    ) -> int:
        """Adds multiple opportunities to the database in a single transaction, updating existing ones."""
        conn = self._get_conn()

        with conn:
            cursor = conn.cursor()
            for opp in opportunities:
                keyword = opp.get("keyword")
                cursor.execute("SELECT id FROM keywords WHERE keyword = ?", (keyword,))
                keyword_row = cursor.fetchone()

                keyword_info = opp.get("keyword_info", {})
                keyword_properties = opp.get("keyword_properties", {})
                search_intent_info = opp.get("search_intent_info", {})

                if keyword_row:
                    keyword_id = keyword_row["id"]
                    # Update existing keyword
                    cursor.execute(
                        """
                        UPDATE keywords
                        SET search_volume = ?, keyword_difficulty = ?, cpc = ?, competition = ?, search_volume_trend = ?, main_intent = ?, core_keyword = ?
                        WHERE id = ?
                    """,
                        (
                            keyword_info.get("search_volume"),
                            keyword_properties.get("keyword_difficulty"),
                            keyword_info.get("cpc"),
                            keyword_info.get("competition"),
                            json.dumps(keyword_info.get("search_volume_trend")),
                            search_intent_info.get("main_intent"),
                            keyword_properties.get("core_keyword"),
                            keyword_id,
                        ),
                    )
                else:
                    # Insert new keyword
                    cursor.execute(
                        """
                        INSERT INTO keywords (keyword, search_volume, keyword_difficulty, cpc, competition, search_volume_trend, main_intent, core_keyword)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            keyword,
                            keyword_info.get("search_volume"),
                            keyword_properties.get("keyword_difficulty"),
                            keyword_info.get("cpc"),
                            keyword_info.get("competition"),
                            json.dumps(keyword_info.get("search_volume_trend")),
                            search_intent_info.get("main_intent"),
                            keyword_properties.get("core_keyword"),
                        ),
                    )
                    keyword_id = cursor.lastrowid

                cursor.execute(
                    "SELECT id, metrics_history FROM opportunities WHERE client_id = ? AND keyword_id = ?",
                    (client_id, keyword_id),
                )
                opportunity_row = cursor.fetchone()

                if opportunity_row:
                    # Update existing opportunity
                    history = (
                        json.loads(opportunity_row["metrics_history"])
                        if opportunity_row["metrics_history"]
                        else []
                    )
                    history.append(
                        {
                            "date": datetime.now().isoformat(),
                            "search_volume": keyword_info.get("search_volume"),
                            "keyword_difficulty": keyword_properties.get(
                                "keyword_difficulty"
                            ),
                            "cpc": keyword_info.get("cpc"),
                        }
                    )
                    cursor.execute(
                        """
                        UPDATE opportunities
                        SET last_seen_at = ?, metrics_history = ?
                        WHERE id = ?
                    """,
                        (
                            datetime.now().isoformat(),
                            json.dumps(history),
                            opportunity_row["id"],
                        ),
                    )
                else:
                    # Insert new opportunity
                    # Extract values for direct columns, potentially nulling them out from JSON if no longer needed there
                    cpc_val = keyword_info.get("cpc")
                    competition_val = keyword_info.get("competition")
                    main_intent_val = search_intent_info.get("main_intent")
                    search_volume_trend_json_val = json.dumps(
                        keyword_info.get("search_volume_trend")
                    )

                    # Aggregate top competitor data for direct columns
                    top_competitor = next(
                        (
                            comp
                            for comp in opp.get("blueprint", {}).get(
                                "competitor_analysis", []
                            )
                            if comp.get("url")
                        ),
                        None,
                    )
                    competitor_social_media_tags_json_val = (
                        json.dumps(top_competitor.get("social_media_tags", {}))
                        if top_competitor
                        else None
                    )
                    competitor_page_timing_json_val = (
                        json.dumps(top_competitor.get("page_timing", {}))
                        if top_competitor
                        else None
                    )

                    cursor.execute(
                        """
                        INSERT INTO opportunities (
                            keyword, client_id, run_id, status, date_added, date_processed,
                            strategic_score, blog_qualification_status, blog_qualification_reason,
                            keyword_info, keyword_properties,
                            search_intent_info, serp_overview, score_breakdown, ai_content_json,
                            keyword_info_normalized_with_bing, keyword_info_normalized_with_clickstream, monthly_searches, traffic_value,
                            check_url, related_keywords, keyword_categories, core_keyword, last_seen_at, metrics_history, keyword_id,
                            full_data,
                            cpc, competition, main_intent, search_volume_trend_json,
                            competitor_social_media_tags_json, competitor_page_timing_json,
                        social_media_posts_status
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            keyword,
                            client_id,
                            run_id,
                            opp.get("status", "pending"),
                            opp.get("date_added", datetime.now().isoformat()),
                            opp.get("date_processed"),
                            opp.get("strategic_score"),
                            opp.get("blog_qualification_status"),
                            opp.get("blog_qualification_reason"),
                            json.dumps(keyword_info),
                            json.dumps(keyword_properties),
                            json.dumps(search_intent_info),
                            json.dumps(opp.get("serp_overview")),
                            json.dumps(opp.get("score_breakdown")),
                            json.dumps(opp.get("ai_content")),
                            json.dumps(opp.get("keyword_info_normalized_with_bing")),
                            json.dumps(
                                opp.get("keyword_info_normalized_with_clickstream")
                            ),
                            json.dumps(keyword_info.get("monthly_searches")),
                            opp.get("traffic_value", 0),
                            opp.get("serp_info", {}).get("check_url"),
                            json.dumps(opp.get("related_keywords")),
                            json.dumps(keyword_info.get("categories")),
                            keyword_properties.get("core_keyword"),
                            datetime.now().isoformat(),
                            json.dumps([]),
                            keyword_id,
                            json.dumps(opp),
                            cpc_val,  # NEW DIRECT COLUMN
                            competition_val,  # NEW DIRECT COLUMN
                            main_intent_val,  # NEW DIRECT COLUMN
                            search_volume_trend_json_val,  # NEW DIRECT COLUMN
                            competitor_social_media_tags_json_val,  # NEW DIRECT COLUMN
                            competitor_page_timing_json_val,  # NEW DIRECT COLUMN
                            opp.get("social_media_posts_status", "draft"),
                        ),
                    )

            return cursor.rowcount

    def get_opportunity_queue(self, client_id: str = "default") -> List[Dict[str, Any]]:
        """Retrieves all pending opportunities for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_PENDING_OPPORTUNITIES, (client_id,))
            return self._deserialize_rows(cursor.fetchall())

    def get_all_opportunities(
        self,
        client_id: str,
        params: Dict[str, Any],
        summary: bool = False,
        select_columns: str = None,
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        Retrieves keyword opportunities for a client, supporting filtering, sorting, and pagination.
        If summary is True, only essential fields for the table view are returned.
        """
        conn = self._get_conn()
        limit = int(params.get("limit", 20))
        page = int(params.get("page", 1))
        offset = (page - 1) * limit

        sort_by_map = {
            "strategic_score": "strategic_score",
            "date_added": "date_added",
            "keyword": "keyword",
            "status": "status",
            "search_volume": "JSON_EXTRACT(full_data, '$.keyword_info.search_volume')",
            "keyword_difficulty": "JSON_EXTRACT(full_data, '$.keyword_properties.keyword_difficulty')",
            "cpc": "JSON_EXTRACT(full_data, '$.keyword_info.cpc')",
        }
        sort_by = sort_by_map.get(params.get("sort_by"), "date_added")
        sort_direction = "ASC" if params.get("sort_direction") == "asc" else "DESC"

        where_parts = ["client_id = ?"]
        query_values = [client_id]

        status_filter = params.get("status")
        if status_filter:
            statuses = [s.strip() for s in status_filter.split(",")]
            placeholders = ",".join(["?"] * len(statuses))
            where_parts.append(f"status IN ({placeholders})")
            query_values.extend(statuses)

        where_clause = " AND ".join(where_parts)

        count_query = f"SELECT COUNT(*) FROM opportunities WHERE {where_clause}"
        with conn:
            cursor = conn.cursor()
            cursor.execute(count_query, query_values)
            total_count = cursor.fetchone()[0]

        select_columns = (
            select_columns
            if select_columns
            else "id, keyword, status, date_added, strategic_score, search_volume, keyword_difficulty, cpc, competition, main_intent, search_volume_trend_json, competitor_social_media_tags_json, competitor_page_timing_json, blog_qualification_status, latest_job_id, cluster_name, score_breakdown, full_data"
        )
        if summary and "full_data" not in select_columns:
            select_columns += ", full_data"

        final_query = f"SELECT {select_columns} FROM opportunities WHERE {where_clause} ORDER BY {sort_by} {sort_direction} LIMIT ? OFFSET ?"

        paged_values = query_values + [limit, offset]
        with conn:
            cursor = conn.cursor()
            cursor.execute(final_query, paged_values)
            opportunities = self._deserialize_rows(cursor.fetchall())

        # Manually extract and add search_volume and keyword_difficulty for the frontend
        for opp in opportunities:
            try:
                if opp.get("full_data"):
                    full_data = opp["full_data"]
                    opp["search_volume"] = full_data.get("keyword_info", {}).get(
                        "search_volume"
                    )
                    opp["keyword_difficulty"] = full_data.get(
                        "keyword_properties", {}
                    ).get("keyword_difficulty")
            except (KeyError, TypeError):
                opp["search_volume"] = None
                opp["keyword_difficulty"] = None

        return opportunities, total_count

    def get_opportunity_by_id(self, opportunity_id: int) -> Optional[Dict[str, Any]]:
        """Retrieves a single opportunity by its primary key ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_OPPORTUNITY_BY_ID, (opportunity_id,))
            row = cursor.fetchone()
            if row:
                return self._deserialize_rows([row])[0]
        return None

    def get_opportunity_summary_by_id(
        self, opportunity_id: int
    ) -> Optional[Dict[str, Any]]:
        """
        Retrieves only essential summary fields for an opportunity. (W13 FIX)
        """
        conn = self._get_conn()
        with conn:
            # Use a selective query to avoid fetching large JSON blobs
            cursor = conn.execute(
                "SELECT id, keyword, status, date_added, strategic_score, blog_qualification_status, featured_image_local_path FROM opportunities WHERE id = ?",
                (opportunity_id,),
            )
            row = cursor.fetchone()
            if row:
                # Need to manually or selectively deserialize, but for simplicity, we treat the row as the summary
                return dict(row)
            return None

    def get_opportunity_by_slug(self, slug: str) -> Optional[Dict[str, Any]]:
        """Retrieves a single opportunity by its URL slug."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_OPPORTUNITY_BY_SLUG, (slug,))
            row = cursor.fetchone()
            if row:
                return self._deserialize_rows([row])[0]
        return None

    def search_opportunities(self, client_id: str, query: str) -> List[Dict[str, Any]]:
        """Searches for opportunities by keyword for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            search_term = f"%{query}%"
            cursor.execute(
                queries.SEARCH_OPPORTUNITIES_BY_KEYWORD, (client_id, search_term)
            )
            # No need for full deserialization as we are fetching simple columns
            return [dict(row) for row in cursor.fetchall()]

    def get_published_articles_for_linking(
        self, client_id: str
    ) -> List[Dict[str, str]]:
        """
        Retrieves a list of published articles (title and slug) for internal linking suggestions.
        """
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            # Select only 'keyword' (as title) and 'slug' for published articles
            # 'full_data' is not needed here to save memory/processing
            cursor.execute(
                """
                SELECT keyword, slug FROM opportunities
                WHERE client_id = ? AND status IN ('generated', 'published') AND slug IS NOT NULL;
            """,
                (client_id,),
            )

            articles = []
            for row in cursor.fetchall():
                articles.append(
                    {
                        "title": row["keyword"],  # Use keyword as a proxy for title
                        "url": f"/article/{row['slug']}",  # Construct the relative URL
                    }
                )
            return articles

    def get_all_processed_keywords_for_client(self, client_id: str) -> List[str]:
        """Retrieves a flat list of all primary keywords for a client that are not in a 'rejected' or 'failed' state."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_ALL_PROCESSED_KEYWORDS, (client_id,))
            return [row["keyword"] for row in cursor.fetchall()]

    def check_existing_keywords(self, client_id: str, keywords: List[str]) -> List[str]:
        """Checks a list of keywords against the DB and returns those that exist."""
        if not keywords:
            return []
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            placeholders = ",".join("?" for _ in keywords)
            query = f"SELECT keyword FROM opportunities WHERE client_id = ? AND keyword IN ({placeholders})"
            cursor.execute(query, [client_id] + keywords)
            return [row["keyword"] for row in cursor.fetchall()]

    def update_opportunity_status(self, opportunity_id: int, new_status: str):
        """Updates a keyword's status in the database."""
        conn = self._get_conn()
        with conn:
            if new_status in ["generated", "analyzed", "failed"]:
                conn.execute(
                    queries.UPDATE_OPPORTUNITY_STATUS_WITH_DATE,
                    (new_status, datetime.now().isoformat(), opportunity_id),
                )
            else:
                conn.execute(
                    queries.UPDATE_OPPORTUNITY_STATUS, (new_status, opportunity_id)
                )

    def update_opportunity_workflow_state(
        self,
        opportunity_id: int,
        step: str,
        status: str = "in_progress",
        error_message: Optional[str] = None,
    ):
        """Updates the workflow step and status for a given opportunity.
        Possible statuses include: 'in_progress', 'completed', 'failed', 'pending', 'validated', 'analyzed', 'generated', 'published', 'rejected', 'paused_for_approval'."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Updating workflow state to step='{step}', status='{status}'"
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_WORKFLOW_STATE,
                (step, status, error_message, opportunity_id),
            )

    def update_opportunity_blueprint(
        self, opportunity_id: int, blueprint_data: Dict[str, Any], slug: str
    ):
        """Stores the generated blueprint data and the URL slug for a specific opportunity."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_BLUEPRINT_AND_SLUG,
                (json.dumps(blueprint_data), slug, opportunity_id),
            )

    def update_opportunity_ai_content(
        self, opportunity_id: int, ai_content_data: Dict[str, Any], ai_model: str
    ):
        """
        Stores the generated AI content package and model used for a specific opportunity.
        Applies server-side sanitization to the HTML body (W20 FIX).
        """
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving AI content generated by model '{ai_model}'."
        )

        # W20 FIX: Sanitize content before saving
        html_body = ai_content_data.get("article_body_html")
        if html_body:
            clean_html = bleach.clean(
                html_body, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES_DB
            )
            ai_content_data["article_body_html"] = clean_html

        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_AI_CONTENT,
                (
                    json.dumps(ai_content_data),
                    ai_model,
                    datetime.now().isoformat(),
                    opportunity_id,
                ),
            )

    def update_opportunity_images(
        self,
        opportunity_id: int,
        featured_image_url: Optional[str],
        featured_image_local_path: Optional[str],
        in_article_images_data: List[Dict[str, Any]],
    ):
        """Stores image generation details for a specific opportunity."""
        # W5 FIX: Ensure path is relative before storing, e.g., by checking if it starts with the expected API prefix
        if featured_image_local_path and os.path.isabs(featured_image_local_path):
            # Assuming the standard path is relative to the API's image mounting point,
            # extract the filename or the relative part required by the API
            # This is a simplified example; robust path management is needed.
            base_path = os.path.abspath(
                os.path.join(os.path.dirname(__file__), "..", "generated_images")
            )
            featured_image_local_path = os.path.relpath(
                featured_image_local_path, base_path
            )

        self.logger.info(
            f"Opportunity {opportunity_id}: Saving Pexels image data (featured URL: {featured_image_url})."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_IMAGES,
                (
                    featured_image_url,
                    featured_image_local_path,
                    json.dumps(in_article_images_data),
                    opportunity_id,
                ),
            )

    def update_opportunity_full_data(
        self, opportunity_id: int, full_data: Dict[str, Any]
    ):
        """Updates the full_data JSON blob for a specific opportunity."""
        self.logger.info(f"Opportunity {opportunity_id}: Updating full_data field.")
        conn = self._get_conn()
        with conn:
            conn.execute(
                "UPDATE opportunities SET full_data = ? WHERE id = ?",
                (json.dumps(full_data), opportunity_id),
            )

    def update_opportunity_scores(
        self,
        opportunity_id: int,
        strategic_score: float,
        score_breakdown: Dict[str, Any],
        blueprint_data: Optional[Dict[str, Any]] = None,
    ):
        """Updates the primary strategic score, breakdown, and blueprint for an opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Updating strategic_score to {strategic_score:.2f} and saving blueprint."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_SCORES,
                (
                    strategic_score,
                    json.dumps(score_breakdown),
                    json.dumps(blueprint_data) if blueprint_data else None,
                    opportunity_id,
                ),
            )

    def update_opportunity_final_package(
        self, opportunity_id: int, final_package: Dict[str, Any]
    ):
        """Stores the generated final content package JSON for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving final standalone content package."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_FINAL_PACKAGE,
                (json.dumps(final_package), opportunity_id),
            )

    def add_client(
        self, client_id: str, client_name: str, default_settings: Dict[str, Any]
    ) -> bool:
        """Adds a new client to the clients table and initializes their settings."""
        conn = self._get_conn()
        try:
            with conn:
                cursor = conn.execute("PRAGMA table_info(client_settings)")
                schema_columns = {row["name"] for row in cursor.fetchall()}

                conn.execute(
                    queries.INSERT_CLIENT,
                    (client_id, client_name, datetime.now().isoformat()),
                )

                settings_to_insert = {
                    k: v for k, v in default_settings.items() if k in schema_columns
                }
                if (
                    "client_knowledge_base" not in settings_to_insert
                    and "client_knowledge_base" in schema_columns
                ):
                    settings_to_insert["client_knowledge_base"] = (
                        ""  # Initialize with empty string
                    )
                settings_to_insert["client_id"] = client_id
                settings_to_insert["last_updated"] = datetime.now().isoformat()

                keys = ", ".join(settings_to_insert.keys())
                placeholders = ", ".join("?" * len(settings_to_insert))

                values = []
                for key in settings_to_insert.keys():
                    value = settings_to_insert[key]
                    if isinstance(value, list):
                        values.append(",".join(map(str, value)))
                    elif isinstance(value, bool):
                        values.append(1 if value else 0)
                    else:
                        values.append(value)

                insert_query = (
                    f"INSERT INTO client_settings ({keys}) VALUES ({placeholders})"
                )

                conn.execute(insert_query, tuple(values))
                self.logger.info(
                    f"Added new client '{client_name}' ({client_id}) and initialized settings."
                )

                # Also initialize qualification settings with a comprehensive set of default values
                conn.execute(
                    """
                    INSERT INTO qualification_settings (
                        client_id, ease_of_ranking_weight, traffic_potential_weight, commercial_intent_weight,
                        serp_features_weight, growth_trend_weight, serp_freshness_weight, serp_volatility_weight,
                        competitor_weakness_weight, competitor_performance_weight, min_search_volume, max_keyword_difficulty,
                        negative_keywords, prohibited_intents, max_y_pixel_threshold,
                        max_forum_results_in_top_10, max_ecommerce_results_in_top_10,
                        disallowed_page_types_in_top_3
                    ) VALUES (?, 40, 15, 5, 5, 5, 5, 5, 20, 5, 100, 80, 'login,free,cheap', 'navigational', 800, 3, 2, 'E-commerce,Forum')
                """,
                    (client_id,),
                )
                self.logger.info(
                    f"Initialized default qualification settings for client '{client_name}' ({client_id})."
                )
            return True
        except sqlite3.IntegrityError:
            self.logger.warning(f"Client with ID '{client_id}' already exists.")
            return False
        except Exception as e:
            self.logger.error(
                f"Error adding client '{client_name}': {e}", exc_info=True
            )
            return False

    def get_clients(self) -> List[Dict[str, str]]:
        """Retrieves all clients from the database."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_ALL_CLIENTS)
            return [dict(row) for row in cursor.fetchall()]

    def get_processed_opportunities(self, client_id: str) -> List[Dict[str, Any]]:
        """Retrieves all opportunities with a generated blueprint for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_PROCESSED_OPPORTUNITIES, (client_id,))
            return self._deserialize_rows(cursor.fetchall())

    def get_client_settings(self, client_id: str) -> Dict[str, Any]:
        """Retrieves settings for a specific client, converting from DB types."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_CLIENT_SETTINGS, (client_id,))
            row = cursor.fetchone()
            if row:
                settings = dict(row)
                settings.pop("client_id", None)
                settings.pop("last_updated", None)

                list_keys = [
                    "allowed_intents",
                    "negative_keywords",
                    "competitor_blacklist_domains",
                    "serp_feature_filters",
                    "serp_features_exclude_filter",
                    "platforms",
                    "default_wordpress_categories",
                    "default_wordpress_tags",
                    "ugc_and_parasite_domains",
                    "high_value_categories",
                    "hostile_serp_features",
                    "final_validation_non_blog_domains",
                    "prohibited_intents",  # NEW
                ]
                for key in list_keys:
                    if settings.get(key) is not None and isinstance(settings[key], str):
                        settings[key] = [
                            item.strip()
                            for item in settings[key].split(",")
                            if item.strip()
                        ]
                    elif settings.get(key) is None:
                        settings[key] = []

                bool_keys = [
                    "enforce_intent_filter",
                    "require_question_keywords",
                    "use_pexels_first",
                    "cleanup_local_images",
                    "onpage_enable_javascript",
                    "onpage_load_resources",
                    "calculate_rectangles",
                    "onpage_disable_cookie_popup",
                    "onpage_return_despite_timeout",
                    "onpage_enable_browser_rendering",
                    "onpage_store_raw_html",
                    "onpage_validate_micromarkup",
                    "discovery_replace_with_core_keyword",
                    "discovery_ignore_synonyms",
                    "enable_automated_internal_linking",  # NEW
                ]
                for key in bool_keys:
                    if settings.get(key) is not None:
                        settings[key] = bool(settings[key])

                int_keys = [
                    "num_in_article_images",
                    "location_code",
                    "serp_freshness_old_threshold_days",
                    "min_competitor_word_count",
                    "max_competitor_technical_warnings",
                    "num_competitors_to_analyze",
                    "num_common_headings",
                    "num_unique_angles",
                    "max_initial_serp_urls_to_analyze",
                    "min_search_volume",
                    "max_keyword_difficulty",
                    "people_also_ask_click_depth",
                    "onpage_max_domains_per_request",
                    "onpage_max_tasks_per_request",
                    "ease_of_ranking_weight",
                    "traffic_potential_weight",
                    "commercial_intent_weight",
                    "growth_trend_weight",
                    "serp_features_weight",
                    "serp_freshness_weight",
                    "serp_volatility_weight",
                    "competitor_weakness_weight",
                    "max_sv_for_scoring",
                    "max_domain_rank_for_scoring",
                    "max_referring_domains_for_scoring",
                    "serp_volatility_stable_threshold_days",
                    "discovery_related_depth",
                    "max_avg_referring_domains_filter",
                    "yearly_trend_decline_threshold",
                    "quarterly_trend_decline_threshold",
                    "max_kd_hard_limit",
                    "max_referring_main_domains_limit",
                    "max_avg_domain_rank_threshold",
                    "min_keyword_word_count",
                    "max_keyword_word_count",
                    "crowded_serp_features_threshold",
                    "min_serp_stability_days",
                    "max_non_blog_results",
                    "max_ai_overview_words",
                    "max_first_organic_y_pixel",
                    "max_words_for_ai_analysis",  # NEW
                    "max_avg_lcp_time",  # NEW
                ]
                for key in int_keys:
                    if settings.get(key) is not None:
                        try:
                            settings[key] = int(settings[key])
                        except (ValueError, TypeError):
                            pass

                float_keys = [
                    "informational_score",
                    "commercial_score",
                    "transactional_score",
                    "navigational_score",
                    "question_keyword_bonus",
                    "max_cpc_for_scoring",
                    "min_monthly_trend_percentage",
                    "featured_snippet_bonus",
                    "ai_overview_bonus",
                    "serp_freshness_bonus_max",
                    "min_cpc_filter_api",
                    "category_intent_bonus",
                    "search_volume_volatility_threshold",
                    "max_paid_competition_score",
                    "max_high_top_of_page_bid",
                    "max_pages_to_domain_ratio",
                    "ai_generation_temperature",  # NEW
                    "recommended_word_count_multiplier",  # NEW
                ]
                for key in float_keys:
                    if settings.get(key) is not None:
                        try:
                            settings[key] = float(settings[key])
                        except (ValueError, TypeError):
                            pass

                return settings
            return {}

    def update_client_settings(self, client_id: str, settings: Dict[str, Any]):
        """Updates client-specific settings, handling type conversions for DB storage."""
        self.logger.info(f"Updating client settings for {client_id}.")
        conn = self._get_conn()
        with conn:
            cursor = conn.execute("PRAGMA table_info(client_settings)")
            schema_columns = {row["name"] for row in cursor.fetchall()}

            current_time = datetime.now().isoformat()

            set_clauses = ["last_updated = ?"]
            values = [current_time]

            for key, value in settings.items():
                if key in schema_columns and key not in ["client_id", "last_updated"]:
                    db_value = value
                    if isinstance(value, list):
                        db_value = ",".join(map(str, value))
                    elif isinstance(value, bool):
                        db_value = 1 if value else 0

                    set_clauses.append(f"{key} = ?")
                    values.append(db_value)

            if len(set_clauses) > 1:
                update_query = f"UPDATE client_settings SET {', '.join(set_clauses)} WHERE client_id = ?"
                values.append(client_id)
                conn.execute(update_query, values)
                self.logger.info(f"Client settings for {client_id} updated in DB.")
            else:
                self.logger.info(
                    f"No valid client settings found to update for {client_id}."
                )

    def get_all_opportunities_for_export(self) -> List[Dict[str, Any]]:
        """Retrieves all opportunities for all clients from the database."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_ALL_OPPORTUNITIES_FOR_EXPORT)
            return self._deserialize_rows(cursor.fetchall())

    def get_dashboard_stats(self, client_id: str) -> Dict[str, Any]:
        """Retrieves statistics for the dashboard UI."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # Get counts by status
            cursor.execute(queries.COUNT_OPPORTUNITIES_BY_STATUS, (client_id,))
            status_counts = {row["status"]: row["count"] for row in cursor.fetchall()}

            # Get recent items
            cursor.execute(queries.SELECT_RECENTLY_GENERATED, (client_id,))
            recent_items = [dict(row) for row in cursor.fetchall()]

            return {"status_counts": status_counts, "recent_items": recent_items}

    def get_total_api_cost(self, client_id: str) -> float:
        """Calculates the total API cost for a client by summing costs from both opportunities and discovery runs."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            
            # Sum cost from opportunities
            cursor.execute(
                "SELECT SUM(total_api_cost) FROM opportunities WHERE client_id = ?",
                (client_id,),
            )
            opportunities_cost = cursor.fetchone()[0] or 0.0
            
            # Sum cost from discovery runs
            cursor.execute(
                "SELECT SUM(total_api_cost) FROM discovery_runs WHERE client_id = ?",
                (client_id,),
            )
            runs_cost = cursor.fetchone()[0] or 0.0
            
            return opportunities_cost + runs_cost

    def get_dashboard_data(self, client_id: str) -> Dict[str, Any]:
        """Retrieves aggregated data for the main dashboard UI."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # 1. KPIs
            cursor.execute(
                "SELECT COUNT(*) FROM opportunities WHERE client_id = ?", (client_id,)
            )
            total_opportunities = cursor.fetchone()[0]
            cursor.execute(
                "SELECT COUNT(*) FROM opportunities WHERE client_id = ? AND status = 'generated'",
                (client_id,),
            )
            content_generated = cursor.fetchone()[0]
            cursor.execute(
                "SELECT SUM(traffic_value) FROM opportunities WHERE client_id = ?",
                (client_id,),
            )
            total_traffic_value = cursor.fetchone()[0] or 0
            total_api_cost = self.get_total_api_cost(client_id)

            kpis = {
                "totalOpportunities": total_opportunities,
                "contentGenerated": content_generated,
                "totalTrafficValue": total_traffic_value,
                "totalApiCost": total_api_cost,
            }

            # 2. Funnel and Stats Data
            dashboard_stats = self.get_dashboard_stats(client_id)
            status_counts = dashboard_stats.get("status_counts", {})
            recent_items = dashboard_stats.get("recent_items", [])

            funnel_data = [
                {"stage": "Total", "count": total_opportunities},
                {"stage": "Validated", "count": status_counts.get("validated", 0)},
                {"stage": "Analyzed", "count": status_counts.get("analyzed", 0)},
                {"stage": "Generated", "count": content_generated},
                {"stage": "Disqualified", "count": (status_counts.get("rejected", 0) or 0) + (status_counts.get("failed", 0) or 0)},
            ]

            # 3. Action Items
            cursor.execute(queries.SELECT_ACTION_ITEMS, (client_id,))
            action_items_raw = [dict(row) for row in cursor.fetchall()]
            action_items = {
                "awaitingApproval": [
                    item
                    for item in action_items_raw
                    if item["status"] == "paused_for_approval"
                ],
                "failed": [
                    item for item in action_items_raw if item["status"] == "failed"
                ],
            }

            return {
                "kpis": kpis,
                "funnelData": funnel_data,
                "actionItems": action_items,
                "status_counts": status_counts,
                "recent_items": recent_items,
            }

    def update_opportunity_wordpress_payload(
        self, opportunity_id: int, wordpress_payload: Dict[str, Any]
    ):
        """Stores the generated WordPress JSON payload for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving final WordPress JSON payload."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_WORDPRESS_PAYLOAD,
                (json.dumps(wordpress_payload), opportunity_id),
            )

    def get_api_cache(self, key: str) -> Optional[Dict[str, Any]]:
        """Retrieves a cached item from the api_cache table."""
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(queries.SELECT_API_CACHE, (key,))
            row = cursor.fetchone()
            if row:
                # Check TTL during retrieval
                if row["timestamp"] + (row["ttl_days"] * 86400) > time.time():
                    return json.loads(row["data"])
                else:
                    self.logger.debug(f"Cache STALE for key: {key}")
                    self.delete_api_cache_by_key(key)  # Clean up stale entry
            return None

    def set_api_cache(self, key: str, value: Any, ttl_days: int = 7):
        """Stores an item in the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.INSERT_API_CACHE,
                (key, json.dumps(value), time.time(), ttl_days),
            )
        self.logger.debug(f"Cache SET for key: {key}")

    def delete_api_cache_by_key(self, key: str):
        """Deletes a specific item from the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.DELETE_API_CACHE_BY_KEY, (key,))

    def clear_api_cache(self):
        """Clears all items from the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.TRUNCATE_API_CACHE)
        self.logger.info("API cache cleared.")

    def clear_expired_api_cache(self):
        """Deletes all expired items from the api_cache table."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.DELETE_EXPIRED_API_CACHE, (time.time(),))
        self.logger.debug("Expired API cache entries cleaned up.")

    # --- Discovery Run Methods ---

    def create_discovery_run(self, client_id: str, parameters: Dict[str, Any]) -> int:
        """Creates a new discovery run record and returns its ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                queries.INSERT_DISCOVERY_RUN,
                (
                    client_id,
                    datetime.now().isoformat(),
                    "running",
                    json.dumps(parameters),
                ),
            )
            return cursor.lastrowid

    def update_discovery_run_status(self, run_id: int, status: str):
        """Updates the status of a discovery run."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.UPDATE_DISCOVERY_RUN_STATUS, (status, run_id))

    def update_discovery_run_completed(
        self, run_id: int, results_summary: Dict[str, Any]
    ):
        """Marks a discovery run as completed and stores the results summary."""
        conn = self._get_conn()
        total_cost = results_summary.get("total_cost", 0.0)
        with conn:
            conn.execute(
                queries.UPDATE_DISCOVERY_RUN_COMPLETED,
                (
                    datetime.now().isoformat(),
                    json.dumps(results_summary),
                    total_cost,
                    run_id,
                ),
            )

    def update_discovery_run_failed(self, run_id: int, error_message: str):
        """Marks a discovery run as failed and stores the error message."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_DISCOVERY_RUN_FAILED,
                (datetime.now().isoformat(), error_message, run_id),
            )

    def get_all_discovery_runs_paginated(
        self, client_id: str, page: int, limit: int
    ) -> Tuple[List[Dict[str, Any]], int]:
        """Retrieves all discovery runs for a specific client with pagination."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # Get total count
            cursor.execute(
                "SELECT COUNT(*) FROM discovery_runs WHERE client_id = ?", (client_id,)
            )
            total_count = cursor.fetchone()[0]

            cursor.execute(
                "SELECT * FROM discovery_runs WHERE client_id = ? ORDER BY start_time DESC LIMIT ? OFFSET ?",
                (client_id, limit, (page - 1) * limit),
            )
            runs = []
            for row in cursor.fetchall():
                run = dict(row)
                try:
                    if run.get("parameters"):
                        run["parameters"] = json.loads(run["parameters"])
                    if run.get("results_summary"):
                        run["results_summary"] = json.loads(run["results_summary"])
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse JSON for discovery run ID {run.get('id')}."
                    )
                runs.append(run)
            return runs, total_count

    def get_discovery_run_by_id(self, run_id: int) -> Optional[Dict[str, Any]]:
        """Retrieves a single discovery run by its ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_DISCOVERY_RUN_BY_ID, (run_id,))
            row = cursor.fetchone()
            if row:
                run = dict(row)
                try:
                    if run.get("parameters"):
                        run["parameters"] = json.loads(run["parameters"])
                    if run.get("results_summary"):
                        run["results_summary"] = json.loads(run["results_summary"])
                except json.JSONDecodeError:
                    self.logger.warning(
                        f"Failed to parse JSON for discovery run ID {run.get('id')}."
                    )
                return run
        return None

    def update_discovery_run_log_path(self, run_id: int, log_path: str):
        """Updates the log file path for a discovery run."""
        conn = self._get_conn()
        with conn:
            conn.execute(queries.UPDATE_DISCOVERY_RUN_LOG_PATH, (log_path, run_id))

    def get_keywords_for_run(self, run_id: int) -> List[Dict[str, Any]]:
        """Retrieves all opportunities associated with a specific discovery run ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM opportunities WHERE run_id = ?", (run_id,))
            return self._deserialize_rows(cursor.fetchall())

    def get_keywords_for_run_by_reason(
        self, run_id: int, reason: str
    ) -> List[Dict[str, Any]]:
        """Retrieves all opportunities associated with a specific discovery run ID that were disqualified for a specific reason."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_KEYWORDS_FOR_RUN_BY_REASON, (run_id, reason))
            return self._deserialize_rows(cursor.fetchall())

    def search_discovery_runs(self, client_id: str, query: str) -> List[Dict[str, Any]]:
        """Searches for discovery runs by seed keywords or status for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            search_term = f"%{query}%"
            cursor.execute(
                """
                SELECT id, client_id, start_time, status, parameters, results_summary
                FROM discovery_runs
                WHERE client_id = ? 
                  AND (parameters LIKE ? OR status LIKE ? OR error_message LIKE ?)
                ORDER BY start_time DESC
                LIMIT 10;
            """,
                (client_id, search_term, search_term, search_term),
            )

            runs = []
            for row in cursor.fetchall():
                run = dict(row)
                if run.get("parameters"):
                    run["parameters"] = json.loads(run["parameters"])
                if run.get("results_summary"):
                    run["results_summary"] = json.loads(run["results_summary"])
                runs.append(run)
            return runs

    def get_job(self, job_id: str) -> Optional[Dict[str, Any]]:
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(queries.GET_JOB, (job_id,))
            row = cursor.fetchone()
            if row:
                job_data = dict(row)
                if job_data.get("result"):
                    job_data["result"] = json.loads(job_data["result"])
                return job_data
        return None

    def get_all_jobs(self) -> List[Dict[str, Any]]:
        """Retrieves all jobs from the database, ordered by start time."""
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(queries.GET_ALL_JOBS)
            jobs = []
            for row in cursor.fetchall():
                job_data = dict(row)
                if job_data.get("result"):
                    try:
                        job_data["result"] = json.loads(job_data["result"])
                    except json.JSONDecodeError:
                        job_data["result"] = {"raw_result": job_data["result"]}
                jobs.append(job_data)
            return jobs

    def update_job(self, job_info: Dict[str, Any]):
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_JOB,
                (
                    job_info["id"],
                    job_info["status"],
                    job_info["progress"],
                    json.dumps(job_info["result"]) if job_info.get("result") else None,
                    job_info.get("error"),
                    job_info["started_at"],
                    job_info.get("finished_at"),
                ),
            )

    def get_client_prompt_templates(self, client_id: str) -> List[Dict[str, Any]]:
        """MOCK: Retrieves all prompt templates for a client."""
        return []

    def save_client_prompt_template(
        self, client_id: str, template_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """MOCK: Saves or updates a client's prompt template."""
        template_data["last_updated"] = datetime.now().isoformat()
        return template_data

    def delete_client_prompt_template(self, client_id: str, template_name: str):
        """MOCK: Deletes a client's prompt template."""
        pass

    def update_opportunity_ai_content_and_status(
        self,
        opportunity_id: int,
        ai_content_data: Dict[str, Any],
        ai_model: str,
        status: str,
    ):
        """Stores the generated AI content package, model, and status used for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving AI content generated by model '{ai_model}' and setting status to '{status}'."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_AI_CONTENT_AND_STATUS,
                (
                    json.dumps(ai_content_data),
                    ai_model,
                    datetime.now().isoformat(),
                    status,
                    opportunity_id,
                ),
            )

    def save_content_version_to_history(
        self,
        opportunity_id: int,
        ai_content_json: Dict[str, Any],
        timestamp: Optional[str] = None,
    ):
        """Saves the current content package of an opportunity to the history table."""
        conn = self._get_conn()
        timestamp = timestamp or datetime.now().isoformat()
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving content version to history at {timestamp}."
        )
        with conn:
            conn.execute(
                queries.INSERT_CONTENT_HISTORY,
                (opportunity_id, timestamp, json.dumps(ai_content_json)),
            )

    def save_content_feedback(
        self, opportunity_id: int, rating: int, comments: Optional[str] = None
    ):
        """Saves user feedback for generated content."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.INSERT_CONTENT_FEEDBACK,
                (opportunity_id, rating, comments, datetime.now().isoformat()),
            )
        self.logger.info(
            f"Saved content feedback for opportunity {opportunity_id} (Rating: {rating})."
        )

    def get_content_history(self, opportunity_id: int) -> List[Dict[str, Any]]:
        """Retrieves all historical content versions for an opportunity."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(queries.SELECT_CONTENT_HISTORY_BY_OPP_ID, (opportunity_id,))
            rows = cursor.fetchall()

            results = []
            for row in rows:
                row_dict = dict(row)
                if row_dict.get("ai_content_json"):
                    row_dict["ai_content_json"] = json.loads(
                        row_dict["ai_content_json"]
                    )
                results.append(row_dict)
            return results

    def restore_content_version(
        self, opportunity_id: int, version_timestamp: str
    ) -> Optional[Dict[str, Any]]:
        """
        Restores a content version from history. Before restoring, it saves the current
        'generated' content to the history table to prevent data loss.
        """
        conn = self._get_conn()
        with conn:
            # First, fetch the current opportunity to save its content
            cursor = conn.execute(queries.SELECT_OPPORTUNITY_BY_ID, (opportunity_id,))
            current_opp_row = cursor.fetchone()
            if not current_opp_row:
                self.logger.error(
                    f"Cannot restore: Opportunity {opportunity_id} not found."
                )
                return None

            current_opp = self._deserialize_rows([current_opp_row])[0]
            current_content = current_opp.get("ai_content")

            # Save the current 'generated' content to history before overwriting
            if current_content and current_opp.get("status") == "generated":
                self.save_content_version_to_history(opportunity_id, current_content)

            # Now, find the historical version to restore
            cursor = conn.execute(
                "SELECT ai_content_json FROM content_history WHERE opportunity_id = ? AND timestamp = ?",
                (opportunity_id, version_timestamp),
            )
            version_to_restore_row = cursor.fetchone()

            if (
                not version_to_restore_row
                or not version_to_restore_row["ai_content_json"]
            ):
                self.logger.error(
                    f"Version not found or content missing for timestamp: {version_timestamp}"
                )
                raise ValueError(f"Content version at {version_timestamp} not found.")

            restored_content_str = version_to_restore_row["ai_content_json"]
            restored_content = json.loads(restored_content_str)

            # Update the main opportunities table with the restored content
            conn.execute(
                queries.UPDATE_OPPORTUNITY_AI_CONTENT_AND_STATUS,
                (
                    restored_content_str,
                    current_opp.get(
                        "ai_content_model", "gpt-4o"
                    ),  # Keep the last used model
                    datetime.now().isoformat(),
                    "generated",  # Reset status to 'generated'
                    opportunity_id,
                ),
            )
            self.logger.info(
                f"Successfully restored content from {version_timestamp} for opportunity {opportunity_id}."
            )

        return restored_content

    def update_opportunity_social_posts(
        self, opportunity_id: int, social_media_posts: List[Dict[str, Any]]
    ):
        """Stores the updated social media posts JSON for a specific opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving updated social media posts."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_OPPORTUNITY_SOCIAL_POSTS,
                (json.dumps(social_media_posts), opportunity_id),
            )

    def update_social_media_posts_status(self, opportunity_id: int, new_status: str):
        """Updates the status of social media posts for an opportunity."""
        self.logger.info(
            f"Opportunity {opportunity_id}: Updating social media posts status to '{new_status}'."
        )
        conn = self._get_conn()
        with conn:
            conn.execute(
                "UPDATE opportunities SET social_media_posts_status = ? WHERE id = ?",
                (new_status, opportunity_id),
            )

    def save_full_content_package(
        self,
        opportunity_id: int,
        ai_content_data: Dict[str, Any],
        ai_model: str,
        featured_image_data: Optional[Dict[str, Any]],
        in_article_images_data: List[Dict[str, Any]],
        social_posts: Optional[List[Dict[str, Any]]],
        final_package: Dict[str, Any],
        total_api_cost: float,
    ):
        """
        Saves the entire generated content package and updates the status to 'generated' in a single transaction.
        Applies server-side sanitization to the main HTML body. (W20 FIX)
        """
        self.logger.info(
            f"Opportunity {opportunity_id}: Saving full content package and finalizing status."
        )

        # W20 FIX: Sanitize the final package HTML content before saving
        html_body = final_package.get("article_html_final")
        if html_body:
            clean_html = bleach.clean(
                html_body, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES_DB
            )
            final_package["article_html_final"] = clean_html

        conn = self._get_conn()
        with conn:
            conn.execute(
                queries.UPDATE_GENERATED_CONTENT_AND_STATUS,
                (
                    json.dumps(ai_content_data),
                    ai_model,
                    featured_image_data.get("remote_url")
                    if featured_image_data
                    else None,
                    featured_image_data.get("local_path")
                    if featured_image_data
                    else None,
                    json.dumps(in_article_images_data),
                    json.dumps(social_posts) if social_posts else None,
                    json.dumps(final_package),
                    datetime.now().isoformat(),
                    total_api_cost,
                    opportunity_id,
                ),
            )
        self.logger.info(
            f"Opportunity {opportunity_id}: Successfully saved full content package and set status to 'generated'."
        )

        def update_opportunity_published_url(self, opportunity_id: int, url: str):
            """Updates the published_url for a specific opportunity."""
            self.logger.info(
                f"Opportunity {opportunity_id}: Storing published URL: {url}"
            )
            conn = self._get_conn()
            with conn:
                conn.execute(
                    "UPDATE opportunities SET published_url = ? WHERE id = ?",
                    (url, opportunity_id),
                )

    def get_high_priority_opportunities(
        self, client_id: str, limit: int = 5
    ) -> List[Dict[str, Any]]:
        """Retrieves the top N validated opportunities with the highest strategic score."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                queries.SELECT_HIGH_PRIORITY_OPPORTUNITIES, (client_id, limit)
            )
            return self._deserialize_rows(cursor.fetchall())

    def get_content_feedback_examples(
        self, client_id: str, limit: int = 2
    ) -> Dict[str, List]:
        """Retrieves examples of good and bad content based on user feedback."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()

            # Get highest rated
            cursor.execute(
                """
                SELECT o.keyword, cf.comments, cf.rating
                FROM content_feedback cf
                JOIN opportunities o ON cf.opportunity_id = o.id
                WHERE o.client_id = ? AND cf.rating >= 4
                ORDER BY cf.rating DESC, cf.timestamp DESC
                LIMIT ?;
            """,
                (client_id, limit),
            )
            good_examples = [dict(row) for row in cursor.fetchall()]

            # Get lowest rated
            cursor.execute(
                """
                SELECT o.keyword, cf.comments, cf.rating
                FROM content_feedback cf
                JOIN opportunities o ON cf.opportunity_id = o.id
                WHERE o.client_id = ? AND cf.rating <= 2
                ORDER BY cf.rating ASC, cf.timestamp DESC
                LIMIT ?;
            """,
                (client_id, limit),
            )
            bad_examples = [dict(row) for row in cursor.fetchall()]

        return {"good_examples": good_examples, "bad_examples": bad_examples}

    def get_qualification_settings(self, client_id: str) -> Dict[str, Any]:
        """Retrieves qualification settings for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT * FROM qualification_settings WHERE client_id = ?", (client_id,)
            )
            row = cursor.fetchone()
            if row:
                return dict(row)
            return {}

    def get_qualification_strategies(self, client_id: str) -> List[Dict[str, Any]]:
        """Retrieves all qualification strategies for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT * FROM qualification_strategies WHERE client_id = ?",
                (client_id,),
            )
            return [dict(row) for row in cursor.fetchall()]

    def get_qualification_strategy_by_id(
        self, strategy_id: int
    ) -> Optional[Dict[str, Any]]:
        """Retrieves a single qualification strategy by its ID."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT * FROM qualification_strategies WHERE id = ?", (strategy_id,)
            )
            row = cursor.fetchone()
            if row:
                return dict(row)
            return None

    def create_qualification_strategy(
        self, client_id: str, strategy: Dict[str, Any]
    ) -> int:
        """Creates a new qualification strategy for a specific client."""
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            keys = ", ".join(strategy.keys())
            placeholders = ", ".join("?" * len(strategy))
            values = list(strategy.values())
            cursor.execute(
                f"INSERT INTO qualification_strategies (client_id, {keys}) VALUES (?, {placeholders})",
                [client_id] + values,
            )
            return cursor.lastrowid

    def update_qualification_strategy(self, strategy_id: int, strategy: Dict[str, Any]):
        """Updates a qualification strategy."""
        conn = self._get_conn()
        with conn:
            set_clauses = []
            values = []
            for key, value in strategy.items():
                set_clauses.append(f"{key} = ?")
                values.append(value)

            if set_clauses:
                update_query = f"UPDATE qualification_strategies SET {', '.join(set_clauses)} WHERE id = ?"
                values.append(strategy_id)
                conn.execute(update_query, values)

    def delete_qualification_strategy(self, strategy_id: int):
        """Deletes a qualification strategy."""
        conn = self._get_conn()
        with conn:
            conn.execute(
                "DELETE FROM qualification_strategies WHERE id = ?", (strategy_id,)
            )

    def update_qualification_settings(self, client_id: str, settings: Dict[str, Any]):
        """Updates qualification settings for a specific client."""
        conn = self._get_conn()
        with conn:
            set_clauses = []
            values = []
            for key, value in settings.items():
                set_clauses.append(f"{key} = ?")
                values.append(value)

            if set_clauses:
                update_query = f"UPDATE qualification_settings SET {', '.join(set_clauses)} WHERE client_id = ?"
                values.append(client_id)
                conn.execute(update_query, values)

    def get_content_snippet_by_slug(self, slug: str) -> Optional[Dict[str, Any]]:
        """Retrieves the title and a content snippet for an opportunity by its slug."""
        conn = self._get_conn()
        with conn:
            cursor = conn.execute(
                """
                SELECT keyword as title,
                       SUBSTR(JSON_EXTRACT(ai_content_json, '$.meta_description'), 1, 200) as snippet_desc
                FROM opportunities WHERE slug = ?;
            """,
                (slug,),
            )
            row = cursor.fetchone()
            if row:
                return dict(row)
        return None

    def fail_stale_jobs(self):
        """Finds all jobs with a 'running' status and marks them as 'failed' on startup."""
        self.logger.info("Scanning for stale jobs from previous sessions...")
        conn = self._get_conn()
        with conn:
            cursor = conn.cursor()
            error_message = "Job failed due to application restart."
            finished_time = time.time()
            cursor.execute(
                """
                UPDATE jobs
                SET status = 'failed', error = ?, finished_at = ?
                WHERE status = 'running';
            """,
                (error_message, finished_time),
            )

            if cursor.rowcount > 0:
                self.logger.warning(
                    f"Marked {cursor.rowcount} stale 'running' jobs as 'failed'."
                )
            else:
                self.logger.info("No stale jobs found.")

        def override_disqualification(self, opportunity_id: int) -> bool:
            """Manually overrides a failed qualification, resetting status to pending."""
            self.logger.info(
                f"Overriding disqualification for opportunity ID: {opportunity_id}"
            )
            conn = self._get_conn()
            with conn:
                cursor = conn.cursor()
                cursor.execute(
                    """
                    UPDATE opportunities
                    SET status = 'pending', blog_qualification_status = 'passed_manual_override', blog_qualification_reason = 'Manually overridden by user.'
                    WHERE id = ? AND status IN ('failed', 'rejected');
                """,
                    (opportunity_id,),
                )

                if cursor.rowcount > 0:
                    self.logger.info(
                        f"Successfully overrode disqualification for opportunity ID: {opportunity_id}"
                    )
                    return True
                else:
                    self.logger.warning(
                        f"Could not override disqualification for ID: {opportunity_id}. Status was not 'failed' or 'rejected'."
                    )
                    return False
```

## File: data_access/initialize.py
```python
import logging
import os
import sys

# Ensure the project root is in sys.path for module imports
# Correctly identify the project root, which is two levels above the 'backend' directory
project_root = os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
backend_root = os.path.join(project_root, "backend")

# Add the backend directory to sys.path to allow for absolute imports from the backend module
if backend_root not in sys.path:
    sys.path.insert(0, backend_root)

# Import necessary classes from your project
try:
    from app_config.manager import ConfigManager
    from data_access.database_manager import DatabaseManager
except ImportError as e:
    print(f"Error importing project modules: {e}")
    print(
        "Please ensure you are running this script from the project's root directory or that paths are set up correctly."
    )
    print(f"Current working directory: {os.getcwd()}")
    print(f"Project root: {project_root}")
    print(f"Backend root added to sys.path: {backend_root}")
    sys.exit(1)

# Configure logging for this script run
logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")
logger = logging.getLogger(__name__)


def main():
    """Initializes the database, runs migrations, and seeds the default client."""

    logger.info(
        "Starting database initialization and default client seeding process..."
    )

    # Construct the absolute path to the settings.ini file
    settings_path = os.path.join(backend_root, "app_config", "settings.ini")
    if not os.path.exists(settings_path):
        logger.error(f"settings.ini file not found at expected path: {settings_path}")
        sys.exit(1)

    # Instantiate ConfigManager with the correct path
    config_manager = ConfigManager(settings_path=settings_path)
    global_cfg = config_manager.get_global_config()
    db_file_name = global_cfg.get(
        "db_file_name", "data/opportunities.db"
    )  # Get DB file name from config

    # The database path is relative to the project root
    db_path = os.path.join(project_root, db_file_name)
    db_data_dir = os.path.dirname(db_path)

    # --- Force re-creation of the database ---
    if os.path.exists(db_path):
        logger.info(
            f"Deleting existing database file at {db_path} to ensure a clean slate."
        )
        os.remove(db_path)
    # Ensure the data directory exists
    if not os.path.exists(db_data_dir):
        os.makedirs(db_data_dir)
        logger.info(f"Created data directory: {db_data_dir}")
    # --- End ---

    try:
        # 1. Initialize DatabaseManager
        db_manager = DatabaseManager(cfg_manager=config_manager, db_path=db_path)

        # 2. Ensure database and all tables are created, and migrations are applied
        db_manager.initialize()
        logger.info("Database initialized and all migrations applied.")

        # 3. Define default client details (using values from settings.ini)
        client_id = global_cfg.get("default_client_id", "Lark_Main_Site")
        client_name = client_id.replace(
            "_", " "
        ).title()  # Simple conversion for display name

        # 4. Get default settings template (which includes brand voice settings from settings.ini)
        default_settings = config_manager.get_default_client_settings_template()

        if not default_settings:
            logger.error("Could not load default client settings template. Aborting.")
            return

        # 5. Add the default client
        success = db_manager.add_client(client_id, client_name, default_settings)

        if success:
            logger.info(
                f"Successfully seeded the database with default client: '{client_name}' ({client_id})"
            )
        else:
            # This would only happen if the client_id already existed, which shouldn't after deleting the DB.
            logger.warning(
                f"Default client '{client_name}' ({client_id}) already existed in the database."
            )

        logger.info("Database setup process complete. No keyword data has been added.")

    except Exception as e:
        logger.error(f"An error occurred during database setup: {e}", exc_info=True)


if __name__ == "__main__":
    main()
```

## File: data_access/models.py
```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional


@dataclass
class KeywordData:
    keyword: str
    search_volume: int
    keyword_difficulty: int
    main_intent: str
    cpc: float
    search_volume_trend: Optional[Dict[str, Any]] = None
    core_keyword: Optional[str] = None
    # Add other fields as they come from DataForSEO API


@dataclass
class CompetitorPage:
    url: str
    rank: int
    word_count: int
    readability_score: float
    technical_warnings: List[str] = field(default_factory=list)
    headings: Dict[str, List[str]] = field(default_factory=dict)
    full_content_plain_text: Optional[str] = None
    error: Optional[str] = None


@dataclass
class SerpOverview:
    serp_has_featured_snippet: bool
    serp_has_video_results: bool
    serp_has_ai_overview: bool
    people_also_ask: List[str] = field(default_factory=list)
    ai_overview_content: Optional[str] = None
    featured_snippet_content: Optional[str] = None
    avg_referring_domains_top5_organic: Optional[float] = None
    avg_main_domain_rank_top5_organic: Optional[float] = None
    serp_last_updated_days_ago: Optional[int] = None
    dominant_content_format: Optional[str] = None


@dataclass
class ContentIntelligence:
    recommended_word_count: int
    average_readability_score: float
    common_headings_to_cover: List[str] = field(default_factory=list)
    unique_angles_to_include: List[str] = field(default_factory=list)
    ai_generated_outline_h2: List[str] = field(default_factory=list)
    ai_generated_outline_h3: List[str] = field(default_factory=list)


@dataclass
class AIBrief:
    target_keyword: str
    content_type: str
    target_audience_persona: str
    primary_goal: str
    target_word_count: int
    mandatory_sections: List[str] = field(default_factory=list)
    unique_angles_to_cover: List[str] = field(default_factory=list)
    questions_to_answer_directly: List[str] = field(default_factory=list)
    internal_linking_suggestions: List[Dict[str, str]] = field(default_factory=list)
    dynamic_serp_instructions: List[str] = field(default_factory=list)
    source_and_inspiration_content: Dict[str, Any] = field(default_factory=dict)
    client_id: str = "default"


@dataclass
class Blueprint:
    metadata: Dict[str, Any]
    winning_keyword: Dict[
        str, Any
    ]  # Full keyword data for the selected winning keyword
    serp_overview: SerpOverview
    content_intelligence: ContentIntelligence
    competitor_analysis: List[CompetitorPage]
    executive_summary: str
    ai_content_brief: AIBrief


@dataclass
class AIContentPackage:
    article_body_html: str
    meta_title: str
    meta_description: str
    social_blurbs: List[Dict[str, Any]]
    editor_notes: str
    ai_focus_keyword: str


@dataclass
class GeneratedImage:
    type: str  # 'featured' or 'in_article_N'
    original_prompt: str
    enhanced_prompt: str
    revised_prompt: str
    local_path: str
    model: str
    wordpress_id: Optional[int] = None
    remote_url: Optional[str] = None
    alt_text: Optional[str] = None
    insertion_marker: Optional[str] = None
    error: Optional[str] = None


@dataclass
class Opportunity:
    id: Optional[int]
    keyword: str  # This is the cluster_topic
    status: str
    client_id: str
    date_added: datetime
    date_processed: Optional[datetime]
    scheduled_for: Optional[datetime]
    full_data: Dict[str, Any]  # Original keyword data
    blueprint_data: Optional[Blueprint]
    ai_content_json: Optional[AIContentPackage]
    ai_content_model: Optional[str]
    featured_image_prompt: Optional[str]
    featured_image_model: Optional[str]
    featured_image_url: Optional[str]
    featured_image_local_path: Optional[str]
    in_article_images_data: List[GeneratedImage] = field(default_factory=list)
    wordpress_post_url: Optional[str]
    wordpress_post_id: Optional[int]
    social_media_posts_json: List[Dict[str, Any]] = field(default_factory=list)
    last_workflow_step: Optional[str]
    error_message: Optional[str]
    search_volume: Optional[int] = None
    keyword_difficulty: Optional[int] = None


@dataclass
class Client:
    client_id: str
    client_name: str
    date_created: datetime


@dataclass
class ClientSettings:
    client_id: str
    openai_api_key: Optional[str] = None
    pexels_api_key: Optional[str] = None  # NEW, was missing from dataclass
    wordpress_url: Optional[str] = None
    wordpress_user: Optional[str] = None
    wordpress_app_password: Optional[str] = None
    hootsuite_api_key: Optional[str] = None
    custom_ai_prompt_template: Optional[str] = None
    ai_image_style_formula: Optional[str] = None
    featured_image_base_prompt: Optional[str] = None
    wordpress_seo_plugin: Optional[str] = None
    ai_content_model: Optional[str] = None
    image_ai_model: Optional[str] = None
    num_in_article_images: Optional[int] = None
    image_quality: Optional[str] = None
    enable_automated_internal_linking: Optional[bool] = False
    default_wordpress_categories: List[str] = field(default_factory=list)
    default_wordpress_tags: List[str] = field(default_factory=list)
    negative_keywords: List[str] = field(default_factory=list)
    competitor_blacklist_domains: List[str] = field(default_factory=list)
    require_question_keywords: Optional[bool] = None
    enforce_intent_filter: Optional[bool] = None
    allowed_intents: List[str] = field(default_factory=list)
    max_competition_level: Optional[str] = None
    discovery_order_by: Optional[str] = None
    db_type: Optional[str] = "sqlite"  # NEW
    max_words_for_ai_analysis: Optional[int] = 1500  # NEW
    ai_generation_temperature: Optional[float] = 0.7  # NEW
    recommended_word_count_multiplier: Optional[float] = 1.2  # NEW
    max_avg_lcp_time: Optional[int] = 4000  # NEW
    prohibited_intents: List[str] = field(default_factory=list)  # NEW
    last_updated: datetime = field(default_factory=datetime.now)
```

## File: data_access/queries.py
```python
# data_access/queries.py

# --- Table Creation ---
CREATE_OPPORTUNITIES_TABLE = """
CREATE TABLE IF NOT EXISTS opportunities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    keyword TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending',
    client_id TEXT NOT NULL DEFAULT 'default',
    date_added TEXT NOT NULL,
    date_processed TEXT,
    strategic_score REAL,
    blog_qualification_status TEXT,
    blog_qualification_reason TEXT,
    keyword_info TEXT,
    keyword_properties TEXT,
    search_intent_info TEXT,
    serp_overview TEXT,
    score_breakdown TEXT,
    full_data TEXT NOT NULL,
    blueprint_data TEXT,
    ai_content_json TEXT,
    ai_content_model TEXT,
    featured_image_url TEXT,
    featured_image_local_path TEXT,
    in_article_images_data TEXT,
    social_media_posts_json TEXT,
    social_media_posts_status TEXT DEFAULT 'draft', -- NEW COLUMN
    last_workflow_step TEXT,
    error_message TEXT,
    wordpress_payload_json TEXT,
    final_package_json TEXT,
    slug TEXT UNIQUE,
    run_id INTEGER,
    keyword_info_normalized_with_bing TEXT,
    keyword_info_normalized_with_clickstream TEXT,
    monthly_searches TEXT,
    traffic_value REAL DEFAULT 0,
    check_url TEXT,
    related_keywords TEXT,
    keyword_categories TEXT,
     core_keyword TEXT,
    published_url TEXT,
     UNIQUE(keyword, client_id)
 );"""

CREATE_CLIENTS_TABLE = """
CREATE TABLE IF NOT EXISTS clients (
    client_id TEXT PRIMARY KEY,
    client_name TEXT NOT NULL,
    date_created TEXT NOT NULL
);
"""

CREATE_CLIENT_SETTINGS_TABLE = """
CREATE TABLE IF NOT EXISTS client_settings (
    client_id TEXT PRIMARY KEY,
    openai_api_key TEXT,
    pexels_api_key TEXT,
    location_code INTEGER,
    language_code TEXT,
    target_domain TEXT,
    device TEXT,
    os TEXT,
    informational_score REAL,
    commercial_score REAL,
    transactional_score REAL,
    navigational_score REAL,
    question_keyword_bonus REAL,
    ease_of_ranking_weight INTEGER,
    traffic_potential_weight INTEGER,
    commercial_intent_weight INTEGER,
    growth_trend_weight INTEGER,
    serp_features_weight INTEGER,
    serp_freshness_weight INTEGER,
    serp_volatility_weight INTEGER,
    competitor_weakness_weight INTEGER,
    max_cpc_for_scoring REAL,
    max_sv_for_scoring INTEGER,
    max_domain_rank_for_scoring INTEGER,
    max_referring_domains_for_scoring INTEGER,
    max_avg_referring_domains_filter INTEGER,
    featured_snippet_bonus REAL,
    ai_overview_bonus REAL,
    serp_freshness_bonus_max REAL,
    serp_freshness_old_threshold_days INTEGER,
    serp_volatility_stable_threshold_days INTEGER,
    enforce_intent_filter INTEGER,
    allowed_intents TEXT,
    require_question_keywords INTEGER,
    negative_keywords TEXT,
    min_monthly_trend_percentage REAL,
    min_competitor_word_count INTEGER,
    max_competitor_technical_warnings INTEGER,
    competitor_blacklist_domains TEXT,
    ugc_and_parasite_domains TEXT,
    num_competitors_to_analyze INTEGER,
    num_common_headings INTEGER,
    num_unique_angles INTEGER,
    max_initial_serp_urls_to_analyze INTEGER,
    calculate_rectangles INTEGER,
    people_also_ask_click_depth INTEGER,
    min_search_volume INTEGER,
    max_keyword_difficulty INTEGER,
    ai_content_model TEXT,
    num_in_article_images INTEGER,
    use_pexels_first INTEGER,
    cleanup_local_images INTEGER,
    onpage_enable_javascript INTEGER,
    onpage_load_resources INTEGER,
    onpage_disable_cookie_popup INTEGER,
    onpage_return_despite_timeout INTEGER,
    onpage_enable_browser_rendering INTEGER,
    onpage_store_raw_html INTEGER,
    onpage_validate_micromarkup INTEGER,
    onpage_check_spell INTEGER,
    onpage_accept_language TEXT,
    onpage_custom_user_agent TEXT,
    onpage_max_domains_per_request INTEGER,
    onpage_max_tasks_per_request INTEGER,
    onpage_enable_switch_pool INTEGER,
    onpage_browser_screen_resolution_ratio REAL,
    discovery_exact_match INTEGER,
    onpage_enable_custom_js INTEGER,
    onpage_custom_js TEXT,
    platforms TEXT,


    enable_automated_internal_linking INTEGER,
    db_type TEXT,
    max_words_for_ai_analysis INTEGER,
    ai_generation_temperature REAL,
    recommended_word_count_multiplier REAL,
    max_avg_lcp_time INTEGER,
    prohibited_intents TEXT,
    load_async_ai_overview INTEGER,
    onpage_custom_checks_thresholds TEXT,
    serp_remove_from_url_params TEXT,
     schema_author_type TEXT,
     client_knowledge_base TEXT,
    wordpress_url TEXT,
    wordpress_user TEXT,
    wordpress_app_password TEXT,
    wordpress_seo_plugin TEXT,
    default_wordpress_categories TEXT,
    default_wordpress_tags TEXT,
     last_updated TEXT NOT NULL,
     FOREIGN KEY (client_id) REFERENCES clients (client_id)
 );"""

INSERT_CLIENT_SETTINGS = """
INSERT INTO client_settings (client_id, brand_tone, target_audience, terms_to_avoid) VALUES (%s, %s, %s, %s)
ON CONFLICT (client_id) DO UPDATE SET brand_tone = %s, target_audience = %s, terms_to_avoid = %s
"""

SELECT_CLIENT_SETTINGS = """
SELECT brand_tone, target_audience, terms_to_avoid FROM client_settings WHERE client_id = %s;
"""

CREATE_DISCOVERY_RUNS_TABLE = """
CREATE TABLE IF NOT EXISTS discovery_runs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    client_id TEXT NOT NULL,
    start_time TEXT NOT NULL,
    end_time TEXT,
    status TEXT NOT NULL,
    parameters TEXT,
    results_summary TEXT,
    log_file_path TEXT,
    error_message TEXT,
    FOREIGN KEY (client_id) REFERENCES clients (client_id)
);
"""

CREATE_SCHEMA_VERSION_TABLE = """
CREATE TABLE IF NOT EXISTS schema_version (
    version INTEGER PRIMARY KEY,
    applied_at TEXT NOT NULL
);
"""

# NEW: Add a query to get current schema version
GET_SCHEMA_VERSION = """
SELECT version FROM schema_version ORDER BY version DESC LIMIT 1;
"""

# NEW: Add a query to insert schema version
INSERT_SCHEMA_VERSION = """
INSERT INTO schema_version (version, applied_at) VALUES (?, ?);
"""

CREATE_API_CACHE_TABLE = """
CREATE TABLE IF NOT EXISTS api_cache (
    key TEXT PRIMARY KEY,
    data TEXT NOT NULL,
    timestamp REAL NOT NULL,
    ttl_days INTEGER NOT NULL
);
"""

# --- Cache table queries (related to migration 002_add_slug_and_cache_table.sql)
INSERT_API_CACHE = """
INSERT OR REPLACE INTO api_cache (key, data, timestamp, ttl_days)
VALUES (?, ?, ?, ?);
"""

SELECT_API_CACHE = """
SELECT data, timestamp, ttl_days FROM api_cache WHERE key = ?;
"""

DELETE_EXPIRED_API_CACHE = """
DELETE FROM api_cache WHERE timestamp + (ttl_days * 86400) < ?;
"""

DELETE_API_CACHE_BY_KEY = """
DELETE FROM api_cache WHERE key = ?;
"""

TRUNCATE_API_CACHE = """
DELETE FROM api_cache;
"""

# --- Opportunity Queries ---
INSERT_OPPORTUNITY_OR_IGNORE = """
INSERT OR IGNORE INTO opportunities 
(keyword, client_id, date_added, full_data)
VALUES (?, ?, ?, ?);
"""

INSERT_OPPORTUNITY_WITH_BLUEPRINT = """
INSERT OR IGNORE INTO opportunities
(keyword, client_id, date_added, full_data, blueprint_data, slug)
VALUES (?, ?, ?, ?, ?, ?);
"""

SELECT_PENDING_OPPORTUNITIES = """
SELECT * FROM opportunities 
WHERE status = 'pending' AND client_id = ?;
"""

SELECT_ALL_OPPORTUNITIES_BY_CLIENT = """
SELECT * FROM opportunities 
WHERE client_id = ?
ORDER BY date_added DESC;
"""

SELECT_OPPORTUNITY_BY_ID = """
SELECT * FROM opportunities WHERE id = ?;
"""

SELECT_ALL_PROCESSED_KEYWORDS = """
SELECT keyword FROM opportunities 
WHERE client_id = ? AND status NOT IN ('rejected', 'failed');
"""

UPDATE_OPPORTUNITY_STATUS_WITH_DATE = """
UPDATE opportunities SET status = ?, date_processed = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_STATUS = """
UPDATE opportunities SET status = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_WORKFLOW_STATE = """
UPDATE opportunities SET last_workflow_step = ?, status = ?, error_message = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_BLUEPRINT = """
UPDATE opportunities
SET blueprint_data = ?
WHERE id = ?;
"""

UPDATE_OPPORTUNITY_BLUEPRINT_AND_SLUG = """
UPDATE opportunities
SET blueprint_data = ?, slug = ?
WHERE id = ?;
"""

UPDATE_OPPORTUNITY_AI_CONTENT = """
UPDATE opportunities
SET ai_content_json = ?, ai_content_model = ?, date_processed = ?
WHERE id = ?;
"""

UPDATE_OPPORTUNITY_IMAGES = """
UPDATE opportunities SET featured_image_url = ?, featured_image_local_path = ?, in_article_images_data = ? WHERE id = ?;
"""

UPDATE_OPPORTUNITY_SOCIAL_POSTS = """
UPDATE opportunities SET social_media_posts_json = ? WHERE id = ?;
"""

SELECT_ALL_OPPORTUNITIES_FOR_EXPORT = """
SELECT * FROM opportunities ORDER BY client_id, date_added DESC;
"""

SELECT_PROCESSED_OPPORTUNITIES = """
SELECT * FROM opportunities
WHERE client_id = ? AND blueprint_data IS NOT NULL
ORDER BY date_processed DESC;
"""

UPDATE_OPPORTUNITY_WORDPRESS_PAYLOAD = (
    "UPDATE opportunities SET wordpress_payload_json = ? WHERE id = ?;"
)

UPDATE_OPPORTUNITY_FINAL_PACKAGE = (
    "UPDATE opportunities SET final_package_json = ? WHERE id = ?;"
)

# --- Dashboard Queries ---
COUNT_OPPORTUNITIES_BY_STATUS = """
SELECT status, COUNT(*) as count FROM opportunities WHERE client_id = ? GROUP BY status;
"""

SUM_DISCOVERY_COST_BY_CLIENT = """
SELECT SUM(CAST(JSON_EXTRACT(results_summary, '$.total_cost') AS REAL)) FROM discovery_runs WHERE client_id = ?;
"""

SUM_ANALYSIS_COST_BY_CLIENT = """
SELECT SUM(CAST(JSON_EXTRACT(blueprint_data, '$.metadata.total_api_cost') AS REAL)) FROM opportunities WHERE client_id = ?;
"""

SELECT_RECENTLY_GENERATED = """
SELECT id, keyword, status, date_processed FROM opportunities 
WHERE client_id = ? AND status = 'generated' 
ORDER BY date_processed DESC 
LIMIT 5;
"""


# --- Client Queries ---
INSERT_CLIENT = """
INSERT INTO clients (client_id, client_name, date_created) VALUES (?, ?, ?);
"""

SELECT_ALL_CLIENTS = """
SELECT client_id, client_name FROM clients ORDER BY date_created DESC;
"""

SELECT_CLIENT_SETTINGS = """
SELECT * FROM client_settings WHERE client_id = ?;
"""

# --- Discovery Run Queries ---
INSERT_DISCOVERY_RUN = """
INSERT INTO discovery_runs (client_id, start_time, status, parameters)
VALUES (?, ?, ?, ?);
"""

UPDATE_DISCOVERY_RUN_STATUS = """
UPDATE discovery_runs SET status = ? WHERE id = ?;
"""

UPDATE_DISCOVERY_RUN_COMPLETED = """
UPDATE discovery_runs SET end_time = ?, status = 'completed', results_summary = ?, total_api_cost = ? WHERE id = ?;
"""

UPDATE_DISCOVERY_RUN_FAILED = """
UPDATE discovery_runs SET end_time = ?, status = 'failed', error_message = ? WHERE id = ?;
"""

SELECT_ALL_DISCOVERY_RUNS_BY_CLIENT = """
SELECT * FROM discovery_runs WHERE client_id = ? ORDER BY start_time DESC;
"""

SELECT_ALL_DISCOVERY_RUNS_BY_CLIENT_PAGINATED = """
SELECT * FROM discovery_runs WHERE client_id = ? ORDER BY start_time DESC LIMIT ? OFFSET ?;
"""

SELECT_DISCOVERY_RUN_BY_ID = """
SELECT * FROM discovery_runs WHERE id = ?;
"""

SELECT_KEYWORDS_FOR_RUN_BY_REASON = """
SELECT * FROM opportunities WHERE run_id = ? AND blog_qualification_reason = ?;
"""

UPDATE_DISCOVERY_RUN_LOG_PATH = """
UPDATE discovery_runs SET log_file_path = ? WHERE id = ?;
"""

SELECT_OPPORTUNITY_BY_SLUG = """

SELECT * FROM opportunities WHERE slug = ?;

"""


CREATE_JOBS_TABLE = """

CREATE TABLE IF NOT EXISTS jobs (

    id TEXT PRIMARY KEY,

    status TEXT NOT NULL,

    progress INTEGER NOT NULL,

    result TEXT,

    error TEXT,

    started_at REAL NOT NULL,

    finished_at REAL

);

"""

GET_JOB = "SELECT * FROM jobs WHERE id = ?;"

UPDATE_JOB_STATUS_DIRECT = """
UPDATE jobs SET status = ?, progress = ?, finished_at = ? WHERE id = ?;
"""

UPDATE_JOB = """

INSERT OR REPLACE INTO jobs (id, status, progress, result, error, started_at, finished_at)

VALUES (?, ?, ?, ?, ?, ?, ?);

"""

GET_ALL_JOBS = "SELECT * FROM jobs ORDER BY started_at DESC LIMIT 100;"

CREATE_CONTENT_HISTORY_TABLE = """
CREATE TABLE IF NOT EXISTS content_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    timestamp TEXT NOT NULL,
    ai_content_json TEXT NOT NULL,
    restored_from_id INTEGER,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);
"""

# In data_access/queries.py, add history queries:
INSERT_CONTENT_HISTORY = """
INSERT INTO content_history
(opportunity_id, timestamp, ai_content_json)
VALUES (?, ?, ?);
"""

SELECT_CONTENT_HISTORY_BY_OPP_ID = """
SELECT id, opportunity_id, timestamp, ai_content_json FROM content_history
WHERE opportunity_id = ?
ORDER BY timestamp DESC;
"""

# Update/Add status update query:
UPDATE_OPPORTUNITY_AI_CONTENT_AND_STATUS = """
UPDATE opportunities
SET ai_content_json = ?, ai_content_model = ?, date_processed = ?, status = ?
WHERE id = ?;
"""

# In data_access/queries.py, define template queries:
COUNT_ALL_OPPORTUNITIES_BY_CLIENT = """
SELECT COUNT(*) FROM opportunities
WHERE client_id = ? 
"""

SELECT_ALL_OPPORTUNITIES_PAGINATED = """
SELECT {select_columns} FROM opportunities
WHERE client_id = ? {where_clause}
ORDER BY {order_by} {order_direction}
LIMIT ? OFFSET ?;
"""

SEARCH_OPPORTUNITIES_BY_KEYWORD = """
SELECT id, keyword, status FROM opportunities
WHERE client_id = ? AND keyword LIKE ?
ORDER BY date_added DESC
LIMIT 20;
"""

SELECT_HIGH_PRIORITY_OPPORTUNITIES = """
SELECT id, keyword, strategic_score, score_breakdown, keyword_info, keyword_properties, traffic_value FROM opportunities
WHERE client_id = ? AND status = 'validated'
ORDER BY strategic_score DESC
LIMIT ?;
"""

SELECT_ACTION_ITEMS = """
SELECT id, keyword, status, error_message, COALESCE(date_processed, date_added) as updated_at, strategic_score FROM opportunities
WHERE client_id = ? AND status IN ('paused_for_approval', 'failed')
ORDER BY updated_at DESC;
"""

CREATE_CONTENT_FEEDBACK_TABLE = """
CREATE TABLE IF NOT EXISTS content_feedback (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    opportunity_id INTEGER NOT NULL,
    rating INTEGER NOT NULL, -- 1-5 scale
    comments TEXT,
    timestamp TEXT NOT NULL,
    FOREIGN KEY (opportunity_id) REFERENCES opportunities (id)
);
"""

INSERT_CONTENT_FEEDBACK = """
INSERT INTO content_feedback (opportunity_id, rating, comments, timestamp)
VALUES (?, ?, ?, ?);
"""

UPDATE_OPPORTUNITY_SCORES = """
UPDATE opportunities
SET strategic_score = ?, score_breakdown = ?, blueprint_data = ?
WHERE id = ?;
"""

UPDATE_GENERATED_CONTENT_AND_STATUS = """
UPDATE opportunities
SET
    ai_content_json = ?,
    ai_content_model = ?,
    featured_image_url = ?,
    featured_image_local_path = ?,
    in_article_images_data = ?,
    social_media_posts_json = ?,
    final_package_json = ?,
    status = 'generated',
    last_workflow_step = 'generation_complete',
    date_processed = ?,
    total_api_cost = ?
WHERE id = ?;
"""

INSERT_DEFAULT_QUALIFICATION_SETTINGS = """
INSERT INTO qualification_settings (
    client_id, ease_of_ranking_weight, traffic_potential_weight, commercial_intent_weight,
    serp_features_weight, growth_trend_weight, serp_freshness_weight, serp_volatility_weight,
    competitor_weakness_weight, competitor_performance_weight, min_search_volume, max_keyword_difficulty,
    negative_keywords, prohibited_intents, max_y_pixel_threshold,
    max_forum_results_in_top_10, max_ecommerce_results_in_top_10,
    disallowed_page_types_in_top_3
) VALUES (?, 40, 15, 5, 5, 5, 5, 5, 20, 5, 100, 80, 'login,free,cheap', 'navigational', 800, 3, 2, 'E-commerce,Forum')
"""

FAIL_STALE_JOBS = """
UPDATE jobs
SET status = 'failed', error = ?, finished_at = ?
WHERE status = 'running';
"""
```

## File: data_mappers/dataforseo_mapper.py
```python
# FILE: data_mappers/dataforseo_mapper.py

from typing import Dict, Any
import logging
from backend.core.utils import parse_datetime_string  # ADDED IMPORT
import json

logger = logging.getLogger(__name__)


class DataForSEOMapper:
    """
    Provides static methods to sanitize and normalize raw DataForSEO API responses
    immediately after they are received, before they enter the main pipeline.
    Ensures consistent data types and handles common API quirks.
    """

    @staticmethod
    def _sanitize_serp_info(serp_info: Dict[str, Any]) -> Dict[str, Any]:
        """Sanitizes the 'serp_info' object, specifically handling 'se_results_count' and datetimes."""
        if isinstance(serp_info.get("se_results_count"), str):
            try:
                serp_info["se_results_count"] = int(serp_info["se_results_count"])
            except (ValueError, TypeError):
                logger.warning(
                    f"Failed to convert 'se_results_count' (was string) to int. Setting to 0. Raw: {serp_info.get('se_results_count')}"
                )
                serp_info["se_results_count"] = 0

        # Sanitize datetime fields
        serp_info["last_updated_time"] = parse_datetime_string(
            serp_info.get("last_updated_time")
        )
        serp_info["previous_updated_time"] = parse_datetime_string(
            serp_info.get("previous_updated_time")
        )

        return serp_info

    @staticmethod
    def sanitize_keyword_data_item(item: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sanitizes a single keyword data item (e.g., from keyword_ideas, keyword_suggestions, related_keywords).
        Applies cleaning to nested structures like 'keyword_info', 'keyword_properties', 'serp_info'.
        """
        if not isinstance(item, dict):
            logger.warning(
                f"Invalid item type received for sanitization: {type(item)}. Skipping."
            )
            return {}

        sanitized_item = item.copy()

        # Sanitize keyword_info
        if isinstance(sanitized_item.get("keyword_info"), dict):
            # Ensure CPC and Competition are floats, defaulting to 0.0 if missing or None
            sanitized_item["keyword_info"]["cpc"] = float(
                sanitized_item["keyword_info"].get("cpc") or 0.0
            )
            sanitized_item["keyword_info"]["competition"] = float(
                sanitized_item["keyword_info"].get("competition") or 0.0
            )

            # Ensure other numeric fields are handled
            sanitized_item["keyword_info"]["search_volume"] = int(
                sanitized_item["keyword_info"].get("search_volume") or 0
            )
            sanitized_item["keyword_info"]["low_top_of_page_bid"] = float(
                sanitized_item["keyword_info"].get("low_top_of_page_bid") or 0.0
            )
            sanitized_item["keyword_info"]["high_top_of_page_bid"] = float(
                sanitized_item["keyword_info"].get("high_top_of_page_bid") or 0.0
            )

            # Sanitize last_updated_time
            sanitized_item["keyword_info"]["last_updated_time"] = parse_datetime_string(
                sanitized_item["keyword_info"].get("last_updated_time")
            )

            # Ensure monthly_searches are properly parsed if they are raw strings
            if isinstance(sanitized_item["keyword_info"].get("monthly_searches"), str):
                try:
                    sanitized_item["keyword_info"]["monthly_searches"] = json.loads(
                        sanitized_item["keyword_info"]["monthly_searches"]
                    )
                except json.JSONDecodeError:
                    logger.warning(
                        f"Failed to parse monthly_searches JSON string for keyword '{sanitized_item.get('keyword')}'. Resetting."
                    )
                    sanitized_item["keyword_info"]["monthly_searches"] = []

            # Ensure individual monthly_searches items are sanitized for type consistency
            if isinstance(sanitized_item["keyword_info"].get("monthly_searches"), list):
                for month_data in sanitized_item["keyword_info"]["monthly_searches"]:
                    if isinstance(month_data, dict):
                        month_data["year"] = int(month_data.get("year") or 0)
                        month_data["month"] = int(month_data.get("month") or 0)
                        month_data["search_volume"] = int(
                            month_data.get("search_volume") or 0
                        )

        # Sanitize keyword_properties
        if isinstance(sanitized_item.get("keyword_properties"), dict):
            sanitized_item["keyword_properties"]["keyword_difficulty"] = int(
                sanitized_item["keyword_properties"].get("keyword_difficulty") or 0
            )

        # Sanitize search_intent_info
        if isinstance(sanitized_item.get("search_intent_info"), dict):
            sanitized_item["search_intent_info"]["foreign_intent"] = (
                sanitized_item["search_intent_info"].get("foreign_intent") or []
            )
            sanitized_item["search_intent_info"]["last_updated_time"] = (
                parse_datetime_string(
                    sanitized_item["search_intent_info"].get("last_updated_time")
                )
            )

        # Sanitize serp_info (crucial for se_results_count string/int issue)
        if isinstance(sanitized_item.get("serp_info"), dict):
            sanitized_item["serp_info"] = DataForSEOMapper._sanitize_serp_info(
                sanitized_item["serp_info"]
            )
            sanitized_item["serp_info"]["serp_item_types"] = (
                sanitized_item["serp_info"].get("serp_item_types") or []
            )

        # Sanitize avg_backlinks_info
        if isinstance(sanitized_item.get("avg_backlinks_info"), dict):
            for key in [
                "backlinks",
                "dofollow",
                "referring_pages",
                "referring_domains",
                "referring_main_domains",
                "rank",
                "main_domain_rank",
            ]:
                sanitized_item["avg_backlinks_info"][key] = float(
                    sanitized_item["avg_backlinks_info"].get(key) or 0.0
                )
            sanitized_item["avg_backlinks_info"]["last_updated_time"] = (
                parse_datetime_string(
                    sanitized_item["avg_backlinks_info"].get("last_updated_time")
                )
            )

        # Keyword Info Normalized with Bing
        for normalized_key in [
            "keyword_info_normalized_with_bing",
            "keyword_info_normalized_with_clickstream",
        ]:
            if isinstance(sanitized_item.get(normalized_key), dict):
                normalized_data = sanitized_item[normalized_key]
                normalized_data["search_volume"] = int(
                    normalized_data.get("search_volume") or 0
                )
                normalized_data["last_updated_time"] = parse_datetime_string(
                    normalized_data.get("last_updated_time")
                )
                if isinstance(normalized_data.get("monthly_searches"), list):
                    for month_data in normalized_data["monthly_searches"]:
                        if isinstance(month_data, dict):
                            month_data["year"] = int(month_data.get("year") or 0)
                            month_data["month"] = int(month_data.get("month") or 0)
                            month_data["search_volume"] = int(
                                month_data.get("search_volume") or 0
                            )

        return sanitized_item

    @staticmethod
    def sanitize_serp_overview_response(serp_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sanitizes a full SERP overview response from FullSerpAnalyzer.analyze_serp.
        Ensures consistent types for nested items, especially 'pixel_rank_data' and 'top_organic_results'.
        """
        sanitized_serp_data = serp_data.copy()

        # Sanitize top-level date/time fields
        sanitized_serp_data["datetime"] = parse_datetime_string(
            sanitized_serp_data.get("datetime")
        )
        sanitized_serp_data["last_updated_time"] = parse_datetime_string(
            sanitized_serp_data.get("last_updated_time")
        )
        sanitized_serp_data["previous_updated_time"] = parse_datetime_string(
            sanitized_serp_data.get("previous_updated_time")
        )

        # Sanitize pixel_rank_data
        if isinstance(sanitized_serp_data.get("raw_pixel_ranking_data"), list):
            for item in sanitized_serp_data["raw_pixel_ranking_data"]:
                if isinstance(item.get("rectangle"), dict):
                    item["rectangle"]["x"] = float(item["rectangle"].get("x") or 0.0)
                    item["rectangle"]["y"] = float(item["rectangle"].get("y") or 0.0)
                    item["rectangle"]["width"] = float(
                        item["rectangle"].get("width") or 0.0
                    )
                    item["rectangle"]["height"] = float(
                        item["rectangle"].get("height") or 0.0
                    )
                item["rank_absolute"] = int(item.get("rank_absolute") or 0)
                item["rank_group"] = int(item.get("rank_group") or 0)

        if sanitized_serp_data.get("first_organic_y_pixel") is not None:
            sanitized_serp_data["first_organic_y_pixel"] = float(
                sanitized_serp_data["first_organic_y_pixel"]
            )

        # Sanitize top_organic_results
        if isinstance(sanitized_serp_data.get("top_organic_results"), list):
            for result in sanitized_serp_data["top_organic_results"]:
                result["rank"] = int(result.get("rank") or 0)
                if isinstance(result.get("rating"), dict):
                    result["rating"]["value"] = float(
                        result["rating"].get("value") or 0.0
                    )
                    result["rating"]["votes_count"] = int(
                        result["rating"].get("votes_count") or 0
                    )
                    result["rating"]["rating_max"] = int(
                        result["rating"].get("rating_max") or 0
                    )

                # Ensure about_this_result_search_terms and related_terms are lists
                result["about_this_result_search_terms"] = (
                    result.get("about_this_result_search_terms") or []
                )
                result["about_this_result_related_terms"] = (
                    result.get("about_this_result_related_terms") or []
                )

        sanitized_serp_data["people_also_ask"] = (
            sanitized_serp_data.get("people_also_ask") or []
        )
        sanitized_serp_data["related_searches"] = (
            sanitized_serp_data.get("related_searches") or []
        )
        sanitized_serp_data["extracted_serp_features"] = (
            sanitized_serp_data.get("extracted_serp_features") or []
        )

        if isinstance(sanitized_serp_data.get("ai_overview_items"), list):
            for ai_item in sanitized_serp_data["ai_overview_items"]:
                ai_item["rank_group"] = int(ai_item.get("rank_group") or 0)
                ai_item["rank_absolute"] = int(ai_item.get("rank_absolute") or 0)
                if isinstance(ai_item.get("references"), list):
                    for ref in ai_item["references"]:
                        ref["date"] = parse_datetime_string(ref.get("date"))
                        ref["timestamp"] = parse_datetime_string(ref.get("timestamp"))
                if isinstance(ai_item.get("table"), dict):
                    if not isinstance(ai_item["table"].get("table_header"), list):
                        ai_item["table"]["table_header"] = []
                    if not isinstance(ai_item["table"].get("table_content"), list):
                        ai_item["table"]["table_content"] = []

        return sanitized_serp_data

    @staticmethod
    def sanitize_onpage_data_item(item: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sanitizes a single OnPage API response item from `get_onpage_data_for_urls`.
        Ensures consistent data types for numeric fields, especially in 'meta' and 'page_timing'.
        """
        sanitized_item = item.copy()

        # Sanitize meta fields
        meta = sanitized_item.get("meta", {})
        if isinstance(meta, dict):
            meta["charset"] = int(meta.get("charset") or 0)
            meta["internal_links_count"] = int(meta.get("internal_links_count") or 0)
            meta["external_links_count"] = int(meta.get("external_links_count") or 0)
            meta["inbound_links_count"] = int(meta.get("inbound_links_count") or 0)
            meta["images_count"] = int(meta.get("images_count") or 0)
            meta["images_size"] = int(meta.get("images_size") or 0)
            meta["scripts_count"] = int(meta.get("scripts_count") or 0)
            meta["scripts_size"] = int(meta.get("scripts_size") or 0)
            meta["stylesheets_count"] = int(meta.get("stylesheets_count") or 0)
            meta["stylesheets_size"] = int(meta.get("stylesheets_size") or 0)
            meta["title_length"] = int(meta.get("title_length") or 0)
            meta["description_length"] = int(meta.get("description_length") or 0)
            meta["render_blocking_scripts_count"] = int(
                meta.get("render_blocking_scripts_count") or 0
            )
            meta["render_blocking_stylesheets_count"] = int(
                meta.get("render_blocking_stylesheets_count") or 0
            )
            meta["cumulative_layout_shift"] = float(
                meta.get("cumulative_layout_shift") or 0.0
            )

            # Sanitize date/time fields
            meta["last_updated_time"] = parse_datetime_string(
                meta.get("last_updated_time")
            )

            content_meta = meta.get("content", {})
            if isinstance(content_meta, dict):
                content_meta["plain_text_size"] = int(
                    content_meta.get("plain_text_size") or 0
                )
                content_meta["plain_text_rate"] = float(
                    content_meta.get("plain_text_rate") or 0.0
                )
                content_meta["plain_text_word_count"] = int(
                    content_meta.get("plain_text_word_count") or 0
                )
                content_meta["automated_readability_index"] = float(
                    content_meta.get("automated_readability_index") or 0.0
                )
                content_meta["coleman_liau_readability_index"] = float(
                    content_meta.get("coleman_liau_readability_index") or 0.0
                )
                content_meta["dale_chall_readability_index"] = float(
                    content_meta.get("dale_chall_readability_index") or 0.0
                )
                content_meta["flesch_kincaid_readability_index"] = float(
                    content_meta.get("flesch_kincaid_readability_index") or 0.0
                )
                content_meta["smog_readability_index"] = float(
                    content_meta.get("smog_readability_index") or 0.0
                )
                content_meta["description_to_content_consistency"] = float(
                    content_meta.get("description_to_content_consistency") or 0.0
                )
                content_meta["title_to_content_consistency"] = float(
                    content_meta.get("title_to_content_consistency") or 0.0
                )
                content_meta["meta_keywords_to_content_consistency"] = float(
                    content_meta.get("meta_keywords_to_content_consistency") or 0.0
                )
            sanitized_item["meta"] = meta

        # Sanitize page_timing fields
        page_timing = sanitized_item.get("page_timing", {})
        if isinstance(page_timing, dict):
            page_timing["time_to_interactive"] = int(
                page_timing.get("time_to_interactive") or 0
            )
            page_timing["dom_complete"] = int(page_timing.get("dom_complete") or 0)
            page_timing["largest_contentful_paint"] = float(
                page_timing.get("largest_contentful_paint") or 0.0
            )
            page_timing["first_input_delay"] = float(
                page_timing.get("first_input_delay") or 0.0
            )
            page_timing["connection_time"] = int(
                page_timing.get("connection_time") or 0
            )
            page_timing["time_to_secure_connection"] = int(
                page_timing.get("time_to_secure_connection") or 0
            )
            page_timing["request_sent_time"] = int(
                page_timing.get("request_sent_time") or 0
            )
            page_timing["waiting_time"] = int(page_timing.get("waiting_time") or 0)
            page_timing["download_time"] = int(page_timing.get("download_time") or 0)
            page_timing["duration_time"] = int(page_timing.get("duration_time") or 0)
            page_timing["fetch_start"] = int(page_timing.get("fetch_start") or 0)
            page_timing["fetch_end"] = int(page_timing.get("fetch_end") or 0)
            sanitized_item["page_timing"] = page_timing

        # Sanitize top-level numeric fields
        sanitized_item["onpage_score"] = float(
            sanitized_item.get("onpage_score") or 0.0
        )
        sanitized_item["total_dom_size"] = int(
            sanitized_item.get("total_dom_size") or 0
        )
        sanitized_item["size"] = int(sanitized_item.get("size") or 0)
        sanitized_item["encoded_size"] = int(sanitized_item.get("encoded_size") or 0)
        sanitized_item["total_transfer_size"] = int(
            sanitized_item.get("total_transfer_size") or 0
        )
        sanitized_item["url_length"] = int(sanitized_item.get("url_length") or 0)
        sanitized_item["relative_url_length"] = int(
            sanitized_item.get("relative_url_length") or 0
        )

        # Sanitize fetch_time
        sanitized_item["fetch_time"] = parse_datetime_string(
            sanitized_item.get("fetch_time")
        )

        # Sanitize cache_control ttl
        if isinstance(sanitized_item.get("cache_control"), dict):
            sanitized_item["cache_control"]["ttl"] = int(
                sanitized_item["cache_control"].get("ttl") or 0
            )

        # Sanitize last_modified dates
        if isinstance(sanitized_item.get("last_modified"), dict):
            sanitized_item["last_modified"]["header"] = parse_datetime_string(
                sanitized_item["last_modified"].get("header")
            )
            sanitized_item["last_modified"]["sitemap"] = parse_datetime_string(
                sanitized_item["last_modified"].get("sitemap")
            )
            sanitized_item["last_modified"]["meta_tag"] = parse_datetime_string(
                sanitized_item["last_modified"].get("meta_tag")
            )

        return sanitized_item
```

## File: data_mappers/keyword_data_mapper.py
```python
# data_mappers/keyword_data_mapper.py

from typing import Dict, Any
from data_access.models import KeywordData


def map_keyword_data(raw_data: Dict[str, Any]) -> KeywordData:
    """Maps raw keyword data from the DataForSEO API to the KeywordData model."""
    keyword_info = raw_data.get("keyword_info", {})
    keyword_properties = raw_data.get("keyword_properties", {})
    search_intent_info = raw_data.get("search_intent_info", {})

    return KeywordData(
        keyword=raw_data.get("keyword"),
        search_volume=keyword_info.get("search_volume"),
        keyword_difficulty=keyword_properties.get("keyword_difficulty"),
        cpc=keyword_info.get("cpc"),
        main_intent=search_intent_info.get("main_intent"),
        search_volume_trend=keyword_info.get("search_volume_trend"),
        core_keyword=keyword_properties.get("core_keyword"),
    )
```

## File: data_mappers/serp_overview_mapper.py
```python
# data_mappers/serp_overview_mapper.py

from typing import Dict, Any
from data_access.models import SerpOverview


def map_serp_overview(raw_data: Dict[str, Any]) -> SerpOverview:
    """Maps raw SERP data from the DataForSEO API to the SerpOverview model."""
    serp_info = raw_data.get("serp_info", {})
    avg_backlinks_info = raw_data.get("avg_backlinks_info", {})

    return SerpOverview(
        serp_has_featured_snippet="featured_snippet"
        in serp_info.get("serp_item_types", []),
        serp_has_video_results="video" in serp_info.get("serp_item_types", []),
        serp_has_ai_overview="ai_overview" in serp_info.get("serp_item_types", []),
        people_also_ask=serp_info.get("people_also_ask", []),
        ai_overview_content=serp_info.get("ai_overview_content"),
        featured_snippet_content=serp_info.get("featured_snippet_content"),
        avg_referring_domains_top5_organic=avg_backlinks_info.get("referring_domains"),
        avg_main_domain_rank_top5_organic=avg_backlinks_info.get("main_domain_rank"),
        serp_last_updated_days_ago=serp_info.get("last_updated_time"),
        dominant_content_format=serp_info.get("dominant_content_format"),
    )
```

## File: external_apis/dataforseo_client_v2.py
```python
"""
This module provides a simplified and corrected client for the DataForSEO OnPage API,
focusing exclusively on the `instant_pages` endpoint.
"""

import base64
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import requests
from urllib.parse import urlparse
import hashlib
from backend.data_access.database_manager import DatabaseManager
from backend.data_mappers.dataforseo_mapper import DataForSEOMapper


class DataForSEOClientV2:
    """
    A simplified client for the DataForSEO API, using only the `instant_pages` endpoint.
    """

    # W17 FIX: Move hardcoded endpoints to constants
    LABS_KEYWORD_IDEAS = "dataforseo_labs/google/keyword_ideas/live"
    LABS_KEYWORD_SUGGESTIONS = "dataforseo_labs/google/keyword_suggestions/live"
    LABS_RELATED_KEYWORDS = "dataforseo_labs/google/related_keywords/live"
    LABS_RANKED_KEYWORDS = "dataforseo_labs/google/ranked_keywords/live"
    LABS_COMPETITORS_DOMAIN = "dataforseo_labs/google/competitors_domain/live"
    SERP_ADVANCED = "serp/google/organic/live/advanced"
    ONPAGE_INSTANT_PAGES = "on_page/instant_pages"
    ONPAGE_CONTENT_PARSING = "on_page/content_parsing/live"  # Add this line

    def __init__(
        self,
        login: str,
        password: str,
        db_manager: DatabaseManager,
        config: Dict[str, Any],
        enable_cache: bool = True,
    ):
        self.base_url = "https://api.dataforseo.com/v3"
        if not login or not password:
            raise ValueError("DataForSEO API login and password cannot be empty.")
        credentials = f"{login}:{password}"
        self.headers = {
            "Authorization": f"Basic {base64.b64encode(credentials.encode()).decode()}",
            "Content-Type": "application/json",
        }
        self.logger = logging.getLogger(self.__class__.__name__)
        self.db_manager = db_manager
        self.config = config  # Store the config object
        self.enable_cache = enable_cache

    def _enforce_api_filter_limit(
        self, filters: Optional[List[Any]], max_limit: int = 8
    ) -> Optional[List[Any]]:
        """
        Enforces the API filter limit (max 8 conditions) by truncating the filter list.
        This is a backend safeguard if frontend validation is bypassed.
        """
        if not filters:
            return None

        # Count actual filter conditions (which are lists, not "and"/"or" strings)
        condition_elements = [f for f in filters if isinstance(f, list)]
        if len(condition_elements) <= max_limit:
            return filters  # No need to truncate

        self.logger.warning(
            f"Backend safeguard: API filter list exceeds {max_limit} conditions ({len(condition_elements)} found). Truncating to the first {max_limit} conditions."
        )

        truncated_filters = []
        condition_count = 0
        for item in filters:
            if isinstance(item, list):  # This is a condition
                if condition_count < max_limit:
                    truncated_filters.append(item)
                    condition_count += 1
                else:
                    break  # Stop adding conditions
            elif truncated_filters and isinstance(truncated_filters[-1], list):
                # Add logical operator only if it follows a condition and we're still building
                truncated_filters.append(item)

        # Ensure the list doesn't end with a dangling logical operator
        if (
            truncated_filters
            and isinstance(truncated_filters[-1], str)
            and truncated_filters[-1].lower() in ["and", "or"]
        ):
            truncated_filters.pop()

        return truncated_filters

    def _post_request(
        self, endpoint: str, data: List[Dict[str, Any]], tag: Optional[str] = None
    ) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Handles the actual POST request to the API, with retries and exponential backoff for rate limits.
        """
        cache_key_string = json.dumps(
            {
                "endpoint": endpoint,
                "data": data,
                "filters": data[0].get("filters") if data else None,
            },
            sort_keys=True,
        )
        cache_key = hashlib.md5(cache_key_string.encode("utf-8")).hexdigest()

        if self.enable_cache:
            cached_response = self.db_manager.get_api_cache(cache_key)
            if cached_response:
                self.logger.info(f"Cache HIT for endpoint {endpoint} with tag '{tag}'.")
                return cached_response, 0.0

        self.logger.info(
            f"Cache MISS for endpoint {endpoint} with tag '{tag}'. Making live API call."
        )

        if tag:
            for task_item in data:
                if isinstance(task_item, dict):
                    task_item["tag"] = tag

        full_url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"
        self.logger.info(
            f"Making POST request to {full_url} with data: {json.dumps(data)}"
        )
        retries = 3
        backoff_factor = 5

        for attempt in range(retries):
            try:
                response = requests.post(
                    full_url, headers=self.headers, data=json.dumps(data), timeout=120
                )

                # W20 FIX: Early exit for critical top-level HTTP errors
                if response.status_code >= 500:
                    self.logger.error(
                        f"DataForSEO API returned a server error ({response.status_code}). Aborting after {attempt + 1} attempts."
                    )
                    return None, 0.0  # Do not retry on server errors

                response.raise_for_status()  # Raise HTTPError for 4xx client errors

                response_json = response.json()

                # W20 FIX: Check top-level status_code from DataForSEO
                if response_json.get("status_code") != 20000:
                    self.logger.error(
                        f"DataForSEO API returned non-20000 status_code: {response_json.get('status_code')} - {response_json.get('status_message')}"
                    )
                    # No retry for auth errors, etc.
                    if response_json.get("status_code") in [40101, 40102, 40103]:
                        return None, 0.0

                # W20 FIX: Log critical task-level errors
                if response_json.get("tasks_error", 0) > 0:
                    for task in response_json.get("tasks", []):
                        if task.get("status_code") != 20000:
                            # Log specific, known critical errors
                            if task.get("status_code") == 40501:  # Duplicate crawl host
                                self.logger.critical(
                                    f"CRITICAL API ERROR (40501): Duplicate crawl host detected for URL {task.get('data', {}).get('url')}. This batch is invalid."
                                )
                            else:
                                self.logger.warning(
                                    f"Task-level error for {task.get('data', {}).get('url', 'N/A')}: {task.get('status_code')} - {task.get('status_message')}"
                                )

                cost = response_json.get("cost", 0.0)

                if self.enable_cache:
                    self.db_manager.set_api_cache(cache_key, response_json)

                return response_json, cost

            except requests.exceptions.HTTPError as e:
                # This will now primarily catch 4xx errors
                if (
                    response.status_code == 429 and attempt < retries - 1
                ):  # Specifically handle rate limits
                    wait_time = backoff_factor * (2**attempt)
                    self.logger.warning(
                        f"Rate limit exceeded (429). Retrying in {wait_time} seconds... (Attempt {attempt + 1}/{retries})"
                    )
                    time.sleep(wait_time)
                    continue
                else:
                    self.logger.error(
                        f"HTTP error during DataForSEO API request to {full_url}: {e}",
                        exc_info=True,
                    )
                    return None, 0.0
            except requests.exceptions.RequestException as e:
                self.logger.error(
                    f"Network error during DataForSEO API request to {full_url}: {e}",
                    exc_info=True,
                )
                if attempt < retries - 1:
                    time.sleep(backoff_factor * (2**attempt))
                    continue
                return None, 0.0

        return None, 0.0

    def _prioritize_and_limit_filters(self, filters: Optional[List[Any]]) -> List[Any]:
        """Enforces the 8-filter maximum rule by prioritizing essential filters."""
        if not filters:
            return []

        # Count actual filter conditions (excluding logical operators like "and", "or")
        condition_count = sum(1 for f in filters if isinstance(f, list))

        # If already within the limit, return as is.
        if condition_count <= 8:
            return filters

        self.logger.warning(
            f"Filter list exceeds 8 conditions ({condition_count} found). Prioritizing essential filters."
        )

        # Simple prioritization logic: Keep filters based on field name presence
        # Prioritized fields (essential for targeting): keyword_difficulty, search_volume, main_intent, competition_level, cpc, competition
        PRIORITIZED_FIELDS = [
            "keyword_difficulty",
            "search_volume",
            "main_intent",
            "competition_level",
            "cpc",
            "competition",
        ]

        prioritized_filters = []
        other_filters = []

        # Iterate through the filters to separate prioritized from others
        for element in filters:
            if isinstance(element, list) and len(element) >= 3:
                # Assuming element[0] is the field name, like "keyword_info.search_volume"
                field_name = element[0].lower()
                is_priority = any(
                    p_field in field_name for p_field in PRIORITIZED_FIELDS
                )

                if is_priority:
                    prioritized_filters.append(element)
                else:
                    other_filters.append(element)
            else:
                # Logical operators ('and', 'or') will be re-added later if needed
                pass

        # Combine prioritized filters (up to 8 slots)
        limited_filters_list = []  # Only actual conditions

        # 1. Add prioritized filters first
        for f in prioritized_filters:
            if len(limited_filters_list) < 8:
                limited_filters_list.append(f)
            else:
                break

        # 2. Fill remaining slots with non-prioritized filters if space permits
        for f in other_filters:
            if len(limited_filters_list) < 8:
                limited_filters_list.append(f)
            else:
                break

        # Reconstruct the filter list with "and" operators
        final_filters_structure = []
        for i, filt in enumerate(limited_filters_list):
            final_filters_structure.append(filt)
            if i < len(limited_filters_list) - 1:
                final_filters_structure.append("and")

        return final_filters_structure

    def get_technical_onpage_data(
        self, urls: List[str], client_cfg: Dict[str, Any]
    ) -> Tuple[Optional[List[Dict[str, Any]]], float]:
        """
        Performs a batch OnPage scan using the Instant Pages endpoint to get technical SEO data.
        """
        if not urls:
            return [], 0.0

        self.logger.info(
            f"Fetching OnPage data for {len(urls)} URLs with device preset '{client_cfg.get('device', 'desktop')}' using Instant Pages..."
        )

        endpoint = self.ONPAGE_INSTANT_PAGES
        # Group URLs into batches that comply with max_domains and max_tasks
        url_batches = self._group_urls_by_domain(
            urls,
            max_domains=client_cfg.get("onpage_max_domains_per_request", 5),
            batch_size=client_cfg.get("onpage_max_tasks_per_request", 20),
        )

        all_results = []
        total_cost = 0.0

        for i, batch in enumerate(url_batches):
            post_data = []
            for url in batch:
                task_data = {
                    "url": url,
                    # W8 FIX: Use configured value, not hardcoded False
                    "enable_browser_rendering": client_cfg.get(
                        "onpage_enable_browser_rendering", False
                    ),
                    # W5 FIX: Inject critical analysis parameters
                    "validate_micromarkup": client_cfg.get(
                        "onpage_validate_micromarkup", False
                    ),
                    "return_despite_timeout": client_cfg.get(
                        "onpage_return_despite_timeout", False
                    ),
                    "check_spell": client_cfg.get("onpage_check_spell", False),
                    # W7 FIX: Inject language header
                    "accept_language": client_cfg.get("onpage_accept_language"),
                    # Include configured user agent if available
                    "custom_user_agent": client_cfg.get("onpage_custom_user_agent"),
                    "switch_pool": client_cfg.get("onpage_enable_switch_pool", False),
                }

                # Add ip_pool_for_scan if it's in the client config
                if "ip_pool_for_scan" in client_cfg:
                    task_data["ip_pool_for_scan"] = client_cfg["ip_pool_for_scan"]

                # W7 FIX & W17 FIX: Include custom screen resolution if rendering is enabled and validate its range
                screen_ratio = client_cfg.get("onpage_browser_screen_resolution_ratio")
                if screen_ratio is not None and (
                    task_data["enable_browser_rendering"]
                    or client_cfg.get("onpage_enable_javascript", False)
                ):
                    # W17 FIX: Validate the range [0.5, 3.0]
                    if 0.5 <= screen_ratio <= 3.0:
                        task_data["browser_screen_resolution_ratio"] = screen_ratio
                    else:
                        self.logger.error(
                            f"Invalid screen ratio configured: {screen_ratio}. Must be between 0.5 and 3.0. Omitting parameter."
                        )
                        # Parameter will be omitted if outside valid range, relying on DataForSEO default.

                # W9 FIX: Include custom checks threshold if provided in config (continues from Task 4.4)
                thresholds_str = client_cfg.get("onpage_custom_checks_thresholds")
                if thresholds_str:
                    try:
                        task_data["checks_threshold"] = json.loads(thresholds_str)
                    except json.JSONDecodeError:
                        self.logger.error(
                            "Failed to parse onpage_custom_checks_thresholds JSON from config. Using default thresholds."
                        )

                # W12 FIX: Include custom JS if enabled (continues from Task 12.4)
                if client_cfg.get("onpage_enable_custom_js", False):
                    custom_js_script = client_cfg.get("onpage_custom_js")
                    if custom_js_script:
                        task_data["custom_js"] = custom_js_script

                # Remove keys if their value is None to maintain a clean API request
                task_data = {k: v for k, v in task_data.items() if v is not None}
                post_data.append(task_data)

            # --- Attempt 1 for the batch ---
            response, cost = self._post_request(
                endpoint, post_data, tag=f"onpage_instant_pages_content:batch{i + 1}"
            )
            total_cost += cost

            current_batch_results = []
            failed_urls_in_batch = []

            if response and response.get("tasks"):
                for task in response["tasks"]:
                    task_url = task.get("data", {}).get("url")

                    # Explicit Failure Criteria Check: Task failed OR result is malformed/empty
                    if (
                        task.get("status_code") != 20000
                        or not task.get("result")
                        or not task["result"][0].get("items")
                    ):
                        self.logger.warning(
                            f"Task for URL {task_url} failed/malformed response (Status: {task.get('status_code')}). Queuing for retry."
                        )
                        failed_urls_in_batch.append(task_url)
                    else:
                        for result_item in task["result"]:
                            if result_item and result_item.get("items"):
                                current_batch_results.extend(
                                    [
                                        DataForSEOMapper.sanitize_onpage_data_item(it)
                                        for it in result_item["items"]
                                    ]
                                )  # ADDED SANITIZATION
            else:
                self.logger.error(
                    f"Failed to get any response for instant_pages batch starting with URL: {batch[0]}"
                )
                failed_urls_in_batch.extend(batch)

            # --- Retry mechanism for failed URLs in this batch ---
            if failed_urls_in_batch:
                self.logger.info(
                    f"Retrying {len(failed_urls_in_batch)} failed URLs from batch {i + 1}..."
                )

                should_switch_pool = client_cfg.get("onpage_enable_switch_pool", False)

                # W15 FIX: Re-group failed URLs to enforce the max_domains limit (5 domains max per request)
                max_domains_per_retry = client_cfg.get(
                    "onpage_max_domains_per_request", 5
                )
                retry_batches = self._group_urls_by_domain(
                    failed_urls_in_batch,
                    max_domains=max_domains_per_retry,
                    batch_size=client_cfg.get("onpage_max_tasks_per_request", 20),
                )

                current_retry_cost = 0.0

                for retry_batch in retry_batches:
                    retry_post_data = []
                    for url in retry_batch:
                        # Reconstruct task_data using original structure but force `return_despite_timeout`
                        original_task_data = next(
                            (item for item in post_data if item.get("url") == url), {}
                        )
                        retry_task_data = {
                            **original_task_data,
                            "return_despite_timeout": True,
                        }

                        if should_switch_pool:
                            retry_task_data["switch_pool"] = True

                        retry_post_data.append(retry_task_data)

                    retry_response, retry_cost = self._post_request(
                        endpoint,
                        retry_post_data,
                        tag=f"onpage_instant_pages_content:retry_batch{i + 1}",
                    )
                    current_retry_cost += retry_cost

                    # Process retry_response (existing logic, moved and adapted)
                    if retry_response and retry_response.get("tasks"):
                        for task in retry_response["tasks"]:
                            task_url = task.get("data", {}).get("url")
                            if task.get("status_code") == 20000 and task.get("result"):
                                for result_item in task["result"]:
                                    if result_item and result_item.get("items"):
                                        current_batch_results.extend(
                                            result_item["items"]
                                        )
                            else:
                                self.logger.warning(
                                    f"Retry task for URL {task_url} failed again (Status: {task.get('status_code')})."
                                )

                total_cost += current_retry_cost

            all_results.extend(current_batch_results)
        return all_results, total_cost

    def get_content_onpage_data(
        self,
        urls: List[str],
        client_cfg: Dict[str, Any],
        enable_javascript: bool = False,
    ) -> Tuple[Optional[List[Dict[str, Any]]], float]:
        """
        Performs OnPage scans using the Content Parsing endpoint, with control over JS rendering.
        This function now sends requests for multiple URLs in parallel using a thread pool
        for improved performance, as the endpoint does not support batch processing.
        """
        if not urls:
            return [], 0.0

        self.logger.info(
            f"Fetching OnPage Content Parsing data for {len(urls)} URLs with enable_javascript={enable_javascript} in parallel..."
        )

        all_tasks = []
        total_cost = 0.0

        # Helper function to be executed in each thread
        def _fetch_single_url(url: str) -> Tuple[Optional[Dict[str, Any]], float]:
            post_data = [
                {
                    "url": url,
                    "enable_javascript": enable_javascript,
                    "store_raw_html": True,
                    "markdown_view": True,
                    "disable_cookie_popup": client_cfg.get(
                        "onpage_disable_cookie_popup", True
                    ),
                }
            ]
            tag = f"onpage_content_parsing_js_{str(enable_javascript).lower()}:{urlparse(url).netloc}"
            return self._post_request(self.ONPAGE_CONTENT_PARSING, post_data, tag=tag)

        # Use a ThreadPoolExecutor to send requests concurrently
        # The number of workers can be tuned, but 5 is a safe default to avoid overwhelming the API
        with ThreadPoolExecutor(max_workers=5) as executor:
            # map executes the function for each item in the urls list
            future_to_url = {
                executor.submit(_fetch_single_url, url): url for url in urls
            }
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    response, cost = future.result()
                    total_cost += cost
                    if response and response.get("tasks"):
                        all_tasks.extend(response["tasks"])
                    else:
                        self.logger.error(
                            f"Failed to get a valid response for content_parsing for URL: {url}"
                        )
                        all_tasks.append(
                            {
                                "status_code": 50000,
                                "status_message": "No response from API",
                                "data": {"url": url},
                            }
                        )
                except Exception as exc:
                    self.logger.error(f"{url} generated an exception: {exc}")
                    all_tasks.append(
                        {
                            "status_code": 50001,
                            "status_message": f"Request generated an exception: {exc}",
                            "data": {"url": url},
                        }
                    )

        if all_tasks:
            return all_tasks, total_cost

        self.logger.error(
            f"Failed to get any response for any of the {len(urls)} URLs."
        )
        return [
            {
                "status_code": 50000,
                "status_message": "No response from API",
                "data": {"url": url},
            }
            for url in urls
        ], 0.0

    def get_serp_results(
        self,
        keyword: str,
        location_code: int,
        language_code: str,
        client_cfg: Dict[str, Any],
        serp_call_params: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Fetches the advanced SERP data for a single keyword, with caching.
        """
        device = client_cfg.get("device", "desktop")
        self.logger.info(
            f"Fetching live SERP results for '{keyword}' on device '{device}'..."
        )
        endpoint = self.SERP_ADVANCED
        base_serp_params = {
            "keyword": keyword,
            "location_code": location_code,
            "language_code": language_code,
            "group_organic_results": False,  # NEW: Ensure no grouping for full analysis
        }
        if serp_call_params:
            base_serp_params.update(serp_call_params)

        if client_cfg.get("calculate_rectangles", False):
            base_serp_params["calculate_rectangles"] = True

        base_serp_params["depth"] = int(base_serp_params.get("depth", 10))

        # Add advanced features from client_cfg if they are enabled.
        if client_cfg.get("calculate_rectangles", False):
            base_serp_params["calculate_rectangles"] = True

        paa_depth = client_cfg.get("people_also_ask_click_depth", 0)
        if isinstance(paa_depth, int) and 1 <= paa_depth <= 4:
            base_serp_params["people_also_ask_click_depth"] = paa_depth

        # W3 FIX: Add support for loading AI overview asynchronously
        if client_cfg.get("load_async_ai_overview", False):
            base_serp_params["load_async_ai_overview"] = True

        # W11 FIX: Include URL removal parameters
        remove_params_str = client_cfg.get("serp_remove_from_url_params")
        if remove_params_str:
            # Assuming config value is a comma-separated string of parameters
            params_list = [p.strip() for p in remove_params_str.split(",") if p.strip()]

            # W14 FIX: Validate and clip URL removal parameters (max 10)
            if len(params_list) > 10:
                self.logger.warning(
                    f"Configuration defined {len(params_list)} parameters for removal, but DataForSEO limit is 10. Truncating."
                )

            base_serp_params["remove_from_url"] = params_list[:10]

        # Ensure device and OS are passed based on client config
        device = client_cfg.get("device", "desktop")
        os_name = client_cfg.get("os", "windows")

        # Adjust OS if device is mobile for compatibility
        if device == "mobile" and os_name not in ["android", "ios"]:
            os_name = "android"

        base_serp_params["device"] = device
        base_serp_params["os"] = os_name

        request_tag = f"serp_advanced:{keyword[:50]}"
        response, cost = self._post_request(
            endpoint, [base_serp_params], tag=request_tag
        )

        if response and response.get("tasks") and response["tasks"][0].get("result"):
            result_data = response["tasks"][0]["result"][0]
            sanitized_result_data = DataForSEOMapper.sanitize_serp_overview_response(
                result_data
            )  # ADDED SANITIZATION
            return sanitized_result_data, cost

        return None, cost

    def post_with_paging(
        self,
        endpoint: str,
        initial_task: Dict[str, Any],
        max_pages: int,
        paginated: bool = True,
        tag: Optional[str] = None,
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Executes a POST request and, if paginated=True, recursively retrieves all results using the correct pagination method.
        """
        all_items = []
        total_cost = 0.0
        current_task = initial_task.copy()

        if "filters" in current_task and (
            current_task["filters"] is None or len(current_task["filters"]) == 0
        ):
            current_task.pop("filters")

        page_count = 0
        previous_offset_token = None  # ADDED: For infinite loop prevention

        while True:
            if not paginated and page_count > 0:
                break

            if page_count >= max_pages:
                self.logger.info(
                    f"Reached max page limit ({max_pages}) for endpoint {endpoint}."
                )
                break

            page_count += 1
            self.logger.info(
                f"Submitting task to {endpoint} (Page {page_count}/{max_pages})..."
            )

            request_tag = (
                tag + f":p{page_count}"
                if tag
                else endpoint.split("/")[-1] + f":p{page_count}"
            )
            response, cost = self._post_request(
                endpoint, [current_task], tag=request_tag
            )
            total_cost += cost

            if (
                not response
                or response.get("status_code") != 20000
                or response.get("tasks_error", 0) > 0
            ):
                self.logger.error(
                    f"Paging for endpoint {endpoint} failed on page {page_count}. Response: {response}"
                )
                break

            tasks = response.get("tasks", [])
            if not tasks or "result" not in tasks[0]:
                self.logger.info(
                    f"No 'result' field in the first task for endpoint {endpoint} on page {page_count}. Stopping pagination."
                )
                break

            task_result = tasks[0].get("result")
            if not task_result:
                self.logger.info(
                    f"Task result is empty for endpoint {endpoint} on page {page_count}. Stopping pagination."
                )
                break

            items_count = 0
            offset_token = None
            if task_result and isinstance(task_result, list) and len(task_result) > 0:
                offset_token = task_result[0].get("offset_token")
                for result_item in task_result:
                    # Capture items from the main list
                    items = result_item.get("items")
                    if items:
                        items_count += len(items)
                        all_items.extend(items)

                    # Capture the valuable seed_keyword_data if it exists (from Keyword Suggestions)
                    # and if this is specifically from the Keyword Suggestions API.
                    # This avoids adding the same seed_keyword twice if it was also in the 'items' list
                    # or if the main search (e.g., Keyword Ideas) already returned it.
                    if endpoint == self.LABS_KEYWORD_SUGGESTIONS:
                        seed_data = result_item.get("seed_keyword_data")
                        if isinstance(seed_data, dict) and seed_data.get("keyword"):
                            seed_data["discovery_source"] = (
                                "keyword_suggestions_seed"  # Mark its source
                            )
                            all_items.append(
                                DataForSEOMapper.sanitize_keyword_data_item(seed_data)
                            )  # ADDED SANITIZATION

            if not paginated or page_count >= max_pages or items_count == 0:
                break

            if offset_token:
                # ADDED: Infinite loop prevention check
                if offset_token == previous_offset_token:
                    self.logger.warning(
                        f"API returned a duplicate offset_token. Halting pagination to prevent infinite loop for endpoint {endpoint}."
                    )
                    break
                previous_offset_token = offset_token  # Update the previous token

                current_task = {
                    "offset_token": offset_token,
                    "limit": initial_task.get("limit", 1000),
                }
                if "filters" in initial_task and initial_task["filters"] is not None:
                    current_task["filters"] = initial_task["filters"]
                if "order_by" in initial_task and initial_task["order_by"] is not None:
                    current_task["order_by"] = initial_task["order_by"]

                time.sleep(1)
            else:
                break

        return all_items, total_cost

    def _group_urls_by_domain(
        self, urls: List[str], max_domains: int = 5, batch_size: int = 20
    ) -> List[List[str]]:
        """
        Groups URLs into batches that comply with the identical-domain limit and batch size.
        """
        from collections import defaultdict, deque

        domain_cache = {}

        def get_domain(url):
            if url not in domain_cache:
                try:
                    domain_cache[url] = urlparse(url).netloc
                except Exception:
                    self.logger.warning(f"Could not parse domain for URL: {url}")
                    domain_cache[url] = url
            return domain_cache[url]

        # Use deques for efficient popping from the left
        domain_groups = defaultdict(deque)
        for url in urls:
            domain_groups[get_domain(url)].append(url)

        batches = []

        # Continue as long as there are URLs to process
        while sum(len(q) for q in domain_groups.values()) > 0:
            current_batch = []
            domain_counts = defaultdict(int)

            # A set of domains that have reached their limit for the current batch
            exhausted_domains = set()

            # Loop until the batch is full or no more URLs can be added
            while len(current_batch) < batch_size:
                url_added_in_this_pass = False

                # Iterate through domains that have URLs and are not exhausted for this batch
                for domain, url_queue in domain_groups.items():
                    if len(current_batch) >= batch_size:
                        break

                    if url_queue and domain not in exhausted_domains:
                        if domain_counts[domain] < max_domains:
                            current_batch.append(url_queue.popleft())
                            domain_counts[domain] += 1
                            url_added_in_this_pass = True
                        else:
                            exhausted_domains.add(domain)

                # If we went through all domains and couldn't add a single URL, stop filling this batch
                if not url_added_in_this_pass:
                    break

            if current_batch:
                batches.append(current_batch)
            # If we created an empty batch and there are still urls, something is wrong.
            # This should not happen with this logic, but as a safeguard:
            elif sum(len(q) for q in domain_groups.values()) > 0:
                self.logger.error(
                    "Could not form a valid batch. Breaking to prevent infinite loop."
                )
                break

        self.logger.info(f"Grouped {len(urls)} URLs into {len(batches)} batches.")
        return batches

    def _convert_filters_to_api_format(
        self, filters: Optional[List[Dict[str, Any]]]
    ) -> Optional[List[Any]]:
        if not filters:
            return None

        api_filters = []
        for i, f in enumerate(filters):
            api_filters.append([f["field"], f["operator"], f["value"]])
            if i < len(filters) - 1:
                api_filters.append("and")
        return api_filters

    def get_keyword_ideas(
        self,
        seed_keywords: List[str],
        location_code: int,
        language_code: str,
        client_cfg: Dict[str, Any],
        discovery_modes: List[str],
        filters: Dict[str, Any],
        order_by: Optional[Dict[str, List[str]]],
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms_override: Optional[bool] = None,
        include_clickstream_override: Optional[bool] = None,
        closely_variants_override: Optional[bool] = None,
        exact_match_override: Optional[bool] = None,
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Performs a comprehensive discovery burst using Keyword Ideas, Suggestions, and Related Keywords endpoints.
        """
        all_items = []
        total_cost = 0.0
        max_pages = client_cfg.get("discovery_max_pages", 1)

        # Dynamic parameters (fall back to client_cfg if override is None)
        ignore_synonyms = (
            ignore_synonyms_override
            if ignore_synonyms_override is not None
            else client_cfg.get("discovery_ignore_synonyms", False)
        )
        include_clickstream = (
            include_clickstream_override
            if include_clickstream_override is not None
            else client_cfg.get("include_clickstream_data", False)
        )
        closely_variants = (
            closely_variants_override
            if closely_variants_override is not None
            else client_cfg.get("closely_variants", False)
        )
        exact_match = (
            exact_match_override
            if exact_match_override is not None
            else client_cfg.get("exact_match", False)
        )

        if "keyword_ideas" in discovery_modes:
            self.logger.info(
                f"Fetching keyword ideas for {len(seed_keywords)} seeds..."
            )
            ideas_endpoint = self.LABS_KEYWORD_IDEAS

            sanitized_ideas_filters = self._prioritize_and_limit_filters(
                self._convert_filters_to_api_format(filters.get("ideas"))
            )

            ideas_task = {
                "keywords": seed_keywords,
                "location_code": location_code,
                "language_code": language_code,
                "limit": int(limit or 100),
                "include_serp_info": True,
                "ignore_synonyms": ignore_synonyms,
                "closely_variants": closely_variants,
                "filters": sanitized_ideas_filters,
                "order_by": order_by.get("ideas") if order_by else None,
                "include_clickstream_data": include_clickstream,
            }
            ideas_items, cost = self.post_with_paging(
                ideas_endpoint, ideas_task, max_pages=max_pages, tag="discovery_ideas"
            )
            total_cost += cost

            for item in ideas_items:
                item["discovery_source"] = "keyword_ideas"
                item["depth"] = 0
                all_items.append(DataForSEOMapper.sanitize_keyword_data_item(item))
            self.logger.info(f"Found {len(ideas_items)} ideas from Keyword Ideas API.")

        if "keyword_suggestions" in discovery_modes:
            self.logger.info("Fetching keyword suggestions...")
            suggestions_endpoint = self.LABS_KEYWORD_SUGGESTIONS
            for seed_keyword in seed_keywords:
                suggestions_task = {
                    "keyword": seed_keyword,
                    "location_code": location_code,
                    "language_code": language_code,
                    "limit": int(limit or 100),
                    "include_serp_info": True,
                    "exact_match": exact_match,
                    "ignore_synonyms": ignore_synonyms,
                    "include_seed_keyword": True,
                    "filters": self._prioritize_and_limit_filters(
                        self._convert_filters_to_api_format(filters.get("suggestions"))
                    ),
                    "order_by": order_by.get("suggestions") if order_by else None,
                    "include_clickstream_data": include_clickstream,
                }
                suggestions_items, cost = self.post_with_paging(
                    suggestions_endpoint,
                    suggestions_task,
                    max_pages=max_pages,
                    tag=f"discovery_suggestions:{seed_keyword[:20]}",
                )
                total_cost += cost
                for item in suggestions_items:
                    item["discovery_source"] = "keyword_suggestions"
                    item["depth"] = 0
                    all_items.append(DataForSEOMapper.sanitize_keyword_data_item(item))
                self.logger.info(
                    f"Found {len(suggestions_items)} suggestions for '{seed_keyword}'."
                )

        if "related_keywords" in discovery_modes:
            self.logger.info("Fetching related keywords...")
            related_endpoint = self.LABS_RELATED_KEYWORDS
            for seed in seed_keywords:
                related_task = {
                    "keyword": seed,
                    "location_code": location_code,
                    "language_code": language_code,
                    "depth": int(depth or client_cfg.get("discovery_related_depth", 3)),
                    "limit": int(limit or 100),
                    "include_serp_info": True,
                    "filters": self._prioritize_and_limit_filters(
                        self._convert_filters_to_api_format(filters.get("related"))
                    ),
                    "order_by": order_by.get("related") if order_by else None,
                    "include_clickstream_data": include_clickstream,
                    "replace_with_core_keyword": client_cfg.get(
                        "discovery_replace_with_core_keyword", False
                    ),
                }

                related_items, cost = self.post_with_paging(
                    related_endpoint,
                    related_task,
                    max_pages=max_pages,
                    tag=f"discovery_related:{seed[:20]}",
                )
                total_cost += cost
                for item in related_items:
                    keyword_data = item.get("keyword_data")
                    if keyword_data:
                        keyword_data["discovery_source"] = "related"
                        keyword_data["depth"] = item.get("depth")
                        all_items.append(
                            DataForSEOMapper.sanitize_keyword_data_item(keyword_data)
                        )
            self.logger.info(f"Total raw items from all sources: {len(all_items)}")

        return all_items, total_cost
```

## File: external_apis/on-page-api.py
```python
import os
import requests
import json
import sys
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# --- Configuration ---
DATAFORSEO_LOGIN = os.getenv("DATAFORSEO_LOGIN")
DATAFORSEO_PASSWORD = os.getenv("DATAFORSEO_PASSWORD")
API_URL = "https://api.dataforseo.com/v3/on_page/content_parsing/live"


def fetch_page_content_live(url: str):
    """
    Fetches the parsed content and raw HTML for a given URL using the synchronous
    'live' endpoint of the Dataforseo On-Page Content Parsing API.

    Args:
        url: The URL of the page to analyze.

    Returns:
        A dictionary containing the API response, or None if an error occurs.
    """
    if not DATAFORSEO_LOGIN or not DATAFORSEO_PASSWORD:
        print("Error: Dataforseo credentials not found in .env file.", file=sys.stderr)
        return None

    # The request body is a list containing a dictionary for the URL.
    post_data = [
        {
            "url": url,
            "store_raw_html": True,
            "enable_javascript": False,
            "convert_to_markdown": True,
        }
    ]

    headers = {"Content-Type": "application/json"}

    try:
        print(f"Sending request for URL: {url}", file=sys.stderr)
        response = requests.post(
            API_URL,
            auth=(DATAFORSEO_LOGIN, DATAFORSEO_PASSWORD),
            headers=headers,
            json=post_data,
        )
        # Raise an exception for bad status codes (4xx or 5xx)
        response.raise_for_status()

        response_json = response.json()

        # Check the response status from the API itself
        if response_json.get("status_code") == 20000:
            print(f"Successfully received data for: {url}", file=sys.stderr)
            return response_json
        else:
            status_msg = response_json.get("status_message", "No status message.")
            print(f"API returned an error for '{url}': {status_msg}", file=sys.stderr)
            return None

    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred for URL '{url}': {http_err}", file=sys.stderr)
        print(f"Response content: {response.text}", file=sys.stderr)
        return None
    except requests.exceptions.RequestException as req_err:
        print(f"A request error occurred for URL '{url}': {req_err}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"An unexpected error occurred for URL '{url}': {e}", file=sys.stderr)
        return None


if __name__ == "__main__":
    test_urls = [
        "https://www.theverge.com/2024/02/15/24074327/openai-sora-text-to-video-generator-ai",
        "https://www.wired.com/story/what-is-generative-ai/",
        "https://blog.google/technology/ai/google-gemini-ai/",
    ]

    all_results = []

    print("--- Starting On-Page Content Parsing (Live API) Tests ---", file=sys.stderr)
    for url in test_urls:
        result = fetch_page_content_live(url)
        if result:
            all_results.append(result)
        else:
            # Add a placeholder for failed requests to keep track
            all_results.append({"url": url, "error": "Failed to fetch content"})
        print("-" * 20, file=sys.stderr)

    print("\n--- All Test Results (JSON Output) ---", file=sys.stderr)
    # Use json.dumps to pretty-print the final list of results
    print(json.dumps(all_results, indent=2))
    print("--- Test Run Complete ---", file=sys.stderr)
```

## File: external_apis/openai_client.py
```python
from openai import OpenAI
import json
import logging
from typing import Dict, Any, List, Optional, Tuple
import time


class OpenAIClientWrapper:
    """
    Provides a robust wrapper for OpenAI API calls,
    handling authentication, retries, and structured outputs (JSON object format).
    """

    def __init__(self, api_key: str, client_cfg: Dict[str, Any]):
        if not api_key:
            raise ValueError("OpenAI API key is required.")
        self.client = OpenAI(api_key=api_key)
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = client_cfg
        self.latest_cost = 0.0

    def _calculate_cost(self, usage: Dict[str, Any], model: str) -> float:
        """Calculates the cost of a chat completion based on token usage."""
        # Pricing per 1M tokens
        pricing = {
            "gpt-4o": {"input": 5.00, "output": 15.00},
            "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
            "gpt-5-mini": {"input": 0.250, "output": 2.000},
            "gpt-5-nano": {"input": 0.050, "output": 0.400},
        }
        model_pricing = pricing.get(
            model, pricing["gpt-4o"]
        )  # Default to gpt-4o pricing

        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)

        input_cost = (prompt_tokens / 1_000_000) * model_pricing["input"]
        output_cost = (completion_tokens / 1_000_000) * model_pricing["output"]

        return input_cost + output_cost
        pass

    def call_chat_completion(
        self,
        messages: List[Dict[str, str]],
        schema: Optional[Dict[str, Any]] = None,
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_completion_tokens: int = 64000,
        retries: int = 3,
    ) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
        """
        Makes a robust OpenAI chat completion call, optionally enforcing JSON output
        and calculating the cost.
        """
        model = 'gpt-5-mini'  # Override to always use gpt-5-mini
        
        self.latest_cost = 0.0
        for attempt in range(retries):
            try:
                response_kwargs = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_completion_tokens": max_completion_tokens,
                }

                # gpt-5-nano and gpt-5-mini do not support temperature
                if model in ['gpt-5-nano', 'gpt-5-mini']:
                    del response_kwargs['temperature']

                if schema:
                    response_kwargs["response_format"] = {
                        "type": "json_schema",
                        "json_schema": {
                            "name": schema.get("name", "structured_output"),
                            "schema": schema,
                            "strict": True,
                        },
                    }

                response = self.client.chat.completions.create(**response_kwargs)

                if response.choices and response.choices[0].finish_reason == "length":
                    self.logger.warning(
                        f"OpenAI API response was truncated because the token limit was reached. "
                        f"Consider increasing 'max_completion_tokens_for_generation' in settings.ini. "
                        f"Current limit for this call: {max_completion_tokens}."
                    )

                if response.usage:
                    self.latest_cost = self._calculate_cost(
                        response.usage.dict(), model
                    )

                if response.choices and response.choices[0].message.content:
                    if schema:
                        try:
                            parsed_output = json.loads(
                                response.choices[0].message.content
                            )
                            self.logger.info(
                                f"Successfully parsed structured output from OpenAI (Attempt {attempt + 1}/{retries}). Cost: ${self.latest_cost:.4f}"
                            )
                            return parsed_output, None
                        except json.JSONDecodeError as e:
                            self.logger.warning(
                                f"Failed to decode JSON from OpenAI (Attempt {attempt + 1}/{retries}): {e}."
                            )
                            continue
                    else:
                        return response.choices[0].message.content, None

                self.logger.warning(
                    f"OpenAI returned no content (Attempt {attempt + 1}/{retries})."
                )
                continue

            except Exception as e:
                self.logger.error(
                    f"OpenAI API call failed (Attempt {attempt + 1}/{retries}): {e}"
                )
                if attempt < retries - 1:
                    time.sleep(5 * (attempt + 1))
                    continue
                return None, str(e)

        return None, "All OpenAI API call attempts failed."

    def call_image_generation(
        self,
        prompt: str,
        style_formula: str,
        quality: str,
        size: str,
        model: Optional[str] = None,
        retries: int = 3,
    ) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """
        Mocks OpenAI image generation. This function is present but *not used* in the final plan
        as Pexels is the exclusive image source. It's kept for potential future re-integration.
        """
        if model is None:
            model = self.client_cfg.get('default_image_model', 'dall-e-3')
        
        self.logger.info(
            "OpenAI image generation is configured but Pexels is prioritized. This function will not be called."
        )
        return (
            None,
            None,
            "OpenAI image generation bypassed; Pexels is the primary source.",
        )
```

## File: external_apis/pexels_client.py
```python
import requests
import logging
import os
from typing import List, Dict, Any, Optional, Tuple


class PexelsClient:
    """
    Manages communication with the Pexels API for free stock photos and videos.
    """

    def __init__(self, api_key: str):
        if not api_key:
            raise ValueError("Pexels API key is required.")
        self.base_url_photos = "https://api.pexels.com/v1/"
        self.base_url_videos = "https://api.pexels.com/videos/"  # Not used in this plan, but included for completeness
        self.headers = {"Authorization": api_key}
        self.logger = logging.getLogger(self.__class__.__name__)

    def search_photos(
        self,
        query: str,
        orientation: Optional[str] = None,
        size: Optional[str] = None,
        per_page: int = 1,
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Searches for photos on Pexels based on a query.
        Returns a list of photo dicts (simplified for direct use) and a dummy cost (Pexels is free).
        """
        endpoint = f"{self.base_url_photos}search"
        params = {
            "query": query,
            "per_page": per_page,
        }
        if orientation:
            params["orientation"] = orientation
        if size:
            params["size"] = size

        try:
            response = requests.get(
                endpoint, headers=self.headers, params=params, timeout=10
            )
            response.raise_for_status()
            data = response.json()

            photos = []
            for photo in data.get("photos", []):
                # Simplify the photo data to what's immediately useful
                photos.append(
                    {
                        "id": photo["id"],
                        "url": photo["url"],
                        "photographer": photo["photographer"],
                        "photographer_url": photo["photographer_url"],
                        "src": photo["src"],  # Contains different sizes
                        "alt": photo.get(
                            "alt", f"Photo by {photo['photographer']} on Pexels"
                        ),
                    }
                )

            self.logger.info(
                f"Found {len(photos)} photos on Pexels for query '{query}'."
            )
            return photos, 0.0  # Pexels is free, so cost is 0

        except requests.exceptions.RequestException as e:
            self.logger.error(
                f"Error searching Pexels photos for '{query}': {e}", exc_info=True
            )
            return [], 0.0
        except Exception as e:
            self.logger.error(
                f"Unexpected error in Pexels photo search for '{query}': {e}",
                exc_info=True,
            )
            return [], 0.0


def download_image_from_url(image_url: str, save_path: str) -> Optional[str]:
    """
    Downloads an image from a given URL and saves it locally.
    Returns the local file path on success, None on failure.
    """
    try:
        response = requests.get(image_url, stream=True, timeout=30)
        response.raise_for_status()

        # Ensure directory exists
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        with open(save_path, "wb") as out_file:
            for chunk in response.iter_content(chunk_size=8192):
                out_file.write(chunk)

        logging.getLogger(__name__).info(
            f"Downloaded image from {image_url} to {save_path}"
        )
        return save_path
    except requests.exceptions.RequestException as e:
        logging.getLogger(__name__).error(
            f"Failed to download image from {image_url}: {e}", exc_info=True
        )
        return None
    except Exception as e:
        logging.getLogger(__name__).error(
            f"An unexpected error occurred during image download: {e}", exc_info=True
        )
        return None
```

## File: pipeline/orchestrator/__init__.py
```python
# backend/pipeline/orchestrator/__init__.py
```

## File: pipeline/orchestrator/analysis_orchestrator.py
```python
# backend/pipeline/orchestrator/analysis_orchestrator.py
import logging
import traceback
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)


class AnalysisOrchestrator:
    def run_analysis_phase(
        self,
        opportunity_id: int,
        selected_competitor_urls: Optional[List[str]] = None,
        use_cached_serp: bool = False,
    ) -> Dict[str, Any]:
        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            return {
                "status": "failed",
                "message": f"Opportunity ID {opportunity_id} not found.",
            }

        keyword = opportunity.get("keyword")
        self.logger.info(
            f"--- Orchestrator: Starting Full Analysis for '{keyword}' ---"
        )
        self.db_manager.update_opportunity_workflow_state(
            opportunity_id, "analysis_started", "in_progress"
        )
        total_api_cost = 0.0

        try:
            # 1. Fetch Live SERP Data
            if use_cached_serp and opportunity.get("full_data", {}).get(
                "serp_overview"
            ):
                self.logger.info(f"Using cached SERP data for '{keyword}'...")
                live_serp_data = opportunity["full_data"]["serp_overview"]
                serp_api_cost = 0.0
            else:
                self.logger.info(f"Running live SERP data fetch for '{keyword}'...")
                from core.serp_analyzer import FullSerpAnalyzer

                serp_analyzer = FullSerpAnalyzer(
                    self.dataforseo_client, self.client_cfg
                )
                live_serp_data, serp_api_cost = serp_analyzer.analyze_serp(keyword)
                total_api_cost += serp_api_cost

            if not live_serp_data:
                raise ValueError("Failed to retrieve live SERP data for analysis.")

            # --- START MODIFICATION ---
            # 2. NEW: Pre-Analysis Validation Gate (Safeguard for AI Calls)
            # Count valid "blog/article" results in top 15
            top_results_for_validation = live_serp_data.get("top_organic_results", [])[
                :15
            ]
            min_relevant_results = self.client_cfg.get(
                "min_relevant_analysis_results", 3
            )
            article_type_results_count = sum(
                1
                for r in top_results_for_validation
                if r.get("page_type") in ["Blog/Article", "News"]
            )

            if article_type_results_count < min_relevant_results:
                reason = f"Analysis failed: SERP is dominated by non-article formats ({article_type_results_count} relevant results found in top 15), making it unsuitable for this workflow."
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id, "pre_analysis_validation_failed", "failed", reason
                )
                self.logger.warning(f"Analysis halted for '{keyword}': {reason}")
                return {
                    "status": "failed",
                    "message": reason,
                    "api_cost": total_api_cost,
                }

            self.logger.info(
                f"Pre-analysis validation passed for '{keyword}' ({article_type_results_count} relevant results in top 15). Proceeding with blueprint generation."
            )
            # --- END MODIFICATION ---

            # 3. Conditional Competitor OnPage Analysis
            competitor_analysis = []
            competitor_api_cost = 0.0

            if self.client_cfg.get("enable_deep_competitor_analysis", False):
                self.logger.info(
                    "Deep competitor analysis is ENABLED. Running OnPage competitor analysis."
                )
                from pipeline.step_04_analysis.competitor_analyzer import (
                    FullCompetitorAnalyzer,
                )

                competitor_analyzer = FullCompetitorAnalyzer(
                    self.dataforseo_client, self.client_cfg
                )
                top_organic_urls = [
                    result["url"]
                    for result in live_serp_data.get("top_organic_results", [])[
                        : self.client_cfg.get("num_competitors_to_analyze", 5)
                    ]
                ]
                competitor_analysis, competitor_api_cost = (
                    competitor_analyzer.analyze_competitors(
                        top_organic_urls, selected_competitor_urls
                    )
                )
                total_api_cost += competitor_api_cost
            else:
                self.logger.info(
                    "Deep competitor analysis is DISABLED. Skipping OnPage competitor analysis."
                )

            # 4. Content Intelligence Synthesis
            from pipeline.step_04_analysis.content_analyzer import ContentAnalyzer

            content_analyzer = ContentAnalyzer(self.openai_client, self.client_cfg)
            content_intelligence, content_api_cost = (
                content_analyzer.synthesize_content_intelligence(
                    keyword,
                    live_serp_data,
                    competitor_analysis,  # Pass this list; it will be empty for the fast workflow
                )
            )
            total_api_cost += content_api_cost

            # 5. Determine Strategy & Generate Outline
            from pipeline.step_05_strategy.decision_engine import (
                StrategicDecisionEngine,
            )

            strategy_engine = StrategicDecisionEngine(self.client_cfg)
            recommended_strategy = strategy_engine.determine_strategy(
                live_serp_data, competitor_analysis, content_intelligence
            )

            ai_outline, outline_api_cost = content_analyzer.generate_ai_outline(
                keyword, live_serp_data, content_intelligence
            )
            total_api_cost += outline_api_cost
            content_intelligence.update(ai_outline)

            if not content_intelligence.get("article_structure"):
                self.logger.critical(
                    "AI outline generation failed to produce an 'article_structure'."
                )
                raise ValueError("AI outline generation failed.")

            # 6. Assemble and Save Blueprint & Re-Score
            analysis_data = {
                "serp_overview": live_serp_data,
                "competitor_analysis": competitor_analysis,
                "content_intelligence": content_intelligence,
                "recommended_strategy": recommended_strategy,
            }

            blueprint = self.blueprint_factory.create_blueprint(
                seed_topic=keyword,
                winning_keyword_data=opportunity.get("full_data", {}).copy(),
                analysis_data=analysis_data,
                total_api_cost=total_api_cost,
                client_id=opportunity.get("client_id"),
            )

            opportunity["blueprint"] = blueprint

            final_score, final_score_breakdown = self.scoring_engine.calculate_score(
                opportunity
            )

            self.db_manager.update_opportunity_scores(
                opportunity_id, final_score, final_score_breakdown, blueprint
            )
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "analysis_completed", "paused_for_approval"
            )

            return {
                "status": "success",
                "message": "Analysis phase completed and opportunity re-scored.",
                "api_cost": total_api_cost,
            }

        except Exception as e:
            error_message = f"Analysis phase failed unexpectedly: {e}"
            self.logger.error(f"{error_message}\n{traceback.format_exc()}")
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "analysis_failed", "failed", error_message=str(e)
            )
            return {"status": "failed", "message": str(e), "api_cost": total_api_cost}

    def _run_analysis_background(
        self,
        job_id: str,
        opportunity_id: int,
        selected_competitor_urls: Optional[List[str]],
    ):
        try:
            self.run_analysis_phase(opportunity_id, selected_competitor_urls)
            self.job_manager.update_job_status(job_id, "completed", progress=100)
        except Exception as e:
            self.job_manager.update_job_status(job_id, "failed", error=str(e))
            raise

    def run_full_analysis(
        self, opportunity_id: int, selected_competitor_urls: Optional[List[str]] = None
    ) -> str:
        job_id = self.job_manager.create_job(
            target_function=self._run_analysis_background,
            args=(opportunity_id, selected_competitor_urls),
        )
        return job_id
```

## File: pipeline/orchestrator/content_orchestrator.py
```python
# backend/pipeline/orchestrator/content_orchestrator.py
import logging
import traceback
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)


class ContentOrchestrator:
    def _build_abstract_content_tree(
        self, opportunity: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Builds the Abstract Content Tree (ACT) from the blueprint's outline."""
        self.logger.info(
            f"Building Abstract Content Tree for opportunity ID: {opportunity['id']}"
        )

        blueprint = opportunity.get("blueprint", {})
        content_intelligence = blueprint.get("content_intelligence", {})
        outline_structure = content_intelligence.get("article_structure", [])

        if not outline_structure:
            raise ValueError(
                "Cannot build ACT: `article_structure` not found in blueprint."
            )

        act = []

        for i, section in enumerate(outline_structure):
            h2_title = section.get("h2")
            h3s = section.get("h3s", [])

            if not h2_title:
                continue

            node_type = "section_h2"
            if h2_title.lower().strip().startswith("introduction"):
                node_type = "introduction"
            elif h2_title.lower().strip().startswith("conclusion"):
                node_type = "conclusion"

            act.append(
                {
                    "id": f"section-{i}",
                    "type": node_type,
                    "title": h2_title,
                    "sub_points": h3s,
                    "status": "pending",
                    "content_html": "",
                }
            )

        self.logger.info(f"Successfully built ACT with {len(act)} nodes.")
        return act

    def _run_full_content_generation_background(
        self,
        job_id: str,
        opportunity_id: int,
        overrides: Optional[Dict[str, Any]] = None,
    ):
        """
        Internal method to execute the full agentic content generation and enrichment pipeline.
        This is the complete, final version of this function.
        """
        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            error_msg = f"Opportunity {opportunity_id} not found."
            self.logger.error(error_msg)
            self.job_manager.update_job_status(job_id, "failed", error=error_msg)
            return

        if opportunity.get("status") not in ["analyzed", "paused_for_approval"]:
            error_msg = "Opportunity not ready for content generation."
            self.job_manager.update_job_status(job_id, "failed", error=error_msg)
            return

        self.db_manager.update_opportunity_workflow_state(
            opportunity_id, "content_creation_started", "running"
        )
        self.job_manager.update_job_status(
            job_id, "running", progress=5, result={"step": "Initializing Generation"}
        )

        try:
            # --- START COST TRACKING MODIFICATION ---
            total_api_cost = opportunity.get("blueprint", {}).get("metadata", {}).get("total_api_cost", 0.0)
            self.logger.info(f"Initial cost from blueprint: ${total_api_cost:.4f}")
            # --- END COST TRACKING MODIFICATION ---

            self.job_manager.update_job_status(
                job_id, "running", progress=10, result={"step": "Building Content Tree"}
            )
            act = self._build_abstract_content_tree(opportunity)

            from agents.article_generator import SectionalArticleGenerator

            sectional_generator = SectionalArticleGenerator(
                self.openai_client, self.client_cfg, self.db_manager
            )

            full_article_context_for_conclusion = ""
            previous_content = ""
            for i, node in enumerate(act):
                progress = 15 + int((i / len(act)) * 40)
                self.job_manager.update_job_status(
                    job_id,
                    "running",
                    progress=progress,
                    result={"step": f"Generating: {node['title']}"},
                )

                content_html, cost = None, 0.0
                if node["type"] == "introduction":
                    content_html, cost = sectional_generator.generate_introduction(
                        opportunity
                    )
                elif node["type"] == "section_h2":
                    content_html, cost = sectional_generator.generate_section(
                        opportunity,
                        node["title"],
                        node.get("sub_points", []),
                        previous_content,
                    )
                elif node["type"] == "conclusion":
                    content_html, cost = sectional_generator.generate_conclusion(
                        opportunity, full_article_context_for_conclusion
                    )
                
                total_api_cost += cost # Aggregate cost

                if content_html:
                    node["content_html"] = content_html
                    full_article_context_for_conclusion += (
                        f"<h2>{node['title']}</h2>\n{content_html}\n"
                    )
                    previous_content = content_html
                else:
                    raise RuntimeError(
                        f"Failed to generate content for section '{node['title']}'."
                    )

            self.job_manager.update_job_status(
                job_id, "running", progress=60, result={"step": "Assembling Article"}
            )
            final_html_parts = [
                f"<h2>{node['title']}</h2>\n{node['content_html']}" for node in act
            ]
            final_article_html = "\n".join(final_html_parts)

            opportunity["ai_content"] = {"article_body_html": final_article_html}

            MAX_REFINEMENT_ATTEMPTS = 3
            current_html = opportunity["ai_content"]["article_body_html"]
            final_audit_results = {}

            for attempt in range(MAX_REFINEMENT_ATTEMPTS):
                self.job_manager.update_job_status(
                    job_id,
                    "running",
                    progress=65 + (attempt * 5),
                    result={"step": f"Auditing Content (Attempt {attempt + 1})"},
                )

                audit_results = self.content_auditor.audit_content(
                    article_html=current_html,
                    primary_keyword=opportunity.get("keyword", ""),
                    blueprint=opportunity.get("blueprint", {}),
                    client_cfg=self.client_cfg,
                )
                final_audit_results = audit_results

                structured_issues = audit_results.get("publish_readiness_issues", [])

                if not structured_issues:
                    self.logger.info(
                        f"Audit passed on attempt {attempt + 1}. No refinement needed."
                    )
                    break

                self.logger.warning(
                    f"Audit failed on attempt {attempt + 1}. Issues found: {len(structured_issues)}. Triggering self-healing."
                )
                self.job_manager.update_job_status(
                    job_id,
                    "running",
                    progress=70 + (attempt * 5),
                    result={"step": f"Self-Healing (Attempt {attempt + 1})"},
                )

                all_refinement_commands = []
                for issue in structured_issues:
                    if issue["issue"] == "unresolved_placeholder":
                        all_refinement_commands.append(
                            "- CRITICAL FIX: Remove all image placeholders like '[[IMAGE_ID:...]]' from the text."
                        )
                    elif issue["issue"] == "empty_heading":
                        all_refinement_commands.append(
                            f"- FIX: The following heading tag is empty: `{issue['context']}`. Based on the surrounding content, either remove this tag entirely or populate it with a relevant heading."
                        )
                    elif issue["issue"] == "short_paragraph":
                        all_refinement_commands.append(
                            f"- FIX: The paragraph `{issue['context']}` is too brief. Expand this paragraph to be at least 3 sentences long, providing more detail, or merge it with an adjacent paragraph if appropriate."
                        )
                    elif issue["issue"] == "word_count_deviation":
                        target_word_count = (
                            opportunity.get("blueprint", {})
                            .get("ai_content_brief", {})
                            .get("target_word_count", 1500)
                        )
                        all_refinement_commands.append(
                            f"- FIX: The article's word count is significantly off target. Review the entire article and expand or condense it to be approximately {target_word_count} words. {issue['context']}"
                        )

                if not all_refinement_commands:
                    break

                combined_command = (
                    "Please refine the entire HTML document by addressing the following issues:\n"
                    + "\n".join(all_refinement_commands)
                )

                refine_prompt_messages = [
                    {
                        "role": "system",
                        "content": "You are an expert content editor. You will receive a full HTML document and a list of specific issues to fix. Apply all fixes and return ONLY the complete, corrected HTML document. Preserve all original HTML tags and structure unless a fix requires changing them. Do not add any introductory text, just the refined HTML.",
                    },
                    {
                        "role": "user",
                        "content": f"COMMANDS:\n{combined_command}\n\nFULL HTML TO FIX:\n```html\n{current_html}\n```",
                    },
                ]

                refined_html, error = self.openai_client.call_chat_completion(
                    messages=refine_prompt_messages,
                    model=self.client_cfg.get("default_model", "gpt-5-nano"),
                    temperature=0.2,
                )
                total_api_cost += self.openai_client.latest_cost # Aggregate cost

                if error or not refined_html:
                    self.logger.error(
                        f"AI Refinement Agent failed on attempt {attempt + 1}: {error}"
                    )
                    break

                current_html = (
                    refined_html.strip()
                    .removeprefix("```html")
                    .removesuffix("```")
                    .strip()
                )

            opportunity["ai_content"]["article_body_html"] = current_html
            opportunity["ai_content"]["audit_results"] = final_audit_results

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=85,
                result={"step": "Generating Images & Social Posts"},
            )
            featured_image_data, image_cost = self.image_generator.generate_featured_image(
                opportunity
            )
            total_api_cost += image_cost
            social_posts, social_cost = self.social_crafter.craft_posts(opportunity)
            total_api_cost += social_cost

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=90,
                result={"step": "Formatting & Internal Linking"},
            )
            internal_link_suggestions, link_cost = (
                self.internal_linking_suggester.suggest_links(
                    opportunity["ai_content"]["article_body_html"],
                    opportunity.get("blueprint", {})
                    .get("ai_content_brief", {})
                    .get("key_entities_to_mention", []),
                    self.client_cfg.get("target_domain"),
                    self.client_id,
                )
            )
            total_api_cost += link_cost

            final_package = self.html_formatter.format_final_package(
                opportunity,
                internal_linking_suggestions=internal_link_suggestions,
                in_article_images_data=[],
            )

            self.job_manager.update_job_status(
                job_id, "running", progress=95, result={"step": "Saving to Database"}
            )
            self.db_manager.save_full_content_package(
                opportunity_id,
                opportunity["ai_content"],
                self.client_cfg.get("ai_content_model", "gpt-4o"),
                featured_image_data,
                [],
                social_posts,
                final_package,
                total_api_cost, # Pass total cost
            )

            self.job_manager.update_job_status(
                job_id,
                "completed",
                progress=100,
                result={
                    "status": "success",
                    "message": "Content generation completed.",
                },
            )

        except Exception as e:
            error_msg = (
                f"Agentic content generation failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "content_generation_failed", "failed", str(e)
            )
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def run_full_content_generation(
        self, opportunity_id: int, overrides: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Public method to initiate content generation asynchronously.
        Returns a job_id.
        """
        self.logger.info(
            f"--- Orchestrator: Initiating Full Content Generation for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_full_content_generation_background,
            args=(opportunity_id, overrides),
        )
        return job_id
```

## File: pipeline/orchestrator/cost_estimator.py
```python
# backend/pipeline/orchestrator/cost_estimator.py
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


class CostEstimator:
    def estimate_action_cost(
        self,
        action: str,
        opportunity_id: Optional[int] = None,
        discovery_params: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Estimates the API cost for a given workflow action without executing it.
        - For 'discovery', uses discovery_params.
        - For other actions, uses opportunity_id.
        """
        estimated_cost = 0.0
        explanation = []

        if action == "discovery":
            if not discovery_params:
                raise ValueError(
                    "discovery_params are required for 'discovery' action estimation."
                )

            KEYWORD_IDEAS_RATE = 0.005
            KEYWORD_SUGGESTIONS_RATE = 0.005
            RELATED_KEYWORDS_RATE = 0.005

            seed_keywords = discovery_params.get("seed_keywords", [])
            discovery_modes = discovery_params.get("discovery_modes", [])
            max_pages = self.client_cfg.get("discovery_max_pages", 1)
            num_seeds = len(seed_keywords)

            if "keyword_ideas" in discovery_modes:
                cost = KEYWORD_IDEAS_RATE * max_pages
                estimated_cost += cost
                explanation.append(
                    {
                        "service": "Keyword Ideas API",
                        "details": f"1 call x {max_pages} page(s) @ ${KEYWORD_IDEAS_RATE}/call",
                        "cost": cost,
                    }
                )

            if "keyword_suggestions" in discovery_modes:
                cost = KEYWORD_SUGGESTIONS_RATE * num_seeds * max_pages
                estimated_cost += cost
                explanation.append(
                    {
                        "service": "Keyword Suggestions API",
                        "details": f"{num_seeds} seed(s) x {max_pages} page(s) @ ${KEYWORD_SUGGESTIONS_RATE}/call",
                        "cost": cost,
                    }
                )

            if "related_keywords" in discovery_modes:
                cost = RELATED_KEYWORDS_RATE * num_seeds * max_pages
                estimated_cost += cost
                explanation.append(
                    {
                        "service": "Related Keywords API",
                        "details": f"{num_seeds} seed(s) x {max_pages} page(s) @ ${RELATED_KEYWORDS_RATE}/call",
                        "cost": cost,
                    }
                )

            return {"total_cost": estimated_cost, "breakdown": explanation}

        if not opportunity_id:
            raise ValueError(f"opportunity_id is required for action '{action}'.")

        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            raise ValueError("Opportunity not found.")

        if action == "analyze" or action == "validate":
            serp_base_cost = 0.005
            serp_cost_explanation = (
                f"1 x SERP Live Advanced call (~${serp_base_cost:.3f} base)"
            )

            if self.client_cfg.get("load_async_ai_overview", False):
                serp_base_cost += 0.002
                serp_cost_explanation += " + $0.002 (Async AI Overview)"

            if self.client_cfg.get("calculate_rectangles", False):
                serp_base_cost += 0.002
                serp_cost_explanation += " + $0.002 (Pixel Ranking)"

            paa_depth = self.client_cfg.get("people_also_ask_click_depth", 0)
            if isinstance(paa_depth, int) and paa_depth > 0:
                paa_cost = paa_depth * 0.00015
                serp_base_cost += paa_cost
                serp_cost_explanation += f" + ${paa_cost:.5f} (PAA Depth {paa_depth})"

            estimated_cost += serp_base_cost
            explanation.append(
                {
                    "service": "SERP Live Advanced Task",
                    "details": serp_cost_explanation,
                    "cost": serp_base_cost,
                }
            )

            if action == "analyze":
                num_competitors = self.client_cfg.get("num_competitors_to_analyze", 5)

                ONPAGE_BASIC_RATE = 0.000125
                ONPAGE_RENDER_RATE = 0.00425
                ONPAGE_CUSTOM_JS_RATE = 0.00025

                if self.client_cfg.get("onpage_enable_browser_rendering", False):
                    onpage_per_task_cost = ONPAGE_RENDER_RATE
                    onpage_cost_explanation = f"x OnPage Instant Pages (Browser Rendering ON @ ${onpage_per_task_cost:.5f} each)"
                else:
                    onpage_per_task_cost = ONPAGE_BASIC_RATE
                    onpage_cost_explanation = f"x OnPage Instant Pages (Basic Crawl @ ${onpage_per_task_cost:.5f} each)"

                if self.client_cfg.get("onpage_enable_custom_js", False):
                    onpage_per_task_cost += ONPAGE_CUSTOM_JS_RATE
                    onpage_cost_explanation += " + $0.00025 (Custom JavaScript)"

                onpage_cost = num_competitors * onpage_per_task_cost
                estimated_cost += onpage_cost

                explanation.append(
                    {
                        "service": f"{num_competitors} Competitor OnPage Tasks",
                        "details": onpage_cost_explanation,
                        "cost": onpage_cost,
                    }
                )

            ai_analysis_cost = 0.05
            estimated_cost += ai_analysis_cost
            explanation.append(
                {
                    "service": "OpenAI Analysis Buffer",
                    "details": "1 x OpenAI GPT-4o call for analysis",
                    "cost": ai_analysis_cost,
                }
            )

        elif action == "generate":
            model = self.client_cfg.get("ai_content_model", "gpt-4o")

            pricing = self.client_cfg.get("OPENAI_PRICING", {})
            input_rate = pricing.get(f"{model}_input", 5.00) / 1000000
            output_rate = pricing.get(f"{model}_output", 15.00) / 1000000

            article_input_tokens = 10000
            article_output_tokens = 5000
            article_cost = (article_input_tokens * input_rate) + (
                article_output_tokens * output_rate
            )

            social_cost = (2000 * input_rate) + (500 * output_rate)

            buffer_tokens = 5000
            buffer_cost = (
                (buffer_tokens * input_rate) + (buffer_tokens * output_rate)
            ) * 0.5

            estimated_cost += article_cost
            explanation.append(
                {
                    "service": "AI Article Generation",
                    "details": f"1 x OpenAI {model} call (10k in, 5k out)",
                    "cost": article_cost,
                }
            )

            estimated_cost += social_cost
            explanation.append(
                {
                    "service": "AI Social Posts",
                    "details": f"1 x OpenAI {model} call (2k in, 0.5k out)",
                    "cost": social_cost,
                }
            )

            estimated_cost += buffer_cost
            explanation.append(
                {
                    "service": "AI Refinement/Linking Buffer",
                    "details": "50% chance of refinement/linking tokens",
                    "cost": buffer_cost,
                }
            )

            if self.client_cfg.get("use_pexels_first", True):
                explanation.append(
                    {
                        "service": "Image Sourcing (Pexels)",
                        "details": "Cost: $0.00",
                        "cost": 0.00,
                    }
                )
            else:
                image_cost = self.client_cfg.get("num_in_article_images", 0) * 0.04
                estimated_cost += image_cost
                explanation.append(
                    {
                        "service": f"Image Generation ({self.client_cfg.get('default_image_model', 'dall-e-3')})",
                        "details": f"Estimated {self.client_cfg.get('num_in_article_images', 0)} images @ $0.04 each",
                        "cost": image_cost,
                    }
                )

        elif action == "validate":
            # Assuming SERP_LIVE_ADVANCED_RATE is 0.020 USD
            SERP_LIVE_ADVANCED_RATE = 0.020
            validation_cost = SERP_LIVE_ADVANCED_RATE
            details = f"1 x SERP Live Advanced call (~${SERP_LIVE_ADVANCED_RATE:.3f})"
            if self.client_cfg.get("load_async_ai_overview", False):
                validation_cost += 0.002
                details += " + $0.002 for asynchronous AI Overview retrieval."
            estimated_cost += validation_cost
            explanation.append(
                {
                    "service": "SERP Validation",
                    "details": details,
                    "cost": validation_cost,
                }
            )

        return {"estimated_cost": round(estimated_cost, 2), "explanation": explanation}
```

## File: pipeline/orchestrator/discovery_orchestrator.py
```python
# backend/pipeline/orchestrator/discovery_orchestrator.py
import logging
import traceback
import os
from typing import Dict, Any, List, Optional

from backend.services.serp_analysis_service import SerpAnalysisService

logger = logging.getLogger(__name__)


class DiscoveryOrchestrator:
    def _run_discovery_background(
        self,
        job_id: str,
        run_id: int,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        filters_override: Optional[Dict[str, Any]],
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = None,
        include_clickstream_data: Optional[bool] = None,
        closely_variants: Optional[bool] = None,
        exact_match: Optional[bool] = None,
    ):
        """Internal method to execute the consolidated discovery phase for a job."""
        log_dir = "discovery_logs"
        os.makedirs(log_dir, exist_ok=True)
        log_file_path = os.path.join(log_dir, f"run_{run_id}.log")

        run_logger = logging.getLogger(f"run_{run_id}")
        handler = logging.FileHandler(log_file_path)
        handler.setFormatter(
            logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        )
        run_logger.addHandler(handler)
        run_logger.setLevel(logging.INFO)

        self.db_manager.update_discovery_run_log_path(run_id, log_file_path)
        self.db_manager.update_discovery_run_status(run_id, "running")
        self.job_manager.update_job_status(job_id, "running", progress=0)

        run_config = self.global_cfg_manager.load_client_config(
            self.client_id, self.db_manager
        )
        if filters_override:
            run_logger.info(f"Applying filter overrides: {filters_override}")
            run_config.update(filters_override)

        run_logger.info(
            f"Starting discovery with modes: {discovery_modes}, filters: {filters}, order_by: {order_by}, limit: {limit}, depth: {depth}"
        )

        try:
            job_status = self.job_manager.get_job_status(job_id)
            if job_status and job_status.get("status") == "failed":
                run_logger.warning(
                    "Job found marked as 'failed' (cancelled). Exiting gracefully."
                )
                self.db_manager.update_discovery_run_status(run_id, "cancelled")
                return {"message": "Job cancelled by user request."}

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=10,
                result={"step": "Fetching & Scoring keywords"},
            )

            from pipeline.step_01_discovery.run_discovery import run_discovery_phase

            discovery_result = run_discovery_phase(
                seed_keywords=seed_keywords,
                dataforseo_client=self.dataforseo_client,
                db_manager=self.db_manager,
                client_id=self.client_id,
                client_cfg=run_config,
                discovery_modes=discovery_modes,
                filters=filters,
                order_by=order_by,
                limit=limit,
                depth=depth,
                ignore_synonyms=ignore_synonyms,
                include_clickstream_data=include_clickstream_data,
                closely_variants=closely_variants,
                run_logger=run_logger,
            )

            stats = discovery_result.get("stats", {})
            total_cost = discovery_result.get("total_cost", 0.0)
            processed_opportunities = discovery_result.get("opportunities", [])

            self.job_manager.update_job_status(
                job_id, "running", progress=75, result={"step": "Saving to Database"}
            )

            num_added = 0
            if processed_opportunities:
                run_logger.info(
                    f"Attempting to save {len(processed_opportunities)} processed opportunities..."
                )
                num_added = self.db_manager.add_opportunities(
                    processed_opportunities, self.client_id, run_id
                )
                run_logger.info(
                    f"Successfully saved {num_added} new keyword records. The database ignored {len(processed_opportunities) - num_added} duplicates."
                )

            results_summary = {
                "total_cost": total_cost,
                "source_counts": stats.get("raw_counts", {}),
                "total_raw_count": stats.get("total_raw_count", 0),
                "total_unique_count": stats.get("total_unique_count", 0),
                "disqualification_reasons": stats.get("disqualification_reasons", {}),
                "disqualified_count": stats.get("disqualified_count", 0),
                "final_qualified_count": stats.get("final_qualified_count", 0),
                "duplicates_removed": len(processed_opportunities) - num_added,
                "final_added_to_db": num_added,
            }

            self.db_manager.update_discovery_run_completed(run_id, results_summary)
            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=results_summary
            )
            run_logger.info("Discovery run completed successfully.")
            return results_summary
        except Exception as e:
            error_message = f"Discovery workflow failed: {e}\n{traceback.format_exc()}"
            run_logger.error(error_message)
            self.db_manager.update_discovery_run_failed(run_id, str(e))
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def run_discovery_and_save(
        self,
        run_id: int,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]] = None,
        order_by: Optional[List[str]] = None,
        filters_override: Optional[Dict[str, Any]] = None,
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = None,
        include_clickstream_data: Optional[bool] = None,
        closely_variants: Optional[bool] = None,
        exact_match: Optional[bool] = None,
    ) -> str:
        """
        Public method to initiate a discovery run asynchronously.
        Returns a job_id.
        """
        self.logger.info(
            f"--- Orchestrator: Initiating Full Discovery & Qualification for Run ID: {run_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_discovery_background,
            args=(
                run_id,
                seed_keywords,
                discovery_modes,
                filters,
                order_by,
                filters_override,
                limit,
                depth,
                ignore_synonyms,
                include_clickstream_data,
                closely_variants,
                exact_match,
            ),
        )
        return job_id
```

## File: pipeline/orchestrator/image_orchestrator.py
```python
# backend/pipeline/orchestrator/image_orchestrator.py
import logging
import traceback
import json

logger = logging.getLogger(__name__)


class ImageOrchestrator:
    def _run_single_image_generation_background(
        self, job_id: str, opportunity_id: int, original_prompt: str, new_prompt: str
    ):
        """Internal method to regenerate a single in-article image."""
        self.job_manager.update_job_status(job_id, "running", progress=0)

        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity or not opportunity.get("ai_content_json"):
                raise ValueError(
                    "Opportunity or content missing for single image regeneration."
                )

            opportunity["client_cfg"] = self.client_cfg

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=30,
                result={"step": "Generating single image"},
            )

            images_data, _ = self.image_generator.generate_images_from_prompts(
                [new_prompt]
            )

            if not images_data or not images_data[0]:
                raise RuntimeError("Image generation failed or returned no data.")

            new_image_data = images_data[0]

            self.job_manager.update_job_status(
                job_id,
                "running",
                progress=70,
                result={"step": "Updating content package"},
            )

            in_article_images = opportunity.get("in_article_images_data", [])
            if isinstance(in_article_images, str):
                in_article_images = (
                    json.loads(in_article_images) if in_article_images else []
                )

            updated_images = []
            found_and_updated = False
            for img in in_article_images:
                if img.get("original_prompt") == original_prompt:
                    img.update(new_image_data)
                    found_and_updated = True
                updated_images.append(img)

            if not found_and_updated:
                new_image_data["original_prompt"] = new_prompt
                updated_images.append(new_image_data)

            self.db_manager.update_opportunity_images(
                opportunity_id,
                opportunity.get("featured_image_url"),
                opportunity.get("featured_image_local_path"),
                updated_images,
            )

            final_package = self.html_formatter.format_final_package(opportunity)
            self.db_manager.update_opportunity_final_package(
                opportunity_id, final_package
            )

            result_message = {
                "status": "success",
                "message": f"Single image regenerated for prompt: {original_prompt}",
            }

            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=result_message
            )
            return result_message

        except Exception as e:
            error_msg = (
                f"Single image regeneration failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def regenerate_single_image(
        self, opportunity_id: int, original_prompt: str, new_prompt: str
    ) -> str:
        """Public method to initiate single image regeneration asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Single Image Regeneration for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_single_image_generation_background,
            args=(opportunity_id, original_prompt, new_prompt),
        )
        return job_id

    def _run_featured_image_regeneration_background(
        self, job_id: str, opportunity_id: int, prompt: str
    ):
        """Internal method to regenerate a featured image."""
        self.job_manager.update_job_status(
            job_id, "running", progress=10, result={"step": "Generating Featured Image"}
        )
        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                raise ValueError(
                    "Opportunity not found for featured image regeneration."
                )

            opportunity["keyword"] = prompt
            opportunity["ai_content"] = {"meta_title": prompt}

            featured_image_data, cost = self.image_generator.generate_featured_image(
                opportunity
            )

            if featured_image_data:
                self.db_manager.update_opportunity_images(
                    opportunity_id,
                    featured_image_data.get("remote_url"),
                    featured_image_data.get("local_path"),
                    json.loads(opportunity.get("in_article_images_data", "[]")),
                )

            result_message = {
                "status": "success",
                "message": "Featured image regenerated successfully.",
                "api_cost": cost,
            }
            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=result_message
            )
            return result_message

        except Exception as e:
            error_msg = (
                f"Featured image regeneration failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def regenerate_featured_image(self, opportunity_id: int, prompt: str) -> str:
        """Public method to initiate featured image regeneration asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Featured Image Regeneration for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_featured_image_regeneration_background,
            args=(opportunity_id, prompt),
        )
        return job_id
```

## File: pipeline/orchestrator/main.py
```python
# backend/pipeline/orchestrator/main.py
import logging

from backend.app_config.manager import ConfigManager
from backend.data_access.database_manager import DatabaseManager
from backend.external_apis.dataforseo_client_v2 import DataForSEOClientV2
from backend.external_apis.openai_client import OpenAIClientWrapper
from backend.agents.image_generator import ImageGenerator
from backend.agents.social_media_crafter import SocialMediaCrafter
from backend.agents.internal_linking_suggester import InternalLinkingSuggester
from backend.agents.html_formatter import HtmlFormatter
from backend.core.blueprint_factory import BlueprintFactory
from backend.agents.content_auditor import ContentAuditor
from backend.agents.prompt_assembler import DynamicPromptAssembler
from backend.services.serp_analysis_service import SerpAnalysisService
from backend.pipeline.step_03_prioritization.scoring_engine import ScoringEngine
from backend.pipeline.step_01_discovery.cannibalization_checker import (
    CannibalizationChecker,
)
from backend.jobs import JobManager

from .discovery_orchestrator import DiscoveryOrchestrator
from .analysis_orchestrator import AnalysisOrchestrator
from .content_orchestrator import ContentOrchestrator
from .image_orchestrator import ImageOrchestrator
from .social_orchestrator import SocialOrchestrator
from .validation_orchestrator import ValidationOrchestrator
from .workflow_orchestrator import WorkflowOrchestrator
from .cost_estimator import CostEstimator

logger = logging.getLogger(__name__)


class WorkflowOrchestrator(
    DiscoveryOrchestrator,
    AnalysisOrchestrator,
    ContentOrchestrator,
    ImageOrchestrator,
    SocialOrchestrator,
    ValidationOrchestrator,
    WorkflowOrchestrator,
    CostEstimator,
):
    def __init__(
        self,
        global_cfg_manager: ConfigManager,
        db_manager: DatabaseManager,
        client_id: str,
        job_manager: "JobManager",
    ):
        self.global_cfg_manager = global_cfg_manager
        self.db_manager = db_manager
        self.client_id = client_id
        self.job_manager = job_manager
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = self.global_cfg_manager.load_client_config(
            self.client_id, self.db_manager
        )

        self.openai_client = OpenAIClientWrapper(
            self.client_cfg.get("openai_api_key"), self.client_cfg
        )
        self.dataforseo_client = DataForSEOClientV2(
            login=self.client_cfg["dataforseo_login"],
            password=self.client_cfg["dataforseo_password"],
            config=self.client_cfg,
            db_manager=self.db_manager,
            enable_cache=self.client_cfg.get("enable_cache", True),
        )

        self.image_generator = ImageGenerator(self.client_cfg)
        self.social_crafter = SocialMediaCrafter(self.openai_client, self.client_cfg)
        self.internal_linking_suggester = InternalLinkingSuggester(
            self.openai_client, self.client_cfg, self.db_manager
        )
        self.html_formatter = HtmlFormatter()
        self.blueprint_factory = BlueprintFactory(
            self.openai_client, self.client_cfg, self.dataforseo_client, self.db_manager
        )
        self.scoring_engine = ScoringEngine(self.client_cfg)
        self.cannibalization_checker = CannibalizationChecker(
            self.client_cfg.get("target_domain"),
            self.dataforseo_client,
            self.client_cfg,
            self.db_manager,
        )
        self.content_auditor = ContentAuditor()
        self.prompt_assembler = DynamicPromptAssembler(self.db_manager)
        self.serp_analysis_service = SerpAnalysisService(self.dataforseo_client, self.client_cfg)
```

## File: pipeline/orchestrator/social_orchestrator.py
```python
# backend/pipeline/orchestrator/social_orchestrator.py
import logging
import traceback

logger = logging.getLogger(__name__)


class SocialOrchestrator:
    def _run_social_posts_regeneration_background(
        self, job_id: str, opportunity_id: int
    ):
        """Internal method to regenerate social media posts."""
        self.job_manager.update_job_status(
            job_id, "running", progress=10, result={"step": "Crafting Social Posts"}
        )
        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                raise ValueError("Opportunity not found for social media regeneration.")

            opportunity["client_cfg"] = self.client_cfg

            social_posts, cost = self.social_crafter.craft_posts(opportunity)
            if social_posts:
                self.db_manager.update_opportunity_social_posts(
                    opportunity_id, social_posts
                )

            result_message = {
                "status": "success",
                "message": "Social media posts regenerated successfully.",
                "api_cost": cost,
            }
            self.job_manager.update_job_status(
                job_id, "completed", progress=100, result=result_message
            )
            return result_message

        except Exception as e:
            error_msg = (
                f"Social media post regeneration failed: {e}\n{traceback.format_exc()}"
            )
            self.logger.error(error_msg)
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def regenerate_social_posts(self, opportunity_id: int) -> str:
        """Public method to initiate social media post regeneration asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Social Media Post Regeneration for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_social_posts_regeneration_background,
            args=(opportunity_id,),
        )
        return job_id
```

## File: pipeline/orchestrator/validation_orchestrator.py
```python
# backend/pipeline/orchestrator/validation_orchestrator.py
import logging
import traceback

logger = logging.getLogger(__name__)


class ValidationOrchestrator:
    def run_validation_phase(self, opportunity_id: int):
        """
        Runs a cost-effective final validation gate before committing to a full analysis.
        Makes one live SERP call and a deep cannibalization check.
        """
        opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
        if not opportunity:
            return {
                "status": "failed",
                "message": f"Opportunity ID {opportunity_id} not found.",
            }

        self.logger.info(
            f"--- Orchestrator: Starting Live SERP Validation for '{opportunity.get('keyword')}' ---"
        )
        self.db_manager.update_opportunity_workflow_state(
            opportunity_id, "validation_started", "in_progress"
        )
        total_cost = 0.0

        try:
            from core.serp_analyzer import FullSerpAnalyzer

            serp_analyzer = FullSerpAnalyzer(self.dataforseo_client, self.client_cfg)
            serp_overview, serp_api_cost = serp_analyzer.analyze_serp(
                opportunity.get("keyword")
            )
            total_cost += serp_api_cost
            if not serp_overview:
                raise ValueError("Failed to retrieve live SERP data for validation.")

            from pipeline.step_04_analysis.run_analysis import run_final_validation

            is_valid, reason = run_final_validation(
                serp_overview, opportunity, self.client_cfg, self.dataforseo_client
            )

            if is_valid:
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id, "validation_passed", "validated"
                )
                self.logger.info(
                    f"Validation PASSED for '{opportunity.get('keyword')}'."
                )
                return {
                    "status": "success",
                    "message": "Validation passed. Ready for full analysis.",
                    "api_cost": total_cost,
                }
            else:
                self.db_manager.update_opportunity_status(opportunity_id, "rejected")
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id,
                    "validation_failed",
                    "rejected",
                    error_message=reason,
                )
                self.logger.warning(
                    f"Validation FAILED for '{opportunity.get('keyword')}': {reason}"
                )
                return {"status": "failed", "message": reason, "api_cost": total_cost}
        except Exception as e:
            error_msg = f"Validation phase failed unexpectedly: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "validation_failed", "failed", error_message=str(e)
            )
            return {"status": "failed", "message": str(e), "api_cost": total_cost}

    def _run_validation_background(self, job_id: str, opportunity_id: int):
        """Internal method to execute the validation phase for a job."""
        self.job_manager.update_job_status(job_id, "running", progress=0)
        try:
            result = self.run_validation_phase(opportunity_id)

            if result["status"] == "success":
                self.job_manager.update_job_status(
                    job_id, "completed", progress=100, result=result
                )
            else:
                self.job_manager.update_job_status(
                    job_id, "failed", progress=100, error=result["message"]
                )
            return result
        except Exception as e:
            error_msg = f"Validation background failed: {e}\n{traceback.format_exc()}"
            self.logger.error(error_msg)
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "validation_failed", "failed", str(e)
            )
            self.job_manager.update_job_status(
                job_id, "failed", progress=100, error=str(e)
            )
            raise

    def run_validation(self, opportunity_id: int) -> str:
        """Public method to initiate the validation phase asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Validation for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_validation_background, args=(opportunity_id,)
        )
        return job_id
```

## File: pipeline/orchestrator/workflow_orchestrator.py
```python
# backend/pipeline/orchestrator/workflow_orchestrator.py
import logging
import traceback
import threading

logger = logging.getLogger(__name__)


class WorkflowOrchestrator:
    def _run_full_auto_workflow_background(
        self, job_id: str, opportunity_id: int, override_validation: bool
    ):
        """Internal method to execute the full workflow from validation to generation."""
        try:
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "in_progress", "running"
            )
            self.job_manager.update_job_progress(job_id, "Workflow Started", "Starting full automation workflow.")

            if not override_validation:
                self.job_manager.update_job_progress(job_id, "Validation", "Running validation checks.")
                validation_result = self.run_validation_phase(opportunity_id)
                if validation_result.get("status") == "failed":
                    self.logger.warning(
                        f"Workflow for opportunity {opportunity_id} stopped due to validation failure: {validation_result.get('message')}"
                    )
                    raise RuntimeError(f"Validation failed: {validation_result.get('message')}")
            else:
                self.logger.info(
                    f"Validation step skipped for opportunity {opportunity_id} due to override."
                )
                self.job_manager.update_job_progress(job_id, "Validation", "Validation skipped by user override.")
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id,
                    "validation_overridden",
                    "validated",
                    error_message="Validation manually overridden.",
                )

            self.job_manager.update_job_progress(job_id, "Analysis", "Starting in-depth analysis.")
            analysis_result = self.run_analysis_phase(
                opportunity_id, selected_competitor_urls=None
            )
            if analysis_result.get("status") == "failed":
                reason = f"Disqualified during analysis: {analysis_result.get('message')}"
                self.logger.warning(
                    f"Full auto workflow for {opportunity_id} stopped: {reason}"
                )
                raise RuntimeError(reason)

            self.db_manager.update_opportunity_workflow_state(
                opportunity_id,
                "analysis_completed",
                "paused_for_approval",
                error_message="Awaiting user approval to proceed to content generation.",
            )
            self.job_manager.update_job_progress(job_id, "Paused", "Analysis complete. Awaiting user approval.", status="paused")
            self.logger.info(
                f"Full auto workflow for {opportunity_id} paused after analysis, awaiting user approval."
            )
            # This background job ends here. A new one is started by the 'approve_analysis' endpoint.
            # So, we set the job to a 'paused' but technically 'completed' state from the runner's perspective.
            self.job_manager.update_job_status(job_id, "paused", progress=50, result={"status": "paused", "message": "Awaiting approval."})

        except Exception as e:
            error_msg = f"Full auto workflow for {opportunity_id} failed: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            # The job's _run_job wrapper will catch this and handle the final 'failed' state.
            raise

    def run_full_auto_workflow(
        self, opportunity_id: int, override_validation: bool = False
    ) -> str:
        """Public method to initiate the full auto workflow asynchronously."""
        self.logger.info(
            f"--- Orchestrator: Initiating Full Auto Workflow for Opportunity ID: {opportunity_id} (Async) with override: {override_validation} ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_full_auto_workflow_background,
            args=(opportunity_id, override_validation),
        )
        return job_id

    def _run_full_automation_workflow_background(
        self, job_id: str, opportunity_id: int, override_validation: bool
    ):
        """Internal method for the true 'fire and forget' full automation workflow."""
        try:
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "in_progress", "running"
            )
            self.job_manager.update_job_progress(job_id, "Workflow Started", "Starting full automation workflow.")

            if not override_validation:
                self.job_manager.update_job_progress(job_id, "Validation", "Running validation checks.")
                validation_result = self.run_validation_phase(opportunity_id)
                if validation_result.get("status") == "failed":
                    self.logger.warning(
                        f"Automation for opportunity {opportunity_id} stopped due to validation failure: {validation_result.get('message')}"
                    )
                    raise RuntimeError(f"Validation failed: {validation_result.get('message')}")
            else:
                self.logger.info(
                    f"Validation step skipped for opportunity {opportunity_id} due to override."
                )
                self.job_manager.update_job_progress(job_id, "Validation", "Validation skipped by user override.")
                self.db_manager.update_opportunity_workflow_state(
                    opportunity_id,
                    "validation_overridden",
                    "validated",
                    error_message="Validation manually overridden.",
                )

            self.job_manager.update_job_progress(job_id, "Analysis", "Starting in-depth analysis.")
            analysis_result = self.run_analysis_phase(
                opportunity_id, selected_competitor_urls=None
            )
            if analysis_result.get("status") == "failed":
                reason = f"Disqualified during analysis: {analysis_result.get('message')}"
                self.logger.warning(
                    f"Full automation for {opportunity_id} stopped: {reason}"
                )
                raise RuntimeError(reason)

            self.logger.info(
                f"Analysis complete for {opportunity_id}, proceeding directly to content generation."
            )
            self.job_manager.update_job_progress(job_id, "Content Generation", "Analysis complete, starting content generation.")

            # This now calls the content generation logic which also uses update_job_progress
            self._run_full_content_generation_background(job_id, opportunity_id, overrides=None)
            
            # The _run_job wrapper will mark the job as completed.
            # We add the redirect_url to the result.
            final_result = {
                "status": "success",
                "message": "Full automation workflow completed successfully.",
                "redirect_url": f"/opportunities/{opportunity_id}"
            }
            self.job_manager.update_job_status(job_id, "completed", progress=100, result=final_result)

        except Exception as e:
            error_msg = f"Full automation workflow for {opportunity_id} failed: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            # The job's _run_job wrapper will catch this and handle the final 'failed' state.
            raise
    
    # ... (keep existing public methods like run_full_automation_workflow) ...

    def continue_workflow_after_approval(self, job_id: str, opportunity_id: int, overrides: dict = None) -> str:
        """
        Continues a paused workflow job after user approval, proceeding to content generation.
        This now creates a NEW job for the content generation phase.
        """
        self.logger.info(
            f"Attempting to resume workflow from job {job_id} on opportunity {opportunity_id}."
        )

        job_info = self.job_manager.get_job_status(job_id)
        if not job_info or job_info.get("status") != "paused":
            error_msg = f"Cannot resume job {job_id}: Job not found or not in 'paused' state. Current status: {job_info.get('status') if job_info else 'Not Found'}."
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        self.db_manager.update_opportunity_status(opportunity_id, "in_progress")

        # Create a new job for the content generation part of the workflow
        new_job_id = self.job_manager.create_job(
            target_function=self._run_full_content_generation_background,
            args=(opportunity_id, overrides),
        )
        
        # Link the old job to the new one for traceability if needed
        self.job_manager.update_job_status(job_id, "completed", progress=100, result={"status": "resumed", "next_job_id": new_job_id})

        return new_job_id

    def _run_content_refresh_workflow_background(
        self, job_id: str, opportunity_id: int
    ):
        """Internal method to execute content refresh for a job."""
        try:
            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity:
                raise ValueError("Opportunity not found for content refresh.")

            self.logger.info(
                f"--- Orchestrator: Starting Content Refresh Workflow for '{opportunity.get('keyword')}' ---"
            )
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "refresh_started", "running"
            )
            self.job_manager.update_job_progress(job_id, "Refresh Started", "Re-analyzing content and SERP data.")

            analysis_result = self.run_analysis_phase(
                opportunity_id, selected_competitor_urls=None
            )
            if analysis_result.get("status") == "failed":
                raise RuntimeError(f"Refresh failed during analysis: {analysis_result.get('message')}")

            opportunity = self.db_manager.get_opportunity_by_id(opportunity_id)
            if not opportunity or opportunity.get("status") != "analyzed":
                raise RuntimeError("Opportunity not in 'analyzed' state after refresh analysis.")

            self.job_manager.update_job_progress(job_id, "Content Generation", "Analysis complete, re-generating content.")
            
            self._run_full_content_generation_background(job_id, opportunity_id, overrides=None)

            self.db_manager.update_opportunity_workflow_state(
                opportunity_id,
                "refresh_completed",
                "generated",
                error_message="Content refreshed.",
            )
            
            final_result = {
                "status": "success",
                "message": "Content refresh completed successfully.",
                "redirect_url": f"/opportunities/{opportunity_id}"
            }
            self.job_manager.update_job_status(job_id, "completed", progress=100, result=final_result)
            self.logger.info(f"Content refresh for opportunity {opportunity_id} completed successfully.")

        except Exception as e:
            error_msg = f"Content refresh workflow failed for opportunity {opportunity_id}: {e}\n{traceback.format_exc()}"
            self.logger.error(error_msg)
            self.db_manager.update_opportunity_workflow_state(
                opportunity_id, "refresh_failed", "failed", str(e)
            )
            # The job's _run_job wrapper will catch this and handle the final 'failed' state.
            raise

    def run_content_refresh_workflow(self, opportunity_id: int) -> str:
        """Public method to initiate an asynchronous content refresh workflow."""
        self.logger.info(
            f"--- Orchestrator: Initiating Content Refresh Workflow for Opportunity ID: {opportunity_id} (Async) ---"
        )
        job_id = self.job_manager.create_job(
            target_function=self._run_content_refresh_workflow_background,
            args=(opportunity_id,),
        )
        return job_id
```

## File: pipeline/step_01_discovery/keyword_discovery/expander.py
```python
# pipeline/step_01_discovery/keyword_discovery/expander.py
import logging
from typing import List, Dict, Any, Optional

from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class NewKeywordExpander:
    def __init__(
        self,
        client: DataForSEOClientV2,
        config: Dict[str, Any],
        logger: Optional[logging.Logger] = None,
    ):
        self.client = client
        self.config = config
        self.logger = logger or logging.getLogger(self.__class__.__name__)

    def expand(
        self,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        existing_keywords: set,
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = False,
    ) -> Dict[str, Any]:
        if not discovery_modes:
            raise ValueError("At least one discovery mode must be selected.")

        # Filter out seed keywords that already exist
        original_seed_count = len(seed_keywords)
        seed_keywords = [
            kw for kw in seed_keywords if kw.lower() not in existing_keywords
        ]
        if not seed_keywords:
            self.logger.info(
                "All seed keywords already exist in the database. Skipping expansion."
            )
            return {
                "total_cost": 0.0,
                "raw_counts": {},
                "total_raw_count": 0,
                "total_unique_count": 0,
                "final_keywords": [],
            }
        self.logger.info(
            f"Filtered seed keywords from {original_seed_count} to {len(seed_keywords)}."
        )

        location_code = self.config.get("location_code")
        language_code = self.config.get("language_code")
        if not location_code or not language_code:
            raise ValueError("Location and language codes must be set.")

        # The frontend provides filters with 'keyword_data.' prefix, suitable for 'related_keywords'.
        # We need to create versions of these filters without the prefix for other modes.

        related_filters = filters
        ideas_filters = []
        if filters:
            for f in filters:
                new_filter = f.copy()
                if "field" in new_filter and new_filter["field"].startswith(
                    "keyword_data."
                ):
                    new_filter["field"] = new_filter["field"][len("keyword_data.") :]
                ideas_filters.append(new_filter)

        # Suggestions filters are the same as ideas filters (no prefix)
        suggestions_filters = ideas_filters

        structured_filters = {
            "ideas": ideas_filters,
            "suggestions": suggestions_filters,
            "related": related_filters,
        }

        # W21 FIX: Set default order_by if not provided, now structured as a dict
        if not order_by:
            ideas_suggestions_orderby = [
                "keyword_info.search_volume,desc",
            ]
            related_orderby = [
                "keyword_data.keyword_info.search_volume,desc",
            ]
            structured_orderby = {
                "ideas": ideas_suggestions_orderby,
                "suggestions": ideas_suggestions_orderby,
                "related": related_orderby,
            }
        else:
            # If order_by is provided from frontend, structure it for each mode
            related_orderby = [f"keyword_data.{rule}" for rule in order_by]
            structured_orderby = {
                "ideas": order_by,
                "suggestions": order_by,
                "related": related_orderby,
            }

        # Make a single burst call to the DataForSEOClientV2
        all_ideas, total_cost = self.client.get_keyword_ideas(
            seed_keywords=seed_keywords,
            location_code=location_code,
            language_code=language_code,
            client_cfg=self.config,
            discovery_modes=discovery_modes,
            filters=structured_filters,  # Use the structured filters directly
            order_by=structured_orderby,
            limit=limit,
            depth=depth,
            ignore_synonyms_override=ignore_synonyms,
        )
        self.logger.info(
            f"Burst discovery completed. Found {len(all_ideas)} raw keyword ideas. Cost: ${total_cost:.4f}"
        )

        # Filter out any duplicates and existing keywords from the burst results
        final_keywords_deduplicated = []
        seen_keywords = set(
            existing_keywords
        )  # Start with already existing to prevent re-adding

        # Recalculate raw counts per source based on `discovery_source` field added by get_keyword_ideas
        raw_counts = {"keyword_ideas": 0, "suggestions": 0, "related": 0}
        for item in all_ideas:
            kw_text = item.get("keyword", "").lower()
            if kw_text and kw_text not in seen_keywords:
                final_keywords_deduplicated.append(item)
                seen_keywords.add(kw_text)
                source = item.get("discovery_source")
                if source in raw_counts:
                    raw_counts[source] += 1
            elif kw_text:
                self.logger.debug(
                    f"Skipping duplicate or existing keyword: {item.get('keyword')}"
                )

        self.logger.info(
            f"Total unique new keywords after deduplication: {len(final_keywords_deduplicated)}"
        )

        return {
            "total_cost": total_cost,
            "raw_counts": raw_counts,
            "total_raw_count": len(all_ideas),  # Total raw from API before processing
            "total_unique_count": len(final_keywords_deduplicated),
            "final_keywords": final_keywords_deduplicated,
        }
```

## File: pipeline/step_01_discovery/keyword_discovery/filters.py
```python
# pipeline/step_01_discovery/keyword_discovery/filters.py
import json
import logging
from typing import List, Any, Tuple, Dict

logger = logging.getLogger(__name__)

FORBIDDEN_API_FILTER_FIELDS = [
    "relevance",
    "sv_bing",
    "sv_clickstream",
]  # Define forbidden fields


def sanitize_filters_for_api(filters: List[Any]) -> List[Any]:
    """
    Removes any filters attempting to use forbidden internal metrics or data sources.
    """
    sanitized = []
    for item in filters:
        if isinstance(item, list) and len(item) >= 1 and isinstance(item[0], str):
            field_path = item[0].lower()
            if any(
                forbidden in field_path for forbidden in FORBIDDEN_API_FILTER_FIELDS
            ):
                logger.warning(
                    f"Forbidden field '{field_path}' detected in API filter. Removing it."
                )
                continue
        sanitized.append(item)
    return sanitized


def build_discovery_filters(config: Dict[str, Any]) -> Tuple[List[Any], List[Any]]:
    """
    Builds filter lists for API-side filtering for KD, SV, Competition, and Intent.
    """
    std_api_filters = []
    rel_api_filters = []

    min_sv = config.get("min_search_volume")
    if min_sv is not None:
        std_api_filters.extend([["keyword_info.search_volume", ">=", min_sv], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword_info.search_volume", ">=", min_sv], "and"]
        )

    max_kd = config.get("max_keyword_difficulty")
    if max_kd is not None:
        std_api_filters.extend(
            [["keyword_properties.keyword_difficulty", "<=", max_kd], "and"]
        )
        rel_api_filters.extend(
            [
                ["keyword_data.keyword_properties.keyword_difficulty", "<=", max_kd],
                "and",
            ]
        )

    allowed_comp_levels = config.get("allowed_competition_levels")
    if allowed_comp_levels:
        std_api_filters.extend(
            [["keyword_info.competition_level", "in", allowed_comp_levels], "and"]
        )
        rel_api_filters.extend(
            [
                [
                    "keyword_data.keyword_info.competition_level",
                    "in",
                    allowed_comp_levels,
                ],
                "and",
            ]
        )

    allowed_intents = config.get("allowed_intents")
    if config.get("enforce_intent_filter", False) and allowed_intents:
        std_api_filters.extend(
            [["search_intent_info.main_intent", "in", allowed_intents], "and"]
        )
        rel_api_filters.extend(
            [
                ["keyword_data.search_intent_info.main_intent", "in", allowed_intents],
                "and",
            ]
        )

    # NEW: Closely Variants
    closely_variants = config.get("closely_variants")
    if closely_variants is not None:
        std_api_filters.extend(
            [["closely_variants", "=", closely_variants], "and"]
        )  # This param is at top level
        # Related keywords endpoint does not have closely_variants

    # NEW: CPC Range Filters
    min_cpc_filter = config.get("min_cpc_filter")
    max_cpc_filter = config.get("max_cpc_filter")
    if min_cpc_filter is not None:
        std_api_filters.extend([["keyword_info.cpc", ">=", min_cpc_filter], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword_info.cpc", ">=", min_cpc_filter], "and"]
        )
    if max_cpc_filter is not None:
        std_api_filters.extend([["keyword_info.cpc", "<=", max_cpc_filter], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword_info.cpc", "<=", max_cpc_filter], "and"]
        )

    # NEW: Competition Range Filters
    min_competition = config.get("min_competition")
    max_competition = config.get("max_competition")
    if min_competition is not None:
        std_api_filters.extend(
            [["keyword_info.competition", ">=", min_competition], "and"]
        )
        rel_api_filters.extend(
            [["keyword_data.keyword_info.competition", ">=", min_competition], "and"]
        )
    if max_competition is not None:
        std_api_filters.extend(
            [["keyword_info.competition", "<=", max_competition], "and"]
        )
        rel_api_filters.extend(
            [["keyword_data.keyword_info.competition", "<=", max_competition], "and"]
        )

    # NEW: Max Competition Level Filter
    max_competition_level = config.get("max_competition_level")
    if max_competition_level:
        levels = ["LOW", "MEDIUM", "HIGH"]
        allowed_levels = levels[: levels.index(max_competition_level) + 1]
        std_api_filters.extend(
            [["keyword_info.competition_level", "in", allowed_levels], "and"]
        )
        rel_api_filters.extend(
            [
                ["keyword_data.keyword_info.competition_level", "in", allowed_levels],
                "and",
            ]
        )

    # NEW: Regex Filter (from Task 34)
    search_phrase_regex = config.get("search_phrase_regex")
    if search_phrase_regex and search_phrase_regex.strip():
        std_api_filters.extend([["keyword", "regex", search_phrase_regex], "and"])
        rel_api_filters.extend(
            [["keyword_data.keyword", "regex", search_phrase_regex], "and"]
        )

    if std_api_filters:
        std_api_filters.pop()
    if rel_api_filters:
        rel_api_filters.pop()

    # Apply sanitation (Weakness 3.7 Fix)
    std_api_filters = sanitize_filters_for_api(std_api_filters)
    rel_api_filters = sanitize_filters_for_api(rel_api_filters)

    logger.info(f"Built standard API filters: {json.dumps(std_api_filters)}")
    logger.info(f"Built related API filters: {json.dumps(rel_api_filters)}")

    return std_api_filters, rel_api_filters
```

## File: pipeline/step_01_discovery/__init__.py
```python
# pipeline/step_01_discovery/__init__.py
```

## File: pipeline/step_01_discovery/blog_content_qualifier.py
```python
# pipeline/step_01_discovery/blog_content_qualifier.py
from typing import Dict, Any, Tuple
from .disqualification_rules import apply_disqualification_rules


def assign_status_from_score(
    opportunity: Dict[str, Any], score: float, client_cfg: Dict[str, Any]
) -> Tuple[str, str]:
    """
    Assigns a final status to a keyword based on its score and hard disqualification rules.
    """
    # First, check for hard-stop, non-negotiable disqualification rules.
    is_disqualified, reason, is_hard_stop = apply_disqualification_rules(
        opportunity, client_cfg, cannibalization_checker=None
    )

    if is_disqualified and is_hard_stop:
        return "rejected", reason

    # If not hard-stopped, categorize based on the strategic score.
    if score >= client_cfg.get("qualified_threshold", 70):
        return "qualified", "Qualified: High strategic score."
    elif score >= client_cfg.get("review_threshold", 50):
        return "review", "Review: Moderate strategic score."
    else:
        return "rejected", f"Rejected: Low strategic score ({score:.1f})."
```

## File: pipeline/step_01_discovery/cannibalization_checker.py
```python
import logging
from typing import List, Dict, Any
from urllib.parse import urlparse

from backend.data_access.database_manager import DatabaseManager


class CannibalizationChecker:
    def __init__(
        self,
        target_domain: str,
        dataforseo_client: Any,
        client_cfg: Dict[str, Any],
        db_manager: DatabaseManager,
    ):
        self.target_domain = (
            target_domain.lower().replace("www.", "") if target_domain else None
        )
        self.logger = logging.getLogger(self.__class__.__name__)
        self.dataforseo_client = dataforseo_client
        self.client_cfg = client_cfg
        self.db_manager = db_manager

    def is_url_in_serp(
        self, serp_results: List[Dict[str, Any]], keyword: str, client_id: str
    ) -> bool:
        """
        Returns True if the target domain is found in the list of SERP results
        OR if the keyword already exists in the opportunities database for the client.
        """
        if self.db_manager.check_existing_keywords(client_id, [keyword]):
            self.logger.warning(
                f"Cannibalization detected: Keyword '{keyword}' already exists in the database for client '{client_id}'."
            )
            return True

        if not self.target_domain:
            return False

        for result in serp_results:
            try:
                url = result.get("url")
                if not url:
                    continue
                url_domain = urlparse(url).netloc.lower().replace("www.", "")
                if url_domain == self.target_domain or url_domain.endswith(
                    f".{self.target_domain}"
                ):
                    self.logger.warning(
                        f"Cannibalization detected: Found '{url}' in SERP for '{keyword}'."
                    )
                    return True
            except Exception:
                continue
        return False
```

## File: pipeline/step_01_discovery/disqualification_rules.py
```python
# pipeline/step_01_discovery/disqualification_rules.py
import logging
import re
from typing import Dict, Any, Tuple, Optional
from datetime import datetime
import numpy as np
from core import utils

from .cannibalization_checker import CannibalizationChecker


def apply_disqualification_rules(
    opportunity: Dict[str, Any],
    client_cfg: Dict[str, Any],
    cannibalization_checker: CannibalizationChecker,
) -> Tuple[bool, Optional[str], bool]:
    """
    Applies the comprehensive 20-rule set to disqualify a keyword based on data from the discovery phase.
    Reads all thresholds from client_cfg.
    Returns (is_disqualified, reason, is_hard_stop).
    """
    keyword = opportunity.get("keyword", "Unknown Keyword")

    # --- Failsafe Validation ---
    required_keys = [
        "keyword_info",
        "keyword_properties",
        "serp_info",
        "search_intent_info",
    ]
    for key in required_keys:
        if key not in opportunity or opportunity[key] is None:
            logging.getLogger(__name__).warning(
                f"Disqualifying '{keyword}' due to missing or null '{key}' data."
            )
            return True, f"Rule 1: Missing critical data structure ({key}).", True

    serp_info = opportunity.get("serp_info", {})
    if not serp_info:
        logging.getLogger(__name__).warning(
            f"Disqualifying '{keyword}' due to empty 'serp_info' data."
        )
        return True, "Rule 1: Missing SERP info data.", True

    keyword_info = opportunity.get("keyword_info") or {}
    keyword_props = opportunity.get("keyword_properties") or {}
    avg_backlinks = opportunity.get("avg_backlinks_info") or {}
    intent_info = opportunity.get("search_intent_info") or {}

    # Tier 1: Foundational Checks
    if not all([keyword_info, keyword_props, intent_info]):
        return (
            True,
            "Rule 1: Missing critical data structures (keyword_info, keyword_properties, or search_intent_info).",
            True,
        )

    # Rule 2: Check primary intent
    allowed_intents = client_cfg.get("allowed_intents", ["informational"])
    main_intent = intent_info.get("main_intent")
    foreign_intents = intent_info.get("foreign_intent", [])

    if main_intent not in allowed_intents:
        return True, f"Rule 2: Non-target main intent ('{main_intent}').", True

    # Rule 2b (NEW): Check secondary intents for prohibitive types
    prohibited_intents = set(client_cfg.get("prohibited_intents", ["navigational"]))
    foreign_intents = intent_info.get("foreign_intent", []) or []
    if not prohibited_intents.isdisjoint(set(foreign_intents)):
        offending_intents = prohibited_intents.intersection(set(foreign_intents))
        return (
            True,
            f"Rule 2b: Contains a prohibited secondary intent ({', '.join(offending_intents)}).",
            True,
        )

    if keyword_props.get("is_another_language"):
        return True, "Rule 3: Language mismatch.", True

    negative_keywords = set(
        kw.lower() for kw in client_cfg.get("negative_keywords", [])
    )
    core_keyword = keyword_props.get("core_keyword")
    if any(neg_kw in keyword.lower() for neg_kw in negative_keywords) or (
        core_keyword
        and any(neg_kw in core_keyword.lower() for neg_kw in negative_keywords)
    ):
        return True, "Rule 4: Contains a negative keyword.", True

    # Tier 2: Volume & Trend Analysis
    if utils.safe_compare(
        keyword_info.get("search_volume"), client_cfg.get("min_search_volume"), "lt"
    ):
        return (
            True,
            f"Rule 5: Below search volume floor (minimum: {client_cfg.get('min_search_volume', 100)} SV). Current: {keyword_info.get('search_volume', 0)} SV.",
            False,
        )

    trends = keyword_info.get("search_volume_trend", {})
    try:
        yearly_trend = trends.get("yearly")
        quarterly_trend = trends.get("quarterly")

        yearly_threshold = client_cfg.get("yearly_trend_decline_threshold", -25)
        quarterly_threshold = client_cfg.get("quarterly_trend_decline_threshold", 0)

        yearly_check = utils.safe_compare(yearly_trend, yearly_threshold, "lt")
        quarterly_check = utils.safe_compare(quarterly_trend, quarterly_threshold, "lt")

        if yearly_check and quarterly_check:
            return (
                True,
                f"Rule 6: Consistently declining trend. Yearly trend: {yearly_trend}% (below {yearly_threshold}% threshold), Quarterly trend: {quarterly_trend}% (below {quarterly_threshold}% threshold). Consider manual review for seasonality.",
                False,
            )
    except TypeError:
        logging.getLogger(__name__).error(
            f"TypeError during trend analysis for keyword '{keyword}'. "
            f"trends.get('yearly') value: {trends.get('yearly')}, type: {type(trends.get('yearly'))}. "
            f"trends.get('quarterly') value: {trends.get('quarterly')}, type: {type(trends.get('quarterly'))}."
        )
        return (
            True,
            "Rule 6: Failed to process trend data due to invalid format.",
            False,
        )

    monthly_searches = keyword_info.get("monthly_searches", [])
    if monthly_searches and len(monthly_searches) > 1:
        volumes = [
            ms["search_volume"]
            for ms in monthly_searches
            if ms.get("search_volume") is not None and ms["search_volume"] > 0
        ]
        if len(volumes) > 1 and np.mean(volumes) > 0:
            volatility_threshold = client_cfg.get(
                "search_volume_volatility_threshold", 1.5
            )
            std_dev_to_mean_ratio = np.std(volumes) / np.mean(volumes)
            if std_dev_to_mean_ratio > volatility_threshold:
                return (
                    True,
                    f"Rule 7: Extreme search volume volatility. Std Dev / Mean ratio: {std_dev_to_mean_ratio:.2f} (above {volatility_threshold} threshold). Could indicate a fleeting trend or strong seasonality. Manual review recommended.",
                    False,
                )

    # Rule 7b: Check for recent sharp decline using raw monthly searches
    monthly_searches = opportunity.get(
        "monthly_searches", []
    )  # Get from opportunity object, which is deserialized
    if monthly_searches and len(monthly_searches) >= 4:
        # Sort by year and month to ensure correctness (most recent first for trend)
        try:
            sorted_searches = sorted(
                monthly_searches, key=lambda x: (x["year"], x["month"]), reverse=True
            )
            if len(sorted_searches) >= 4:
                # Compare latest month with 3 months prior (index 0 vs index 3)
                latest_vol = sorted_searches[0].get("search_volume")
                past_vol = sorted_searches[3].get("search_volume")

                if latest_vol is not None and past_vol is not None and past_vol > 0:
                    if (
                        latest_vol / past_vol
                    ) < 0.5:  # If volume has dropped by more than 50% in 3 months
                        return (
                            True,
                            "Rule 7b: Recent sharp decline in search volume (>50% drop in last 3 months).",
                            False,
                        )
        except (TypeError, KeyError):
            logging.getLogger(__name__).warning(
                f"Could not parse monthly_searches for recent trend analysis on keyword '{keyword}'."
            )

    # Tier 3: Commercial & Competitive Analysis
    if utils.safe_compare(
        keyword_info.get("competition"),
        client_cfg.get("max_paid_competition_score", 0.8),
        "gt",
    ) and (keyword_info.get("competition_level") == "HIGH"):
        return True, "Rule 8: Excessive paid competition.", False

    if utils.safe_compare(
        keyword_info.get("high_top_of_page_bid"),
        client_cfg.get("max_high_top_of_page_bid", 15.0),
        "gt",
    ):
        return (
            True,
            f"Rule 9: Prohibitively high CPC bids (${client_cfg.get('max_high_top_of_page_bid', 15.00)}).",
            False,
        )

    if utils.safe_compare(
        keyword_props.get("keyword_difficulty"),
        client_cfg.get("max_kd_hard_limit", 70),
        "gt",
    ):
        return (
            True,
            f"Rule 10: Extreme keyword difficulty (>{client_cfg.get('max_kd_hard_limit', 70)}).",
            False,
        )

    if utils.safe_compare(
        avg_backlinks.get("referring_main_domains"),
        client_cfg.get("max_referring_main_domains_limit", 100),
        "gt",
    ):
        return (
            True,
            f"Rule 11: Overly authoritative competitor domains (>{client_cfg.get('max_referring_main_domains_limit', 100)} referring main domains).",
            False,
        )

    if utils.safe_compare(
        avg_backlinks.get("main_domain_rank"),
        client_cfg.get("max_avg_domain_rank_threshold", 500),
        "lt",
    ):
        return (
            True,
            f"Rule 12: SERP dominated by high-authority domains (avg rank < {client_cfg.get('max_avg_domain_rank_threshold', 500)}).",
            False,
        )

    if (avg_backlinks.get("referring_domains") or 0) > 0:
        pages_to_domain_ratio = (avg_backlinks.get("referring_pages") or 0) / (
            avg_backlinks.get("referring_domains") or 1
        )
        if pages_to_domain_ratio > client_cfg.get("max_pages_to_domain_ratio", 15):
            return (
                True,
                "Rule 13: Potential spammy competitor profile (high page/domain ratio).",
                False,
            )

    # Tier 4: Content, SERP & Keyword Structure

    # Rule: Check for hostile SERP environment
    is_hostile, hostile_reason = _check_hostile_serp_environment(opportunity)
    if is_hostile:
        return True, hostile_reason, True

    non_evergreen_pattern = _get_non_evergreen_year_pattern()
    if non_evergreen_pattern and re.search(non_evergreen_pattern, keyword):
        return (
            True,
            "Rule 14: Non-evergreen temporal keyword (matches pattern for past/current years).",
            False,
        )

    word_count = len(keyword.split())
    is_question = utils.is_question_keyword(keyword)  # This now exists

    min_wc = client_cfg.get("min_keyword_word_count", 2)
    max_wc = client_cfg.get("max_keyword_word_count", 8)

    is_outside_range = word_count < min_wc or word_count > max_wc

    # Rule 15 (Refined with override): Check word count and potentially override for high-value keywords
    if is_outside_range and not is_question:
        sv = keyword_info.get("search_volume", 0)
        cpc = keyword_info.get("cpc")  # Get the value, which could be None
        if cpc is None:
            cpc = 0.0  # Default to 0.0 if it's None

        high_sv_override = client_cfg.get("high_value_sv_override_threshold", 10000)
        high_cpc_override = client_cfg.get("high_value_cpc_override_threshold", 5.0)

        if sv >= high_sv_override or cpc >= high_cpc_override:
            logging.getLogger(__name__).info(
                f"Override: High value SV/CPC bypasses word count rule for '{keyword}'."
            )
            pass  # Allow the keyword to proceed
        else:
            return (
                True,
                f"Rule 15: Non-question keyword word count ({word_count}) is outside the acceptable range ({min_wc}-{max_wc} words).",
                False,
            )

    serp_info = opportunity.get("serp_info", {})
    serp_types = set(serp_info.get("serp_item_types", []))

    crowded_features = {
        "video",
        "images",
        "people_also_ask",
        "carousel",
        "featured_snippet",
        "short_videos",
    }
    if len(serp_types.intersection(crowded_features)) > client_cfg.get(
        "crowded_serp_features_threshold", 4
    ):
        return (
            True,
            f"Rule 17: SERP is overly crowded (>{client_cfg.get('crowded_serp_features_threshold', 4)} attention-grabbing features).",
            False,
        )

    # Rule 18: Check for navigational intent safely
    is_navigational = False
    if intent_info:
        if intent_info.get("main_intent") == "navigational":
            is_navigational = True
        else:
            foreign_intent = intent_info.get("foreign_intent")
            if foreign_intent and "navigational" in foreign_intent:
                is_navigational = True
    if is_navigational:
        return True, "Rule 18: Strong navigational intent.", True

    if serp_info.get("last_updated_time") and serp_info.get("previous_updated_time"):
        try:
            last_update = datetime.fromisoformat(
                serp_info["last_updated_time"].replace(" +00:00", "")
            )
            prev_update = datetime.fromisoformat(
                serp_info["previous_updated_time"].replace(" +00:00", "")
            )
            days_between_updates = (last_update - prev_update).days
            if days_between_updates < client_cfg.get("min_serp_stability_days", 14):
                return (
                    True,
                    f"Rule 19: Unstable SERP (updated every {days_between_updates} days).",
                    False,
                )
        except ValueError:
            logging.getLogger(__name__).warning(
                f"Could not parse SERP update times for '{keyword}': {serp_info.get('last_updated_time')}, {serp_info.get('previous_updated_time')}"
            )

    cpc_value = keyword_info.get("cpc")
    if cpc_value is None:
        cpc_value = 0.0
    if (
        intent_info.get("main_intent") in ["commercial", "transactional"]
        and cpc_value == 0
    ):
        return True, "Rule 20: Low-value commercial intent (zero CPC).", False

    return False, None, False


def _check_hostile_serp_environment(
    opportunity: Dict[str, Any],
) -> Tuple[bool, Optional[str]]:
    """
    Rule 16: Disqualifies keywords where the SERP is dominated by features hostile to blog content.
    """
    serp_info = opportunity.get("serp_info", {})
    if not serp_info:
        return False, None  # Cannot analyze if SERP info is missing

    serp_types = set(serp_info.get("serp_item_types", []))

    # Define hostile features based on detailed SERP analysis
    HOSTILE_FEATURES = {
        # Strong transactional/e-commerce intent
        "shopping",
        "popular_products",
        "refine_products",
        "explore_brands",
        # Strong local intent
        "local_pack",
        "map",
        "local_services",
        # Purely transactional/utility intent (Google-owned tools)
        "google_flights",
        "google_hotels",
        "hotels_pack",
        # App-related intent
        "app",
        # Job-seeking intent
        "jobs",
        # Direct utility/tool intent
        "math_solver",
        "currency_box",
        "stocks_box",
    }

    found_hostile_features = serp_types.intersection(HOSTILE_FEATURES)

    if found_hostile_features:
        return (
            True,
            f"Rule 16: SERP is hostile to blog content. Contains dominant non-article features: {', '.join(found_hostile_features)}.",
        )

    return False, None


def _get_non_evergreen_year_pattern() -> str:
    """
    Generates a regex pattern to find past years up to the current year,
    dynamically adjusting to avoid disqualifying valid keywords in the future.
    Example for current year 2024: \b(201\d|202[0-4])\b
    """
    current_year = datetime.now().year

    patterns = []
    # Handle decades before the current one (e.g., 2010s)
    for decade_start in range(2010, (current_year // 10) * 10, 10):
        patterns.append(
            f"{decade_start}|{decade_start + 1}|{decade_start + 2}|{decade_start + 3}|{decade_start + 4}|{decade_start + 5}|{decade_start + 6}|{decade_start + 7}|{decade_start + 8}|{decade_start + 9}"
        )

    # Handle years in the current decade up to the current year
    current_decade_start_year = (current_year // 10) * 10
    current_decade_years = [
        str(year) for year in range(current_decade_start_year, current_year + 1)
    ]
    if current_decade_years:
        patterns.append("|".join(current_decade_years))

    if not patterns:
        return ""  # Should not happen unless current_year is before 2010

    return r"\b(" + "|".join(patterns) + r")\b"
```

## File: pipeline/step_01_discovery/keyword_expander.py
```python
# pipeline/step_01_discovery/keyword_expander.py
import logging
from typing import List, Dict, Any, Optional
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from .keyword_discovery.expander import NewKeywordExpander


class KeywordExpander:
    """
    A wrapper class that uses the new modular keyword expansion system.
    """

    def __init__(
        self,
        client: DataForSEOClientV2,
        config: Dict[str, Any],
        run_logger: Optional[logging.Logger] = None,
    ):
        self.client = client
        self.config = config
        self.logger = run_logger or logging.getLogger(self.__class__.__name__)
        self.expander = NewKeywordExpander(client, config, self.logger)

    def expand_seed_keyword(
        self,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        existing_keywords: set,
        limit: Optional[int] = None,
        depth: Optional[int] = None,
        ignore_synonyms: Optional[bool] = False,
    ) -> Dict[str, Any]:
        """
        Delegates the keyword expansion to the new NewKeywordExpander.
        """
        self.logger.info(
            f"Starting keyword expansion with {len(seed_keywords)} seeds and modes: {discovery_modes}"
        )

        results = self.expander.expand(
            seed_keywords,
            discovery_modes,
            filters,
            order_by,
            existing_keywords,
            limit,
            depth,
            ignore_synonyms,
        )

        self.logger.info(
            f"Keyword expansion complete. Found {results['total_unique_count']} unique keywords."
        )

        return results
```

## File: pipeline/step_01_discovery/run_discovery.py
```python
import logging
from typing import List, Dict, Any, Optional

from data_access.database_manager import DatabaseManager
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from pipeline.step_01_discovery.keyword_expander import KeywordExpander
from pipeline.step_01_discovery.disqualification_rules import (
    apply_disqualification_rules,
)
from pipeline.step_01_discovery.cannibalization_checker import CannibalizationChecker
from pipeline.step_03_prioritization.scoring_engine import ScoringEngine
from pipeline.step_01_discovery.blog_content_qualifier import assign_status_from_score
from backend.services.serp_analysis_service import SerpAnalysisService


def run_discovery_phase(
    seed_keywords: List[str],
    dataforseo_client: DataForSEOClientV2,
    db_manager: "DatabaseManager",
    client_id: str,
    client_cfg: Dict[str, Any],
    discovery_modes: List[str],
    filters: Optional[List[Any]],
    order_by: Optional[List[str]],
    limit: Optional[int] = None,
    depth: Optional[int] = None,
    ignore_synonyms: Optional[bool] = False,
    include_clickstream_data: Optional[bool] = None,
    closely_variants: Optional[bool] = None,
    run_logger: Optional[logging.Logger] = None,
) -> Dict[str, Any]:
    logger = run_logger or logging.getLogger(__name__)
    logger.info("--- Starting Consolidated Keyword Discovery & Scoring Phase ---")

    expander = KeywordExpander(dataforseo_client, client_cfg, logger)
    cannibalization_checker = CannibalizationChecker(
        client_cfg.get("target_domain"), dataforseo_client, client_cfg, db_manager
    )
    scoring_engine = ScoringEngine(client_cfg)

    # 1. Get keywords that already exist for this client to avoid API calls for them.
    existing_keywords = set(db_manager.get_all_processed_keywords_for_client(client_id))
    logger.info(
        f"Found {len(existing_keywords)} existing keywords to exclude from API request."
    )

    # 2. Expand seed keywords into a large list of opportunities.
    expansion_result = expander.expand_seed_keyword(
        seed_keywords,
        discovery_modes,
        filters,
        order_by,
        existing_keywords,
        limit,
        depth,
        ignore_synonyms,
    )

    all_expanded_keywords = expansion_result.get("final_keywords", [])
    total_cost = expansion_result.get("total_cost", 0.0)

    # --- Scoring and Disqualification Loop (Consolidated Logic) ---
    processed_opportunities = []
    disqualification_reasons = {}
    status_counts = {"qualified": 0, "review": 0, "rejected": 0}
    required_keys = [
        "keyword_info",
        "keyword_properties",
        "serp_info",
        "search_intent_info",
    ]

    for opp in all_expanded_keywords:
        # Pre-validation of opportunity structure
        missing_keys = [
            key for key in required_keys if key not in opp or opp[key] is None
        ]
        if missing_keys:
            logger.warning(
                f"Skipping opportunity '{opp.get('keyword')}' due to missing required data: {', '.join(missing_keys)}"
            )
            continue

        # 3. Apply Hard Disqualification Rules (Cannibalization, Negative Keywords, etc.)
        is_disqualified, reason, is_hard_stop = apply_disqualification_rules(
            opp, client_cfg, cannibalization_checker
        )

        if is_disqualified and is_hard_stop:
            opp["status"] = "rejected"
            opp["blog_qualification_status"] = "rejected"
            opp["blog_qualification_reason"] = reason
            status_counts["rejected"] += 1
            disqualification_reasons[reason] = (
                disqualification_reasons.get(reason, 0) + 1
            )
        else:
            # 4. Score the remaining keywords
            score, breakdown = scoring_engine.calculate_score(opp)
            opp["strategic_score"] = score
            opp["score_breakdown"] = breakdown

            # 5. Assign Status based on Strategic Score
            status, reason = assign_status_from_score(opp, score, client_cfg)
            opp["status"] = status
            opp["blog_qualification_status"] = status
            opp["blog_qualification_reason"] = reason
            status_counts[status.split("_")[0]] = (
                status_counts.get(status.split("_")[0], 0) + 1
            )  # count qualified/review/rejected

        processed_opportunities.append(opp)

    disqualified_count = status_counts.get("rejected", 0)
    passed_count = status_counts.get("qualified", 0) + status_counts.get("review", 0)

    logger.info(
        f"Scoring and Qualification complete. Passed: {passed_count}, Rejected: {disqualified_count}."
    )

    stats = {
        **expansion_result,
        "disqualification_reasons": disqualification_reasons,
        "disqualified_count": disqualified_count,
        "final_qualified_count": passed_count,
    }

    return {
        "stats": stats,
        "total_cost": total_cost,
        "opportunities": processed_opportunities,
    }
```

## File: pipeline/step_02_qualification/__init__.py
```python
# pipeline/step_02_qualification/__init__.py
```

## File: pipeline/step_02_qualification/competitor_analyzer.py
```python
import logging
from typing import List, Dict, Any, Tuple

from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class CompetitorAnalyzer:
    """
    Analyzes top organic competitors from SERP data.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.min_word_count = self.config.get("min_competitor_word_count", 300)

    def analyze_competitors(
        self, top_results: List[Dict[str, Any]]
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Fetches OnPage data for top competitors and performs a basic analysis.
        """
        if not top_results:
            return [], 0.0

        # Limit the number of competitors to analyze based on config
        num_to_analyze = self.config.get("num_competitors_to_analyze", 5)
        competitor_urls = [
            res["url"] for res in top_results[:num_to_analyze] if res.get("url")
        ]

        onpage_results, cost = self.client.get_onpage_data_for_urls(competitor_urls)

        if not onpage_results:
            return [], cost

        analyzed_competitors = []
        for result in onpage_results:
            if "error" in result:
                self.logger.warning(
                    f"Could not analyze competitor {result.get('url')}: {result.get('error')}"
                )
                continue

            content_meta = result.get("meta", {}).get("content", {})
            word_count = content_meta.get("plain_text_word_count")
            if word_count and word_count >= self.min_word_count:
                analyzed_competitors.append(
                    {
                        "url": result.get("url"),
                        "word_count": word_count,
                        "readability_score": content_meta.get(
                            "flesch_kincaid_readability_index"
                        ),
                        "onpage_score": result.get("onpage_score"),
                        "internal_links": result.get("meta", {}).get(
                            "internal_links_count"
                        ),
                        "external_links": result.get("meta", {}).get(
                            "external_links_count"
                        ),
                        "headings": result.get("meta", {}).get("htags"),
                    }
                )

        return analyzed_competitors, cost
```

## File: pipeline/step_02_qualification/serp_analyzer.py
```python
import logging
from typing import Dict, Any, Tuple, Optional

from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from datetime import datetime


class SerpAnalyzer:
    """
    Analyzes the SERP for a given keyword to extract key insights.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def analyze_serp(self, keyword: str) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        Fetches SERP data and extracts insights like featured snippets, PAA, etc.
        """
        location_code = self.config.get("location_code")
        language_code = self.config.get("language_code")

        serp_results, cost = self.client.get_serp_results(
            keyword, location_code, language_code
        )

        if not serp_results:
            return None, cost

        analysis = {
            "serp_has_featured_snippet": False,
            "serp_has_video_results": False,
            "serp_has_ai_overview": False,
            "people_also_ask": [],
            "top_organic_results": [],
            "serp_last_updated_days_ago": None,
        }

        item_types = serp_results.get("item_types", [])
        if "featured_snippet" in item_types:
            analysis["serp_has_featured_snippet"] = True
        if "video" in item_types:
            analysis["serp_has_video_results"] = True
        if "ai_overview" in item_types:
            analysis["serp_has_ai_overview"] = True

        # Extract PAA and top organic results
        for item in serp_results.get("items", []):
            if item.get("type") == "people_also_ask":
                analysis["people_also_ask"] = [
                    q.get("title") for q in item.get("items", []) if q.get("title")
                ]
            elif item.get("type") == "organic":
                analysis["top_organic_results"].append(
                    {
                        "rank": item.get("rank_absolute"),
                        "url": item.get("url"),
                        "title": item.get("title"),
                        "domain": item.get("domain"),
                        "main_domain_rank": item.get(
                            "main_domain_rank", 1000
                        ),  # Default to low rank
                    }
                )

        # Calculate SERP freshness
        datetime_str = serp_results.get("datetime")
        if datetime_str:
            try:
                serp_date = datetime.strptime(datetime_str, "%Y-%m-%d %H:%M:%S +00:00")
                analysis["serp_last_updated_days_ago"] = (
                    datetime.utcnow() - serp_date
                ).days
            except ValueError:
                self.logger.warning(f"Could not parse SERP datetime: {datetime_str}")

        return analysis, cost
```

## File: pipeline/step_03_prioritization/scoring_components/__init__.py
```python
# pipeline/step_03_prioritization/scoring_components/__init__.py
from .ease_of_ranking import calculate_ease_of_ranking_score
from .traffic_potential import calculate_traffic_potential_score
from .commercial_intent import calculate_commercial_intent_score
from .growth_trend import calculate_growth_trend_score
from .serp_features import calculate_serp_features_score
from .serp_volatility import calculate_serp_volatility_score
from .competitor_weakness import calculate_competitor_weakness_score
from .serp_crowding import calculate_serp_crowding_score
from .keyword_structure import calculate_keyword_structure_score
from .serp_threat import calculate_serp_threat_score
from .volume_volatility import calculate_volume_volatility_score
from .serp_freshness import calculate_serp_freshness_score
from .competitor_performance import calculate_competitor_performance_score

__all__ = [
    "calculate_ease_of_ranking_score",
    "calculate_traffic_potential_score",
    "calculate_commercial_intent_score",
    "calculate_growth_trend_score",
    "calculate_serp_features_score",
    "calculate_serp_volatility_score",
    "calculate_competitor_weakness_score",
    "calculate_serp_crowding_score",
    "calculate_keyword_structure_score",
    "calculate_serp_threat_score",
    "calculate_volume_volatility_score",
    "calculate_serp_freshness_score",
    "calculate_competitor_performance_score",  # ADDED THIS LINE
]
```

## File: pipeline/step_03_prioritization/scoring_components/commercial_intent.py
```python
# pipeline/step_03_prioritization/scoring_components/commercial_intent.py
from typing import Dict, Any, Tuple
from backend.core import utils  # NEW: Import the utils module


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_commercial_intent_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the keyword's strategic value for blog content,
    balancing commercial indicators with the type of user intent.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    intent_info = (
        data.get("search_intent_info")
        if isinstance(data.get("search_intent_info"), dict)
        else {}
    )
    keyword = data.get("keyword", "")

    cpc = keyword_info.get("cpc", 0.0)
    if cpc is None:
        cpc = 0.0
    max_cpc = config.get("max_cpc_for_scoring", 10.0)
    cpc_score = _normalize_value(cpc, max_cpc)

    # Add bonus for wide CPC bid spread, indicating market inefficiency
    low_bid = keyword_info.get("low_top_of_page_bid", 0.0) or 0.0
    high_bid = keyword_info.get("high_top_of_page_bid", 0.0) or 0.0
    if low_bid > 0 and high_bid > low_bid:
        bid_spread_ratio = high_bid / low_bid
        if bid_spread_ratio > 5:  # e.g., low is $1, high is >$5
            cpc_score = min(100, cpc_score + 15)

    main_intent = intent_info.get("main_intent", "informational")
    foreign_intents = intent_info.get("foreign_intent", []) or []

    intent_scores = {
        "informational": 75,
        "commercial": 60,
        "transactional": 10,
        "navigational": 0,
    }
    intent_score = intent_scores.get(main_intent, 75)
    explanation = f"Base score for '{main_intent}' intent is {intent_score}."

    if main_intent == "informational" and (
        "commercial" in foreign_intents or "transactional" in foreign_intents
    ):
        intent_score = min(100, intent_score + 25)
        explanation += " Bonus for commercial secondary intent."

    # REPLACED: Use the centralized utility function
    if utils.is_question_keyword(keyword):
        intent_score = min(100, intent_score + 15)
        explanation += " Bonus for being a question keyword."

    competition_level = keyword_info.get("competition_level")
    if competition_level == "LOW":
        cpc_score = min(100, cpc_score + 20)

    final_score = (cpc_score * 0.5) + (intent_score * 0.5)
    breakdown = {
        "CPC & Competition": {
            "value": f"${cpc:.2f} ({competition_level})",
            "score": round(cpc_score),
            "explanation": "Normalized CPC, with bonuses for low competition and wide bid spread.",
        },
        "Strategic Intent": {
            "value": main_intent.title(),
            "score": round(intent_score),
            "explanation": explanation,
        },
    }
    return round(final_score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/competitor_performance.py
```python
from typing import Dict, Any, Tuple
import logging

logger = logging.getLogger(__name__)


def _normalize_value(
    value: float, target_value: float, is_lower_better: bool = True
) -> float:
    """Helper to normalize a value to a 0-100 scale, with target_value being ideal."""
    if value is None or target_value is None or target_value == 0:
        return 50.0  # Neutral score if data is missing or target is zero

    if is_lower_better:
        # Example: LCP target 2500ms.
        # If value is 1250, score = 100. If value is 5000, score = 0.
        # This formula provides 100 at 0, 50 at target, 0 at 2*target
        score = max(0.0, min(100.0, 100 * (1 - (value / (2 * target_value)))))
    else:
        # Example: High metric, higher is better. e.g. High security score
        score = max(
            0.0, min(100.0, 100 * (value / (2 * target_value)))
        )  # Max out at 2*target for 100, linear

    return score


def calculate_competitor_performance_score(
    opportunity: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the technical performance (e.g., Core Web Vitals)
    of top organic competitors. Weak competitor performance indicates a higher opportunity.
    """
    if not isinstance(opportunity, dict) or not opportunity.get("blueprint"):
        return 50.0, {"message": "Invalid opportunity data for scoring."}

    competitor_analysis = opportunity["blueprint"].get("competitor_analysis", [])
    if not competitor_analysis:
        return 50.0, {
            "message": "No competitor analysis available for performance scoring."
        }

    lcp_times = []
    for comp in competitor_analysis:
        if (
            comp.get("page_timing")
            and comp["page_timing"].get("largest_contentful_paint") is not None
        ):
            lcp_times.append(comp["page_timing"]["largest_contentful_paint"])

    if not lcp_times:
        return 50.0, {"message": "No LCP data available from competitors."}

    avg_lcp_ms = sum(lcp_times) / len(lcp_times)

    # Get the target LCP from client config (lower is better for performance)
    # This target defines what "good" performance is. Competitors worse than this are an opportunity.
    target_good_lcp_ms = config.get(
        "max_avg_lcp_time", 2500
    )  # Default to 2.5s as a good target

    # Score: higher if competitors' LCP is high (poor performance)
    # We want to invert the normalization: a higher LCP (worse performance) means higher score (better opportunity)
    # If avg_lcp_ms is 2*target_good_lcp_ms, score is 100. If it's target_good_lcp_ms, score is 50.
    score = _normalize_value(avg_lcp_ms, target_good_lcp_ms, is_lower_better=False)

    explanation = f"Average competitor LCP is {avg_lcp_ms:.0f}ms. Higher value indicates worse competitor performance, which is a better opportunity. Target LCP for good performance is {target_good_lcp_ms}ms."
    breakdown = {
        "Avg. Competitor LCP": {
            "value": f"{avg_lcp_ms:.0f}ms",
            "score": round(score),
            "explanation": explanation,
        }
    }

    return round(score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/competitor_weakness.py
```python
# pipeline/step_03_prioritization/scoring_components/competitor_weakness.py
from typing import Dict, Any, Tuple


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_competitor_weakness_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the authority of ranking competitors using
    data available at the discovery phase (avg_backlinks_info).
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    breakdown = {}
    avg_backlinks = (
        data.get("avg_backlinks_info")
        if isinstance(data.get("avg_backlinks_info"), dict)
        else {}
    )

    # 1. Average Main Domain Rank of competitors
    domain_rank = avg_backlinks.get("main_domain_rank", 500)
    max_rank = config.get("max_domain_rank_for_scoring", 1000)
    domain_rank_score = _normalize_value(domain_rank, max_rank, invert=True)
    breakdown["Avg. Domain Rank"] = {
        "value": f"{domain_rank:.0f}",
        "score": round(domain_rank_score),
        "explanation": f"Normalized against a max of {max_rank}. Lower is better.",
    }

    # 2. Average Referring Main Domains
    ref_domains = avg_backlinks.get("referring_main_domains", 50)
    max_ref_domains = config.get("max_referring_domains_for_scoring", 100)
    ref_domains_score = _normalize_value(ref_domains, max_ref_domains, invert=True)
    breakdown["Avg. Referring Domains"] = {
        "value": f"{ref_domains:.1f}",
        "score": round(ref_domains_score),
        "explanation": f"Normalized against a max of {max_ref_domains}. Lower is better.",
    }

    # Weighted average for final score
    final_score = (domain_rank_score * 0.6) + (ref_domains_score * 0.4)
    return round(final_score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/ease_of_ranking.py
```python
# pipeline/step_03_prioritization/scoring_components/ease_of_ranking.py
import math
from typing import Dict, Any, Tuple


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_ease_of_ranking_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """Calculates a score based on how easy it is to rank for the keyword."""
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    breakdown = {}
    keyword_props = (
        data.get("keyword_properties")
        if isinstance(data.get("keyword_properties"), dict)
        else {}
    )
    avg_backlinks = (
        data.get("avg_backlinks_info")
        if isinstance(data.get("avg_backlinks_info"), dict)
        else {}
    )
    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}

    # 1. Keyword Difficulty (KD)
    kd = keyword_props.get("keyword_difficulty", 50)
    kd_score = _normalize_value(kd, 100, invert=True)
    breakdown["Keyword Difficulty"] = {
        "value": kd,
        "score": round(kd_score),
        "explanation": "Lower is better.",
    }

    # 2. Average Main Domain Rank of competitors
    domain_rank = avg_backlinks.get("main_domain_rank", 500)
    max_rank = config.get("max_domain_rank_for_scoring", 1000)
    domain_rank_score = _normalize_value(domain_rank, max_rank, invert=True)
    breakdown["Avg. Domain Rank"] = {
        "value": f"{domain_rank:.0f}",
        "score": round(domain_rank_score),
        "explanation": f"Normalized against a max of {max_rank}. Lower is better.",
    }

    # 3. Average Page Rank of top competing pages
    page_rank = avg_backlinks.get("rank", 50)
    page_rank_score = _normalize_value(page_rank, 100, invert=True)
    breakdown["Avg. Page Rank"] = {
        "value": f"{page_rank:.0f}",
        "score": round(page_rank_score),
        "explanation": "Represents page-level authority. Lower is better.",
    }

    # 4. Dofollow Ratio
    total_backlinks = avg_backlinks.get("backlinks", 0)
    dofollow_backlinks = avg_backlinks.get("dofollow", 0)
    dofollow_ratio = dofollow_backlinks / total_backlinks if total_backlinks > 0 else 0
    dofollow_score = _normalize_value(
        dofollow_ratio, 1, invert=True
    )  # Lower ratio is better
    breakdown["Dofollow Ratio"] = {
        "value": f"{dofollow_ratio:.1%}",
        "score": round(dofollow_score),
        "explanation": "Ratio of dofollow links. A lower ratio indicates weaker competitor backlink profiles.",
    }

    # 5. Search Engine Results Count
    results_count = serp_info.get("se_results_count", 1_000_000)
    if results_count > 0:
        log_score = _normalize_value(
            math.log(results_count + 1), math.log(1_000_000_000 + 1), invert=True
        )
    else:
        log_score = 100.0
    breakdown["Total Results"] = {
        "value": f"{results_count:,}",
        "score": round(log_score),
        "explanation": "Log-normalized. Fewer competing pages is better.",
    }

    # Weighted average for final score
    final_score = (
        (kd_score * 0.40)
        + (domain_rank_score * 0.25)
        + (page_rank_score * 0.20)
        + (dofollow_score * 0.10)
        + (log_score * 0.05)
    )
    return round(final_score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/growth_trend.py
```python
# pipeline/step_03_prioritization/scoring_components/growth_trend.py
from typing import Dict, Any, Tuple
import math


def calculate_growth_trend_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a volume-weighted score based on the keyword's search volume trend.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    trends = (
        keyword_info.get("search_volume_trend")
        if isinstance(keyword_info.get("search_volume_trend"), dict)
        else {}
    )
    sv = keyword_info.get("search_volume", 0) or 0

    yearly = trends.get("yearly", 0)
    quarterly = trends.get("quarterly", 0)
    monthly = trends.get("monthly", 0)

    def score_trend(value):
        if value is None:
            return 50  # Neutral score for missing data
        if value > 25:
            return 100
        if value > 10:
            return 75
        if value < -25:
            return 0
        if value < -10:
            return 25
        return 50

    yearly_score = score_trend(yearly)
    quarterly_score = score_trend(quarterly)
    monthly_score = score_trend(monthly)

    base_trend_score = (
        (yearly_score * 0.3) + (quarterly_score * 0.4) + (monthly_score * 0.3)
    )

    # Weight the trend score by search volume magnitude
    # A log scale helps moderate the effect of massive search volumes
    sv_weight = min(
        math.log(sv + 1) / math.log(100000), 1.0
    )  # Normalize against 100k SV

    # Final score is a blend: 70% trend, 30% volume weight. This prevents tiny keywords with huge trends from dominating.
    final_score = (base_trend_score * 0.7) + (sv_weight * 100 * 0.3)

    explanation = f"Weighted score from trends (Y:{yearly}%, Q:{quarterly}%, M:{monthly}%) and search volume."
    breakdown = {
        "Growth Trend": {
            "value": f"{yearly}% YoY",
            "score": round(final_score),
            "explanation": explanation,
        }
    }
    return round(final_score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/keyword_structure.py
```python
# pipeline/step_03_prioritization/scoring_components/keyword_structure.py
from typing import Dict, Any, Tuple


def calculate_keyword_structure_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Scores the keyword based on its structure, rewarding the "long-tail sweet spot"
    and adding a bonus for search depth.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword = data.get("keyword", "")
    word_count = len(keyword.split())
    depth = data.get("depth", 0)  # From related_keywords endpoint

    # Score is based on word count, with a peak at the 4-6 word sweet spot
    if word_count >= 4 and word_count <= 6:
        score = 100.0
    elif word_count == 3 or word_count == 7:
        score = 75.0
    elif word_count == 2 or word_count == 8:
        score = 50.0
    else:  # 1 word or 9+ words
        score = 25.0

    # Add a bonus for depth, rewarding more specific queries
    if depth > 0:
        score = min(100, score + (depth * 5))  # +5 points per depth level

    explanation = f"Keyword has {word_count} words and search depth of {depth}. The 4-6 word range is the sweet spot."
    breakdown = {
        "Keyword Structure": {
            "value": f"{word_count} words (Depth: {depth})",
            "score": score,
            "explanation": explanation,
        }
    }

    return score, breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/serp_crowding.py
```python
# pipeline/step_03_prioritization/scoring_components/serp_crowding.py
from typing import Dict, Any, Tuple


def calculate_serp_crowding_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on how crowded the SERP is with attention-grabbing features.
    A less crowded SERP is a better opportunity.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    serp_types = set(serp_info.get("serp_item_types", []))

    # Define features that compete for user attention
    CROWDING_FEATURES = {
        "video",
        "short_videos",
        "images",
        "people_also_ask",
        "carousel",
        "multi_carousel",
        "featured_snippet",
        "ai_overview",
    }

    crowding_feature_count = len(serp_types.intersection(CROWDING_FEATURES))

    # The score is inverted: more features = lower score
    if crowding_feature_count >= 5:
        score = 0.0
    elif crowding_feature_count == 4:
        score = 25.0
    elif crowding_feature_count == 3:
        score = 50.0
    elif crowding_feature_count == 2:
        score = 75.0
    elif crowding_feature_count == 1:
        score = 90.0
    else:
        score = 100.0

    explanation = f"{crowding_feature_count} attention-grabbing features found. A lower count is better."
    breakdown = {
        "SERP Crowding": {
            "value": crowding_feature_count,
            "score": score,
            "explanation": explanation,
        }
    }

    return score, breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/serp_features.py
```python
# pipeline/step_03_prioritization/scoring_components/serp_features.py
from typing import Dict, Any, Tuple


def calculate_serp_features_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the SERP environment, rewarding opportunities
    and penalizing attention-grabbing distractions.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    serp_types = set(serp_info.get("serp_item_types", []))

    score = 50.0  # Start with a neutral base score
    notes = []

    # Positive modifiers for high-value features
    if "featured_snippet" in serp_types:
        score += config.get("featured_snippet_bonus", 40)
        notes.append("Featured Snippet (+40)")
    if "people_also_ask" in serp_types:
        score += 25
        notes.append("People Also Ask (+25)")

    # Negative modifiers for threats and attention-grabbing features
    if "ai_overview" in serp_types:
        score -= config.get("ai_overview_penalty", 20)
        notes.append("AI Overview (-20)")
    if "video" in serp_types or "short_videos" in serp_types:
        score -= 15
        notes.append("Video Results (-15)")
    if "images" in serp_types:
        score -= 10
        notes.append("Image Carousel (-10)")

    final_score = max(0, min(100.0, score))
    explanation = (
        "Score reflects SERP opportunities. " + ", ".join(notes)
        if notes
        else "Neutral SERP environment."
    )

    breakdown = {
        "SERP Opportunity": {
            "value": len(notes),
            "score": final_score,
            "explanation": explanation.strip(),
        }
    }
    return final_score, breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/serp_freshness.py
```python
# pipeline/step_03_prioritization/scoring_components/serp_freshness.py
from typing import Dict, Any, Tuple
from datetime import datetime


def calculate_serp_freshness_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on SERP freshness. An older SERP is a better opportunity.
    """
    if not isinstance(data, dict):
        return 50.0, {
            "Freshness": {
                "value": "N/A",
                "score": 50,
                "explanation": "Invalid data format.",
            }
        }

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    last_update_str = serp_info.get("last_updated_time")

    if not last_update_str:
        return 50.0, {
            "Freshness": {
                "value": "N/A",
                "score": 50,
                "explanation": "No freshness data.",
            }
        }

    try:
        last_update = datetime.fromisoformat(last_update_str.replace(" +00:00", ""))
        days_since_update = (datetime.now() - last_update).days

        # Score increases as the SERP gets older
        if days_since_update > 90:
            score = 100.0
        elif days_since_update > 60:
            score = 80.0
        elif days_since_update > 30:
            score = 60.0
        elif days_since_update > 14:
            score = 40.0
        else:
            score = 20.0

        explanation = f"SERP last updated {days_since_update} days ago. Older SERPs are better opportunities."
        breakdown = {
            "Freshness": {
                "value": f"{days_since_update} days",
                "score": score,
                "explanation": explanation,
            }
        }
        return score, breakdown

    except (ValueError, TypeError):
        return 50.0, {
            "Freshness": {
                "value": "Error",
                "score": 50,
                "explanation": "Could not parse update timestamp.",
            }
        }
```

## File: pipeline/step_03_prioritization/scoring_components/serp_threat.py
```python
# pipeline/step_03_prioritization/scoring_components/serp_threat.py
from typing import Dict, Any, Tuple


def calculate_serp_threat_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a unified "threat" score for the SERP. A lower score is better.
    This score is inverted for the final calculation (higher threat = lower opportunity).
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    serp_types = set(serp_info.get("serp_item_types", []))

    threat_level = 0
    notes = []

    # Threat 1: Hostile, non-blog features
    HOSTILE_FEATURES = {
        "shopping",
        "popular_products",
        "local_pack",
        "google_flights",
        "google_hotels",
        "app",
        "jobs",
        "math_solver",
        "currency_box",
    }
    found_hostile = serp_types.intersection(HOSTILE_FEATURES)
    if found_hostile:
        threat_level += 50
        notes.append(f"Hostile features found ({', '.join(found_hostile)})")

    # Threat 2: AI Overview
    if "ai_overview" in serp_types:
        threat_level += config.get("ai_overview_penalty", 25)
        notes.append("AI Overview is present")

    # Threat 3: Paid Ads (implicit threat)
    if "paid" in serp_types:
        threat_level += 10
        notes.append("Paid ads are present")

    # Normalize the threat level to a 0-100 score
    final_threat_score = min(100, threat_level)

    # The final score is inverted: 100 is low threat, 0 is high threat.
    opportunity_score = 100 - final_threat_score

    explanation = (
        "Score reflects threats to organic CTR. " + "; ".join(notes)
        if notes
        else "No major threats found."
    )
    breakdown = {
        "SERP Threat": {
            "value": f"{final_threat_score}%",
            "score": opportunity_score,
            "explanation": explanation,
        }
    }

    return opportunity_score, breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/serp_volatility.py
```python
# pipeline/step_03_prioritization/scoring_components/serp_volatility.py
from typing import Dict, Any, Tuple
from datetime import datetime


def calculate_serp_volatility_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on SERP stability. A more volatile SERP can be an opportunity.
    """
    if not isinstance(data, dict):
        return 50.0, {
            "SERP Stability": {
                "value": "N/A",
                "score": 50,
                "explanation": "Invalid data format.",
            }
        }

    serp_info = data.get("serp_info") if isinstance(data.get("serp_info"), dict) else {}
    breakdown = {}

    last_update_str = serp_info.get("last_updated_time")
    prev_update_str = serp_info.get("previous_updated_time")

    if not last_update_str or not prev_update_str:
        return 50.0, {
            "SERP Stability": {
                "value": "N/A",
                "score": 50,
                "explanation": "Insufficient data to calculate SERP volatility.",
            }
        }

    try:
        last_update = datetime.fromisoformat(last_update_str.replace(" +00:00", ""))
        prev_update = datetime.fromisoformat(prev_update_str.replace(" +00:00", ""))
        days_between_updates = (last_update - prev_update).days

        stable_threshold = config.get("serp_volatility_stable_threshold_days", 30)

        score = 0.0
        if days_between_updates < 7:  # Highly volatile
            score = 100.0
        elif days_between_updates < 21:  # Moderately volatile
            score = 75.0
        elif days_between_updates < stable_threshold:  # Relatively stable
            score = 50.0
        else:  # Very stable
            score = 25.0

        explanation = f"SERP updated every {days_between_updates} days. More frequent updates can signal an opportunity."
        breakdown["SERP Stability"] = {
            "value": f"{days_between_updates} days",
            "score": score,
            "explanation": explanation,
        }
        return score, breakdown

    except (ValueError, TypeError):
        return 50.0, {
            "SERP Stability": {
                "value": "Error",
                "score": 50,
                "explanation": "Could not parse SERP update timestamps.",
            }
        }
```

## File: pipeline/step_03_prioritization/scoring_components/traffic_potential.py
```python
# pipeline/step_03_prioritization/scoring_components/traffic_potential.py
from typing import Dict, Any, Tuple


def _normalize_value(value: float, max_value: float, invert: bool = False) -> float:
    """Helper to normalize a value to a 0-100 scale."""
    if value is None or max_value is None or max_value == 0:
        return 0.0

    normalized = min(float(value) / float(max_value), 1.0)

    if invert:
        return (1 - normalized) * 100
    return normalized * 100


def calculate_traffic_potential_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a blended score based on both commercial traffic value and raw audience size.
    """
    if not isinstance(data, dict):
        return 0, {"message": "Invalid data format for scoring."}

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    sv = keyword_info.get("search_volume", 0) or 0
    cpc = keyword_info.get("cpc", 0.0) or 0.0

    # 1. Calculate Traffic Value Score
    traffic_value = sv * cpc
    max_traffic_value = config.get("max_traffic_value_for_scoring", 50000)
    traffic_value_score = _normalize_value(traffic_value, max_traffic_value)

    # 2. Calculate Raw Search Volume Score
    max_sv = config.get("max_sv_for_scoring", 100000)
    raw_sv_score = _normalize_value(sv, max_sv)

    # 3. Blend the scores to balance commercial value and audience size
    final_score = (traffic_value_score * 0.7) + (raw_sv_score * 0.3)

    explanation = f"Blended score: 70% from Est. Traffic Value (${traffic_value:,.0f}) and 30% from Raw SV ({sv})."
    breakdown = {
        "Traffic Potential": {
            "value": f"{sv} SV | ${cpc:.2f} CPC",
            "score": round(final_score),
            "explanation": explanation,
        }
    }

    return round(final_score), breakdown
```

## File: pipeline/step_03_prioritization/scoring_components/volume_volatility.py
```python
# pipeline/step_03_prioritization/scoring_components/volume_volatility.py
import numpy as np
from typing import Dict, Any, Tuple


def calculate_volume_volatility_score(
    data: Dict[str, Any], config: Dict[str, Any]
) -> Tuple[float, Dict[str, Any]]:
    """
    Calculates a score based on the stability of monthly search volume.
    Lower volatility is generally better for long-term planning.
    """
    if not isinstance(data, dict):
        return 50.0, {
            "Volatility": {
                "value": "N/A",
                "score": 50,
                "explanation": "Invalid data format.",
            }
        }

    keyword_info = (
        data.get("keyword_info") if isinstance(data.get("keyword_info"), dict) else {}
    )
    monthly_searches = keyword_info.get("monthly_searches", [])

    if not monthly_searches or len(monthly_searches) < 3:
        return 50.0, {
            "Volatility": {
                "value": "N/A",
                "score": 50,
                "explanation": "Insufficient data.",
            }
        }

    volumes = [
        ms["search_volume"]
        for ms in monthly_searches
        if isinstance(ms, dict)
        and ms.get("search_volume") is not None
        and ms["search_volume"] > 0
    ]
    if len(volumes) < 3:
        return 50.0, {
            "Volatility": {
                "value": "N/A",
                "score": 50,
                "explanation": "Insufficient data.",
            }
        }

    mean_volume = np.mean(volumes)
    std_dev = np.std(volumes)

    if mean_volume == 0:
        return 0.0, {
            "Volatility": {"value": "0", "score": 0, "explanation": "No search volume."}
        }

    coeff_of_variation = std_dev / mean_volume

    # Score is inverted: higher volatility = lower score
    # A CoV of 0.5 (50%) is considered moderately high.
    score = max(0, 100 - (coeff_of_variation * 150))  # Scale the penalty

    explanation = (
        f"Coefficient of Variation: {coeff_of_variation:.2%}. Lower is more stable."
    )
    breakdown = {
        "Volatility": {
            "value": f"{coeff_of_variation:.2%}",
            "score": round(score),
            "explanation": explanation,
        }
    }

    return round(score), breakdown
```

## File: pipeline/step_03_prioritization/__init__.py
```python
# pipeline/step_03_prioritization/__init__.py
```

## File: pipeline/step_03_prioritization/run_prioritization.py
```python
import logging
from typing import List, Dict, Any

from .scoring_engine import ScoringEngine


def run_prioritization_phase(
    opportunities: List[Dict[str, Any]], client_cfg: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Orchestrates the prioritization phase.

    1. Scores each opportunity based on a weighted formula.
    2. Sorts the opportunities by their calculated score.

    Returns a sorted list of opportunities with scoring data.
    """
    logger = logging.getLogger(__name__)
    logger.info("--- Starting Prioritization Phase ---")

    scoring_engine = ScoringEngine(client_cfg)

    # 1. Score Opportunities
    scored_opportunities = []
    for opp in opportunities:
        score, score_breakdown = scoring_engine.calculate_score(opp)
        opp["strategic_score"] = score
        # Add the focused competition score directly into the breakdown for persistence
        if "low_competition_score" in score_breakdown:
            opp["low_competition_score"] = score_breakdown["low_competition_score"][
                "score"
            ]
        opp["score_breakdown"] = score_breakdown
        scored_opportunities.append(opp)

    # 2. Sort Opportunities
    sorted_opportunities = sorted(
        scored_opportunities, key=lambda x: x["strategic_score"], reverse=True
    )

    logger.info(f"  -> Scored and sorted {len(sorted_opportunities)} opportunities.")
    logger.info("--- Prioritization Phase Complete ---")

    return sorted_opportunities
```

## File: pipeline/step_03_prioritization/scoring_engine.py
```python
import logging
from typing import Dict, Any, Tuple
from .scoring_components import (
    calculate_ease_of_ranking_score,
    calculate_traffic_potential_score,
    calculate_commercial_intent_score,
    calculate_growth_trend_score,
    calculate_serp_features_score,
    calculate_serp_volatility_score,
    calculate_competitor_weakness_score,
    calculate_serp_crowding_score,
    calculate_keyword_structure_score,
    calculate_serp_threat_score,
    calculate_volume_volatility_score,
    calculate_serp_freshness_score,
    calculate_competitor_performance_score,  # ADDED THIS IMPORT
)


class ScoringEngine:
    """
    Calculates a strategic score for each keyword opportunity by orchestrating
    a suite of modular scoring components.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def calculate_score(
        self, opportunity: Dict[str, Any]
    ) -> Tuple[float, Dict[str, Any]]:
        """
        Calculates the final opportunity score by combining weighted scores
        from all registered scoring components.
        """
        if not isinstance(opportunity, dict):
            self.logger.warning(
                "Invalid data format passed to calculate_score. Expected a dictionary."
            )
            return 0.0, {"error": "Invalid data format."}

        breakdown = {}
        data_source = opportunity.get("full_data", opportunity)

        # --- Execute all scoring components ---
        ease_score, ease_breakdown = calculate_ease_of_ranking_score(
            data_source, self.config
        )
        breakdown["ease_of_ranking"] = {
            "name": "Ease of Ranking",
            "score": ease_score,
            "breakdown": ease_breakdown,
        }

        traffic_score, traffic_breakdown = calculate_traffic_potential_score(
            data_source, self.config
        )
        breakdown["traffic_potential"] = {
            "name": "Traffic Potential",
            "score": traffic_score,
            "breakdown": traffic_breakdown,
        }

        intent_score, intent_breakdown = calculate_commercial_intent_score(
            data_source, self.config
        )
        breakdown["commercial_intent"] = {
            "name": "Commercial Intent",
            "score": intent_score,
            "breakdown": intent_breakdown,
        }

        trend_score, trend_breakdown = calculate_growth_trend_score(
            data_source, self.config
        )
        breakdown["growth_trend"] = {
            "name": "Growth Trend",
            "score": trend_score,
            "breakdown": trend_breakdown,
        }

        features_score, features_breakdown = calculate_serp_features_score(
            data_source, self.config
        )
        breakdown["serp_features"] = {
            "name": "SERP Opportunity",
            "score": features_score,
            "breakdown": features_breakdown,
        }

        volatility_score, volatility_breakdown = calculate_serp_volatility_score(
            data_source, self.config
        )
        breakdown["serp_volatility"] = {
            "name": "SERP Volatility",
            "score": volatility_score,
            "breakdown": volatility_breakdown,
        }

        weakness_score, weakness_breakdown = calculate_competitor_weakness_score(
            data_source, self.config
        )
        breakdown["competitor_weakness"] = {
            "name": "Competitor Weakness",
            "score": weakness_score,
            "breakdown": weakness_breakdown,
        }

        crowding_score, crowding_breakdown = calculate_serp_crowding_score(
            data_source, self.config
        )
        breakdown["serp_crowding"] = {
            "name": "SERP Crowding",
            "score": crowding_score,
            "breakdown": crowding_breakdown,
        }

        structure_score, structure_breakdown = calculate_keyword_structure_score(
            data_source, self.config
        )
        breakdown["keyword_structure"] = {
            "name": "Keyword Structure",
            "score": structure_score,
            "breakdown": structure_breakdown,
        }

        threat_score, threat_breakdown = calculate_serp_threat_score(
            data_source, self.config
        )
        breakdown["serp_threat"] = {
            "name": "SERP Threat",
            "score": threat_score,
            "breakdown": threat_breakdown,
        }

        volume_volatility_score, volume_volatility_breakdown = (
            calculate_volume_volatility_score(data_source, self.config)
        )
        breakdown["volume_volatility"] = {
            "name": "Volume Volatility",
            "score": volume_volatility_score,
            "breakdown": volume_volatility_breakdown,
        }

        freshness_score, freshness_breakdown = calculate_serp_freshness_score(
            data_source, self.config
        )
        breakdown["serp_freshness"] = {
            "name": "SERP Freshness",
            "score": freshness_score,
            "breakdown": freshness_breakdown,
        }

        performance_score, performance_breakdown = (
            calculate_competitor_performance_score(opportunity, self.config)
        )
        breakdown["competitor_performance"] = {
            "name": "Competitor Tech Performance",
            "score": performance_score,
            "breakdown": performance_breakdown,
        }
        # --- Apply weights from config and calculate final score ---
        weights = {
            "ease": self.config.get("ease_of_ranking_weight", 25),
            "traffic": self.config.get("traffic_potential_weight", 20),
            "intent": self.config.get("commercial_intent_weight", 15),
            "weakness": self.config.get("competitor_weakness_weight", 10),
            "structure": self.config.get("keyword_structure_weight", 5),
            "trend": self.config.get("growth_trend_weight", 5),
            "features": self.config.get("serp_features_weight", 5),
            "crowding": self.config.get("serp_crowding_weight", 5),
            "volatility": self.config.get("serp_volatility_weight", 5),
            "threat": self.config.get("serp_threat_weight", 5),
            "freshness": self.config.get("serp_freshness_weight", 0),
            "competitor_performance": self.config.get(
                "competitor_performance_weight", 5
            ),  # ADDED THIS LINE
            "volume_volatility": self.config.get("volume_volatility_weight", 0),
        }

        total_weight = sum(weights.values())
        if total_weight == 0:
            return 0.0, breakdown  # Avoid division by zero

        final_score = (
            (ease_score * weights["ease"])
            + (traffic_score * weights["traffic"])
            + (intent_score * weights["intent"])
            + (weakness_score * weights["weakness"])
            + (structure_score * weights["structure"])
            + (trend_score * weights["trend"])
            + (features_score * weights["features"])
            + (crowding_score * weights["crowding"])
            + (volatility_score * weights["volatility"])
            + (threat_score * weights["threat"])
            + (freshness_score * weights["freshness"])
            + (volume_volatility_score * weights["volume_volatility"])
            + (performance_score * weights["competitor_performance"])  # ADDED THIS LINE
        ) / total_weight

        for key, breakdown_data in breakdown.items():
            # Map breakdown key to weight key
            weight_key_map = {
                "ease_of_ranking": "ease",
                "traffic_potential": "traffic",
                "commercial_intent": "intent",
                "competitor_weakness": "weakness",
                "keyword_structure": "structure",
                "growth_trend": "trend",
                "serp_features": "features",
                "serp_crowding": "crowding",
                "serp_volatility": "volatility",
                "serp_threat": "threat",
                "volume_volatility": "volume_volatility",
                "serp_freshness": "freshness",
                "competitor_performance": "competitor_performance",  # ADDED THIS LINE
            }
            weight_key = weight_key_map.get(key, "")
            breakdown_data["weight"] = weights.get(weight_key, 0)

        return round(final_score, 2), breakdown
```

## File: pipeline/step_04_analysis/content_analysis_modules/ai_intelligence_caller.py
```python
from typing import List, Dict, Any, Tuple
from external_apis.openai_client import OpenAIClientWrapper


def get_ai_content_analysis(
    openai_client: OpenAIClientWrapper,
    messages: List[Dict[str, str]],
    model: str,
    max_completion_tokens: int,
) -> Tuple[Dict[str, Any], str]:
    """
    Calls the OpenAI API to get content analysis and returns the response and any error.
    """
    schema = {
        "name": "extract_deep_content_insights",
        "type": "object",
        "properties": {
            "unique_angles_to_include": {"type": "array", "items": {"type": "string"}},
            "key_entities_from_competitors": {
                "type": "array",
                "items": {"type": "string"},
            },
            "core_questions_answered_by_competitors": {
                "type": "array",
                "items": {"type": "string"},
            },
            "identified_content_gaps": {"type": "array", "items": {"type": "string"}},
        },
        "required": [
            "unique_angles_to_include",
            "key_entities_from_competitors",
            "core_questions_answered_by_competitors",
            "identified_content_gaps",
        ],
        "additionalProperties": False,
    }

    response, error = openai_client.call_chat_completion(
        messages=messages,
        schema=schema,
        model=model,
        max_completion_tokens=max_completion_tokens,
    )

    return response, error
```

## File: pipeline/step_04_analysis/content_analysis_modules/heading_analyzer.py
```python
from typing import List, Dict, Any
from collections import Counter


def extract_common_headings(
    competitor_analysis: List[Dict[str, Any]], num_headings: int
) -> List[str]:
    """Extracts the most common H2 and H3 headings from competitor data."""
    all_headings = Counter(
        h
        for c in competitor_analysis
        if c.get("headings")
        for h_type in ["h2", "h3"]
        for h in c["headings"].get(h_type, [])
    )
    return [h for h, count in all_headings.most_common(num_headings)]
```

## File: pipeline/step_04_analysis/content_analysis_modules/metric_analyzer.py
```python
from typing import List, Dict, Any, Optional


def calculate_average_word_count(competitor_analysis: List[Dict[str, Any]]) -> int:
    """Calculates the average word count from a list of competitor data."""
    word_counts = [
        c.get("word_count") for c in competitor_analysis if c and c.get("word_count")
    ]
    return int(sum(word_counts) / len(word_counts)) if word_counts else 1500


def calculate_average_readability(
    competitor_analysis: List[Dict[str, Any]],
) -> Optional[float]:
    """Calculates the average readability score from a list of competitor data."""
    readability_scores = [
        c.get("readability_score")
        for c in competitor_analysis
        if c.get("readability_score") is not None
    ]
    return (
        sum(readability_scores) / len(readability_scores)
        if readability_scores
        else None
    )
```

## File: pipeline/step_04_analysis/__init__.py
```python
# pipeline/step_04_analysis/__init__.py
```

## File: pipeline/step_04_analysis/competitor_analyzer.py
```python
import logging
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import urlparse
import textstat

from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class FullCompetitorAnalyzer:
    """
    Performs a deep-dive analysis of top organic competitors using the OnPage Instant Pages API.
    """

    def __init__(self, client: DataForSEOClientV2, config: Dict[str, Any]):
        self.client = client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.min_word_count = self.config.get("min_competitor_word_count", 300)

        # Combine all blacklists dynamically

        excluded_domains_config = self.config.get(
            "competitor_analysis_excluded_domains", []
        )

        if isinstance(excluded_domains_config, str):
            excluded_domains = set(
                d.strip() for d in excluded_domains_config.split(",")
            )

        else:
            excluded_domains = set(excluded_domains_config)

        self.blacklist_domains = excluded_domains.union(
            set(self.config.get("ugc_and_parasite_domains", []))
        )

    def analyze_competitors(
        self, competitor_urls: List[str], selected_urls: Optional[List[str]] = None
    ) -> Tuple[List[Dict[str, Any]], float]:
        """
        Fetches and analyzes competitor data using a two-tier, adaptive fetching strategy.
        First attempts a cheap scan without JS, then retries failures with JS enabled.
        """
        urls_to_scan = selected_urls or competitor_urls
        if not urls_to_scan:
            return [], 0.0

        total_api_cost = 0.0
        successful_results = []
        urls_that_need_js_retry = []

        # --- Tier 1: Fast, cheap scan with JavaScript DISABLED ---
        self.logger.info(
            f"Starting Tier 1 analysis for {len(urls_to_scan)} URLs (JS disabled)."
        )
        try:
            initial_tasks, initial_cost = self.client.get_content_onpage_data(
                urls_to_scan, self.config, enable_javascript=False
            )
            total_api_cost += initial_cost

            for task in initial_tasks:
                task_url = task.get("data", {}).get("url")

                if task.get("result") is None:
                    self.logger.warning(
                        f"Tier 1 scan for {task_url} returned a null result. Queuing for JS-enabled retry."
                    )
                    urls_that_need_js_retry.append(task_url)
                    continue

                result = task.get("result", [{}])[0]

                if (
                    task.get("status_code") == 20000
                    and result.get("crawl_status") != "Page content is empty"
                    and result.get("items_count", 0) > 0
                ):
                    successful_results.extend(result.get("items", []))
                else:
                    self.logger.warning(
                        f"Tier 1 scan failed for {task_url}. Reason: {result.get('crawl_status', task.get('status_message'))}. Queuing for JS-enabled retry."
                    )
                    urls_that_need_js_retry.append(task_url)
        except Exception as e:
            self.logger.error(
                f"Error during Tier 1 competitor analysis: {e}", exc_info=True
            )

        # --- Tier 2: Slower, more expensive scan with JavaScript ENABLED for failures ---
        if urls_that_need_js_retry:
            self.logger.info(
                f"Starting Tier 2 analysis for {len(urls_that_need_js_retry)} failed URLs (JS enabled)."
            )
            try:
                retry_tasks, retry_cost = self.client.get_content_onpage_data(
                    urls_that_need_js_retry, self.config, enable_javascript=True
                )
                total_api_cost += retry_cost

                for task in retry_tasks:
                    task_url = task.get("data", {}).get("url")

                    if task.get("result") is None:
                        self.logger.error(
                            f"Tier 2 retry FAILED for {task_url}. Reason: API returned a null result. This URL will be excluded from analysis."
                        )
                        continue

                    result = task.get("result", [{}])[0]

                    if (
                        task.get("status_code") == 20000
                        and result.get("items_count", 0) > 0
                    ):
                        self.logger.info(
                            f"Tier 2 JS-enabled retry SUCCEEDED for {task_url}."
                        )
                        successful_results.extend(result.get("items", []))
                    else:
                        self.logger.error(
                            f"Tier 2 retry FAILED for {task_url}. Reason: {result.get('crawl_status', task.get('status_message'))}. This URL will be excluded from analysis."
                        )
            except Exception as e:
                self.logger.error(
                    f"Error during Tier 2 competitor analysis: {e}", exc_info=True
                )

        # --- Final Processing ---
        final_competitor_list = self._process_content_parsing_results(
            successful_results
        )

        return final_competitor_list, total_api_cost

    def _process_content_parsing_results(
        self, results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Processes the successful results from the Content Parsing API call into a standardized competitor object.
        """
        final_competitors = []
        for result in results:
            url = result.get("url")  # URL is at the top level in the new API response
            if not url or result.get("status_code") != 200:
                continue

            domain = urlparse(url).netloc
            if domain in self.blacklist_domains:
                self.logger.info(f"Skipping blacklisted competitor: {domain}")
                continue

            page_content = result.get("page_content", {})
            main_topic_content = ""
            headings = {"h1": [], "h2": [], "h3": [], "h4": [], "h5": [], "h6": []}

            # Extract main content and headings from the structured 'main_topic' array
            if page_content and page_content.get("main_topic"):
                for topic in page_content["main_topic"]:
                    h_level = topic.get("level")
                    h_title = topic.get("h_title")
                    if h_level and h_title:
                        tag = f"h{h_level}"
                        if tag in headings:
                            headings[tag].append(h_title)

                    if topic.get("primary_content"):
                        for pc in topic["primary_content"]:
                            if pc and pc.get("text"):
                                main_topic_content += pc["text"] + " "

            main_topic_content = main_topic_content.strip()

            # Manually calculate word count and readability
            word_count = len(main_topic_content.split())
            readability_score = None
            if (
                word_count > 100
            ):  # textstat needs a reasonable amount of text to be accurate
                try:
                    readability_score = textstat.flesch_kincaid_grade(
                        main_topic_content
                    )
                except Exception as e:
                    self.logger.warning(
                        f"Could not calculate readability for {url}: {e}"
                    )

            if word_count >= self.min_word_count:
                processed_competitor = {
                    "url": url,
                    "title": headings["h1"][0] if headings.get("h1") else None,
                    "word_count": word_count,
                    "readability_score": readability_score,
                    "headings": headings,
                    "main_content_text": main_topic_content,  # Clean text for readability calculation
                    "full_content_markdown": result.get(
                        "page_as_markdown"
                    ),  # Clean markdown for AI analysis
                    # Set technical fields to defaults as they are not available from this endpoint
                    "technical_warnings": [],
                    "page_timing": {},
                    "onpage_score": None,
                }
                final_competitors.append(processed_competitor)
            else:
                self.logger.info(
                    f"Skipping competitor {url} due to low parsed word count: {word_count}"
                )

        return final_competitors
```

## File: pipeline/step_04_analysis/content_analyzer.py
```python
import logging
from typing import List, Dict, Any, Tuple

from external_apis.openai_client import OpenAIClientWrapper

# Keep these imports if you want to reuse them for the deep-dive path
from .content_analysis_modules.ai_intelligence_caller import get_ai_content_analysis


class ContentAnalyzer:
    """
    Orchestrates the analysis of competitor content (or SERP data) to synthesize intelligence.
    """

    def __init__(self, openai_client: OpenAIClientWrapper, config: Dict[str, Any]):
        self.openai_client = openai_client
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.num_common_headings = self.config.get("num_common_headings", 8)
        self.num_unique_angles = self.config.get("num_unique_angles", 5)
        self.max_words_for_ai_analysis = self.config.get(
            "max_words_for_ai_analysis", 2000
        )
        self.num_competitors_for_ai_analysis = self.config.get(
            "num_competitors_for_ai_analysis", 3
        )

    # --- START MODIFICATION ---
    def synthesize_content_intelligence(
        self,
        keyword: str,
        serp_overview: Dict[str, Any],
        competitor_analysis: List[
            Dict[str, Any]
        ],  # This will be empty if deep analysis is skipped
    ) -> Tuple[Dict[str, Any], float]:
        """
        Synthesizes content intelligence by orchestrating data preparation and AI analysis.
        Conditionally uses deep competitor content or rich SERP data.
        """
        if competitor_analysis:
            from .content_analysis_modules.ai_content_preparer import (
                prepare_competitor_content_for_ai,
            )
            from .content_analysis_modules.ai_prompt_builder import (
                get_ai_prompt_messages,
            )

            self.logger.info(
                "Synthesizing intelligence from deep competitor content analysis (legacy path)."
            )

            # 1. Prepare Competitor Content for AI
            content_for_ai, using_markdown = prepare_competitor_content_for_ai(
                competitor_analysis,
                self.num_competitors_for_ai_analysis,
                self.max_words_for_ai_analysis,
            )

            # 2. Build AI Prompt (legacy)
            ai_prompt_messages = get_ai_prompt_messages(
                keyword, content_for_ai, using_markdown
            )

        else:
            self.logger.info(
                "Synthesizing intelligence from rich SERP data (new path)."
            )
            # 1. Prepare SERP Data for AI (already extracted by FullSerpAnalyzer)
            # We just need to ensure it's structured for the prompt.
            # All the new fields are already in serp_overview.

            # 2. Build AI Prompt (new, SERP-only)
            ai_prompt_messages = self._build_synthesis_prompt_from_serp(
                keyword, serp_overview
            )

        # 3. Call AI for Analysis (common to both paths)
        ai_analysis_response, error = get_ai_content_analysis(
            openai_client=self.openai_client,
            messages=ai_prompt_messages,
            model=self.config.get("default_model", "gpt-5-nano"),
            max_completion_tokens=self.config.get(
                "max_completion_tokens_for_generation", 4096
            ),
        )
        total_ai_cost = self.openai_client.latest_cost

        if error or not ai_analysis_response:
            self.logger.error(f"Failed to get deep content analysis from AI: {error}")
            return {
                "analysis_error": f"AI-powered content intelligence failed. Reason: {error}"
            }, total_ai_cost

        # 4. Assemble Final Intelligence Object
        ai_analysis_response["unique_angles_to_include"] = list(
            set(ai_analysis_response.get("unique_angles_to_include", []))
        )[: self.num_unique_angles]

        # --- NEW: Incorporate AI Overview Sources into AI Content Brief
        if (
            serp_overview.get("ai_overview_sources") and not competitor_analysis
        ):  # Only for SERP-only mode
            if "source_and_inspiration_content" not in ai_analysis_response:
                ai_analysis_response["source_and_inspiration_content"] = {}
            ai_analysis_response["source_and_inspiration_content"][
                "ai_overview_sources"
            ] = serp_overview["ai_overview_sources"]

        return ai_analysis_response, total_ai_cost

    # New private method for SERP-only prompt building
    def _build_synthesis_prompt_from_serp(
        self, keyword: str, serp_data: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """
        Builds a comprehensive prompt for AI content intelligence synthesis
        based purely on rich SERP data.
        """
        system_prompt = "You are a world-class SEO content strategist. Your task is to analyze structured SERP data to reverse-engineer a winning content strategy. Your insights must be actionable and highly specific."

        prompt_sections = [f'**Primary Keyword:** "{keyword}"\n']

        if serp_data.get("knowledge_graph_facts"):
            facts_list = '\n- '.join(serp_data['knowledge_graph_facts'])
            prompt_sections.append(
                f"**Verified Facts from Knowledge Graph (Incorporate these as core facts):**\n- {facts_list}\n"
            )

        if serp_data.get("paid_ad_copy"):
            paid_titles = [ad["title"] for ad in serp_data["paid_ad_copy"]]
            paid_descriptions = [ad["description"] for ad in serp_data["paid_ad_copy"]]
            prompt_sections.append(
                f"**High-Conversion Language from Top Paid Ads (Analyze for compelling headlines/intro/CTAs):**\n- Titles: {paid_titles}\n- Descriptions: {paid_descriptions}\n"
            )

        if serp_data.get("top_organic_sitelinks"):
            sitelinks_list = '\n- '.join(serp_data['top_organic_sitelinks'])
            prompt_sections.append(
                f"**High-Priority Subtopics from Competitor Sitelinks (Must include these as H2/H3s):**\n- {sitelinks_list}\n"
            )

        if serp_data.get("top_organic_faqs"):
            faqs_list = '\n- '.join(serp_data['top_organic_faqs'])
            prompt_sections.append(
                f"**High-Priority Questions from Competitor FAQ Snippets (Must include these in a dedicated FAQ section):**\n- {faqs_list}\n"
            )

        if serp_data.get("ai_overview_sources"):
            sources_list = '\n- '.join(serp_data['ai_overview_sources'])
            prompt_sections.append(
                f"**Authoritative Sources Used by Google's AI Overview (Give analytical priority to concepts from these sources):**\n- {sources_list}\n"
            )

        if serp_data.get("discussion_snippets"):
            snippets_list = '\n- '.join(serp_data['discussion_snippets'])
            prompt_sections.append(
                f"**Voice of the Customer from Discussions/Forums (Analyze for tone, pain points, and authentic perspective):**\n- {snippets_list}\n"
            )

        # Add basic organic results for general context
        if serp_data.get("top_organic_results"):
            org_titles_desc_list = '\n- '.join([
                f"Title: {r['title']}\nDescription: {r['description']}"
                for r in serp_data["top_organic_results"]
            ])
            prompt_sections.append(
                f"**Top Organic Result Snippets (for general content analysis):**\n- {org_titles_desc_list}\n"
            )

        if serp_data.get("people_also_ask"):
            paa_list = '\n- '.join(serp_data['people_also_ask'])
            prompt_sections.append(
                f"**People Also Ask Questions:**\n- {paa_list}\n"
            )
        if serp_data.get("related_searches"):
            related_list = '\n- '.join(serp_data['related_searches'])
            prompt_sections.append(
                f"**Related Searches:**\n- {related_list}\n"
            )
        if serp_data.get("ai_overview_content"):
            prompt_sections.append(
                f"**Google's AI Overview Content:**\n{serp_data['ai_overview_content']}\n"
            )
        if serp_data.get("featured_snippet_content"):
            prompt_sections.append(
                f"**Featured Snippet Content:**\n{serp_data['featured_snippet_content']}\n"
            )

        user_prompt_content = f"""
        Analyze the following comprehensive SERP intelligence report to generate a content strategy blueprint.

        {"".join(prompt_sections)}

        **Your Analysis Task:**
        1.  **Unique Angles & Insights:** Based on ALL the provided SERP data (Knowledge Graph, Paid Ads, FAQs, Sitelinks, AI Overview sources, discussions, organic snippets), identify 2-3 truly unique value propositions or content differentiation angles. Where are the gaps and opportunities for our content to stand out as superior?
        2.  **Key Entities:** List the 5-10 most critical entities (people, products, brands, concepts) from the entire SERP. These must be central to the topic.
        3.  **Core Questions Answered:** Synthesize the 5-7 most fundamental user questions that this keyword intends to answer, drawing from PAA, FAQ snippets, and top organic descriptions. These should form the backbone of the article's problem-solving narrative.
        4.  **Identified Content Gaps:** What specific sub-topics are implied or partially covered in the SERP, but could be expanded into full, authoritative sections in our article? What related long-tail questions (from PAA or Related Searches) are not adequately addressed by top results?

        Provide your analysis in the required structured JSON format.
        """
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt_content.strip()},
        ]

    # --- END MODIFICATION ---

    def generate_ai_outline(
        self,
        keyword: str,
        serp_overview: Dict[str, Any],
        content_intelligence: Dict[str, Any],
    ) -> Tuple[Dict[str, List[str]], float]:
        """
        Uses OpenAI to generate a structured content outline with H2s and corresponding H3s.
        (This can also be refactored into a separate module if desired)
        """
        prompt_messages = self._build_outline_prompt(
            keyword, serp_overview, content_intelligence
        )

        schema = {
            "name": "generate_structured_content_outline",
            "type": "object",
            "properties": {
                "article_structure": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "h2": {
                                "type": "string",
                                "description": "The H2 heading of the section.",
                            },
                            "h3s": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "A list of H3 subheadings for this H2 section.",
                            },
                        },
                        "required": ["h2", "h3s"],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["article_structure"],
            "additionalProperties": False,
        }

        response, error = self.openai_client.call_chat_completion(
            messages=prompt_messages,
            schema=schema,
            model=self.config.get("default_model", "gpt-5-nano"),
            max_completion_tokens=self.config.get(
                "max_completion_tokens_for_generation", 4096
            ),
        )
        total_ai_cost = self.openai_client.latest_cost

        if error or not response or not response.get("article_structure"):
            self.logger.error(f"Failed to generate structured AI outline: {error}")
            return {"article_structure": []}, total_ai_cost

        return response, total_ai_cost

    def _build_outline_prompt(
        self,
        keyword: str,
        serp_overview: Dict[str, Any],
        content_intelligence: Dict[str, Any],
    ) -> List[Dict[str, str]]:
        """Builds the prompt for the AI structured outline generation."""
        prompt = f"""
        You are an expert SEO content strategist. Create a logical and comprehensive content outline for an article about "{keyword}". The output must be a structured list of sections, each with an H2 and a list of corresponding H3 subheadings.

        **Analysis Data:**
        - **Common Competitor Headings to Incorporate:** {", ".join(content_intelligence.get("common_headings_to_cover", []))}
        - **Unique Angles & Gaps to Address:** {", ".join(content_intelligence.get("unique_angles_to_include", []))}
        - **Key Entities to Mention:** {", ".join(content_intelligence.get("key_entities_from_competitors", []))}
        - **People Also Ask Questions to Answer:** {", ".join(serp_overview.get("paa_questions", []))}

        **Instructions:**
        1. Create a logical flow for the article.
        2. The first section must be titled 'Introduction'.
        3. The last section must be titled 'Conclusion'.
        4. If there are 'People Also Ask' questions, create a dedicated H2 section titled 'Frequently Asked Questions' and use the questions as H3s.
        5. Structure the entire output as a JSON object matching the requested schema.
        """
        return [{"role": "user", "content": prompt}]
```

## File: pipeline/step_04_analysis/run_analysis.py
```python
import logging
from typing import Dict, Any, Optional, List, Tuple

from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from external_apis.openai_client import OpenAIClientWrapper
from core.blueprint_factory import BlueprintFactory
from core.serp_analyzer import FullSerpAnalyzer
from .competitor_analyzer import FullCompetitorAnalyzer
from .content_analyzer import ContentAnalyzer
from pipeline.step_05_strategy.decision_engine import StrategicDecisionEngine
from pipeline.step_03_prioritization.scoring_engine import ScoringEngine

# --- NEW FUNCTION: run_final_validation ---
from urllib.parse import urlparse

# Then, replace the entire `run_final_validation` function with this new version.


def run_final_validation(
    live_serp_data: Dict[str, Any],
    opportunity: Dict[str, Any],
    client_cfg: Dict[str, Any],
    dataforseo_client: Any,
) -> Tuple[bool, Optional[str]]:
    full_data = opportunity.get("full_data", {})
    if not isinstance(full_data, dict):
        full_data = {}
    cached_serp_info = full_data.get("serp_info", {})
    cached_features = set(cached_serp_info.get("serp_item_types", []))
    live_features = set(live_serp_data.get("item_types", []))

    # NEW: Definitive Cannibalization Check (FIXED LOGIC)
    target_domain = client_cfg.get("target_domain", "").lower().replace("www.", "")
    if target_domain:
        for result in live_serp_data.get("top_organic_results", []):
            try:
                url_domain = (
                    urlparse(result.get("url", "")).netloc.lower().replace("www.", "")
                )
                # CRITICAL FIX: Check for exact match or subdomain suffix to avoid 'pet.com' matching 'competitor.com'
                if url_domain == target_domain or url_domain.endswith(
                    f".{target_domain}"
                ):
                    return (
                        False,
                        f"Final Validation Failed (Cannibalization): Target domain '{target_domain}' found in live SERP at URL '{result.get('url')}'.",
                    )
            except Exception:
                continue

    # Hostile features (configurable)
    hostile_features = set(
        client_cfg.get(
            "hostile_serp_features",
            [
                "shopping",
                "local_pack",
                "google_flights",
                "google_hotels",
                "popular_products",
            ],
        )
    )
    newly_added_hostile = live_features.intersection(
        hostile_features
    ) - cached_features.intersection(hostile_features)
    if newly_added_hostile:
        return (
            False,
            f"Final Validation Failed: Live SERP contains new hostile features: {', '.join(newly_added_hostile)}.",
        )

    # Non-blog content check (configurable domains & threshold)
    top_5_organic = live_serp_data.get("top_organic_results", [])[:5]
    non_blog_domains_cfg = set(client_cfg.get("final_validation_non_blog_domains", []))
    ugc_domains_cfg = set(
        client_cfg.get("ugc_and_parasite_domains", [])
    )  # Get from config

    hostile_domains = non_blog_domains_cfg.union(ugc_domains_cfg).union(
        client_cfg.get("competitor_blacklist_domains", [])
    )  # Combine all relevant hostile domains

    non_blog_count = sum(
        1
        for item in top_5_organic
        if any(domain in item.get("url", "") for domain in hostile_domains)
    )
    if non_blog_count >= client_cfg.get("max_non_blog_results", 4):
        return (
            False,
            "Final Validation Failed: Live SERP is dominated by non-blog/UGC/blacklisted content.",
        )

    # AI Overview comprehensiveness check (configurable threshold)
    disable_ai_overview_check = client_cfg.get("disable_ai_overview_check", False)
    if not disable_ai_overview_check:
        ai_overview_content = live_serp_data.get("ai_overview_content", "")
        if ai_overview_content and len(ai_overview_content.split()) > client_cfg.get(
            "max_ai_overview_words", 250
        ):
            return (
                False,
                "Final Validation Failed: AI Overview is too comprehensive, making a blog post redundant.",
            )

    # Organic visibility check (pixel ranking, configurable threshold)
    max_pixel_y = client_cfg.get("max_first_organic_y_pixel")
    if max_pixel_y is not None:
        first_organic_y = live_serp_data.get("first_organic_y_pixel")
        if first_organic_y is None:
            # Cannot check visibility without pixel data, proceed if other checks pass
            pass
        elif first_organic_y > max_pixel_y:
            return (
                False,
                f"Final Validation Failed: First organic result is too far down ({first_organic_y}px > {max_pixel_y}px).",
            )

    # NEW: LCP Check
    avg_lcp = live_serp_data.get("avg_page_timing", {}).get("largest_contentful_paint")
    if avg_lcp is not None and avg_lcp > client_cfg.get("max_avg_lcp_time", 4000):
        return (
            False,
            f"Final Validation Failed: Live SERP indicates poor page speed (Avg LCP: {avg_lcp}ms).",
        )

    return True, "Final validation passed."


# --- END NEW FUNCTION ---


def run_analysis_phase(
    opportunity: Dict[str, Any],
    openai_client: OpenAIClientWrapper,
    dataforseo_client: DataForSEOClientV2,
    client_cfg: Dict[str, Any],
    blueprint_factory: BlueprintFactory,
    scoring_engine: ScoringEngine,
    selected_competitor_urls: Optional[List[str]] = None,
) -> Tuple[Dict[str, Any], float]:
    logger = logging.getLogger(__name__)
    keyword = opportunity.get("keyword")
    logger.info(f"--- Starting Deep-Dive Analysis Phase for '{keyword}' ---")

    total_api_cost = 0.0

    serp_analyzer = FullSerpAnalyzer(dataforseo_client, client_cfg)
    competitor_analyzer = FullCompetitorAnalyzer(dataforseo_client, client_cfg)
    content_analyzer = ContentAnalyzer(openai_client, client_cfg)
    strategy_engine = StrategicDecisionEngine(client_cfg)
    # Blueprint factory is passed in, no need to re-initialize

    # 1. Make the single expensive, live SERP call for analysis
    logger.info(f"Making live SERP call for analysis of '{keyword}'...")
    serp_overview, serp_api_cost = serp_analyzer.analyze_serp(keyword)
    total_api_cost += serp_api_cost
    if not serp_overview:
        raise ValueError("Failed to retrieve live SERP data for analysis.")

    # VALIDATION GATE IS NOW REMOVED FROM THIS FUNCTION
    logger.info(f"Proceeding with full analysis for '{keyword}'.")

    # 2. On-Page competitor metadata and content analysis
    top_organic_urls = [
        result["url"]
        for result in serp_overview.get("top_organic_results", [])[
            : client_cfg.get("num_competitors_to_analyze", 5)
        ]
    ]
    competitor_analysis, competitor_api_cost = competitor_analyzer.analyze_competitors(
        top_organic_urls, selected_competitor_urls
    )
    total_api_cost += competitor_api_cost

    # 3. Content Intelligence Synthesis using the full content
    content_intelligence, content_api_cost = (
        content_analyzer.synthesize_content_intelligence(
            competitor_analysis,
            keyword,
            serp_overview.get("dominant_content_format", "Comprehensive Article"),
        )
    )
    total_api_cost += content_api_cost

    # 4. Determine Strategy
    recommended_strategy = strategy_engine.determine_strategy(
        serp_overview, competitor_analysis, content_intelligence
    )

    # 5. AI Content Outline Generation
    ai_outline, outline_api_cost = content_analyzer.generate_ai_outline(
        keyword, serp_overview, content_intelligence
    )
    total_api_cost += outline_api_cost
    content_intelligence.update(ai_outline)

    # 6. Assemble the final Blueprint
    analysis_data = {
        "serp_overview": serp_overview,
        "competitor_analysis": competitor_analysis,
        "content_intelligence": content_intelligence,
        "recommended_strategy": recommended_strategy,
    }

    blueprint = blueprint_factory.create_blueprint(
        seed_topic=keyword,
        winning_keyword_data=opportunity.get("full_data", {}).copy(),
        analysis_data=analysis_data,
        total_api_cost=total_api_cost,
        client_id=opportunity.get("client_id"),
    )

    opportunity["blueprint"] = blueprint

    # --- RE-SCORING ---
    # Re-calculate the strategic score with the new, rich data from the live SERP call
    # This ensures the score is based on the most accurate, up-to-date information
    final_score, final_score_breakdown = scoring_engine.calculate_score(opportunity)
    opportunity["strategic_score"] = final_score
    opportunity["score_breakdown"] = final_score_breakdown
    opportunity["full_data"]["strategic_score"] = final_score
    opportunity["full_data"]["score_breakdown"] = final_score_breakdown

    logger.info(f"  -> Final, updated strategic score: {final_score}")
    logger.info(f"  -> Total API Cost for Blueprint Generation: ${total_api_cost:.4f}")
    logger.info("--- Deep-Dive Analysis Phase Complete ---")

    return opportunity, total_api_cost
```

## File: pipeline/step_05_strategy/decision_engine.py
```python
import logging
from typing import Dict, Any, List
import json


class StrategicDecisionEngine:
    """
    Analyzes SERP and competitor data to recommend a specific content strategy.
    """

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)

    def determine_strategy(
        self,
        serp_overview: Dict[str, Any],
        competitor_analysis: List[Dict[str, Any]],
        content_intelligence: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Determines the optimal content format and strategic goal based on the analysis data.
        """
        content_format = serp_overview.get(
            "dominant_content_format", "Comprehensive Article"
        )
        strategic_goal = "Create a definitive guide that outranks competitors through superior depth and quality."

        top_results = serp_overview.get("top_organic_results", [])

        # --- START MODIFICATION ---
        # Check if deep analysis was performed to adjust logic
        deep_analysis_enabled = self.config.get(
            "enable_deep_competitor_analysis", False
        )

        # NEW: Repurpose focus_competitors for SERP-only mode
        focus_competitors_info = []
        if deep_analysis_enabled and competitor_analysis:
            focus_competitors_info = [
                {"url": c.get("url"), "onpage_score": c.get("onpage_score")}
                for c in competitor_analysis
                if c and c.get("url")
            ][:3]
        else:
            # In SERP-only mode, we can show top organic results as "focus competitors"
            focus_competitors_info = [
                {"url": r.get("url"), "title": r.get("title")} for r in top_results[:3]
            ]

        # NEW: Detect rating-heavy SERPs based on serp_overview data
        rating_count = sum(
            1
            for r in top_results
            if r.get("rating") and r["rating"].get("value") is not None
        )
        avg_rating_value = (
            sum(
                r["rating"]["value"]
                for r in top_results
                if r.get("rating") and r["rating"].get("value")
            )
            / rating_count
            if rating_count > 0
            else 0
        )

        if rating_count >= 3 and avg_rating_value >= 4.0:
            content_format = "Review Article"
            strategic_goal = "Produce an authoritative review or comparison that leverages strong social proof and clearly outlines pros/cons, aiming for rich snippets."
            # Prioritize this strategy by returning early after setting it
            return {
                "content_format": content_format,
                "strategic_goal": strategic_goal,
                "focus_competitors": focus_competitors_info,
                "final_qualification_assessment": {
                    "scorecard": self.generate_qualification_scorecard(
                        {
                            "serp_overview": serp_overview,
                            "competitor_analysis": competitor_analysis,
                            "content_intelligence": content_intelligence,
                        }
                    ),
                    **self._determine_final_recommendation(
                        self.generate_qualification_scorecard(
                            {
                                "serp_overview": serp_overview,
                                "competitor_analysis": competitor_analysis,
                                "content_intelligence": content_intelligence,
                            }
                        )
                    ),
                },
            }

        # ... (existing dynamic content format recommendations, e.g., Recipe, Scholarly, etc. - no change) ...

        # Rule: Weak competition (applies only if deep analysis was performed)
        if (
            content_format == "Comprehensive Article"
            and deep_analysis_enabled
            and competitor_analysis
        ):
            onpage_scores = [
                c.get("onpage_score")
                for c in competitor_analysis
                if c and c.get("onpage_score")
            ]
            if onpage_scores and (sum(onpage_scores) / len(onpage_scores)) < 60:
                strategic_goal = "Exploit the technical weaknesses of competitors by creating a fast, well-structured, and technically superior article."

        # FINAL QUALIFICATION GATE
        scorecard = self.generate_qualification_scorecard(
            {
                "serp_overview": serp_overview,
                "competitor_analysis": competitor_analysis,
                "content_intelligence": content_intelligence,
            }
        )
        recommendation = self._determine_final_recommendation(scorecard)
        # --- END MODIFICATION ---

        return {
            "content_format": content_format,
            "strategic_goal": strategic_goal,
            "focus_competitors": focus_competitors_info,  # Use the conditionally populated info
            "final_qualification_assessment": {
                "scorecard": scorecard,
                **recommendation,
            },
        }

    def generate_qualification_scorecard(self, analysis_data: dict) -> dict:
        """Generates a scorecard of qualification factors, adapted for SERP-only mode."""
        serp_overview = analysis_data.get("serp_overview", {})
        competitor_analysis = analysis_data.get("competitor_analysis", [])

        # --- START MODIFICATION ---
        deep_analysis_enabled = self.config.get(
            "enable_deep_competitor_analysis", False
        )

        hostility_score = 0
        for item in serp_overview.get("items", []):  # Iterate through raw SERP items
            if item.get("rank_absolute", 99) <= 10:
                # Count known hostile/attention-grabbing features
                if item.get("type") in [
                    "video",
                    "local_pack",
                    "carousel",
                    "twitter",
                    "shopping",
                    "app",
                    "short_videos",
                    "images",
                ]:
                    hostility_score += 1
                # Knowledge Graph with AI Overview is a stronger signal
                elif item.get("type") == "knowledge_graph" and (
                    "ai_overview_item" in json.dumps(item)
                ):  # Check for AI overview within KG items
                    hostility_score += 2
                elif item.get("type") == "ai_overview":  # Direct AI overview item
                    hostility_score += 2

        is_hostile_serp_environment = hostility_score > 5
        has_ai_overview = serp_overview.get("serp_has_ai_overview", False) or (
            "ai_overview_content" in serp_overview
            and serp_overview["ai_overview_content"] is not None
        )

        # Average competitor weaknesses calculation
        average_competitor_weaknesses = 0
        if deep_analysis_enabled and competitor_analysis:
            technical_warnings = [
                w
                for comp in competitor_analysis
                for w in comp.get("technical_warnings", [])
            ]
            average_competitor_weaknesses = (
                (len(technical_warnings) / len(competitor_analysis))
                if competitor_analysis
                else 0
            )
        else:
            # If deep analysis is disabled, we cannot assess technical weaknesses directly,
            # so we could default to a neutral or slightly positive value to avoid premature disqualification.
            average_competitor_weaknesses = 2  # Assume a moderate level if unknown

        # Has clear content angle (now based purely on content_intelligence from SERP)
        content_intelligence = analysis_data.get("content_intelligence", {})
        has_clear_content_angle = bool(
            content_intelligence.get("unique_angles_to_include")
            or content_intelligence.get("core_questions_answered_by_competitors")
        )

        # Is intent well-defined (now based on all SERP features)
        is_intent_well_defined = bool(
            serp_overview.get("paa_questions")
            or serp_overview.get("extracted_serp_features")
            or serp_overview.get("top_organic_faqs")  # NEW
            or serp_overview.get("top_organic_sitelinks")  # NEW
        )

        return {
            "hostility_score": hostility_score,
            "is_hostile_serp_environment": is_hostile_serp_environment,
            "has_ai_overview": has_ai_overview,
            "average_competitor_weaknesses": average_competitor_weaknesses,
            "has_clear_content_angle": has_clear_content_angle,
            "is_intent_well_defined": is_intent_well_defined,
        }
        # --- END MODIFICATION ---

    def _determine_final_recommendation(self, scorecard: dict) -> dict:
        """Determines the final go/no-go recommendation."""
        confidence_score = 100
        positive_factors = []
        negative_factors = []

        if scorecard["is_hostile_serp_environment"]:
            confidence_score -= 30
            negative_factors.append("SERP is dominated by non-article formats.")
        if scorecard["has_ai_overview"]:
            confidence_score -= 15
            negative_factors.append(
                "Google AI Overview is present, increasing ranking difficulty."
            )
        if scorecard["average_competitor_weaknesses"] < 2:
            confidence_score -= 20
            negative_factors.append("Competitors are technically strong.")
        if scorecard["average_competitor_weaknesses"] > 4:
            confidence_score += 10
            positive_factors.append(
                "Competitors show significant technical weaknesses."
            )
        if not scorecard["has_clear_content_angle"]:
            confidence_score -= 40
            negative_factors.append("No clear content differentiation angle was found.")
        if scorecard["has_clear_content_angle"]:
            positive_factors.append("A unique content angle has been identified.")
        if scorecard["is_intent_well_defined"]:
            positive_factors.append("User intent is well-defined by SERP features.")

        if confidence_score >= 80:
            recommendation = "Proceed"
        elif 50 <= confidence_score < 80:
            recommendation = "Proceed with Caution"
        else:
            recommendation = "Reject"

        return {
            "recommendation": recommendation,
            "confidence_score": confidence_score,
            "positive_factors": positive_factors,
            "negative_factors": negative_factors,
        }
```

## File: pipeline/step_06_content_creation/__init__.py
```python
# This file marks the directory as a Python package.
```

## File: pipeline/__init__.py
```python
# backend/pipeline/__init__.py
from .orchestrator.main import WorkflowOrchestrator as WorkflowOrchestrator
```

## File: pipeline/orchestrator.py
```python
# backend/pipeline/orchestrator.py
import logging

from app_config.manager import ConfigManager
from data_access.database_manager import DatabaseManager
from external_apis.dataforseo_client_v2 import DataForSEOClientV2
from external_apis.openai_client import OpenAIClientWrapper
from agents.image_generator import ImageGenerator
from agents.social_media_crafter import SocialMediaCrafter
from agents.internal_linking_suggester import InternalLinkingSuggester
from agents.html_formatter import HtmlFormatter
from core.blueprint_factory import BlueprintFactory
from agents.content_auditor import ContentAuditor
from agents.prompt_assembler import DynamicPromptAssembler
from pipeline.step_03_prioritization.scoring_engine import ScoringEngine
from pipeline.step_01_discovery.cannibalization_checker import (
    CannibalizationChecker,
)
from jobs import JobManager

from .orchestrator.discovery_orchestrator import DiscoveryOrchestrator
from .orchestrator.analysis_orchestrator import AnalysisOrchestrator
from .orchestrator.content_orchestrator import ContentOrchestrator
from .orchestrator.image_orchestrator import ImageOrchestrator
from .orchestrator.social_orchestrator import SocialOrchestrator
from .orchestrator.validation_orchestrator import ValidationOrchestrator
from .orchestrator.workflow_orchestrator import WorkflowOrchestrator
from .orchestrator.cost_estimator import CostEstimator

logger = logging.getLogger(__name__)


class WorkflowOrchestrator(
    DiscoveryOrchestrator,
    AnalysisOrchestrator,
    ContentOrchestrator,
    ImageOrchestrator,
    SocialOrchestrator,
    ValidationOrchestrator,
    WorkflowOrchestrator,
    CostEstimator,
):
    def __init__(
        self,
        global_cfg_manager: ConfigManager,
        db_manager: DatabaseManager,
        client_id: str,
        job_manager: "JobManager",
    ):
        self.global_cfg_manager = global_cfg_manager
        self.db_manager = db_manager
        self.client_id = client_id
        self.job_manager = job_manager
        self.logger = logging.getLogger(self.__class__.__name__)
        self.client_cfg = self.global_cfg_manager.load_client_config(
            self.client_id, self.db_manager
        )

        self.openai_client = OpenAIClientWrapper(
            api_key=self.client_cfg.get("openai_api_key"), client_cfg=self.client_cfg
        )
        self.dataforseo_client = DataForSEOClientV2(
            login=self.client_cfg["dataforseo_login"],
            password=self.client_cfg["dataforseo_password"],
            config=self.client_cfg,
            db_manager=self.db_manager,
            enable_cache=self.client_cfg.get("enable_cache", True),
        )

        self.image_generator = ImageGenerator(self.client_cfg)
        self.social_crafter = SocialMediaCrafter(self.openai_client, self.client_cfg)
        self.internal_linking_suggester = InternalLinkingSuggester(
            self.openai_client, self.client_cfg, self.db_manager
        )
        self.html_formatter = HtmlFormatter()
        self.blueprint_factory = BlueprintFactory(
            self.openai_client, self.client_cfg, self.dataforseo_client, self.db_manager
        )
        self.scoring_engine = ScoringEngine(self.client_cfg)
        self.cannibalization_checker = CannibalizationChecker(
            self.client_cfg.get("target_domain"),
            self.dataforseo_client,
            self.client_cfg,
            self.db_manager,
        )
        self.content_auditor = ContentAuditor()
        self.prompt_assembler = DynamicPromptAssembler(self.db_manager)
```

## File: services/discovery_service.py
```python
# services/discovery_service.py

from typing import Dict, Any
from data_access.database_manager import DatabaseManager


class DiscoveryService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def create_discovery_run(self, client_id: str, parameters: Dict[str, Any]) -> int:
        """Creates a new discovery run record and returns its ID."""
        return self.db_manager.create_discovery_run(client_id, parameters)

    def get_disqualification_reasons(self, run_id: int) -> Dict[str, int]:
        """
        Retrieves a summary of disqualification reasons for a specific discovery run.
        """
        keywords = self.db_manager.get_keywords_for_run(run_id)

        disqualification_reasons = {}
        for keyword in keywords:
            if keyword.get("blog_qualification_status") == "rejected":
                reason = keyword.get("blog_qualification_reason")
                if reason:
                    disqualification_reasons[reason] = (
                        disqualification_reasons.get(reason, 0) + 1
                    )

        return disqualification_reasons
```

## File: services/disqualification_service.py
```python
# services/disqualification_service.py
import json
from typing import List, Dict, Any
from data_access.database_manager import DatabaseManager


class DisqualificationService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def disqualify(
        self, client_id: str, keywords: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Applies disqualification rules to a list of keywords.
        """
        qualification_settings = self.db_manager.get_qualification_settings(client_id)
        disqualification_rules = json.loads(
            qualification_settings.get("disqualification_rules", "[]")
        )

        brand_keywords = qualification_settings.get("brand_keywords", [])
        competitor_brand_keywords = qualification_settings.get(
            "competitor_brand_keywords", []
        )

        qualified_keywords = []
        for keyword in keywords:
            disqualified = False
            keyword_text = keyword.get("keyword", "").lower()

            if any(brand_kw in keyword_text for brand_kw in brand_keywords):
                continue

            if any(brand_kw in keyword_text for brand_kw in competitor_brand_keywords):
                continue

            for rule in disqualification_rules:
                field = rule.get("field")
                operator = rule.get("operator")
                value = rule.get("value")

                field_value = keyword
                for key in field.split("."):
                    field_value = field_value.get(key, {})

                if operator == "=" and field_value == value:
                    disqualified = True
                    break
                elif operator == ">" and field_value > value:
                    disqualified = True
                    break
                elif operator == "<" and field_value < value:
                    disqualified = True
                    break

            if not disqualified:
                qualified_keywords.append(keyword)

        return qualified_keywords
```

## File: services/keyword_data_aggregator.py
```python
# services/keyword_data_aggregator.py

from typing import List, Dict, Any, Optional
from external_apis.dataforseo_client_v2 import DataForSEOClientV2


class KeywordDataAggregator:
    def __init__(
        self, dataforseo_client: DataForSEOClientV2, client_cfg: Dict[str, Any]
    ):
        self.dataforseo_client = dataforseo_client
        self.client_cfg = client_cfg

    def get_keyword_data(
        self,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        limit: Optional[int],
        depth: Optional[int],
        ignore_synonyms: Optional[bool],
    ) -> List[Dict[str, Any]]:
        """
        Calls the keyword discovery endpoints, deduplicates the results, and returns a unified list of keyword data objects.
        """
        # W21 FIX: Set default order_by if not provided, now structured as a dict
        if not order_by:
            ideas_suggestions_orderby = [
                "keyword_properties.keyword_difficulty,asc",
                "keyword_info.search_volume,desc",
            ]
            related_orderby = [
                "keyword_data.keyword_properties.keyword_difficulty,asc",
                "keyword_data.keyword_info.search_volume,desc",
            ]
            structured_orderby = {
                "ideas": ideas_suggestions_orderby,
                "suggestions": ideas_suggestions_orderby,
                "related": related_orderby,
            }
        else:
            # If order_by is provided from frontend, structure it for each mode
            related_orderby = [f"keyword_data.{rule}" for rule in order_by]
            structured_orderby = {
                "ideas": order_by,
                "suggestions": order_by,
                "related": related_orderby,
            }

        # Structure filters for the client
        ideas_suggestions_filters = []
        if filters:
            for f in filters:
                new_filter = f.copy()
                if "field" in new_filter and new_filter["field"].startswith(
                    "keyword_data."
                ):
                    new_filter["field"] = new_filter["field"][len("keyword_data.") :]
                ideas_suggestions_filters.append(new_filter)

        structured_filters = {
            "ideas": ideas_suggestions_filters,
            "suggestions": ideas_suggestions_filters,
            "related": filters,  # Related keeps the prefix
        }

        all_ideas, _ = self.dataforseo_client.get_keyword_ideas(
            seed_keywords=seed_keywords,
            location_code=self.client_cfg.get("location_code"),
            language_code=self.client_cfg.get("language_code"),
            client_cfg=self.client_cfg,
            discovery_modes=discovery_modes,
            filters=structured_filters,
            order_by=structured_orderby,
            limit=limit,
            depth=depth,
            ignore_synonyms=ignore_synonyms,
        )

        final_keywords_deduplicated = []
        seen_keywords = set()
        for item in all_ideas:
            kw_text = item.get("keyword", "").lower()
            if kw_text and kw_text not in seen_keywords:
                final_keywords_deduplicated.append(item)
                seen_keywords.add(kw_text)

        return final_keywords_deduplicated
```

## File: services/opportunities_service.py
```python
# services/opportunities_service.py

from typing import List, Dict, Any, Tuple
from data_access.database_manager import DatabaseManager


class OpportunitiesService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def get_all_opportunities(
        self, client_id: str, params: Dict[str, Any]
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        Retrieves keyword opportunities for a client, supporting filtering, sorting, and pagination.
        Returns (opportunities_list, total_count).
        """
        return self.db_manager.get_all_opportunities(client_id, params)

    def get_all_opportunities_summary(
        self, client_id: str, params: Dict[str, Any], select_columns: str = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        Retrieves a lightweight summary of keyword opportunities for a client.
        Returns (opportunities_list, total_count).
        """
        return self.db_manager.get_all_opportunities(
            client_id, params, summary=True, select_columns=select_columns
        )

    def get_opportunities_by_category(
        self, client_id: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Retrieves all opportunities for a client, grouped by category.
        """
        opportunities, _ = self.db_manager.get_all_opportunities(client_id, {})

        opportunities_by_category = {}
        for opportunity in opportunities:
            categories = opportunity.get("keyword_info", {}).get("categories", [])
            for category in categories:
                if category not in opportunities_by_category:
                    opportunities_by_category[category] = []
                opportunities_by_category[category].append(opportunity)

        return opportunities_by_category

    def get_opportunities_by_cluster(
        self, client_id: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Retrieves all opportunities for a client, grouped by cluster.
        """
        opportunities, _ = self.db_manager.get_all_opportunities(client_id, {})

        opportunities_by_cluster = {}
        for opportunity in opportunities:
            cluster_name = opportunity.get("cluster_name")
            if cluster_name:
                if cluster_name not in opportunities_by_cluster:
                    opportunities_by_cluster[cluster_name] = []
                opportunities_by_cluster[cluster_name].append(opportunity)

        return opportunities_by_cluster
```

## File: services/qualification_service.py
```python
# services/qualification_service.py

from typing import List, Dict, Any, Optional
from .keyword_data_aggregator import KeywordDataAggregator
from .disqualification_service import DisqualificationService
from .scoring_service import ScoringService
from .serp_analysis_service import SerpAnalysisService


class QualificationService:
    def __init__(
        self,
        keyword_data_aggregator: KeywordDataAggregator,
        disqualification_service: DisqualificationService,
        scoring_service: ScoringService,
        serp_analysis_service: SerpAnalysisService,
    ):
        self.keyword_data_aggregator = keyword_data_aggregator
        self.disqualification_service = disqualification_service
        self.scoring_service = scoring_service
        self.serp_analysis_service = serp_analysis_service

    def qualify_keywords(
        self,
        client_id: str,
        seed_keywords: List[str],
        discovery_modes: List[str],
        filters: Optional[List[Any]],
        order_by: Optional[List[str]],
        limit: Optional[int],
        depth: Optional[int],
        ignore_synonyms: Optional[bool],
    ) -> List[Dict[str, Any]]:
        """
        Orchestrates the entire qualification flow.
        """
        keyword_data = self.keyword_data_aggregator.get_keyword_data(
            seed_keywords,
            discovery_modes,
            filters,
            order_by,
            limit,
            depth,
            ignore_synonyms,
        )

        analyzed_keywords = self.serp_analysis_service.analyze_keywords_serp(
            keyword_data
        )

        qualified_keywords = self.disqualification_service.disqualify(
            client_id, analyzed_keywords
        )

        scored_keywords = []
        for keyword in qualified_keywords:
            score, breakdown = self.scoring_service.calculate_score(client_id, keyword)
            keyword["strategic_score"] = score
            keyword["score_breakdown"] = breakdown
            scored_keywords.append(keyword)

        return scored_keywords
```

## File: services/scoring_service.py
```python
# services/scoring_service.py

from typing import Dict, Any, Tuple
from data_access.database_manager import DatabaseManager


class ScoringService:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    def calculate_score(
        self, client_id: str, keyword_data: Dict[str, Any]
    ) -> Tuple[float, Dict[str, Any]]:
        """
        Calculates a strategic score for a keyword based on the client's qualification settings.
        """
        qualification_settings = self.db_manager.get_qualification_settings(client_id)

        traffic_potential_weight = qualification_settings.get(
            "traffic_potential_weight", 0
        )
        cpc_weight = qualification_settings.get("cpc_weight", 0)
        search_intent_weight = qualification_settings.get("search_intent_weight", 0)
        competitor_strength_weight = qualification_settings.get(
            "competitor_strength_weight", 0
        )
        serp_features_weight = qualification_settings.get("serp_features_weight", 0)
        trend_weight = qualification_settings.get("trend_weight", 0)
        seasonality_weight = qualification_settings.get("seasonality_weight", 0)
        serp_volatility_weight = qualification_settings.get("serp_volatility_weight", 0)

        search_volume = keyword_data.get(
            "keyword_info_normalized_with_clickstream", {}
        ).get("search_volume", 0)
        keyword_difficulty = keyword_data.get("keyword_properties", {}).get(
            "keyword_difficulty", 0
        )
        cpc = keyword_data.get("keyword_info", {}).get("cpc", 0)
        main_intent = keyword_data.get("search_intent_info", {}).get("main_intent")
        avg_referring_domains = keyword_data.get("avg_backlinks_info", {}).get(
            "referring_domains", 0
        )
        serp_item_types = keyword_data.get("serp_info", {}).get("serp_item_types", [])
        monthly_searches = keyword_data.get("keyword_info", {}).get(
            "monthly_searches", []
        )
        serp_last_updated_days_ago = keyword_data.get("serp_overview", {}).get(
            "serp_last_updated_days_ago"
        )
        serp_update_interval_days = keyword_data.get("serp_overview", {}).get(
            "serp_update_interval_days"
        )

        traffic_potential_score = search_volume * (1 - (keyword_difficulty / 100))
        cpc_score = cpc * 100
        competitor_strength_score = 100 - (avg_referring_domains / 10)
        serp_features_score = 0
        if "featured_snippet" in serp_item_types:
            serp_features_score += 20
        if "video" in serp_item_types:
            serp_features_score += 10
        if "ai_overview" in serp_item_types:
            serp_features_score -= 10

        trend_score = 0
        if len(monthly_searches) > 1:
            latest_search_volume = monthly_searches[0]["search_volume"]
            oldest_search_volume = monthly_searches[-1]["search_volume"]
            if oldest_search_volume > 0:
                trend_score = (
                    (latest_search_volume - oldest_search_volume) / oldest_search_volume
                ) * 100

        seasonality_score = 0
        if len(monthly_searches) > 11:
            # Calculate the average search volume for each month
            monthly_averages = [0] * 12
            for i in range(12):
                monthly_averages[i] = monthly_searches[i]["search_volume"]

            # Calculate the standard deviation of the monthly averages
            mean = sum(monthly_averages) / 12
            variance = sum([((x - mean) ** 2) for x in monthly_averages]) / 12
            std_dev = variance**0.5

            # Normalize the standard deviation to a score between 0 and 100
            if mean > 0:
                seasonality_score = 100 - (std_dev / mean) * 100

        serp_volatility_score = 0
        if (
            serp_last_updated_days_ago is not None
            and serp_update_interval_days is not None
        ):
            if serp_update_interval_days > 0:
                serp_volatility_score = (
                    100 - (serp_last_updated_days_ago / serp_update_interval_days) * 100
                )

        search_intent_score = 0
        if main_intent == "informational":
            search_intent_score = 100 * qualification_settings.get(
                "informational_intent_weight", 0
            )
        elif main_intent == "navigational":
            search_intent_score = 50 * qualification_settings.get(
                "navigational_intent_weight", 0
            )
        elif main_intent == "commercial":
            search_intent_score = 75 * qualification_settings.get(
                "commercial_intent_weight", 0
            )
        elif main_intent == "transactional":
            search_intent_score = 90 * qualification_settings.get(
                "transactional_intent_weight", 0
            )

        score = (
            (traffic_potential_score * traffic_potential_weight)
            + (cpc_score * cpc_weight)
            + (search_intent_score * search_intent_weight)
            + (competitor_strength_score * competitor_strength_weight)
            + (serp_features_score * serp_features_weight)
            + (trend_score * trend_weight)
            + (seasonality_score * seasonality_weight)
            + (serp_volatility_score * serp_volatility_weight)
        )

        breakdown = {
            "traffic_potential_score": traffic_potential_score,
            "cpc_score": cpc_score,
            "search_intent_score": search_intent_score,
            "competitor_strength_score": competitor_strength_score,
            "serp_features_score": serp_features_score,
            "trend_score": trend_score,
            "seasonality_score": seasonality_score,
            "serp_volatility_score": serp_volatility_score,
        }

        return score, breakdown
```

## File: services/serp_analysis_service.py
```python
# backend/services/serp_analysis_service.py

from typing import Dict, Any, List
from backend.core.serp_analyzer import FullSerpAnalyzer
from backend.external_apis.dataforseo_client_v2 import DataForSEOClientV2


class SerpAnalysisService:
    def __init__(self, dataforseo_client: DataForSEOClientV2, config: Dict[str, Any]):
        self.serp_analyzer = FullSerpAnalyzer(dataforseo_client, config)
        self.dataforseo_client = dataforseo_client
        self.config = config

    def analyze_serp_for_blog_opportunity(
        self, serp_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Analyzes the SERP data to determine if there is a good opportunity for a blog article.
        """
        if not serp_results or not serp_results.get("top_organic_results"):
            return {
                "blog_opportunity": False,
                "opportunity_score": 0,
                "competitor_urls": [],
            }

        top_results = serp_results.get("top_organic_results", [])
        blog_count = 0
        competitor_urls = []

        for result in top_results[:10]:
            page_type = result.get("page_type")
            if page_type == "blog" or page_type == "news":
                blog_count += 1
                competitor_urls.append(result.get("url"))

        # Simple logic: if there are at least 3 blog/news articles in the top 10,
        # it's a good opportunity.
        blog_opportunity = blog_count >= 3
        opportunity_score = blog_count / 10.0

        return {
            "blog_opportunity": blog_opportunity,
            "opportunity_score": opportunity_score,
            "competitor_urls": competitor_urls,
        }

    def analyze_keywords_serp(
        self, keywords_data: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Analyzes the SERP data for a list of keywords.
        """
        for keyword_data in keywords_data:
            keyword = keyword_data.get("keyword")
            if keyword:
                serp_results, _ = self.serp_analyzer.analyze_serp(keyword)
                serp_analysis = self.analyze_serp_for_blog_opportunity(serp_results)

                # Add competitor content if it's a blog opportunity
                if serp_analysis["blog_opportunity"]:
                    competitor_content, _ = (
                        self.dataforseo_client.get_content_onpage_data(
                            serp_analysis["competitor_urls"], self.config
                        )
                    )
                    serp_analysis["competitor_content"] = competitor_content
                else:
                    serp_analysis["competitor_content"] = []

                keyword_data["serp_analysis"] = serp_analysis

        return keywords_data
```

## File: tests/test_content_generation.py
```python
import pytest
from unittest.mock import MagicMock, patch
from backend.external_apis.openai_client import OpenAIClientWrapper
from backend.pipeline.step_04_analysis.content_analyzer import ContentAnalyzer

@pytest.fixture
def mock_openai_client():
    """Mocks the OpenAIClientWrapper to avoid actual API calls."""
    client = MagicMock(spec=OpenAIClientWrapper)
    client.latest_cost = 0.1
    return client

@pytest.fixture
def content_analyzer(mock_openai_client):
    """Provides a ContentAnalyzer instance with a mocked OpenAI client."""
    config = {
        "default_model": "gpt-5-nano",
        "max_completion_tokens_for_generation": 8192,
    }
    return ContentAnalyzer(openai_client=mock_openai_client, config=config)

def test_openai_client_enforces_json_schema_mode():
    """
    Verifies that the OpenAI client wrapper correctly uses 'json_schema' mode
    for gpt-5-nano and gpt-5-mini when a schema is provided.
    """
    with patch('openai.OpenAI') as mock_openai:
        # Arrange
        mock_create = MagicMock()
        mock_openai.return_value.chat.completions.create = mock_create

        client_wrapper = OpenAIClientWrapper(api_key="fake_key", client_cfg={})
        messages = [{"role": "user", "content": "Test prompt"}]
        schema = {"type": "object", "properties": {"key": {"type": "string"}}}

        # Act
        client_wrapper.call_chat_completion(
            messages=messages,
            schema=schema,
            model='gpt-5-nano' # Test with one of the target models
        )

        # Assert
        mock_create.assert_called_once()
        call_args = mock_create.call_args.kwargs
        assert "response_format" in call_args
        assert call_args["response_format"]["type"] == "json_schema"
        assert "json_schema" in call_args["response_format"]

def test_full_content_analysis_and_outline_workflow(content_analyzer, mock_openai_client):
    """
    Tests the full content analysis and outline generation workflow,
    ensuring it handles mocked AI responses correctly and produces a valid output.
    """
    # Arrange: Mock the return values for the two AI calls
    mock_synthesis_response = {
        "unique_angles_to_include": ["Angle 1", "Angle 2"],
        "key_entities_from_competitors": ["Entity A", "Entity B"],
        "core_questions_answered_by_serp": ["Question 1?", "Question 2?"],
        "identified_content_gaps": ["Gap A", "Gap B"],
    }
    mock_outline_response = {
        "article_structure": [
            {"h2": "Introduction", "h3s": []},
            {"h2": "Main Topic", "h3s": ["Sub-topic 1", "Sub-topic 2"]},
            {"h2": "Conclusion", "h3s": []},
        ]
    }
    # The client will return these values in order for the two calls
    mock_openai_client.call_chat_completion.side_effect = [
        (mock_synthesis_response, None),
        (mock_outline_response, None)
    ]

    keyword = "test keyword"
    serp_overview = {"paa_questions": ["PAA Question 1?"]}

    # Act: Run the synthesis and outline generation
    content_intelligence, total_cost_synthesis = content_analyzer.synthesize_content_intelligence(
        keyword=keyword,
        serp_overview=serp_overview,
        competitor_analysis=[] # Use the SERP-only path
    )

    outline, total_cost_outline = content_analyzer.generate_ai_outline(
        keyword=keyword,
        serp_overview=serp_overview,
        content_intelligence=content_intelligence
    )

    # Assert
    assert total_cost_synthesis == 0.1
    assert total_cost_outline == 0.1
    assert "unique_angles_to_include" in content_intelligence
    assert "article_structure" in outline
    assert len(outline["article_structure"]) == 3
    assert outline["article_structure"][1]["h2"] == "Main Topic"
    assert "Sub-topic 1" in outline["article_structure"][1]["h3s"]
    assert mock_openai_client.call_chat_completion.call_count == 2

print("Test script created at backend/tests/test_content_generation.py")
print("You can run this test using pytest:")
print("pytest backend/tests/test_content_generation.py")
```

## File: tests/test_filter_transformation.py
```python
# tests/test_filter_transformation.py
from pipeline.step_01_discovery.keyword_discovery.expander import (
    _transform_filters_for_api,
)


def test_no_filters():
    """Test that None is returned when no filters are provided."""
    assert _transform_filters_for_api(None) is None


def test_single_filter():
    """Test that a single filter is transformed into a flat list."""
    filters = [{"field": "keyword_info.search_volume", "operator": ">", "value": 100}]
    expected = ["keyword_info.search_volume", ">", 100]
    assert _transform_filters_for_api(filters) == expected


def test_multiple_filters():
    """Test that multiple filters are correctly interleaved with the 'and' operator."""
    filters = [
        {"field": "keyword_info.search_volume", "operator": ">", "value": 100},
        {
            "field": "keyword_properties.keyword_difficulty",
            "operator": "<",
            "value": 50,
        },
    ]
    expected = [
        ["keyword_info.search_volume", ">", 100],
        "and",
        ["keyword_properties.keyword_difficulty", "<", 50],
    ]
    assert _transform_filters_for_api(filters) == expected


def test_three_filters():
    """Test that three filters are correctly interleaved with the 'and' operator."""
    filters = [
        {"field": "keyword_info.search_volume", "operator": ">", "value": 100},
        {
            "field": "keyword_properties.keyword_difficulty",
            "operator": "<",
            "value": 50,
        },
        {"field": "keyword_info.cpc", "operator": ">", "value": 0.5},
    ]
    expected = [
        ["keyword_info.search_volume", ">", 100],
        "and",
        ["keyword_properties.keyword_difficulty", "<", 50],
        "and",
        ["keyword_info.cpc", ">", 0.5],
    ]
    assert _transform_filters_for_api(filters) == expected


def test_empty_filter_list():
    """Test that an empty list of filters returns None."""
    assert _transform_filters_for_api([]) is None
```

## File: tests/test_onpage_instant_pages.py
```python
# tests/test_onpage_instant_pages.py
import os
import json
import logging
import base64
import requests
from dotenv import load_dotenv

TEST_URL = "https://www.wikipedia.org/"
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def test_instant_pages_workflow():
    load_dotenv()
    api_login = os.getenv("DATAFORSEO_LOGIN")
    api_password = os.getenv("DATAFORSEO_PASSWORD")
    assert api_login and api_password, "DATAFORSEO credentials not found in .env file."

    credentials = f"{api_login}:{api_password}"
    headers = {
        "Authorization": f"Basic {base64.b64encode(credentials.encode()).decode()}",
        "Content-Type": "application/json",
    }

    post_data = [{"url": TEST_URL, "enable_browser_rendering": True}]

    response = requests.post(
        "https://api.dataforseo.com/v3/on_page/instant_pages",
        headers=headers,
        data=json.dumps(post_data),
        timeout=120,
    )
    response.raise_for_status()
    response_json = response.json()

    assert response_json["status_code"] == 20000, "API call was not successful."
    task_result = response_json["tasks"][0]["result"][0]
    item = task_result["items"][0]

    assert "meta" in item, "Response missing 'meta' object."
    assert "content" in item["meta"], "Response missing 'meta.content' object."
    assert "plain_text_word_count" in item["meta"]["content"], (
        "Content parsing failed: word count is missing."
    )

    word_count = item["meta"]["content"]["plain_text_word_count"]
    assert isinstance(word_count, int) and word_count > 50, (
        f"Expected a valid word count, got {word_count}."
    )

    logging.info(
        f"SUCCESS: 'instant_pages' test passed. Found word count: {word_count}."
    )
    print(json.dumps(item["meta"]["content"], indent=2))


if __name__ == "__main__":
    test_instant_pages_workflow()
```

## File: Dockerfile
```dockerfile
# Use an official Python runtime as a parent image
FROM python:3.9-slim

# Set the working directory in the container
WORKDIR /app

# Copy the dependencies file to the working directory
COPY requirements.txt .

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copy the content of the local src directory to the working directory
COPY . ./backend/

# Make port 8000 available to the world outside this container
EXPOSE 8000

# Command to run the application
CMD ["uvicorn", "backend.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## File: export_db.py
```python
import json
from datetime import datetime
import os
import sys

# Add project root to sys.path to resolve imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from backend.app_config.manager import ConfigManager
from backend.data_access.database_manager import DatabaseManager

def json_serial(obj):
    """JSON serializer for objects not serializable by default json code"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Type {type(obj)} not serializable")

def export_database_to_json():
    """
    Connects to the database, fetches all opportunities, and exports them to a JSON file.
    """
    print("Starting database export...")
    try:
        config_manager = ConfigManager()
        db_manager = DatabaseManager(cfg_manager=config_manager)
        
        print("Fetching all opportunities from the database...")
        all_opportunities = db_manager.get_all_opportunities_for_export()
        print(f"Found {len(all_opportunities)} opportunities to export.")
        
        # Define the output file path in the project root
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
        output_file_path = os.path.join(project_root, "database_export.json")
        
        print(f"Exporting data to {output_file_path}...")
        with open(output_file_path, "w") as f:
            json.dump(all_opportunities, f, indent=4, default=json_serial)
            
        print("Database export completed successfully.")
        
    except Exception as e:
        print(f"An error occurred during the export process: {e}")

if __name__ == "__main__":
    export_database_to_json()
```

## File: jobs.py
```python
# jobs.py
import threading
import time
import uuid
import logging
from typing import Dict, Any, Callable, Optional
from datetime import datetime
from backend.data_access import queries

# Import DatabaseManager
from backend.data_access.database_manager import DatabaseManager

logger = logging.getLogger(__name__)


class JobManager:
    """Manages asynchronous jobs, their status, and results, backed by a database."""

    # MODIFIED: __init__ now requires a db_manager
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.db_manager.fail_stale_jobs()
        # The in-memory job store and lock are no longer needed.
        # self.jobs: Dict[str, Dict[str, Any]] = {}
        # self.lock = threading.Lock()

    def create_job(
        self, target_function: Callable, args: tuple = (), kwargs: dict = {}
    ) -> str:
        """
        Creates a new job, saves its initial state to the DB, starts it in a
        separate thread, and returns its ID.
        """
        job_id = str(uuid.uuid4())
        job_info = {
            "id": job_id,
            "status": "pending",
            "progress": 0,
            "result": None,
            "error": None,
            "started_at": time.time(),
            "finished_at": None,
            "function_name": target_function.__name__,
        }

        # MODIFIED: Save job to DB instead of in-memory dict
        self.db_manager.update_job(job_info)

        logger.info(f"Job {job_id} created for function {target_function.__name__}")
        thread = threading.Thread(
            target=self._run_job, args=(job_id, target_function, args, kwargs)
        )
        thread.daemon = True
        thread.start()
        return job_id

    def update_job_progress(self, job_id: str, step: str, message: str, status: Optional[str] = None):
        """Appends a progress log to the job record in the database."""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "message": message,
        }
        
        # This operation needs to be atomic to prevent race conditions.
        # We'll fetch the current job, update the log, and save it back.
        # A more advanced setup might use a database transaction or a JSON_APPEND function.
        job_info = self.get_job_status(job_id)
        if job_info:
            progress_log = job_info.get("progress_log", [])
            if isinstance(progress_log, str): # Handle case where it might be a JSON string
                try:
                    progress_log = json.loads(progress_log)
                except json.JSONDecodeError:
                    progress_log = []
            
            progress_log.append(log_entry)
            job_info["progress_log"] = progress_log

            # Optionally update the overall job status at the same time
            if status:
                job_info["status"] = status

            self.db_manager.update_job(job_info)

    def _run_job(
        self, job_id: str, target_function: Callable, args: tuple, kwargs: dict
    ):
        """Internal method to execute the target function and update job status in the DB."""
        logger.info(f"Job {job_id} starting. DB manager: {self.db_manager}")
        try:
            # Initialize the progress log
            self.update_job_status(job_id, "running", progress=5)
            self.update_job_progress(job_id, "Job Started", "The workflow is initializing.")
            
            result = target_function(job_id, *args, **kwargs)
            
            self.update_job_progress(job_id, "Job Finished", "The workflow completed successfully.")
            self.update_job_status(job_id, "completed", progress=100, result=result)
            logger.info(f"Job {job_id} completed successfully.")
        except Exception as e:
            error_message = f"Job {job_id} failed: {e}"
            logger.error(error_message, exc_info=True)
            self.update_job_progress(job_id, "Job Failed", str(e))
            self.update_job_status(job_id, "failed", progress=100, error=str(e))


    def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """Retrieves the current status of a job from the database."""
        # MODIFIED: Fetch from DB
        return self.db_manager.get_job(job_id)

    def update_job_status(
        self,
        job_id: str,
        status: str,
        progress: int,
        result: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None,
    ):
        """
        Updates job status using a direct UPDATE query (W10 FIX).
        """
        conn = self.db_manager._get_conn()
        finished_at = (
            datetime.now().timestamp() if status in ["completed", "failed"] else None
        )

        if result or error:
            # If result/error is present, use the original UPDATE_JOB (INSERT OR REPLACE)
            # that handles all fields.
            job_info = self.db_manager.get_job(job_id)
            if job_info:
                job_info["status"] = status
                job_info["progress"] = progress
                job_info["result"] = result
                job_info["error"] = error
                job_info["finished_at"] = finished_at
                self.db_manager.update_job(job_info)
        else:
            # Execute the direct, optimized status/progress update
            # This avoids fetching the entire job record first. (W10 FIX)
            with conn:
                conn.execute(
                    queries.UPDATE_JOB_STATUS_DIRECT,
                    (status, progress, finished_at, job_id),
                )

    # Global job manager instance is no longer initialized here.
    # It will be initialized in api/main.py where it has access to the db_manager.
    # job_manager = JobManager()

    def cancel_job(self, job_id: str) -> bool:
        """Marks a job as 'failed' with a 'cancelled by user' message."""
        job_info = self.get_job_status(job_id)
        if job_info and job_info["status"] in ["pending", "running", "paused"]:
            # The crucial part: mark as failed in the DB so the running thread sees it
            self.update_job_status(
                job_id,
                "failed",
                job_info.get("progress", 0),
                error="Cancelled by user.",
            )
            logger.info(f"Job {job_id} was marked as 'failed' (cancelled by user).")
            return True
        return False
```

## File: requirements.txt
```
fastapi
uvicorn
python-dotenv
scikit-learn
sentence-transformers
requests
textstat
bleach
openai
beautifulsoup4
markdown
```
